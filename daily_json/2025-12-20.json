[
    {
        "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
        "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",
        "url": "http://arxiv.org/abs/2512.16918v1",
        "published_date": "2025-12-18T18:59:55+00:00",
        "updated_date": "2025-12-18T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaoyang Wang",
            "Kaituo Feng",
            "Dongyang Chen",
            "Zhongyu Wang",
            "Zhixun Li",
            "Sicheng Gao",
            "Meng Meng",
            "Xu Zhou",
            "Manyuan Zhang",
            "Yuzhang Shang",
            "Xiangyu Yue"
        ],
        "tldr": "The paper introduces AdaTooler-V, an MLLM with adaptive tool-use capabilities, which uses reinforcement learning to determine when visual tools are necessary, improving performance and reducing unnecessary tool invocations. It outperforms GPT-4o and Gemini 1.5 Pro on a high-resolution benchmark.",
        "tldr_zh": "该论文介绍了 AdaTooler-V，一种具有自适应工具使用能力的多模态大型语言模型，通过强化学习来判断何时需要视觉工具，从而提高性能并减少不必要的工具调用。 在高分辨率基准测试中，其性能优于 GPT-4o 和 Gemini 1.5 Pro。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "最新进展表明，多模态大型语言模型（MLLMs）受益于多模态交错式思维链（CoT）以及视觉工具交互。然而，现有的开源模型常常表现出盲目的工具使用推理模式，即使在不必要的情况下也会调用视觉工具，这显著增加了推理开销并降低了模型性能。为此，我们提出了AdaTooler-V，一个通过确定视觉问题是否真正需要工具来实现自适应工具使用的MLLM。首先，我们引入了AT-GRPO，一种强化学习算法，该算法基于每个样本的工具效益评分自适应地调整奖励尺度，从而鼓励模型仅在工具能提供实际改进时才调用它们。此外，我们构建了两个数据集以支持训练：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于RL，该数据集包含跨单张图像、多张图像和视频数据的可验证奖励。在十二个基准测试上的实验证明了AdaTooler-V强大的推理能力，在各种视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准测试V*上实现了89.8%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已发布。"
    },
    {
        "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning",
        "summary": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.",
        "url": "http://arxiv.org/abs/2512.16911v1",
        "published_date": "2025-12-18T18:59:17+00:00",
        "updated_date": "2025-12-18T18:59:17+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Andrew Wagenmaker",
            "Perry Dong",
            "Raymond Tsao",
            "Chelsea Finn",
            "Sergey Levine"
        ],
        "tldr": "The paper proposes Posterior Behavioral Cloning (PostBC) as a pretraining method for RL finetuning. PostBC ensures coverage over demonstrator actions, leading to improved finetuning performance compared to standard BC, while maintaining comparable pretraining performance.",
        "tldr_zh": "本文提出了后验行为克隆（PostBC），作为一种用于强化学习微调的预训练方法。PostBC确保了对演示者行为的覆盖，与标准行为克隆相比，从而提高了微调性能，同时保持了相当的预训练性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在从机器人到自然语言处理的各个领域，标准做法是首先在大规模演示数据集上预训练一个策略，然后通常使用强化学习（RL）对该策略进行微调，以提高在部署领域的性能。微调步骤已被证明对于实现人类或超人类性能至关重要，然而，虽然已经有很多注意力集中在开发更有效的微调算法上，但很少有注意力集中在确保预训练的策略是强化学习微调的有效初始化。在这项工作中，我们试图理解预训练策略如何影响微调性能，以及如何预训练策略以确保它们是有效的微调初始化。我们首先从理论上表明，标准的行为克隆（BC）——训练策略以直接匹配演示者执行的动作——可能无法确保覆盖演示者的动作，这是有效RL微调的必要最低条件。然后我们表明，如果我们训练一个策略来模拟在给定演示数据集下演示者行为的后验分布，而不是完全拟合观察到的演示，那么我们可以获得一个确保覆盖演示者动作的策略，从而实现更有效的微调。此外，这个我们称之为后验行为克隆（PostBC）策略的策略能够在确保预训练性能不低于BC策略的同时实现这一点。然后，我们表明PostBC可以在机器人控制领域中使用现代生成模型实际实现——仅依赖于标准监督学习——并且与标准行为克隆相比，在现实的机器人控制基准和实际机器人操作任务上，导致强化学习微调性能显著提高。"
    },
    {
        "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning",
        "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.",
        "url": "http://arxiv.org/abs/2512.16909v1",
        "published_date": "2025-12-18T18:59:03+00:00",
        "updated_date": "2025-12-18T18:59:03+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yuanchen Ju",
            "Yongyuan Liang",
            "Yen-Jen Wang",
            "Nandiraju Gireesh",
            "Yuanliang Ju",
            "Seungjae Lee",
            "Qiao Gu",
            "Elvis Hsieh",
            "Furong Huang",
            "Koushil Sreenath"
        ],
        "tldr": "The paper introduces MomaGraph, a unified scene graph representation for embodied agents, along with a large-scale dataset and benchmark for task-driven scene understanding, and a vision-language model trained to predict these graphs for zero-shot task planning.",
        "tldr_zh": "该论文介绍了MomaGraph，一种用于具身代理的统一场景图表示，以及用于任务驱动场景理解的大规模数据集和基准，以及一个视觉-语言模型，该模型经过训练可预测这些图以进行零样本任务规划。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "家用移动操作机器人既需要导航，也需要操作。这需要一种紧凑、语义丰富的场景表示，以捕捉物体的位置、功能以及哪些部分是可操作的。场景图是一个自然的选择，然而以往的工作通常将空间和功能关系分离，将场景视为静态快照，忽略物体状态或时间更新，并且忽视了与完成当前任务最相关的信息。为了解决这些局限性，我们引入了 MomaGraph, 一种用于具身智能体的统一场景表示，集成了空间-功能关系和零件级别的交互元素。然而，推进这种表示既需要合适的数据，也需要严格的评估，而这两者在很大程度上都缺失了。因此，我们贡献了 MomaGraph-Scenes，这是第一个大规模、具有丰富标注的、任务驱动的家庭环境场景图数据集，以及 MomaGraph-Bench，一个系统性的评估套件，涵盖从高级规划到细粒度场景理解的六种推理能力。在此基础上，我们进一步开发了 MomaGraph-R1，一个在 MomaGraph-Scenes 上通过强化学习训练的 7B 视觉-语言模型。MomaGraph-R1 预测面向任务的场景图，并在“图-然后-规划”框架下充当零样本任务规划器。大量实验表明，我们的模型在开源模型中取得了最先进的效果，在基准测试中达到了 71.6% 的准确率（比最佳基线高出 11.4%），同时在公共基准测试中具有泛化能力，并有效地迁移到真实机器人实验中。"
    },
    {
        "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos",
        "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.",
        "url": "http://arxiv.org/abs/2512.16907v1",
        "published_date": "2025-12-18T18:59:01+00:00",
        "updated_date": "2025-12-18T18:59:01+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Mingfei Chen",
            "Yifan Wang",
            "Zhengqin Li",
            "Homanga Bharadhwaj",
            "Yujin Chen",
            "Chuan Qin",
            "Ziyi Kou",
            "Yuan Tian",
            "Eric Whitmire",
            "Rajinder Sodhi",
            "Hrvoje Benko",
            "Eli Shlizerman",
            "Yue Liu"
        ],
        "tldr": "This paper introduces EgoMAN, a large-scale egocentric dataset and a reasoning-to-motion framework for 3D hand trajectory prediction during human interactions, linking vision-language reasoning and motion generation.",
        "tldr_zh": "本文介绍了EgoMAN，一个大规模的以自我为中心的交互数据集，以及一个用于3D手部轨迹预测的推理到运动框架，该框架将视觉-语言推理和运动生成联系起来。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "以往关于3D手部轨迹预测的研究受到数据集的限制，这些数据集将运动与语义监督解耦，并且模型对推理和动作的联系较弱。为了解决这些问题，我们首先提出了EgoMAN数据集，这是一个大规模的以自我为中心的数据集，用于交互阶段感知的3D手部轨迹预测，包含21.9万条6自由度轨迹和300万个结构化问答对，用于语义、空间和运动推理。然后，我们介绍了EgoMAN模型，这是一个推理到运动的框架，通过轨迹-token接口将视觉-语言推理和运动生成连接起来。通过逐步训练，使推理与运动动力学对齐，我们的方法能够生成准确且阶段感知的轨迹，并能在真实场景中进行泛化。"
    },
    {
        "title": "Sceniris: A Fast Procedural Scene Generation Framework",
        "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris",
        "url": "http://arxiv.org/abs/2512.16896v1",
        "published_date": "2025-12-18T18:55:03+00:00",
        "updated_date": "2025-12-18T18:55:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Jinghuan Shang",
            "Harsh Patel",
            "Ran Gong",
            "Karl Schmeckpeper"
        ],
        "tldr": "Sceniris is a new procedural scene generation framework that achieves significant speedups over existing methods by using batch sampling and faster collision checking, making it suitable for large-scale dataset creation and robot manipulation tasks.",
        "tldr_zh": "Sceniris是一个新的程序化场景生成框架，通过使用批量采样和更快的碰撞检测，显著提高了现有方法的效率，使其适用于大规模数据集创建和机器人操作任务。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "合成3D场景对于开发物理人工智能和生成模型至关重要。现有的程序化生成方法通常输出吞吐量较低，从而在扩大数据集创建规模方面造成显著瓶颈。在这项工作中，我们介绍Sceniris，一种高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体。Sceniris还提供可选的机器人可达性检查，为机器人任务提供可操作的场景。Sceniris旨在通过解决先前方法Scene Synthesizer的主要性能限制来实现最大效率。利用批量采样和cuRobo中更快的碰撞检测，Sceniris实现了至少234倍于Scene Synthesizer的加速。Sceniris还将现有工作中可用的对象级空间关系扩展至支持多样化的场景需求。我们的代码可在https://github.com/rai-inst/sceniris获取。"
    },
    {
        "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies",
        "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.",
        "url": "http://arxiv.org/abs/2512.16881v1",
        "published_date": "2025-12-18T18:49:41+00:00",
        "updated_date": "2025-12-18T18:49:41+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Arhan Jain",
            "Mingtong Zhang",
            "Kanav Arora",
            "William Chen",
            "Marcel Torne",
            "Muhammad Zubair Irshad",
            "Sergey Zakharov",
            "Yue Wang",
            "Sergey Levine",
            "Chelsea Finn",
            "Wei-Chiu Ma",
            "Dhruv Shah",
            "Abhishek Gupta",
            "Karl Pertsch"
        ],
        "tldr": "The paper introduces PolaRiS, a real-to-sim framework for evaluating generalist robot policies by reconstructing real-world scenes into interactive simulation environments and using co-training to bridge the reality gap.",
        "tldr_zh": "该论文介绍了PolaRiS，一个用于评估通用机器人策略的真实到模拟框架，通过将真实世界的场景重建为交互式模拟环境，并使用联合训练来弥合现实差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "机器人学习研究面临的一个重要挑战是我们准确衡量和比较机器人策略性能的能力。由于现实世界轨迹的随机性、可复现性和耗时性，机器人领域的基准测试历来具有挑战性。对于最近的通用策略，这一挑战更加严峻，因为它们必须在各种场景和任务中进行评估。在仿真中进行评估为现实世界评估提供了一种可扩展的补充，但现有仿真基准与现实世界之间的视觉和物理领域差异使其成为策略改进的不可靠信号。此外，构建逼真且多样的仿真环境历来需要大量的人力和专业知识。为了弥合这一差距，我们推出了“仿真中策略评估和环境重建”（PolaRiS），这是一个可扩展的实物到仿真的框架，用于高保真度的仿真机器人评估。PolaRiS利用神经重建方法将现实世界场景的短视频扫描转化为交互式仿真环境。此外，我们开发了一种简单的仿真数据协同训练方法，弥合了剩余的实物到仿真差距，并实现了在未见过的仿真环境中进行零样本评估。通过仿真和现实世界之间的大量配对评估，我们证明PolaRiS评估比现有的仿真基准更能有力地体现现实世界通用策略的性能。它的简洁性也使得能够快速创建多样化的仿真环境。因此，这项工作朝着为下一代机器人基础模型实现分布式和民主化的评估迈出了一步。"
    },
    {
        "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning",
        "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/",
        "url": "http://arxiv.org/abs/2512.16861v1",
        "published_date": "2025-12-18T18:32:39+00:00",
        "updated_date": "2025-12-18T18:32:39+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zihan Zhou",
            "Animesh Garg",
            "Ajay Mandlekar",
            "Caelan Garrett"
        ],
        "tldr": "ReinforceGen uses task decomposition, imitation learning, and motion planning, followed by reinforcement learning fine-tuning, to improve performance in long-horizon robotic manipulation tasks, achieving 80% success on Robosuite.",
        "tldr_zh": "ReinforceGen结合了任务分解、模仿学习和运动规划，然后通过强化学习微调，以提高长时程机器人操作任务的性能，在Robosuite上达到了80%的成功率。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "摘要：\n长时程操控一直是机器人领域一个长期存在的挑战。我们提出 ReinforceGen，该系统结合了任务分解、数据生成、模仿学习和运动规划来形成初始解决方案，并通过基于强化学习的微调来改进每个组件。ReinforceGen 首先将任务分割成多个局部化的技能，这些技能通过运动规划连接。技能和运动规划目标通过模仿学习在 10 个人工演示生成的数据集上进行训练，然后通过在线自适应和强化学习进行微调。在 Robosuite 数据集上进行基准测试时，ReinforceGen 在最高重置范围设置下，所有具有视觉运动控制的任务成功率达到 80%。 额外的消融研究表明，我们的微调方法对平均性能提升贡献率达到 89%。 更多结果和视频见 https://reinforcegen.github.io/"
    },
    {
        "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction",
        "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.",
        "url": "http://arxiv.org/abs/2512.16842v1",
        "published_date": "2025-12-18T18:18:17+00:00",
        "updated_date": "2025-12-18T18:18:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Yuxin Ray Song",
            "Jinzhou Li",
            "Rao Fu",
            "Devin Murphy",
            "Kaichen Zhou",
            "Rishi Shiv",
            "Yaqi Li",
            "Haoyu Xiong",
            "Crystal Elaine Owens",
            "Yilun Du",
            "Yiyue Luo",
            "Xianyi Cheng",
            "Antonio Torralba",
            "Wojciech Matusik",
            "Paul Pu Liang"
        ],
        "tldr": "The paper introduces OpenTouch, a novel egocentric full-hand tactile dataset with synchronized video-touch-pose data, and presents retrieval and classification benchmarks for grasp understanding, cross-modal alignment, and video query retrieval.",
        "tldr_zh": "该论文介绍了OpenTouch，一种新颖的以自我为中心的完整手部触觉数据集，包含同步的视频-触觉-姿势数据，并提出了用于抓取理解、跨模态对齐和视频查询检索的检索和分类基准。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人体是与物理世界交互的主要界面，然而以自我为中心的感知往往无法得知何时、何地或以多大力度进行接触。可靠的可穿戴触觉传感器依然稀缺，且尚无现有的实际场景数据集能够将第一人称视角视频与全手部触觉对齐。为了弥合视觉感知与物理交互之间的差距，我们提出了 OpenTouch，这是首个实际场景的以自我为中心的全手触觉数据集，包含 5.1 小时的同步视频-触觉-姿态数据，以及 2900 个带有详细文本标注的精选片段。基于 OpenTouch，我们提出了检索和分类的基准测试，旨在探索触觉如何为感知和行动提供基础。我们证明了触觉信号为抓取理解提供了一种紧凑但强大的线索，能够加强跨模态对齐，并且可以从实际场景的视频查询中可靠地检索出来。通过发布这个带有标注的视觉-触觉-姿态数据集和基准，我们的目标是推进多模态的以自我为中心感知、具身学习以及富含接触的机器人操作。"
    },
    {
        "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning",
        "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.",
        "url": "http://arxiv.org/abs/2512.16917v1",
        "published_date": "2025-12-18T18:59:54+00:00",
        "updated_date": "2025-12-18T18:59:54+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Qihao Liu",
            "Luoxin Ye",
            "Wufei Ma",
            "Yu-Cheng Chou",
            "Alan Yuille"
        ],
        "tldr": "The paper introduces Generative Adversarial Reasoner, a novel adversarial reinforcement learning framework that improves LLM reasoning in mathematical tasks by co-evolving a reasoner and a discriminator. The method demonstrates consistent gains over strong baselines.",
        "tldr_zh": "本文介绍了一种名为 Generative Adversarial Reasoner 的新型对抗强化学习框架，通过共同进化推理器和判别器，提升 LLM 在数学任务中的推理能力。该方法在强大的基线上表现出持续的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "具有显式推理能力的大型语言模型(LLM)在数学推理方面表现出色，但仍然会犯过程性错误，如不正确的计算、脆弱的逻辑以及表面上合理但无效的步骤。在本文中，我们提出了一种生成对抗推理器(Generative Adversarial Reasoner)，一种旨在通过对抗性强化学习协同进化LLM推理器和基于LLM的判别器来增强推理能力的在线联合训练框架。一种计算效率高的审查调度策略将每个推理链分割成逻辑上完整的、长度相当的片段，判别器使用简洁、结构化的论证来评估每个片段的合理性。学习耦合了互补信号：LLM推理器因产生正确答案的逻辑一致步骤而获得奖励，而判别器因正确检测推理过程中的错误或区分轨迹而获得奖励。这产生了密集的、良好校准的、在线的步骤级别奖励，补充了稀疏的精确匹配信号，改善了信用分配，提高了样本效率，并增强了LLM的整体推理质量。在各种数学基准测试中，该方法相对于使用标准强化学习后训练的强大基线，实现了持续的提升。具体来说，在AIME24上，我们改进了DeepSeek-R1-Distill-Qwen-7B从54.0到61.3(+7.3)，以及DeepSeek-R1-Distill-Llama-8B从43.7到53.7(+10.0)。模块化判别器还支持灵活的奖励塑造，以实现诸如教师蒸馏、偏好对齐和基于数学证明的推理等目标。"
    },
    {
        "title": "Meta-RL Induces Exploration in Language Agents",
        "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
        "url": "http://arxiv.org/abs/2512.16848v1",
        "published_date": "2025-12-18T18:22:17+00:00",
        "updated_date": "2025-12-18T18:22:17+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Yulun Jiang",
            "Liangze Jiang",
            "Damien Teney",
            "Michael Moor",
            "Maria Brbic"
        ],
        "tldr": "The paper introduces LaMer, a Meta-RL framework for LLM agents that improves exploration and adaptation in reinforcement learning tasks via cross-episode training and in-context policy adaptation, demonstrating performance gains across multiple environments.",
        "tldr_zh": "该论文介绍了一种名为LaMer的Meta-RL框架，该框架通过跨episode训练和上下文策略适应来改进LLM智能体在强化学习任务中的探索和适应能力，并在多个环境中展示了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "强化学习（RL）已驱动大型语言模型（LLM）智能体的训练，使其能够与环境交互并解决多轮次长时域任务。然而，经强化学习训练的智能体常常在需要主动探索的任务中表现不佳，并且无法有效地从试错经验中进行适应。在本文中，我们提出了 LaMer，一种通用的元强化学习框架，它使 LLM 智能体能够在测试时主动探索并从环境反馈中学习。LaMer 由两个关键组件组成：（i）一种跨回合训练框架，旨在鼓励探索和长期奖励优化；以及（ii）通过反思实现的上下文策略适应，允许智能体在无需梯度更新的情况下，通过任务反馈信号来调整策略。在多种环境下的实验表明，LaMer 显著提升了性能，相较于强化学习基线，在 Sokoban、MineSweeper 和 Webshop 上分别获得了 11%、14% 和 19% 的性能提升。此外，与经强化学习训练的智能体相比，LaMer 在更具挑战性或先前未见过的任务中也表现出更好的泛化能力。总而言之，我们的结果表明，元强化学习提供了一种有原则的方法来诱导语言智能体进行探索，从而通过学习到的探索策略实现对新环境的更稳健的适应。"
    },
    {
        "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text",
        "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",
        "url": "http://arxiv.org/abs/2512.16924v1",
        "published_date": "2025-12-18T18:59:59+00:00",
        "updated_date": "2025-12-18T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanlin Wang",
            "Hao Ouyang",
            "Qiuyu Wang",
            "Yue Yu",
            "Yihao Meng",
            "Wen Wang",
            "Ka Leong Cheng",
            "Shuailei Ma",
            "Qingyan Bai",
            "Yixuan Li",
            "Cheng Chen",
            "Yanhong Zeng",
            "Xing Zhu",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "tldr": "WorldCanvas is a framework that generates videos of controllable world events by combining text prompts, object trajectories, and reference images, offering fine-grained control over multi-agent interactions, object appearance, and event timing.",
        "tldr_zh": "WorldCanvas是一个框架，它结合文本提示、物体轨迹和参考图像来生成可控世界事件的视频，从而能够对多智能体交互、物体外观和事件时序进行精细控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了 WorldCanvas，一个用于可提示世界事件的框架，它通过结合文本、轨迹和参考图像来实现丰富的、用户引导的模拟。与纯文本方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法将轨迹（编码运动、时间安排和可见性）与自然语言（用于语义意图）以及参考图像（用于对象身份的视觉基础）相结合，从而能够生成连贯、可控的事件，包括多智能体交互、对象进入/退出、参考引导的外观以及违反直觉的事件。由此产生的视频不仅展示了时间连贯性，而且展示了涌现的一致性，即使在临时消失的情况下也能保持对象身份和场景。通过支持富有表现力的世界事件生成，WorldCanvas 将世界模型从被动预测器推进到交互式的、用户塑造的模拟器。我们的项目主页可在以下网址访问：https://worldcanvas.github.io/。"
    },
    {
        "title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection",
        "summary": "We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.",
        "url": "http://arxiv.org/abs/2512.16908v1",
        "published_date": "2025-12-18T18:59:02+00:00",
        "updated_date": "2025-12-18T18:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuqun Wu",
            "Chih-hao Lin",
            "Henry Che",
            "Aditi Tiwari",
            "Chuhang Zou",
            "Shenlong Wang",
            "Derek Hoiem"
        ],
        "tldr": "The paper introduces SceneDiff, a new benchmark and training-free method for multiview object change detection leveraging 3D alignment and semantic feature comparison, achieving significant performance improvements.",
        "tldr_zh": "该论文介绍了SceneDiff，一个新的多视角物体变化检测基准和无训练方法，利用3D对齐和语义特征比较，取得了显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们研究了识别在同一场景不同时间的两次捕捉（图像或视频）之间新增、移除或移动的物体的问题。检测此类变化对于诸多应用至关重要，例如机器人整理、建筑进度以及安全监控。一个主要的挑战是，不同的视角可能导致物体错误地显示为发生改变。我们推出了SceneDiff Benchmark，这是首个具有物体实例标注的多视角变化检测基准，包含350个不同的视频对，带有数千个发生变化的物体。我们还介绍了SceneDiff方法，这是一种新的无需训练的多视角物体变化检测方法，它利用预训练的3D、分割和图像编码模型，可以在多个基准上进行稳健的预测。我们的方法在3D中对齐捕捉，提取物体区域，并比较空间和语义区域特征以检测变化。在多视角和双视角基准上的实验表明，我们的方法大幅优于现有方法（相对AP提升分别为94%和37.4%）。基准和代码将公开。"
    },
    {
        "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization",
        "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io",
        "url": "http://arxiv.org/abs/2512.16906v1",
        "published_date": "2025-12-18T18:58:42+00:00",
        "updated_date": "2025-12-18T18:58:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyan Cong",
            "Haotian Yang",
            "Angtian Wang",
            "Yizhi Wang",
            "Yiding Yang",
            "Canyu Zhang",
            "Chongyang Ma"
        ],
        "tldr": "VIVA is a framework for instruction-based video editing that utilizes VLM-guided encoding and reward optimization to improve generalization to complex, real-world instructions, showcasing superior editing quality compared to existing approaches.",
        "tldr_zh": "VIVA是一个基于指令的视频编辑框架，它利用VLM引导的编码和奖励优化来提高对复杂真实世界指令的泛化能力，并展示了优于现有方法的编辑质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "基于指令的视频编辑旨在根据自然语言指令修改输入视频，同时保持内容保真度和时间连贯性。然而，现有的基于扩散的方法通常在简单编辑操作的配对数据上进行训练，这从根本上限制了它们泛化到多样化和复杂、真实世界指令的能力。为了解决这种泛化差距，我们提出了VIVA，一个可扩展的基于指令的视频编辑框架，它利用VLM引导的编码和奖励优化。首先，我们引入了一个基于VLM的指导器，它将文本指令、源视频的第一帧和一个可选的参考图像编码成视觉定位的指令表示，为扩散Transformer主干提供细粒度的空间和语义上下文。其次，我们提出了一个后训练阶段，Edit-GRPO，它将群组相对策略优化（Group Relative Policy Optimization）应用到视频编辑领域，使用相对奖励直接优化模型，以实现指令忠实的、内容保留的、以及美观的编辑效果。此外，我们还提出了一个数据构建流程，旨在合成生成多样化的、高保真度的基本编辑操作配对视频-指令数据。大量实验表明，VIVA在指令遵循、泛化和编辑质量方面优于最先进的方法。网站：https://viva-paper.github.io"
    },
    {
        "title": "Tiny Recursive Control: Iterative Reasoning for Efficient Optimal Control",
        "summary": "Neural network controllers increasingly demand millions of parameters, and language model approaches push into the billions. For embedded aerospace systems with strict power and latency constraints, this scaling is prohibitive. We present Tiny Recursive Control (TRC), a neural architecture based on a counterintuitive principle: capacity can emerge from iteration depth rather than parameter count. TRC applies compact networks (approximately 1.5M parameters) repeatedly through a two-level hierarchical latent structure, refining control sequences by simulating trajectories and correcting based on tracking error. Because the same weights process every refinement step, adding iterations increases computation without increasing memory. We evaluate TRC on nonlinear control problems including oscillator stabilization and powered descent with fuel constraints. Across these domains, TRC achieves near-optimal control costs while requiring only millisecond-scale inference on GPU and under 10~MB memory, two orders of magnitude smaller than language model baselines. These results demonstrate that recursive reasoning, previously confined to discrete tasks, transfers effectively to continuous control synthesis.",
        "url": "http://arxiv.org/abs/2512.16824v1",
        "published_date": "2025-12-18T18:05:05+00:00",
        "updated_date": "2025-12-18T18:05:05+00:00",
        "categories": [
            "cs.LG",
            "math.DS"
        ],
        "authors": [
            "Amit Jain",
            "Richard Linares"
        ],
        "tldr": "The paper introduces Tiny Recursive Control (TRC), a compact neural network architecture for efficient and near-optimal control in embedded aerospace systems, achieving millisecond-scale inference and low memory usage by iteratively refining control sequences.",
        "tldr_zh": "本文介绍了Tiny Recursive Control (TRC)，一种紧凑的神经网络架构，用于嵌入式航空航天系统中的高效且近乎最优的控制，通过迭代改进控制序列实现毫秒级的推理和低内存使用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "神经网络控制器对参数的需求日益增长，动辄数百万，而语言模型方法更是推向了数十亿。对于具有严格功率和延迟约束的嵌入式航空航天系统而言，这种规模化是难以承受的。我们提出了微型递归控制(TRC)，一种基于反直觉原理的神经架构：容量可以从迭代深度而非参数数量中产生。TRC通过一个双层分级潜在结构，重复应用紧凑型网络（约 150 万个参数），通过模拟轨迹并根据跟踪误差进行校正，从而改进控制序列。由于相同的权重处理每个改进步骤，因此增加迭代次数会增加计算量，但不会增加内存。我们在非线性控制问题上评估了 TRC，包括振荡器稳定和具有燃料约束的动力下降。在这些领域中，TRC实现了接近最优的控制成本，同时在 GPU 上仅需毫秒级的推理时间，并且仅占用 10MB 以下的内存，比语言模型基线小两个数量级。这些结果表明，先前仅限于离散任务的递归推理，可以有效地转移到连续控制综合中。"
    },
    {
        "title": "DVGT: Driving Visual Geometry Transformer",
        "summary": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.",
        "url": "http://arxiv.org/abs/2512.16919v1",
        "published_date": "2025-12-18T18:59:57+00:00",
        "updated_date": "2025-12-18T18:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Sicheng Zuo",
            "Zixun Xie",
            "Wenzhao Zheng",
            "Shaoqing Xu",
            "Fang Li",
            "Shengyin Jiang",
            "Long Chen",
            "Zhi-Xin Yang",
            "Jiwen Lu"
        ],
        "tldr": "The paper introduces DVGT, a Driving Visual Geometry Transformer, that reconstructs dense 3D point maps from unposed multi-view visual inputs using attention mechanisms and trained on diverse driving datasets, outperforming existing methods.",
        "tldr_zh": "该论文介绍了DVGT，一种驾驶视觉几何Transformer，它使用注意力机制从无姿态的多视角视觉输入中重建密集的3D点云图，并在各种驾驶数据集上进行训练，性能优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "从视觉输入感知和重建3D场景几何对于自动驾驶至关重要。然而，仍然缺乏一种能够适应不同场景和相机配置的、面向驾驶的密集几何感知模型。为了弥合这一差距，我们提出了一种驾驶视觉几何变换器（DVGT），它可以从一系列无位姿的多视角视觉输入中重建全局密集3D点云图。我们首先使用DINO骨干网络提取每张图像的视觉特征，并采用交替的视内局部注意力、跨视空间注意力和跨帧时间注意力来推断图像间的几何关系。然后我们使用多头解码器来解码第一帧自车坐标系下的全局点云图以及每一帧的自车位姿。与依赖精确相机参数的传统方法不同，DVGT不需要显式的3D几何先验，能够灵活处理任意相机配置。DVGT直接从图像序列预测度量尺度的几何信息，无需与外部传感器进行后对齐。DVGT在大规模驾驶数据集（包括nuScenes、OpenScene、Waymo、KITTI和DDAD）的混合数据集上训练，在各种场景下显著优于现有模型。代码已发布于https://github.com/wzzheng/DVGT。"
    },
    {
        "title": "Distributional AGI Safety",
        "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.",
        "url": "http://arxiv.org/abs/2512.16856v1",
        "published_date": "2025-12-18T18:29:50+00:00",
        "updated_date": "2025-12-18T18:29:50+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Nenad Tomašev",
            "Matija Franklin",
            "Julian Jacobs",
            "Sébastien Krier",
            "Simon Osindero"
        ],
        "tldr": "The paper argues for a shift in AI safety research from individual AGI alignment to considering the risks of distributed AGI systems composed of coordinated sub-AGI agents and proposes a framework based on virtual agentic sandbox economies.",
        "tldr_zh": "本文认为AI安全研究应该从关注单个AGI对齐转向关注由协同运作的子AGI组成的分布式AGI系统的风险，并提出了一个基于虚拟代理沙盒经济的框架。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "人工智能安全和对齐研究主要集中于保护单个AI系统的方法，其前提是最终会涌现出单一的人工通用智能（AGI）。另一种AGI涌现假设，即通用能力水平首先通过具有互补技能和可供性的亚AGI个体智能体群体的协调来体现，受到的关注要少得多。在此，我们认为应该认真考虑这种拼凑式AGI假设，并依此指导相应安全保障和缓解措施的开发。具有工具使用能力且能够沟通和协调的先进AI智能体的快速部署使其成为一个紧迫的安全问题。因此，我们提出了一个分布式AGI安全框架，该框架超越了评估和对齐单个智能体。该框架的核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体之间的交易受稳健的市场机制的约束，并辅以适当的可审计性、声誉管理和监督，以减轻集体风险。"
    },
    {
        "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI",
        "summary": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.",
        "url": "http://arxiv.org/abs/2512.16873v1",
        "published_date": "2025-12-18T18:42:16+00:00",
        "updated_date": "2025-12-18T18:42:16+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Otman A. Basir"
        ],
        "tldr": "The paper introduces the Social Responsibility Stack (SRS), an architectural framework for embedding societal values into AI systems using control-theoretic methods, enabling accountability and auditability.",
        "tldr_zh": "本文介绍了一种名为社会责任堆栈（SRS）的架构框架，该框架使用控制理论方法将社会价值观嵌入到人工智能系统中，从而实现问责制和可审计性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "人工智能系统正日益部署在塑造人类行为、机构决策和社会结果的领域中。现有的负责任人工智能和治理工作提供了重要的规范性原则，但通常缺乏在系统生命周期中运行的可执行工程机制。本文介绍了社会责任堆栈（SRS），这是一个六层架构框架，它将社会价值观嵌入到人工智能系统中，作为显式约束、保障措施、行为接口、审计机制和治理流程。SRS将责任建模为社会技术系统的闭环监督控制问题，集成了设计时保障措施与运行时监控和机构监督。我们开发了一个统一的基于约束的公式，引入了安全包络和反馈解释，并展示了如何连续监测和执行公平性、自主性、认知负担和解释质量。临床决策支持、协同自动驾驶车辆和公共部门系统中的案例研究表明，SRS如何将规范性目标转化为可操作的工程和运营控制。该框架将伦理学、控制理论和人工智能治理联系起来，为负责任的、适应性强的和可审计的社会技术人工智能系统提供了实际基础。"
    },
    {
        "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy",
        "summary": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.",
        "url": "http://arxiv.org/abs/2512.16851v1",
        "published_date": "2025-12-18T18:23:06+00:00",
        "updated_date": "2025-12-18T18:23:06+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Ripan Kumar Kundu",
            "Istiak Ahmed",
            "Khaza Anuarul Hoque"
        ],
        "tldr": "This paper introduces PrivateXR, a framework that uses explainable AI (XAI) to selectively apply differential privacy (DP) to AI XR models, improving privacy against membership inference attacks and re-identification while preserving model accuracy and inference time, demonstrated on XR tasks.",
        "tldr_zh": "该论文介绍了一个名为PrivateXR的框架，该框架利用可解释人工智能（XAI）选择性地将差分隐私（DP）应用于AI XR模型，从而在保护模型准确性和推理时间的同时，提高针对成员推理攻击和重新识别的隐私保护能力，并在XR任务中进行了演示。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "人工智能和扩展现实（AI XR）技术的融合为许多领域带来了创新应用。然而，这些系统中使用的敏感数据（例如，眼动追踪数据）引发了严重的隐私问题，因为攻击者可以利用这些数据和模型，通过成员推断攻击（MIA）和重新识别（RDA）来推断和泄露个人信息，且成功率很高。研究人员已经提出了各种技术来缓解此类隐私攻击，包括差分隐私（DP）。然而，AI XR数据集通常包含大量特征，统一应用DP可能会给不太相关的特征引入不必要的噪声，降低模型精度，并增加推理时间，从而限制了实时XR部署。为此，我们提出了一种新的框架，结合了可解释人工智能（XAI）和支持DP的隐私保护机制，以防御隐私攻击。具体而言，我们利用事后解释来识别AI XR模型中最具影响力的特征，并在推理过程中选择性地对这些特征应用DP。我们在三个最先进的AI XR模型和三个数据集上评估了我们的XAI引导的DP方法：晕动症、情绪和活动分类。结果表明，对于晕动症任务，该方法可分别降低高达43%和39%的MIA和RDA成功率，同时使用Transformer模型保持高达97%的模型效用。此外，与传统DP方法相比，它将推理时间缩短了高达~2倍。为了展示实用性，我们将XAI引导的DP AI XR模型部署在HTC VIVE Pro头显上，并开发了一个用户界面（UI），即PrivateXR，允许用户在接收实时任务预测的同时调整隐私级别（例如，低、中、高），从而在XR游戏过程中保护用户隐私。"
    },
    {
        "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors",
        "summary": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.",
        "url": "http://arxiv.org/abs/2512.16915v1",
        "published_date": "2025-12-18T18:59:50+00:00",
        "updated_date": "2025-12-18T18:59:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guibao Shen",
            "Yihua Du",
            "Wenhang Ge",
            "Jing He",
            "Chirui Chang",
            "Donghao Zhou",
            "Zhen Yang",
            "Luozhou Wang",
            "Xin Tao",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces StereoPilot, a feed-forward model for monocular-to-stereo video conversion, addressing limitations of existing methods through a unified dataset and efficient architecture.",
        "tldr_zh": "该论文介绍了StereoPilot，一个用于单目到立体视频转换的前馈模型，通过统一的数据集和高效的架构，解决了现有方法的局限性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "立体显示设备的快速发展，包括VR头显和3D影院，导致对高质量立体视频内容的需求日益增长。然而，制作3D视频仍然成本高昂且复杂，而自动单目转立体的转换则受到多阶段“深度-扭曲-修复”(DWI)流程局限性的阻碍。这种范式存在误差传播、深度模糊以及平行和会聚立体配置之间的格式不一致问题。为了应对这些挑战，我们推出了UniStereo，这是首个用于立体视频转换的大规模统一数据集，涵盖了两种立体格式，以实现公平的基准测试和稳健的模型训练。基于此数据集，我们提出StereoPilot，一种高效的馈送前向模型，可以直接合成目标视图，而无需依赖显式的深度图或迭代扩散采样。StereoPilot配备了可学习的域切换器和循环一致性损失，可以无缝地适应不同的立体格式，并实现更好的一致性。大量的实验证明，StereoPilot在视觉保真度和计算效率方面均显著优于最先进的方法。项目主页：https://hit-perfect.github.io/StereoPilot/。"
    },
    {
        "title": "M-PhyGs: Multi-Material Object Dynamics from Video",
        "summary": "Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.",
        "url": "http://arxiv.org/abs/2512.16885v1",
        "published_date": "2025-12-18T18:50:08+00:00",
        "updated_date": "2025-12-18T18:50:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Norika Wada",
            "Kohei Yamashita",
            "Ryo Kawahara",
            "Ko Nishino"
        ],
        "tldr": "The paper introduces M-PhyGs, a method for estimating the material composition and parameters of multi-material objects (specifically flowers) from video, by jointly segmenting the object and recovering their continuum mechanical parameters.",
        "tldr_zh": "本文介绍了M-PhyGs，一种通过视频估计多材料物体（特别是花朵）的材料组成和参数的方法，该方法通过联合分割物体并恢复其连续介质力学参数来实现。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "理解真实世界物体动力学行为的物理材料属性知识，对于准确预测其对未知交互的响应至关重要。现有的从视觉数据估计此类物理材料参数的方法，通常假定物体是均质的单材料物体，具有预先学习的动力学特性或简单的拓扑结构。然而，真实世界的物体在材料组成和几何形状上通常都很复杂，超出了这些假设的范畴。在本文中，我们特别关注花朵作为一种具有代表性的常见物体。我们引入多材料物理高斯模型（M-PhyGs），用于从视频中估计这种多材料复杂自然物体的材料组成和参数。从在自然环境中捕获的短视频中，M-PhyGs联合分割物体为相似的材料，并恢复它们的连续介质力学参数，同时考虑重力。M-PhyGs通过新引入的级联3D和2D损失，以及利用时间小批量处理，高效地实现这一点。我们引入了一个数据集Phlowers，包含人与花朵的互动，作为一个新的平台，用于评估这种具有挑战性的多材料物理参数估计任务的准确性。在Phlowers数据集上的实验结果证明了M-PhyGs及其组件的准确性和有效性。"
    },
    {
        "title": "Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation",
        "summary": "Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.",
        "url": "http://arxiv.org/abs/2512.16880v1",
        "published_date": "2025-12-18T18:49:33+00:00",
        "updated_date": "2025-12-18T18:49:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Valay Bundele",
            "Mehran Hosseinzadeh",
            "Hendrik P. A. Lensch"
        ],
        "tldr": "The paper introduces ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3 for robust surgical instrument segmentation in endoscopic videos, addressing occlusion, memory capacity, and identity recovery issues, achieving significant improvements over SAM3 in zero-shot settings.",
        "tldr_zh": "该论文介绍了ReMeDI-SAM3，一种无需训练的、内存增强的SAM3扩展，用于内窥镜视频中稳健的手术器械分割，解决了遮挡、内存容量和身份恢复等问题，在零样本设置下对SAM3实现了显著改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "内窥镜视频中准确的手术器械分割对于计算机辅助干预至关重要，但由于频繁的遮挡、快速运动、镜面伪影以及器械长时间重入，仍然具有挑战性。虽然SAM3为视频对象分割提供了一个强大的时空框架，但其在手术场景中的性能受到不加区分的记忆更新、固定的记忆容量以及遮挡后较弱的身份恢复能力的限制。我们提出了ReMeDI-SAM3，一个无需训练的SAM3记忆增强扩展，它通过三个组件解决了这些限制：（i）相关性感知记忆过滤，带有一个专门的遮挡感知记忆，用于存储预遮挡帧；（ii）分段插值方案，扩展了有效记忆容量；以及（iii）一个基于特征的重识别模块，带有时间投票机制，用于可靠的遮挡后身份消歧。总而言之，这些组件缓解了误差累积，并能够在遮挡后实现可靠的恢复。在零样本设置下对EndoVis17和EndoVis18的评估表明，相比原版SAM3，mcIoU分别提升了约7%和16%，甚至优于先前基于训练的方法。项目主页：https://valaybundele.github.io/remedi-sam3/。"
    }
]