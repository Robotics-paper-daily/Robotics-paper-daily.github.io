[
    {
        "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models",
        "summary": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.",
        "url": "http://arxiv.org/abs/2512.09619v1",
        "published_date": "2025-12-10T13:07:27+00:00",
        "updated_date": "2025-12-10T13:07:27+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Minghao Guo",
            "Meng Cao",
            "Jiachen Tao",
            "Rongtao Xu",
            "Yan Yan",
            "Xiaodan Liang",
            "Ivan Laptev",
            "Xiaojun Chang"
        ],
        "tldr": "The paper introduces GLaD, a Vision-Language-Action model that incorporates 3D geometric priors during pretraining by distilling geometric features into the LLM's hidden states, leading to improved performance on robotic manipulation tasks.",
        "tldr_zh": "该论文介绍了GLaD，一种视觉-语言-动作模型，通过将几何特征提取到LLM的隐藏状态中，在预训练期间融入了3D几何先验，从而提高了机器操作任务的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "大多数现有的视觉-语言-动作（VLA）模型主要依赖于RGB信息，而忽略了几何线索，这些线索对于空间推理和操控至关重要。在这项工作中，我们引入了GLaD，一个几何感知的VLA框架，它通过知识蒸馏在预训练期间整合了3D几何先验。与仅将几何特征蒸馏到视觉编码器不同，我们将对应于视觉token的LLM隐藏状态与来自冻结的几何感知视觉Transformer (VGGT) 的特征对齐，确保几何理解被深度整合到驱动动作预测的多模态表示中。通过这种几何蒸馏机制在Bridge数据集上进行预训练，GLaD在四个LIBERO任务套件上取得了94.1%的平均成功率，优于使用相同预训练数据的UniVLA（92.5%）。这些结果验证了几何感知预训练增强了空间推理和策略泛化能力，而无需显式的深度传感器或3D标注。"
    },
    {
        "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories",
        "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.",
        "url": "http://arxiv.org/abs/2512.09607v1",
        "published_date": "2025-12-10T12:54:04+00:00",
        "updated_date": "2025-12-10T12:54:04+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yanghong Mei",
            "Yirong Yang",
            "Longteng Guo",
            "Qunbo Wang",
            "Ming-Ming Yu",
            "Xingjian He",
            "Wenjun Wu",
            "Jing Liu"
        ],
        "tldr": "UrbanNav introduces a framework for training embodied agents to navigate urban environments using language instructions, leveraging web-scale city walking videos and a large dataset of instruction-trajectory-landmark triplets. The results demonstrate improved navigation policies and generalization capabilities.",
        "tldr_zh": "UrbanNav 提出了一个框架，利用大规模网络城市步行视频和一个包含指令-轨迹-地标的大型数据集，训练具身智能体使用语言指令在城市环境中导航。结果表明，该方法改进了导航策略和泛化能力。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "利用自然语言指令在复杂的城市环境中进行导航，对具身智能体提出了重大挑战，包括嘈杂的语言指令、模糊的空间参照、多样的地标以及动态的街道场景。当前的视觉导航方法通常局限于模拟环境或非街道环境，并且经常依赖精确的目标格式，例如具体的坐标或图像。这限制了它们在自主智能体（例如最后一英里配送机器人）在不熟悉的城市中导航的有效性。为了解决这些局限性，我们提出了UrbanNav，一个可扩展的框架，用于训练具身智能体在各种城市环境中遵循自由形式的语言指令。通过利用网络规模的城市步行视频，我们开发了一个可扩展的标注流程，该流程将人类导航轨迹与基于真实地标物的语言指令对齐。UrbanNav包含了超过1500小时的导航数据以及300万个指令-轨迹-地标三元组，捕捉了广泛的城市场景。我们的模型学习了鲁棒的导航策略来应对复杂的城市场景，展现了卓越的空间推理能力、对嘈杂指令的鲁棒性以及对未见过的城市环境的泛化能力。实验结果表明，UrbanNav显著优于现有方法，突出了大规模网络视频数据在实现具身智能体以语言引导的真实世界城市导航方面的潜力。"
    },
    {
        "title": "COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning",
        "summary": "End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\\% in trained driving environments and by 50\\% in previously unseen environments, highlighting its strong generalization capability.",
        "url": "http://arxiv.org/abs/2512.09349v1",
        "published_date": "2025-12-10T06:18:16+00:00",
        "updated_date": "2025-12-10T06:18:16+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Lin Li",
            "Yuxin Cai",
            "Jianwu Fang",
            "Jianru Xue",
            "Chen Lv"
        ],
        "tldr": "The paper introduces COVLM-RL, a novel autonomous driving framework that integrates VLM-guided reinforcement learning with critical object-oriented reasoning to improve generalization, training efficiency, and interpretability, demonstrating significant improvement in success rates.",
        "tldr_zh": "该论文介绍了 COVLM-RL，一种新型自动驾驶框架，它将 VLM 指导的强化学习与关键的面向对象推理相结合，以提高泛化性、训练效率和可解释性，并在成功率方面表现出显著提高。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "端到端自动驾驶框架在泛化性、训练效率和可解释性方面面临持续的挑战。虽然最近的方法利用视觉-语言模型（VLMs）通过大规模数据集上的监督学习来提高推理能力，但它们在新场景中通常缺乏稳健性。相反，基于强化学习（RL）的方法增强了适应性，但仍然存在数据效率低下和缺乏透明决策的问题。为了解决这些局限性，我们提出了COVLM-RL，一种新颖的端到端驾驶框架，它将关键物体导向（CO）推理与VLM引导的RL相结合。具体而言，我们设计了一种思维链（CoT）提示策略，使VLM能够推理关键交通元素并生成高级语义决策，从而有效地将多视角视觉输入转换为结构化的语义决策先验。这些先验降低了输入维度，并将任务相关的知识注入到RL循环中，从而加速训练并提高策略的可解释性。然而，将高级语义指导与连续的低级控制联系起来仍然具有挑战性。为此，我们引入了一致性损失，以鼓励VLM的语义计划与RL代理的控制输出之间的一致性，从而提高可解释性和训练稳定性。在CARLA模拟器中进行的实验表明，COVLM-RL在已训练的驾驶环境中显著提高了成功率30%，在以前未见过的环境中提高了50%，突显了其强大的泛化能力。"
    },
    {
        "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
        "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",
        "url": "http://arxiv.org/abs/2512.09487v1",
        "published_date": "2025-12-10T10:05:31+00:00",
        "updated_date": "2025-12-10T10:05:31+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.IR"
        ],
        "authors": [
            "Yucan Guo",
            "Miao Su",
            "Saiping Guan",
            "Zihao Sun",
            "Xiaolong Jin",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "tldr": "The paper introduces RouteRAG, a Reinforcement Learning-based framework for efficient and adaptive retrieval-augmented generation from both text and graph knowledge sources, outperforming existing RAG baselines on question answering tasks.",
        "tldr_zh": "该论文介绍了一种基于强化学习的框架RouteRAG，用于从文本和图知识源进行高效且自适应的检索增强生成，在问答任务中优于现有的RAG基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "检索增强生成 (RAG) 将非参数知识集成到大型语言模型 (LLM) 中，这些知识通常来自非结构化文本和结构化图数据。虽然最近的进展通过强化学习 (RL) 将基于文本的 RAG 推向多轮推理，但将这些进展扩展到混合检索引入了额外的挑战。现有的基于图或混合系统通常依赖于固定或手工设计的检索流程，缺乏在推理展开时整合补充证据的能力。此外，虽然图证据提供了对多跳推理至关重要的关系结构，但其检索成本要高得多。为了解决这些局限性，我们提出了 \\model{}，一个基于 RL 的框架，使 LLM 能够执行多轮和自适应的图文混合 RAG。\\model{} 通过 RL 联合优化整个生成过程，从而使模型能够在统一的生成策略中学习何时进行推理、从文本或图中检索什么，以及何时生成最终答案。为了指导这个学习过程，我们设计了一个两阶段训练框架，其中考虑了任务结果和检索效率，从而使模型能够利用混合证据，同时避免不必要的检索开销。在五个问答基准测试上的实验结果表明，\\model{} 显著优于现有的 RAG 基线，突显了端到端 RL 在支持复杂推理的自适应和高效检索方面的优势。"
    },
    {
        "title": "Astra: General Interactive World Model with Autoregressive Denoising",
        "summary": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.",
        "url": "http://arxiv.org/abs/2512.08931v1",
        "published_date": "2025-12-09T18:59:57+00:00",
        "updated_date": "2025-12-09T18:59:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yixuan Zhu",
            "Jiaqi Feng",
            "Wenzhao Zheng",
            "Yuan Gao",
            "Xin Tao",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "The paper introduces Astra, a general interactive world model that uses an autoregressive denoising architecture to predict long-horizon futures in diverse real-world scenarios with precise action control, demonstrating improvements over existing world models. Astra utilizes noise-augmented history, action-aware adapters and a mixture of action experts.",
        "tldr_zh": "该论文介绍了Astra，一个通用交互式世界模型，它使用自回归去噪架构来预测各种现实场景中具有精确动作控制的长期未来，并通过实验证明了它相对于现有世界模型的改进。Astra利用了噪声增强的历史记忆，动作感知适配器和动作专家混合。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "扩散Transformer的最新进展使视频生成模型能够从文本或图像生成高质量视频片段。然而，具有从过去观察和行动预测长期未来能力的世界模型仍然有待探索，特别是对于通用场景和各种形式的行动而言。为了弥合这一差距，我们推出了Astra，一个交互式通用世界模型，可以为各种场景（例如，自动驾驶、机器人抓取）生成现实世界的未来，并具有精确的行动交互（例如，相机运动、机器人动作）。我们提出了一种自回归去噪架构，并使用时间因果注意力来聚合过去的观察结果并支持流式输出。我们使用带噪历史记忆来避免过度依赖过去的帧，以平衡响应性和时间连贯性。为了实现精确的动作控制，我们引入了一个动作感知适配器，该适配器将动作信号直接注入到去噪过程中。我们进一步开发了一个动作专家混合器，可以动态地路由异构动作模态，从而增强跨各种现实世界任务（例如，探索、操作和相机控制）的通用性。Astra实现了交互式、一致且通用的长期视频预测，并支持各种形式的交互。在多个数据集上的实验表明，与现有最先进的世界模型相比，Astra在保真度、远程预测和动作对齐方面都有所改进。"
    },
    {
        "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers",
        "summary": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/",
        "url": "http://arxiv.org/abs/2512.08889v1",
        "published_date": "2025-12-09T18:30:23+00:00",
        "updated_date": "2025-12-09T18:30:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Damiano Marsili",
            "Georgia Gkioxari"
        ],
        "tldr": "The paper introduces an annotation-free training framework using LLM and VLM verifiers to improve visual reasoning and grounding, outperforming existing methods on spatial reasoning tasks by combining the strengths of language-only and vision models.",
        "tldr_zh": "该论文介绍了一种无需标注的训练框架，利用LLM和VLM验证器来提高视觉推理和定位能力。通过结合语言模型和视觉模型的优势，该方法在空间推理任务上表现优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "视觉推理极具挑战性，既要求精确的目标定位，也需要理解复杂的空间关系。现有方法分为两类：依赖大规模（图像、问题、答案）监督的纯语言链式思考方法，以及使用预训练模型并避免训练的程序合成方法，但后者面临逻辑缺陷和错误的定位问题。我们提出了一种无需标注的训练框架，可以同时改进推理和定位。我们的框架使用了 AI 驱动的验证器：一个 LLM 验证器通过强化学习来改进 LLM 的推理，而一个 VLM 验证器通过自动硬样本挖掘来加强视觉定位，从而消除了对真值标签的需求。这种设计结合了现代 AI 系统的优势：用于将空间查询分解为更简单子任务的先进纯语言推理模型，以及通过高性能 VLM critics 改进的强大视觉专家模型。我们在各种空间推理任务上评估了我们的方法，结果表明我们的方法改进了视觉推理并超越了开源和专有模型，此外，借助我们改进的视觉定位模型，我们进一步超越了最近的纯文本视觉推理方法。项目网页：https://glab-caltech.github.io/valor/"
    },
    {
        "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
        "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.",
        "url": "http://arxiv.org/abs/2512.09864v1",
        "published_date": "2025-12-10T17:50:29+00:00",
        "updated_date": "2025-12-10T17:50:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Lu",
            "Ziyang Liu",
            "Guangfeng Jiang",
            "Yuanfei Luo",
            "Sheng Chen",
            "Yangang Zhang",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces UniUGP, a unified framework for autonomous driving that combines scene understanding, future video generation, and trajectory planning using a hybrid expert architecture, achieving SOTA performance and generalization in long-tail scenarios.",
        "tldr_zh": "该论文介绍了 UniUGP，一个统一的自动驾驶框架，它结合了场景理解、未来视频生成和轨迹规划，使用混合专家架构，并在长尾场景中实现了最先进的性能和泛化能力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "摘要：\n自动驾驶（AD）系统由于世界知识有限和视觉动态建模薄弱，在长尾场景中表现不佳。现有的基于视觉-语言-动作（VLA）的方法无法利用未标记视频进行视觉因果学习，而基于世界模型的方法则缺乏来自大型语言模型的推理能力。在本文中，我们构建了多个专门的数据集，为复杂场景提供推理和规划标注。然后，我们提出了一个统一的理解-生成-规划框架，名为UniUGP，通过混合专家架构协同实现场景推理、未来视频生成和轨迹规划。通过整合预训练的VLM和视频生成模型，UniUGP利用视觉动态和语义推理来增强规划性能。以多帧观测数据和语言指令作为输入，UniUGP生成可解释的思维链推理、在物理上一致的轨迹以及连贯的未来视频。我们引入了一种四阶段训练策略，该策略通过多个已有的AD数据集以及所提出的专用数据集逐步构建这些能力。实验结果表明，在感知、推理和决策方面取得了最先进的性能，并在应对具有挑战性的长尾场景时具有出色的泛化能力。"
    },
    {
        "title": "ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors",
        "summary": "Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.",
        "url": "http://arxiv.org/abs/2512.09056v1",
        "published_date": "2025-12-09T19:16:28+00:00",
        "updated_date": "2025-12-09T19:16:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liming Kuang",
            "Yordanka Velikova",
            "Mahdi Saleh",
            "Jan-Nico Zaech",
            "Danda Pani Paudel",
            "Benjamin Busam"
        ],
        "tldr": "ConceptPose introduces a training-free, model-free object pose estimation framework using vision-language models for concept vector-based 3D mapping, achieving SOTA zero-shot performance.",
        "tldr_zh": "ConceptPose 提出了一个无需训练、不依赖模型的目标姿态估计框架，它利用视觉-语言模型进行基于概念向量的 3D 建图，实现了最先进的零样本性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "物体姿态估计是计算机视觉和机器人技术中的一项基本任务，然而大多数方法都需要大量特定于数据集的训练。与此同时，大规模视觉语言模型展现出卓越的零样本能力。在这项工作中，我们通过引入ConceptPose，一个无需训练且无模型的物体姿态估计框架，将这两个领域连接起来。ConceptPose利用视觉语言模型（VLM）创建开放词汇的3D概念图，其中每个点都用一个从显著性图导出的概念向量进行标记。通过建立跨概念图的鲁棒3D-3D对应关系，我们的方法能够精确估计6自由度的相对姿态。在没有任何物体或数据集特定训练的情况下，我们的方法在常见的零样本相对姿态估计基准测试中取得了最先进的结果，在ADD(-S)评分上显著优于现有方法超过62%，包括那些利用大量数据集特定训练的方法。"
    },
    {
        "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation",
        "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.",
        "url": "http://arxiv.org/abs/2512.09851v1",
        "published_date": "2025-12-10T17:35:13+00:00",
        "updated_date": "2025-12-10T17:35:13+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yuyang Li",
            "Yinghan Chen",
            "Zihang Zhao",
            "Puhao Li",
            "Tengyu Liu",
            "Siyuan Huang",
            "Yixin Zhu"
        ],
        "tldr": "The paper introduces TacThru, a novel see-through-skin sensor offering simultaneous tactile and visual perception, and TacThru-UMI, an imitation learning framework utilizing these multimodal signals for improved robot manipulation, demonstrating superior performance on real-world tasks.",
        "tldr_zh": "该论文介绍了 TacThru，一种新型的透视皮肤传感器，可同时提供触觉和视觉感知；以及 TacThru-UMI，一个利用这些多模态信号的模仿学习框架，从而改进机器人操作，并在真实世界任务中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "机器人操作需要丰富的多模态感知和有效的学习框架来处理复杂的真实世界任务。结合触觉和视觉感知的透视皮肤（STS）传感器提供了有前景的传感能力，而现代模仿学习则为策略获取提供了强大的工具。然而，现有的STS设计缺乏同步多模态感知，并且触觉跟踪不可靠。此外，将这些丰富的多模态信号整合到基于学习的机械臂控制流程中仍然是一个公开的挑战。我们介绍了TacThru，一个能够同时进行视觉感知和稳健触觉信号提取的STS传感器，以及TacThru-UMI，一个利用这些多模态信号进行操作的模仿学习框架。我们的传感器采用完全透明的弹性体、持续照明、新型关键线标记和高效跟踪，而我们的学习系统通过基于Transformer的扩散策略整合这些信号。在五个具有挑战性的真实世界任务上的实验表明，TacThru-UMI的平均成功率达到85.5%，显著优于交替触觉-视觉（66.3%）和仅视觉（55.4%）的基线。该系统在关键场景中表现出色，包括与薄而软物体的接触检测以及需要多模态协调的精确操作。这项工作表明，将同步多模态感知与现代学习框架相结合，可以实现更精确、更具适应性的机器人操作。"
    },
    {
        "title": "Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing",
        "summary": "Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/",
        "url": "http://arxiv.org/abs/2512.09571v2",
        "published_date": "2025-12-10T12:02:48+00:00",
        "updated_date": "2025-12-11T04:26:24+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Feng Yu",
            "Yu Hu",
            "Yang Su",
            "Yang Deng",
            "Linzuo Zhang",
            "Danping Zou"
        ],
        "tldr": "This paper presents a reinforcement learning framework for drone racing in diverse, unknown, and cluttered environments, using a two-phase training approach with noise augmentation and Lipschitz constraints to achieve robust and generalizable performance.",
        "tldr_zh": "本文提出了一种强化学习框架，用于在多样、未知和杂乱环境中进行无人机竞速。该框架采用两阶段训练方法，并结合噪声增强和Lipschitz约束，以实现鲁棒和泛化的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大多数基于强化学习（RL）的无人机竞速方法都针对固定的、无障碍赛道，使得在未知、杂乱环境中泛化的能力很大程度上未得到解决。这一挑战源于需要平衡竞速速度和避障，有限的可行空间导致策略探索在训练期间陷入局部最优，以及深度图中门和障碍物之间的感知模糊性——尤其是在门的位置仅被粗略指定时。为了克服这些问题，我们提出了一种两阶段学习框架：一个初始的软碰撞训练阶段，以保持策略探索以实现高速飞行，随后是一个硬碰撞细化阶段，以强制执行稳健的避障。一个自适应的、噪声增强的课程，结合非对称的Actor-Critic架构，逐渐将策略的依赖性从特权门-状态信息转移到基于深度的视觉输入。我们进一步施加了Lipschitz约束，并集成了一个赛道基元生成器，以增强运动稳定性和跨环境泛化能力。我们通过广泛的模拟和消融研究评估了我们的框架，并在计算资源受限的四旋翼飞行器上进行了真实世界的实验验证。该系统实现了敏捷飞行，同时保持了对门位置误差的鲁棒性，开发了一种具有通用性的无人机竞速框架，能够在各种部分未知的杂乱环境中运行。 https://yufengsjtu.github.io/MasterRacing.github.io/"
    },
    {
        "title": "REASAN: Learning Reactive Safe Navigation for Legged Robots",
        "summary": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.",
        "url": "http://arxiv.org/abs/2512.09537v1",
        "published_date": "2025-12-10T11:23:32+00:00",
        "updated_date": "2025-12-10T11:23:32+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Qihao Yuan",
            "Ziyu Cao",
            "Ming Cao",
            "Kailai Li"
        ],
        "tldr": "The paper introduces REASAN, a modular end-to-end framework for reactive safe navigation of legged robots in dynamic environments using reinforcement learning, a LiDAR sensor and a transformer-based estimator, achieving real-time onboard performance.",
        "tldr_zh": "该论文介绍了一种名为REASAN的模块化端到端框架，用于在动态环境中实现腿式机器人的反应式安全导航，它使用强化学习、激光雷达传感器和一个基于Transformer的估计器，实现了实时的机载性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了一种新颖的模块化端到端框架，用于在复杂动态环境中，利用单个激光雷达(LiDAR)传感器实现足式机器人的响应式导航。该系统包含四个经仿真训练的模块：三个分别用于运动、安全保护和导航的强化学习(RL)策略，以及一个基于Transformer的外部状态估计器，用于处理原始点云输入。这种复杂足式运动控制任务的模块化分解使得可以使用具有简单架构的轻量级神经网络，通过标准RL实践进行训练，并结合有针对性的奖励塑造和课程设计，无需依赖启发式方法或复杂的策略切换机制。我们进行了全面的消融实验，以验证我们的设计选择，并证明在具有挑战性的导航任务中，与现有方法相比，提高了鲁棒性。最终的响应式安全导航(REASAN)系统实现了在复杂环境中的单机器人和多机器人设置下，完全板载和实时的响应式导航。我们在https://github.com/ASIG-X/REASAN上发布了我们的训练和部署代码。"
    },
    {
        "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics",
        "summary": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.",
        "url": "http://arxiv.org/abs/2512.09510v1",
        "published_date": "2025-12-10T10:34:43+00:00",
        "updated_date": "2025-12-10T10:34:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Donato Caramia",
            "Florian T. Pokorny",
            "Giuseppe Triggiani",
            "Denis Ruffino",
            "David Naso",
            "Paolo Roberto Massenio"
        ],
        "tldr": "The paper introduces ViTA-Seg, a Vision Transformer-based framework for real-time amodal segmentation for robotic bin picking, achieving state-of-the-art results on amodal segmentation benchmarks.",
        "tldr_zh": "该论文介绍了ViTA-Seg，一个基于Vision Transformer的实时非模态分割框架，用于机器人箱拣选，并在非模态分割基准测试中取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "机器人箱内拣选中的遮挡会影响抓取规划的准确性和可靠性。我们提出ViTA-Seg，一个类别无关的视觉Transformer框架，用于实时非模态分割，它利用全局注意力来恢复完整的物体掩码，包括隐藏区域。我们提出了两种架构：a) 单头结构，用于非模态掩码预测；b) 双头结构，用于非模态和遮挡掩码预测。我们还引入了ViTA-SimData，一个为工业箱内拣选场景定制的逼真合成数据集。在COOCA和KINS两个非模态基准测试上的大量实验表明，ViTA-Seg双头结构以计算效率实现了强大的非模态和遮挡分割精度，从而实现了鲁棒的实时机器人操作。"
    },
    {
        "title": "Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation",
        "summary": "Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.",
        "url": "http://arxiv.org/abs/2512.09410v1",
        "published_date": "2025-12-10T08:09:12+00:00",
        "updated_date": "2025-12-10T08:09:12+00:00",
        "categories": [
            "cs.RO",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Jialin Ying",
            "Zhihao Li",
            "Zicheng Dong",
            "Guohua Wu",
            "Yihuan Liao"
        ],
        "tldr": "This paper introduces Path-Guided Frontier MAPPO (PGF-MAPPO), a hierarchical MARL framework for collaborative pursuit-evasion in cluttered environments, demonstrating superior capture efficiency and robust zero-shot generalization.",
        "tldr_zh": "该论文介绍了路径引导前沿MAPPO (PGF-MAPPO)，一个用于杂乱环境中协作追逐逃避的分层MARL框架，展示了卓越的捕获效率和强大的零样本泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "在杂乱环境中进行协作追逐躲避面临着显著的挑战，因为奖励稀疏且视场 (FOV) 受限。标准的多智能体强化学习 (MARL) 常常遭受低效探索的问题，并且无法扩展到大规模场景。我们提出 PGF-MAPPO（路径引导的前沿 MAPPO），一个连接拓扑规划与反应式控制的分层框架。为了解决局部最小值和稀疏奖励问题，我们集成了一个基于 A* 算法的势场，用于密集奖励塑造。此外，我们引入方向性前沿分配，结合最远点采样 (FPS) 与几何角度抑制，以强制空间分散并加速覆盖。该架构采用参数共享的去中心化评论家，保持 O(1) 的模型复杂度，适合于机器人集群。实验表明，PGF-MAPPO 实现了优于更快躲避者的捕获效率。在 10x10 地图上训练的策略对未见过的 20x20 环境表现出强大的零样本泛化能力，显著优于基于规则的和基于学习的基线方法。"
    },
    {
        "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos",
        "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/",
        "url": "http://arxiv.org/abs/2512.09406v1",
        "published_date": "2025-12-10T07:59:45+00:00",
        "updated_date": "2025-12-10T07:59:45+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Hai Ci",
            "Xiaokang Liu",
            "Pei Yang",
            "Yiren Song",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces H2R-Grounder, a paired-data-free video-to-video translation framework that converts human interaction videos into realistic, physically grounded robot manipulation videos using a transferable representation and in-context learning with a video diffusion model.",
        "tldr_zh": "该论文介绍了H2R-Grounder，一个无需配对数据的视频到视频翻译框架，使用可转移的表示和视频扩散模型进行上下文学习，将人类交互视频转换为逼真、物理上合理的机器人操作视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "从日常人类视频中学习操作技能的机器人有望在无需繁琐的机器人数据收集的情况下获得广泛的能力。 我们提出了一种视频到视频的转换框架，该框架将普通的人与物交互视频转换为具有逼真、物理上可靠的交互且运动一致的机器人操作视频。 我们的方法不需要任何配对的人-机器人视频进行训练，只需要一组未配对的机器人视频，从而使系统易于扩展。 我们引入了一种可迁移的表示，用于弥合具身性差距：通过在训练视频中对机器人手臂进行图像修复以获得干净的背景，并叠加一个简单的视觉提示（指示夹爪位置和方向的标记和箭头），我们可以调节生成模型以将机器人手臂重新插入到场景中。 在测试时，我们对人类视频应用相同的过程（对人物进行图像修复并叠加人类姿势提示），并生成模仿人类动作的高质量机器人视频。 我们以上下文学习的方式微调了最先进的视频扩散模型 (Wan 2.2)，以确保时间连贯性并利用其丰富的先验知识。 实验结果表明，与基线方法相比，我们的方法实现了明显更逼真和可靠的机器人运动，为从未标记的人类视频中扩展机器人学习指明了一个有希望的方向。 项目主页：https://showlab.github.io/H2R-Grounder/"
    },
    {
        "title": "Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning",
        "summary": "Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.",
        "url": "http://arxiv.org/abs/2512.09310v1",
        "published_date": "2025-12-10T04:37:07+00:00",
        "updated_date": "2025-12-10T04:37:07+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Kwang Bin Lee",
            "Jiho Kang",
            "Sung-Hee Lee"
        ],
        "tldr": "This paper introduces a scene-agnostic bimanual task planning framework that combines visual affordance reasoning with a structured skill library to enable robots to plan and execute two-handed tasks in novel environments.",
        "tldr_zh": "本文介绍了一个场景无关的双手动任务规划框架，该框架结合了视觉可供性推理和结构化技能库，使机器人能够在新的环境中规划和执行双手任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在开放环境中操作的具身智能体必须将高级指令转化为可执行的具象行为，这通常需要协调双手的使用。虽然近期的基础模型提供了强大的语义推理能力，但现有的机器人任务规划器主要还是单手的，无法解决与场景无关环境中双手操作固有的空间、几何和协调挑战。我们提出了一个统一的、与场景无关的双手动任务规划框架，它将高级推理与基于3D的双手执行连接起来。我们的方法集成了三个关键模块。视觉点定位 (VPG) 分析单个场景图像以检测相关对象并生成世界坐标对齐的交互点。双手动子目标规划器 (BSP) 基于空间邻接和跨对象可达性进行推理，以产生紧凑的、运动中性的子目标，从而利用协同双手动作的机会。交互点驱动的双手动提示 (IPBP) 将这些子目标绑定到一个结构化的技能库，实例化满足手部状态和可供性约束的同步单手或双手动作序列。总而言之，这些模块使智能体能够在混乱、未见过的场景中规划语义上有意义、物理上可行且可并行化的双手行为。实验表明，它能够生成连贯、可行且紧凑的双手计划，并且无需重新训练即可推广到杂乱的场景中，从而展示了用于双手任务的鲁棒的、与场景无关的可供性推理能力。"
    },
    {
        "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation",
        "summary": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.",
        "url": "http://arxiv.org/abs/2512.09297v1",
        "published_date": "2025-12-10T03:48:16+00:00",
        "updated_date": "2025-12-10T03:48:16+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Huayi Zhou",
            "Kui Jia"
        ],
        "tldr": "The paper introduces BiDemoSyn, a framework that generates high-quality, diverse bimanual manipulation demonstrations from a single real-world example by decomposing tasks into invariant coordination blocks and object-dependent adjustments, enabling scalable imitation learning.",
        "tldr_zh": "该论文介绍了一种名为BiDemoSyn的框架，它通过将任务分解为不变的协调块和依赖于对象的信息调整，从一个真实世界的例子中生成高质量、多样的双手操作演示，从而实现可扩展的模仿学习。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "学习灵巧的双臂操作策略关键在于大规模、高质量的示范数据，然而目前的方法面临着内在的权衡：遥操作提供物理上可靠的数据，但劳动成本极高；而基于仿真的合成可以高效地扩展规模，但存在从仿真到真实世界的差距。我们提出了BiDemoSyn，一个从单个真实世界示例中合成富含接触、物理上可行的双臂操作示范的框架。其核心思想是将任务分解为不变的协调块和可变的、依赖于对象的调整，然后通过视觉引导的对齐和轻量级的轨迹优化来调整它们。这使得在几个小时内生成数千个多样且可行的示范成为可能，而无需重复的遥操作或依赖于不完善的仿真。在六个双臂任务中，我们表明基于BiDemoSyn数据训练的策略能够稳健地泛化到新的物体姿势和形状，显著优于最近的基线方法。通过弥合效率和真实世界保真度之间的差距，BiDemoSyn为复杂双臂操作的实际模仿学习提供了一条可扩展的路径，同时不损害物理基础。"
    },
    {
        "title": "Masked Generative Policy for Robotic Control",
        "summary": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.",
        "url": "http://arxiv.org/abs/2512.09101v1",
        "published_date": "2025-12-09T20:37:40+00:00",
        "updated_date": "2025-12-09T20:37:40+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Lipeng Zhuang",
            "Shiyu Fan",
            "Florent P. Audonnet",
            "Yingdong Ru",
            "Gerardo Aragon Camarasa",
            "Paul Henderson"
        ],
        "tldr": "The paper introduces Masked Generative Policy (MGP), a novel masked transformer framework for visuomotor imitation learning that achieves faster inference and higher success rates compared to SOTA methods in complex robotic manipulation tasks, especially in non-Markovian environments.",
        "tldr_zh": "本文介绍了一种名为Masked Generative Policy (MGP) 的新型masked transformer框架，用于视觉运动模仿学习。相比于当前最优的方法，该方法在复杂的机器人操作任务中实现了更快的推理速度和更高的成功率，尤其是在non-Markovian环境中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了掩码生成策略（MGP），一种用于视觉运动模仿学习的新颖框架。我们将动作表示为离散符号，并训练一个条件掩码Transformer，该Transformer并行生成符号，然后快速精炼置信度低的符号。我们进一步提出了两种新的采样范式：MGP-Short，它为马尔可夫任务执行基于得分精炼的并行掩码生成；以及MGP-Long，它以单次遍历预测完整轨迹，并基于新的观测动态精炼置信度低的动作符号。凭借全局连贯的预测和强大的自适应执行能力，MGP-Long能够在复杂且非马尔可夫任务上实现可靠控制，而先前的方法难以应对这些任务。在涵盖Meta-World和LIBERO基准测试的150个机器人操作任务上的广泛评估表明，与最先进的扩散策略和自回归策略相比，MGP实现了快速推理和更高的成功率。具体而言，MGP在150个任务上的平均成功率提高了9%，同时每次序列推理时间最多缩短了35倍。它还在动态和缺失观测环境中将平均成功率提高了60%，并解决了其他最先进方法失败的两个非马尔可夫场景。"
    },
    {
        "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer",
        "summary": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.",
        "url": "http://arxiv.org/abs/2512.08920v1",
        "published_date": "2025-12-09T18:56:30+00:00",
        "updated_date": "2025-12-09T18:56:30+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Jessica Yin",
            "Haozhi Qi",
            "Youngsun Wi",
            "Sayantan Kundu",
            "Mike Lambeta",
            "William Yang",
            "Changhao Wang",
            "Tingfan Wu",
            "Jitendra Malik",
            "Tess Hellebrekers"
        ],
        "tldr": "The paper introduces OSMO, an open-source tactile glove that facilitates human-to-robot skill transfer by minimizing the embodiment gap through tactile feedback, and demonstrates its effectiveness in a real-world wiping task.",
        "tldr_zh": "本文介绍了一个开源触觉手套OSMO，它通过触觉反馈最小化了具身差距，从而促进了人到机器人的技能转移，并在一个真实的擦拭任务中展示了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人类视频演示为学习机器人策略提供了丰富的训练数据，但仅凭视频无法捕捉掌握操作所需的关键接触信号。我们推出OSMO，一个开源的可穿戴触觉手套，专为人类到机器人的技能迁移而设计。该手套在指尖和手掌上配备了12个三轴触觉传感器，并设计为与最先进的手部跟踪方法兼容，以便进行野外数据收集。 我们证明，仅使用通过OSMO收集的人类演示训练的机器人策略，无需任何真实机器人数据，就能够执行一项具有挑战性的、接触丰富的操作任务。通过为人类和机器人配备相同的手套，OSMO最大限度地减少了视觉和触觉的具身差异，从而能够传递连续的剪切力和法向力反馈，同时避免了图像修复或其他基于视觉的力推断的需求。 在一项需要持续接触压力的真实擦拭任务中，我们基于触觉的策略实现了72%的成功率，通过消除与接触相关的失效模式，优于仅使用视觉的基线方法。 我们发布完整的硬件设计、固件和组装说明，以支持社区采用。"
    },
    {
        "title": "An End-to-end Planning Framework with Agentic LLMs and PDDL",
        "summary": "We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.",
        "url": "http://arxiv.org/abs/2512.09629v1",
        "published_date": "2025-12-10T13:17:08+00:00",
        "updated_date": "2025-12-10T13:17:08+00:00",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Emanuele La Malfa",
            "Ping Zhu",
            "Samuele Marro",
            "Sara Bernardini",
            "Michael Wooldridge"
        ],
        "tldr": "This paper presents an end-to-end planning framework using LLMs to convert natural language specifications into PDDL, generate plans, and translate plans back into natural language, demonstrating its effectiveness across diverse planning tasks.",
        "tldr_zh": "本文提出了一个端到端的规划框架，使用LLM将自然语言规范转换为PDDL，生成计划，并将计划翻译回自然语言，并在各种规划任务中展示了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出一个由验证器支撑的端到端规划框架。一个协调器接收用自然语言编写的人类规范，并将其转换成 PDDL（规划领域定义语言）模型，其中领域和问题通过子模块（代理）迭代细化，以解决常见的规划需求，如时间约束和最优性，以及人类规范中可能存在的歧义和矛盾。经过验证的领域和问题随后被传递给一个外部规划引擎以生成规划。协调器和代理由大型语言模型 (LLM) 驱动，在过程的任何阶段都不需要人工干预。最后，一个模块将最终规划翻译回自然语言，以提高人类可读性，同时保持每一步的正确性。我们通过各种领域和任务展示了我们框架的灵活性和有效性，包括 Google NaturalPlan 基准测试和 PlanBench，以及积木世界和汉诺塔等规划问题（众所周知，即使对于小规模实例，LLM 在这些问题上也表现挣扎）。我们的框架可以与任何 PDDL 规划引擎和验证器（例如我们已经测试过的 Fast Downward、LPG、POPF、VAL 和 uVAL）集成，并代表着在 LLM 辅助下实现端到端规划的重要一步。"
    },
    {
        "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
        "summary": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.\n  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.\n  We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
        "url": "http://arxiv.org/abs/2512.09108v1",
        "published_date": "2025-12-09T20:48:45+00:00",
        "updated_date": "2025-12-09T20:48:45+00:00",
        "categories": [
            "cs.SE",
            "cs.AI"
        ],
        "authors": [
            "Paul Brookes",
            "Vardan Voskanyan",
            "Rafail Giavrimis",
            "Matthew Truscott",
            "Mina Ilieva",
            "Chrystalla Pavlou",
            "Alexandru Staicu",
            "Manal Adham",
            "Will Evers- Hood",
            "Jingzhi Gong",
            "Kejia Zhang",
            "Matvey Fedoseev",
            "Vishal Sharma",
            "Roman Bauer",
            "Zheng Wang",
            "Hema Nair",
            "Wei Jie",
            "Tianhua Xu",
            "Aurora Constantin",
            "Leslie Kanthan",
            "Michail Basios"
        ],
        "tldr": "The paper presents ARTEMIS, a no-code evolutionary optimization platform for LLM-based agents, demonstrating significant performance improvements across several benchmark tasks by jointly optimizing agent configurations.",
        "tldr_zh": "该论文介绍了 ARTEMIS，一个用于基于 LLM 的 Agent 的无代码进化优化平台，通过联合优化 Agent 配置，在多个基准测试任务中展示了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于大型语言模型（LLM）构建的主动式AI系统在自动化复杂工作流程方面具有巨大潜力，涵盖从软件开发到客户支持的领域。然而，LLM Agent常常由于次优配置而表现不佳，例如需要数周手动调整的不良提示、工具描述和参数。现有的优化方法要么过于复杂难以通用，要么孤立地处理各个组件，忽略了关键的相互依赖关系。\n\n我们提出了ARTEMIS，一个无需代码的进化优化平台，通过语义感知的遗传算子联合优化Agent配置。仅需一个基准测试脚本和自然语言目标，ARTEMIS即可自动发现可配置组件，从执行日志中提取性能信号，并在无需架构修改的情况下进化配置。\n\n我们在四个代表性Agent系统上评估了ARTEMIS：针对AtCoder Heuristic Contest上的竞技编程任务的\\emph{ALE Agent}，使其接受率提高了\\textbf{$13.6\\%$ }；针对SWE-Perf上的代码优化任务的\\emph{Mini-SWE Agent}，获得了统计显著的\\textbf{10.1\\%性能提升}；以及针对Math Odyssey上的成本和数学推理任务的\\emph{CrewAI Agent}，使其评估所需的Token数量减少了统计显著的\\textbf{$36.9\\%$ }。我们还在GSM8K小学数学问题上评估了由较小的开源模型（Qwen2.5-7B）驱动的\\emph{MathTales-Teacher Agent}，实现了\\textbf{22%的准确率提升}，证明了ARTEMIS可以优化基于商业和本地模型的Agent。"
    },
    {
        "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
        "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
        "url": "http://arxiv.org/abs/2512.08870v1",
        "published_date": "2025-12-09T18:04:41+00:00",
        "updated_date": "2025-12-09T18:04:41+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Xiang Chen",
            "Yuling Shi",
            "Qizhen Lan",
            "Yuchao Qiu",
            "Xiaodong Gu"
        ],
        "tldr": "The paper introduces Fed-SE, a federated learning framework for LLM agents that addresses the challenges of heterogeneous tasks and sparse rewards by using parameter-efficient fine-tuning and low-rank subspace aggregation to improve cross-environment knowledge transfer while preserving privacy.",
        "tldr_zh": "该论文介绍了Fed-SE，一个用于LLM代理的联邦学习框架，通过参数高效的微调和低秩子空间聚合来解决异构任务和稀疏奖励的挑战，从而在保护隐私的同时，提高跨环境的知识迁移。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "LLM智能体已被广泛应用于复杂的交互式任务中，但隐私限制通常会阻碍跨动态环境的集中优化和协同进化。虽然联邦学习(FL)已被证明在静态数据集上有效，但其在智能体的开放式自我进化中的扩展仍未得到充分探索。直接应用标准FL具有挑战性：异构任务和稀疏的轨迹级奖励会引入严重的梯度冲突，从而破坏全局优化过程的稳定性。为了弥合这一差距，我们提出了Fed-SE，一种用于LLM智能体的联邦自我进化框架。Fed-SE建立了一个本地进化-全局聚合范例。在本地，智能体采用参数高效的微调，对过滤后的高回报轨迹进行训练，以实现稳定的梯度更新。在全局，Fed-SE在低秩子空间内聚合更新，从而解耦特定于环境的动态特性，有效减少跨客户端的负迁移。在五个异构环境中的实验表明，Fed-SE将平均任务成功率提高了约18%，超过了联邦基线，验证了其在隐私受限部署中实现稳健的跨环境知识迁移的有效性。"
    },
    {
        "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce",
        "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.",
        "url": "http://arxiv.org/abs/2512.08868v2",
        "published_date": "2025-12-09T18:00:26+00:00",
        "updated_date": "2025-12-11T16:38:57+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Rui Min",
            "Zile Qiao",
            "Ze Xu",
            "Jiawen Zhai",
            "Wenyu Gao",
            "Xuanzhong Chen",
            "Haozhen Sun",
            "Zhen Zhang",
            "Xinyu Wang",
            "Hong Zhou",
            "Wenbiao Yin",
            "Bo Zhang",
            "Xuan Zhou",
            "Ming Yan",
            "Yong Jiang",
            "Haicheng Liu",
            "Liang Ding",
            "Ling Zou",
            "Yi R. Fung",
            "Yalong Li",
            "Pengjun Xie"
        ],
        "tldr": "EcomBench is introduced as a new benchmark for evaluating foundation agents in realistic e-commerce environments, addressing the gap between academic benchmarks and real-world application challenges.",
        "tldr_zh": "EcomBench被提出，这是一个新的基准，用于评估在真实电商环境中基础代理的表现，填补了学术基准和现实应用挑战之间的差距。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基础代理在推理和与真实环境交互方面的能力迅速发展，使得对其核心能力的评估变得越来越重要。虽然已经开发了许多基准测试来评估代理的性能，但大多数集中在学术环境或人为设计的场景中，而忽略了真实应用中出现的挑战。为了解决这个问题，我们专注于一个高度实用的真实世界场景——电子商务领域，该领域涉及大量不同的用户交互、动态的市场条件以及直接与真实决策过程相关的任务。为此，我们推出了 EcomBench，一个旨在评估代理在真实电子商务环境中性能的整体性电子商务基准。EcomBench 构建于嵌入全球领先电子商务生态系统的真实用户需求，并由人类专家精心策划和注释，以确保清晰性、准确性和领域相关性。它涵盖了电子商务场景中的多个任务类别，并定义了三个难度级别，用于评估代理的关键能力，如深度信息检索、多步推理和跨源知识集成。通过将评估建立在真实的电子商务环境中，EcomBench 为衡量代理在现代电子商务中的实际能力提供了一个严格而动态的测试平台。"
    },
    {
        "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models",
        "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.",
        "url": "http://arxiv.org/abs/2512.09841v1",
        "published_date": "2025-12-10T17:22:42+00:00",
        "updated_date": "2025-12-10T17:22:42+00:00",
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yijing Chen",
            "Yihan Wu",
            "Kaisi Guan",
            "Yuchen Ren",
            "Yuyue Wang",
            "Ruihua Song",
            "Liyun Ru"
        ],
        "tldr": "The paper introduces ChronusOmni, an omni large language model that improves temporal awareness in multimodal contexts, using timestamp tokens, reinforcement learning, and a new dataset, ChronusAV, achieving state-of-the-art performance in audiovisual temporal grounding.",
        "tldr_zh": "本文介绍 ChronusOmni，一个全能大型语言模型，通过使用时间戳令牌、强化学习和一个新的数据集 ChronusAV 来提高多模态环境中的时间感知能力，从而在视听时间定位方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "时间感知能力是全能大语言模型的一项基本能力，尤其是在理解长视频和回答复杂问题方面。以往的方法主要针对视觉-语言场景，并侧重于显式的时间定位问题，例如识别视觉事件发生的时间，或确定特定时间发生了什么事件。然而，它们通常对音频模态的利用不足，并且忽略了跨模态的隐式时间定位——例如，识别角色说话时视觉上呈现的内容，或确定视觉事件发生时所说的内容——尽管这种跨模态时间关系在现实场景中普遍存在。本文中, 我们提出了 ChronusOmni，一个旨在增强显式和隐式视听时间定位的时间感知能力的全能大语言模型。首先，我们在每个时间单元中，将基于文本的时间戳标记与视觉和音频表征交织排列，从而实现跨模态的统一时间建模。其次，为了强制执行正确的时间顺序并加强细粒度的时间推理，我们结合了强化学习以及专门设计的奖励函数。此外，我们构建了 ChronusAV，一个时间精确、模态完整且跨模态对齐的数据集，以支持视听时间定位任务的训练和评估。实验结果表明，ChronusOmni 在 ChronusAV 上实现了最先进的性能，提升超过 30%，并在其他时间定位基准测试的大多数指标上取得了最佳结果。这突显了我们的模型在跨模态方面的强大时间感知能力，同时保留了通用的视频和音频理解能力。"
    },
    {
        "title": "View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs",
        "summary": "3D visual grounding (3DVG) identifies objects in 3D scenes from language descriptions. Existing zero-shot approaches leverage 2D vision-language models (VLMs) by converting 3D spatial information (SI) into forms amenable to VLM processing, typically as composite inputs such as specified view renderings or video sequences with overlaid object markers. However, this VLM + SI paradigm yields entangled visual representations that compel the VLM to process entire cluttered cues, making it hard to exploit spatial semantic relationships effectively. In this work, we propose a new VLM x SI paradigm that externalizes the 3D SI into a form enabling the VLM to incrementally retrieve only what it needs during reasoning. We instantiate this paradigm with a novel View-on-Graph (VoG) method, which organizes the scene into a multi-modal, multi-layer scene graph and allows the VLM to operate as an active agent that selectively accesses necessary cues as it traverses the scene. This design offers two intrinsic advantages: (i) by structuring 3D context into a spatially and semantically coherent scene graph rather than confounding the VLM with densely entangled visual inputs, it lowers the VLM's reasoning difficulty; and (ii) by actively exploring and reasoning over the scene graph, it naturally produces transparent, step-by-step traces for interpretable 3DVG. Extensive experiments show that VoG achieves state-of-the-art zero-shot performance, establishing structured scene exploration as a promising strategy for advancing zero-shot 3DVG.",
        "url": "http://arxiv.org/abs/2512.09215v1",
        "published_date": "2025-12-10T00:59:17+00:00",
        "updated_date": "2025-12-10T00:59:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanyuan Liu",
            "Haiyang Mei",
            "Dongyang Zhan",
            "Jiayue Zhao",
            "Dongsheng Zhou",
            "Bo Dong",
            "Xin Yang"
        ],
        "tldr": "The paper introduces View-on-Graph (VoG), a novel zero-shot 3D visual grounding method that leverages vision-language models to reason on scene graphs, improving performance and interpretability by externalizing 3D spatial information.",
        "tldr_zh": "该论文介绍了View-on-Graph (VoG)，一种新颖的零样本3D视觉定位方法，它利用视觉-语言模型在场景图上进行推理，通过外化3D空间信息来提高性能和可解释性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "3D视觉定位（3DVG）旨在根据语言描述识别3D场景中的物体。现有的零样本方法通常利用2D视觉-语言模型（VLMs），通过将3D空间信息（SI）转换为适合VLM处理的形式，通常是作为复合输入，例如指定视角渲染或叠加了物体标记的视频序列。然而，这种VLM + SI范式会产生纠缠的视觉表示，迫使VLM处理整个杂乱的线索，使其难以有效地利用空间语义关系。在这项工作中，我们提出了一种新的VLM x SI范式，该范式将3D SI外部化成一种形式，使VLM能够在推理过程中逐步检索所需的信息。我们通过一种新颖的视图图（VoG）方法来实现这一范式，该方法将场景组织成多模态、多层场景图，并允许VLM作为一个主动代理，在遍历场景时选择性地访问必要的线索。这种设计提供了两个内在优势：（i）通过将3D上下文构建成空间和语义上连贯的场景图，而不是用密集的纠缠视觉输入混淆VLM，它降低了VLM的推理难度；（ii）通过主动探索和推理场景图，它自然地产生了透明的、逐步的追踪，用于可解释的3DVG。大量的实验表明，VoG实现了最先进的零样本性能，证明了结构化场景探索是推进零样本3DVG的一种有前景的策略。"
    },
    {
        "title": "Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters",
        "summary": "Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.",
        "url": "http://arxiv.org/abs/2512.09092v1",
        "published_date": "2025-12-09T20:10:43+00:00",
        "updated_date": "2025-12-09T20:10:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mizanur Rahman Jewel",
            "Mohamed Elmahallawy",
            "Sanjay Madria",
            "Samuel Frimpong"
        ],
        "tldr": "The paper introduces MDSE, a multimodal vision-language framework for generating textual explanations of underground mining disaster scenes, along with the UMD dataset for training and evaluation, achieving state-of-the-art performance in describing obscured environments.",
        "tldr_zh": "该论文介绍了 MDSE，一种用于生成地下矿难场景文本解释的多模态视觉语言框架，并提出了 UMD 数据集用于训练和评估，在描述遮蔽环境方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "摘要：\n地下矿业灾难会产生弥漫的黑暗、灰尘和坍塌，这使得视线模糊，并使人类和传统系统难以进行情境感知。为了解决这个问题，我们提出MDSE，即多模态灾难情境解释器，一种新型视觉-语言框架，可以自动生成灾后地下场景的详细文本解释。MDSE具有三重创新：（i）上下文感知交叉注意力，即使在严重退化情况下也能实现视觉和文本特征的稳健对齐；（ii）分割感知双通道视觉编码，融合全局和区域特定嵌入；以及（iii）资源高效的基于Transformer的语言模型，以最小的计算成本生成富有表现力的字幕。为了支持这项任务，我们提出了地下矿难（UMD）数据集——首个真实地下灾难场景的图像-字幕语料库——从而能够进行严格的训练和评估。在UMD和相关基准测试中的大量实验表明，MDSE显著优于最先进的字幕生成模型，产生更准确和情境相关的描述，捕捉到模糊环境中的关键细节，从而提高地下应急响应的情境感知能力。代码位于https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer。"
    },
    {
        "title": "Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning",
        "summary": "The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA",
        "url": "http://arxiv.org/abs/2512.09706v1",
        "published_date": "2025-12-10T14:52:29+00:00",
        "updated_date": "2025-12-10T14:52:29+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Kaichen He",
            "Zihao Wang",
            "Muyao Li",
            "Anji Liu",
            "Yitao Liang"
        ],
        "tldr": "The paper introduces CrossAgent, a reinforcement learning model that learns to dynamically switch between heterogeneous action spaces for enhanced adaptability and performance in complex, open-world environments like Minecraft, surpassing fixed-action baselines.",
        "tldr_zh": "该论文介绍了 CrossAgent，一种强化学习模型，它能够动态地在不同的动作空间之间切换，从而增强在像 Minecraft 这样复杂的开放世界环境中的适应性和性能，超越了固定动作基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自主人工智能的范式正在从工程化的复杂工作流转向后训练的原生模型。然而，现有的智能体通常局限于静态的、预定义的动作空间——例如，仅仅使用API、GUI事件或机器人命令。这种刚性限制了它们在动态环境中的适应性，在这些环境中，最佳交互粒度会根据上下文变化。为了弥合这一差距，我们提出了CrossAgent，一个统一的自主体模型，能够掌握异构动作空间，并自主选择每个轨迹步骤的最有效界面。我们引入了一个综合的训练流程，该流程将冷启动监督微调与多轮组相对策略优化（GRPO）算法相结合。这种方法使智能体能够学习自适应的动作切换——在高级效率和低级精度之间取得平衡——而无需人工指定的规则。在开放世界的Minecraft环境中进行的超过800项任务的大量实验表明，CrossAgent达到了最先进的性能。通过动态地利用不同动作空间的优势，我们的模型显著优于固定动作的基线，在长时程推理中表现出卓越的泛化能力和效率。所有代码和模型均可在https://github.com/CraftJarvis/OpenHA获得。"
    },
    {
        "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat",
        "summary": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.",
        "url": "http://arxiv.org/abs/2512.09656v1",
        "published_date": "2025-12-10T13:52:08+00:00",
        "updated_date": "2025-12-10T13:52:08+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Nicolas Marticorena",
            "Tobias Fischer",
            "Niko Suenderhauf"
        ],
        "tldr": "The paper presents ReMoSPLAT, a reactive controller for mobile manipulators that uses Gaussian Splat representations for efficient collision avoidance in cluttered environments, achieving performance comparable to controllers using ground truth information.",
        "tldr_zh": "该论文提出了一种名为 ReMoSPLAT 的移动操作臂反应式控制器，该控制器利用高斯溅射表示来实现有效的避障， 在复杂环境中实现了与使用真实信息控制器的相当性能。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "反应式控制能够优雅地协调移动机械臂底座和手臂的运动。然而，如何在不涉及昂贵规划的情况下，融入对环境的精确表示以避开障碍物仍然是一个挑战。在这项工作中，我们提出了ReMoSPLAT，一种基于二次规划公式的移动操作反应式控制器，其利用高斯Splat表示来实现碰撞避免。通过将额外的约束和成本整合到优化公式中，移动机械臂平台可以到达其预期的末端执行器位姿，同时避开障碍物，即使是在拥挤的场景中。我们研究了两种有效计算机器人-障碍物距离方法的权衡，比较了一种纯几何方法和一种基于栅格化的方法。我们在合成扫描和真实世界扫描的模拟实验中验证了我们方法的可行性，表明所提出的方法实现了与依赖于完美真实信息的控制器相当的性能。"
    },
    {
        "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer",
        "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.",
        "url": "http://arxiv.org/abs/2512.09431v1",
        "published_date": "2025-12-10T08:58:37+00:00",
        "updated_date": "2025-12-10T08:58:37+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Quanyou Wang",
            "Mingzhang Zhu",
            "Ruochen Hou",
            "Kay Gillespie",
            "Alvin Zhu",
            "Shiqi Wang",
            "Yicheng Wang",
            "Gaberiel I. Fernandez",
            "Yeting Liu",
            "Colin Togashi",
            "Hyunwoo Nam",
            "Aditya Navghare",
            "Alex Xu",
            "Taoyuanmin Zhu",
            "Min Sung Ahn",
            "Arturo Flores Alvarez",
            "Justin Quan",
            "Ethan Hong",
            "Dennis W. Hong"
        ],
        "tldr": "This paper presents the hardware and software innovations that enabled the ARTEMIS team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition, highlighting advancements in humanoid robot design, perception, navigation, and high-level decision-making.",
        "tldr_zh": "本文介绍了使ARTEMIS团队在RoboCup 2024成人尺寸人形机器人足球比赛中获胜的软硬件创新，重点介绍了人形机器人设计、感知、导航和高级决策方面的进展。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "运动型人形机器人的发展受到了广泛关注，这得益于驱动、传感和控制技术的进步，从而实现了日益动态的、真实世界的能力。RoboCup，一项完全自主的人形机器人国际竞赛，为这些系统提供了一个独特的、具有挑战性的基准，其最终目标是在2050年与人类足球运动员进行比赛。本文介绍了我们团队在RoboCup 2024成人组人形足球比赛中获胜的硬件和软件创新。在硬件方面，我们介绍了一个成人尺寸的人形机器人平台，该平台采用轻量化结构部件、高扭矩准直接驱动执行器和专门的足部设计，从而能够在步内进行强力踢球，同时保持运动的鲁棒性。在软件方面，我们开发了一个集成的感知和定位框架，该框架结合了立体视觉、目标检测和基于地标的融合，以提供对球、球门、队友和对手的可靠估计。一个中级导航栈随后生成避碰的、动态可行的轨迹，而一个集中的行为管理器基于不断变化的游戏状态协调高级决策、角色选择和踢球执行。这些子系统的无缝集成带来了快速、精确和战术有效的游戏体验，从而在真实比赛的动态和对抗性条件下实现了强大的性能。本文介绍了促成ARTEMIS作为2024年成人组人形足球冠军的设计原则、系统架构和实验结果。"
    },
    {
        "title": "UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects",
        "summary": "Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.",
        "url": "http://arxiv.org/abs/2512.09283v1",
        "published_date": "2025-12-10T03:14:12+00:00",
        "updated_date": "2025-12-10T03:14:12+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Fan Wu",
            "Chenguang Yang",
            "Haibin Yang",
            "Shuo Wang",
            "Yanrui Xu",
            "Xing Zhou",
            "Meng Gao",
            "Yaoqi Xian",
            "Zhihong Zhu",
            "Shifeng Huang"
        ],
        "tldr": "The paper introduces UPETrack, a novel geometry-driven framework for real-time tracking of Deformable Linear Objects (DLOs) that addresses the challenges of occlusion and high dimensionality without relying on physical modeling or visual markers, achieving better accuracy and efficiency than state-of-the-art methods.",
        "tldr_zh": "该论文介绍了一种名为UPETrack的新型几何驱动框架，用于实时跟踪可变形线性物体 (DLO)，该框架解决了遮挡和高维度的挑战，无需依赖物理建模或视觉标记，并且比最先进的方法实现了更高的精度和效率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "可变形线性物体（DLO）的实时状态跟踪对于实现DLO在工业装配、医疗程序和日常生活应用中的机器人操作至关重要。然而，高维配置空间、非线性动力学和频繁的局部遮挡对稳健的实时DLO跟踪提出了根本性的障碍。为了解决这些限制，本研究引入了UPETrack，一个基于单向位置估计（UPE）的几何驱动框架，该框架能够在无需物理建模、虚拟仿真或视觉标记的情况下进行跟踪。该框架分两个阶段运行：（1）可见片段跟踪基于通过期望最大化（EM）算法拟合的高斯混合模型（GMM），以及（2）采用我们提出的UPE算法进行遮挡区域预测。UPE利用DLO形状固有的几何连续性及其时间演化模式，通过三个主要机制推导出一个闭合形式的位置估计器：（i）局部线性组合位移项，（ii）近端线性约束项，以及（iii）历史曲率项。这种解析公式允许通过几何组件的显式线性组合有效地且稳定地估计遮挡节点，从而无需额外的迭代优化。实验结果表明，UPETrack在定位精度和计算效率方面均优于两种最先进的跟踪算法，包括TrackDLO和CDCPD2。"
    },
    {
        "title": "Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous",
        "summary": "Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions. This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.",
        "url": "http://arxiv.org/abs/2512.09111v2",
        "published_date": "2025-12-09T20:53:16+00:00",
        "updated_date": "2025-12-11T04:52:52+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "math.OC"
        ],
        "authors": [
            "Yuji Takubo",
            "Arpit Dwivedi",
            "Sukeerth Ramkumar",
            "Luis A. Pabon",
            "Daniele Gammelli",
            "Marco Pavone",
            "Simone D'Amico"
        ],
        "tldr": "The paper introduces SAGES, a natural-language-to-trajectory framework for spacecraft rendezvous, enabling intuitive guidance through natural language commands while respecting nonconvex constraints.",
        "tldr_zh": "该论文介绍了一种名为SAGES的航天器交会对接框架，该框架可将自然语言命令转换为轨迹，从而通过自然语言命令进行直观引导，同时遵守非凸约束。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "可靠的实时轨迹生成对于未来的自主航天器至关重要。尽管非凸导航和控制领域的最新进展为星上自主轨迹优化铺平了道路，但这些方法仍然依赖于大量的专家输入（例如，航路点、约束、任务时间线等），这限制了真实交会任务中的操作可扩展性。本文介绍了一种轨迹生成框架SAGES（空间语义自主制导引擎），它可以将自然语言命令转换为符合非凸约束并反映高层意图的航天器轨迹。在两种场景下的实验——具有连续时间约束实施的容错近距离操作和一个自由漂浮的机器人平台——表明SAGES能够可靠地生成与人类命令对齐的轨迹，在不同的行为模式下实现了超过90%的语义-行为一致性。最终，这项工作标志着语言条件下的、具有约束意识的航天器轨迹生成的初步进展，使操作人员能够通过直观的自然语言命令交互式地引导安全性和行为，从而减少专家的负担。"
    },
    {
        "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors",
        "summary": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.",
        "url": "http://arxiv.org/abs/2512.09065v1",
        "published_date": "2025-12-09T19:33:19+00:00",
        "updated_date": "2025-12-09T19:33:19+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Shivendra Agrawal",
            "Jake Brawer",
            "Ashutosh Naik",
            "Alessandro Roncone",
            "Bradley Hayes"
        ],
        "tldr": "ShelfAware introduces a novel semantic particle filter for robust global localization in quasi-static indoor environments, achieving high success rates and low RMSE with low-cost sensors.",
        "tldr_zh": "ShelfAware引入了一种新颖的语义粒子滤波器，用于在准静态室内环境中实现鲁棒的全局定位，使用低成本传感器实现了高成功率和低RMSE。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "许多室内工作空间是准静态的：整体布局稳定，但局部语义不断变化，产生重复的几何结构、动态杂波和感知噪声，这些都阻碍了基于视觉的定位。我们提出了ShelfAware，一种用于稳健全局定位的语义粒子滤波器，它将场景语义视为物体类别的统计证据，而不是固定的地标。ShelfAware融合了深度似然性与以类别为中心的语义相似性，并使用预先计算的语义视点库在MCL内部执行逆语义提议，从而在低成本、纯视觉硬件上快速、有针对性地生成假设。在一个语义密集的零售环境中，跨越四种条件（小车安装、可穿戴、动态障碍物和稀疏语义）的100次全局定位试验中，ShelfAware的成功率为96%（相比之下，MCL为22%，AMCL为10%），平均收敛时间为1.91秒，在所有条件下都实现了最低的平移均方根误差，并在80%的测试序列中保持了稳定的跟踪，所有这些都在消费级笔记本电脑平台上实时运行。通过在类别层面对语义进行分布建模并利用逆提议，ShelfAware解决了准静态领域中常见的几何混叠和语义漂移问题。由于该方法仅需要视觉传感器和视觉惯性里程计（VIO），因此它可以集成作为仓库、实验室和零售环境中移动机器人的免基础设施构建块；作为一个代表性的应用，它还支持创建辅助设备，为视力障碍人士提供随时启动的、共享控制的辅助导航。"
    },
    {
        "title": "Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing",
        "summary": "The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.",
        "url": "http://arxiv.org/abs/2512.09463v1",
        "published_date": "2025-12-10T09:33:03+00:00",
        "updated_date": "2025-12-10T09:33:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sander De Coninck",
            "Emilio Gamba",
            "Bart Van Doninck",
            "Abdellatif Bey-Temsamani",
            "Sam Leroux",
            "Pieter Simoens"
        ],
        "tldr": "This paper validates a privacy-preserving computer vision framework in real-world industrial settings, showcasing its effectiveness in balancing utility and privacy across three human-centric manufacturing use cases. The framework uses learned visual transformations to obscure sensitive information while preserving task performance.",
        "tldr_zh": "本文在真实工业环境中验证了一种保护隐私的计算机视觉框架，展示了其在三个人本制造用例中平衡效用和隐私方面的有效性。该框架使用学习到的视觉变换来模糊敏感信息，同时保留任务性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "在工业领域采用人工智能驱动的计算机视觉技术往往受到限制，需要在运营效用和工人隐私之间取得平衡。本文建立在我们先前提出的隐私保护框架之上，首次对其在活跃生产环境中由工业合作伙伴直接收集的真实数据进行了全面的验证。我们评估了该框架在三个代表性用例中的表现：木工生产监控、人感知的AGV导航以及多摄像头的人体工学风险评估。该方法采用学习到的视觉变换，模糊敏感或与任务无关的信息，同时保留对任务执行至关重要的特征。通过对隐私-效用权衡的定量评估以及来自工业合作伙伴的定性反馈，我们评估了该框架的有效性、部署可行性以及信任影响。结果表明，针对特定任务的混淆处理能够以降低的隐私风险实现有效的监控，从而确立了该框架在现实世界中应用的准备度，并为工业领域负责任的、以人为本的AI部署提供了跨领域建议。"
    },
    {
        "title": "Architectures for Building Agentic AI",
        "summary": "This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.",
        "url": "http://arxiv.org/abs/2512.09458v1",
        "published_date": "2025-12-10T09:28:40+00:00",
        "updated_date": "2025-12-10T09:28:40+00:00",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Sławomir Nowaczyk"
        ],
        "tldr": "The paper argues for an architectural approach to building reliable agentic AI systems, proposing a taxonomy and design guidance for various agent types with a focus on components, interfaces, and control loops.",
        "tldr_zh": "本文认为可靠的智能体AI系统的构建主要依赖于架构设计，提出了一种架构方法，包括智能体类型的分类和设计指导，重点在于组件、接口和控制环路。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本章论证了自主和生成式人工智能的可靠性主要是一种架构属性。我们将自主系统定义为在闭环中运行的、以目标为导向、使用工具的决策者，并展示了可靠性如何从原则性的组件化（目标管理器、规划器、工具路由、执行器、记忆、验证器、安全监控器、遥测）、规范的接口（模式约束、验证的、最小特权工具调用）以及显式的控制和保障循环中涌现。在经典基础之上，我们提出了一种实用的分类法：工具使用型自主体、记忆增强型自主体、规划和自我改进型自主体、多自主体系统以及具身或网络自主体，并分析了每种模式如何重塑可靠性范围和失效模式。我们提炼了关于类型化模式、幂等性、权限管理、事务性语义、记忆来源和卫生、运行时治理（预算、终止条件）以及行动前模拟安全措施的设计指导。"
    },
    {
        "title": "GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection",
        "summary": "Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.",
        "url": "http://arxiv.org/abs/2512.09396v1",
        "published_date": "2025-12-10T07:40:23+00:00",
        "updated_date": "2025-12-10T07:40:23+00:00",
        "categories": [
            "cs.MA",
            "cs.AI"
        ],
        "authors": [
            "Zishu Wei",
            "Qixiang Ma",
            "Xavier Hu",
            "Yuhang Liu",
            "Hui Zang",
            "Yudong Zhao",
            "Tao Wang",
            "Shengyu Zhang",
            "Fei Wu"
        ],
        "tldr": "The paper introduces GAIR, a GUI automation framework that leverages a general-purpose MLLM for information-joint reasoning and a group reflection mechanism to combine the strengths of multiple GUI-specific MLLMs, improving performance on GUI automation tasks.",
        "tldr_zh": "这篇论文介绍了GAIR，一个GUI自动化框架，它利用通用MLLM进行信息联合推理，并采用群体反思机制来结合多个GUI特定MLLM的优势，从而提高GUI自动化任务的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "为GUI自动化任务构建AI系统吸引了显著的研究投入，其中多模态大型语言模型 (MLLM) 被用于处理用户需求并给出操作指令。然而，GUI自动化涵盖了广泛的任务，从文档处理到在线购物，从CAD到视频编辑。特定任务之间的多样性要求用于GUI自动化的MLLM具备异构能力并精通多维专业知识，这给构建此类模型带来了难题。为应对这一挑战，我们提出了GAIR：基于信息联合推理和群体反思的GUI自动化，一种新颖的基于MLLM的GUI自动化代理框架，旨在整合知识并结合来自异构模型的能力，以构建具有更高性能的GUI自动化代理系统。由于不同的GUI特定MLLM在不同的数据集上训练，因此具有不同的优势，GAIR引入了一种通用MLLM，用于联合处理来自多个GUI特定模型的信息，进一步增强了代理框架的性能。该通用MLLM还充当决策者，尝试基于先前搜集的信息执行合理的操作。当通用模型认为没有足够的信息用于合理的决策时，GAIR将进入群体反思状态，通用模型将根据GUI特定模型的优势和劣势，为其提供不同的指令和提示，从而驱动它们搜集具有更重要性和准确性的信息，以支持更深入的推理和决策。我们通过在GUI基准测试上进行的大量实验验证了GAIR的有效性和可靠性。"
    },
    {
        "title": "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities",
        "summary": "Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.",
        "url": "http://arxiv.org/abs/2512.09847v1",
        "published_date": "2025-12-10T17:31:44+00:00",
        "updated_date": "2025-12-10T17:31:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijia Feng",
            "Michael Wray",
            "Walterio Mayol-Cuevas"
        ],
        "tldr": "This paper presents a system for online detection and anticipation of human struggle during tasks and activities, adapting existing models for real-time assistive applications and demonstrating generalization across different scenarios.",
        "tldr_zh": "本文提出了一种在线检测和预测人类在任务和活动中挣扎行为的系统，通过调整现有模型以适应实时辅助应用，并展示了在不同场景中的泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "理解人类技能表现对于智能辅助系统至关重要，其中，识别困难为识别用户难点提供了一种自然的线索。虽然先前的研究侧重于离线困难分类和定位，但实时应用需要能够在线检测和预测困难的模型。我们将困难定位重新定义为在线检测任务，并将其扩展为预测，即在困难发生之前预测困难时刻。我们改编了两种现成的模型作为在线困难检测和预测的基线。在线困难检测实现了70-80%的逐帧mAP，而提前2秒的困难预测也获得了相当的性能，只有轻微的下降。我们进一步研究了跨任务和活动的泛化能力，并分析了技能演进的影响。尽管在活动层面的泛化中存在更大的领域差距，但模型仍然优于随机基线4-20%。我们的基于特征的模型运行速度高达143 FPS，包括特征提取在内的整个流程的运行速度约为20 FPS，足以满足实时辅助应用的需求。"
    },
    {
        "title": "FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation",
        "summary": "Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of \"apparent rotation\", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.",
        "url": "http://arxiv.org/abs/2512.09792v1",
        "published_date": "2025-12-10T16:11:00+00:00",
        "updated_date": "2025-12-10T16:11:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pierre Ancey",
            "Andrew Price",
            "Saqib Javed",
            "Mathieu Salzmann"
        ],
        "tldr": "FastPose-ViT introduces a Vision Transformer-based approach for real-time spacecraft pose estimation, demonstrating competitive accuracy and efficiency on edge devices, enabling autonomous space operations.",
        "tldr_zh": "FastPose-ViT 提出了一种基于视觉Transformer的实时航天器姿态估计方法，在边缘设备上展示了有竞争力的精度和效率，从而实现了自主空间操作。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "从单张图像估计航天器的六自由度(6DoF)姿态对于在轨服务和空间碎片移除等自主操作至关重要。现有的最先进方法通常依赖于迭代的基于透视-n-点(PnP)的算法，这些算法计算量大，不适合在资源受限的边缘设备上进行实时部署。为了克服这些限制，我们提出了一种基于视觉Transformer (ViT)的架构FastPose-ViT，该架构直接回归6DoF姿态。 我们的方法处理来自对象边界框裁剪的图像，并引入一种新的数学形式主义，将这些局部预测映射回完整的图像尺度。这种形式主义源于射影几何的原理和“表观旋转”的概念，其中模型预测一个表观旋转矩阵，然后对其进行校正以找到真实的姿态。我们证明了我们的方法优于其他非PnP策略，并在SPEED数据集上实现了与最先进的PnP技术具有竞争力的性能。 此外，我们通过量化模型并将其部署在功耗受限的边缘硬件上来验证我们模型对实际空间任务的适用性。 在NVIDIA Jetson Orin Nano上，我们的端到端pipeline在顺序执行下实现了约75毫秒/帧的延迟，并且当各个阶段并发调度时，实现了高达33 FPS的非阻塞吞吐量。"
    },
    {
        "title": "An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence",
        "summary": "The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.",
        "url": "http://arxiv.org/abs/2512.09670v1",
        "published_date": "2025-12-10T14:14:08+00:00",
        "updated_date": "2025-12-10T14:14:08+00:00",
        "categories": [
            "cs.CV",
            "eess.SY"
        ],
        "authors": [
            "Gil Weissman",
            "Amir Ivry",
            "Israel Cohen"
        ],
        "tldr": "This paper introduces an automated tip-and-cue framework for satellite imaging tasking and scheduling, leveraging AI models for image processing and generating visual reports, demonstrated on a maritime vessel tracking scenario.",
        "tldr_zh": "该论文介绍了一个用于卫星成像任务和调度的自动化Tip-and-Cue框架，利用人工智能模型进行图像处理并生成可视化报告，并通过海上船舶跟踪场景进行了演示。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "卫星星座的激增，以及任务延迟的减少和多样化的传感器能力，扩展了自动化地球观测的机会。本文介绍了一个全自动化“Tip-and-Cue”（提示与响应）框架，专为卫星成像任务规划和调度而设计。在该框架中，“提示”由外部数据源或先前卫星图像的分析结果生成，用于识别时空目标并对其进行优先级排序，以供后续规划。“响应”则是对应的成像任务，其中包括传感器约束、时间要求和效用函数。该系统自主生成候选任务，利用连续效用函数（反映每次观测的预期价值）优化多个卫星的任务调度，并使用基于人工智能的模型处理生成的图像，包括目标检测器和视觉-语言模型。生成结构化的视觉报告，以支持可解释性并识别下游任务的新见解。通过一个海上船舶跟踪场景，展示了该框架的有效性，该场景利用船舶自动识别系统（AIS）数据进行轨迹预测、有针对性的观测以及生成可操作的输出。海上船舶跟踪是一个广泛研究的应用，通常用于评估卫星任务规划、预测和分析的新方法。该系统可以扩展到更广泛的应用，如智慧城市监测和灾害响应，在这些应用中，及时的任务规划和自动化分析至关重要。"
    },
    {
        "title": "VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification",
        "summary": "Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.",
        "url": "http://arxiv.org/abs/2512.09646v1",
        "published_date": "2025-12-10T13:40:24+00:00",
        "updated_date": "2025-12-10T13:40:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wanyue Zhang",
            "Lin Geng Foo",
            "Thabo Beeler",
            "Rishabh Dabral",
            "Christian Theobalt"
        ],
        "tldr": "The paper introduces VHOI, a novel two-stage framework for controllable video generation of human-object interactions from sparse trajectories, using a HOI-aware motion representation for improved realism.",
        "tldr_zh": "该论文介绍了VHOI，一种新颖的两阶段框架，用于从稀疏轨迹生成可控的人-物交互视频，并使用HOI-aware运动表示来提高真实感。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "视频中合成逼真的人与物交互（HOI）极具挑战性，这源于人类和物体之间复杂的、特定于实例的交互动态。在视频生成中融入可控性进一步增加了复杂性。现有的可控视频生成方法面临着权衡：诸如关键点轨迹之类的稀疏控制易于指定，但缺乏实例感知；而光流、深度或3D网格等稠密信号信息丰富，但获取成本高昂。我们提出了VHOI，一个两阶段框架，首先将稀疏轨迹稠密化为HOI掩码序列，然后以这些稠密掩码为条件微调视频扩散模型。我们引入了一种新颖的HOI感知运动表示，该表示使用颜色编码来区分不仅是人类和物体的运动，还有特定于身体部位的动态。这种设计将人类先验知识融入到条件信号中，并增强了模型理解和生成逼真HOI动态的能力。实验表明，在可控HOI视频生成方面取得了最先进的结果。VHOI不局限于仅交互场景，还可以端到端地生成完整的人类导航，直至与物体交互。项目主页：https://vcai.mpi-inf.mpg.de/projects/vhoi/。"
    },
    {
        "title": "FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds",
        "summary": "Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.",
        "url": "http://arxiv.org/abs/2512.09423v1",
        "published_date": "2025-12-10T08:46:53+00:00",
        "updated_date": "2025-12-10T08:46:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marco Pegoraro",
            "Evan Atherton",
            "Bruno Roy",
            "Aliasghar Khani",
            "Arianna Rampini"
        ],
        "tldr": "FunPhase introduces a functional periodic autoencoder for motion generation, representing motion in phase manifolds to achieve smooth, scalable, and generalizable motion prediction and generation capabilities across various downstream tasks and datasets.",
        "tldr_zh": "FunPhase 提出了一种功能性周期自编码器，用于运动生成。它将运动表示在相位流形中，从而实现平滑、可扩展和可泛化的运动预测和生成，适用于各种下游任务和数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "由于空间几何与时间动态之间的强耦合，学习自然人体运动仍然具有挑战性。将运动嵌入到相位流形（一种捕捉局部周期性的潜在空间）中，已被证明对运动预测有效；然而，现有方法缺乏可扩展性，并且仍然局限于特定场景。我们引入了 FunPhase，一种函数式周期自编码器，它学习运动的相位流形，并用函数空间公式代替离散时间解码，从而实现可以在任意时间分辨率下采样的平滑轨迹。FunPhase 支持诸如下采样和部分身体运动补全等下游任务，可推广到不同骨骼和数据集，并在单个可解释流形中统一了运动预测和生成。我们的模型比以往的周期自编码器基线实现了大大降低的重建误差，同时支持更广泛的应用，并且性能与最先进的运动生成方法相当。"
    },
    {
        "title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
        "summary": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.",
        "url": "http://arxiv.org/abs/2512.09112v1",
        "published_date": "2025-12-09T20:54:35+00:00",
        "updated_date": "2025-12-09T20:54:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Frédéric Fortier-Chouinard",
            "Yannick Hold-Geoffroy",
            "Valentin Deschaintre",
            "Matheus Gadelha",
            "Jean-François Lalonde"
        ],
        "tldr": "GimbalDiffusion introduces a novel framework for gravity-aware camera control in text-to-video generation, enabling precise and interpretable camera trajectory manipulation in a physically grounded coordinate system.",
        "tldr_zh": "GimbalDiffusion 引入了一个新的框架，用于在文本到视频生成中实现重力感知的相机控制，从而在物理坐标系中实现精确且可解释的相机轨迹操作。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "文本到视频生成领域的最新进展已经实现了显著的逼真度，但对相机移动和朝向的精细控制仍然难以实现。现有的方法通常通过相对或模糊的表示来编码相机轨迹，限制了显式的几何控制。我们引入了GimbalDiffusion，一个框架，它能够实现基于物理世界坐标的相机控制，并使用重力作为全局参考。我们的方法不是描述相对于前一帧的运动，而是在绝对坐标系中定义相机轨迹，从而允许精确且可解释地控制相机参数，而无需初始参考帧。我们利用全景360度视频来构建各种各样的相机轨迹，远超传统视频数据中常见的以直线、前向为主的轨迹。为了进一步增强相机引导，我们引入了零俯仰角条件化，一种注释策略，可以减少模型在文本内容与相机规格冲突时对文本内容的依赖（例如，当相机指向天空时生成草地）。最后，我们通过重新平衡SpatialVID-HQ来建立一个摄像机感知视频生成的基准，以便在广泛的相机俯仰角变化下进行全面评估。总而言之，这些贡献提高了文本到视频模型的可控性和鲁棒性，从而能够在生成框架内实现精确的、重力对齐的相机操作。"
    },
    {
        "title": "Knowledge Diversion for Efficient Morphology Control and Policy Transfer",
        "summary": "Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \\textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \\textit{learngenes} and morphology- and task-specific \\textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\\times$ reduction in model size for single-agent deployment.",
        "url": "http://arxiv.org/abs/2512.09796v1",
        "published_date": "2025-12-10T16:11:51+00:00",
        "updated_date": "2025-12-10T16:11:51+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Fu Feng",
            "Ruixiao Shi",
            "Yucheng Xie",
            "Jianlu Shen",
            "Jing Wang",
            "Xin Geng"
        ],
        "tldr": "The paper introduces DivMorph, a modular training paradigm for universal morphology control that uses knowledge diversion to learn decomposable Transformer-based controllers, achieving improved sample efficiency and model size reduction.",
        "tldr_zh": "本文介绍了一种用于通用形态控制的模块化训练范式 DivMorph，该范式利用知识转移来学习可分解的基于 Transformer 的控制器，从而提高了样本效率并缩小了模型尺寸。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "通用形态控制旨在学习一种能够泛化于异构智能体形态的通用策略，而基于Transformer的控制器正成为一种流行的选择。然而，这种架构会带来巨大的计算成本，导致较高的部署开销。现有方法在跨任务泛化能力上存在局限性，需要为每个新任务从头开始训练。为此，我们提出了\\textbf{DivMorph}，一种模块化训练范式，它利用知识转移来学习可分解的控制器。DivMorph在训练前通过奇异值分解（SVD）将随机初始化的Transformer权重分解成因子单元，并采用动态软门控，以便基于任务和形态嵌入来调节这些单元，从而将它们分离成共享的\\textit{learngenes}和形态与任务特定的\\textit{tailors}，进而实现知识解耦。通过选择性地激活相关组件，DivMorph实现了可扩展且高效的策略部署，同时支持将策略有效迁移到新的任务。大量实验表明，DivMorph实现了最先进的性能，在跨任务迁移的样本效率方面比直接微调提高了3倍，在单个智能体部署的模型尺寸方面减少了17倍。"
    },
    {
        "title": "CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning",
        "summary": "Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.",
        "url": "http://arxiv.org/abs/2512.09368v1",
        "published_date": "2025-12-10T06:59:46+00:00",
        "updated_date": "2025-12-10T06:59:46+00:00",
        "categories": [
            "cs.LG",
            "stat.ME"
        ],
        "authors": [
            "Mingyuan Li",
            "Chunyu Liu",
            "Zhuojun Li",
            "Xiao Liu",
            "Guangsheng Yu",
            "Bo Du",
            "Jun Shen",
            "Qiang Wu"
        ],
        "tldr": "The paper introduces CFLight, a counterfactual learning framework for enhancing safety in Traffic Signal Control (TSC) by minimizing collisions using RL, achieving improvements over existing RL methods and safe RL baselines. The code is available.",
        "tldr_zh": "该论文介绍了一种名为CFLight的基于反事实学习的交通信号控制（TSC）框架，通过使用强化学习来最小化碰撞，从而提高安全性，并在现有强化学习方法和安全强化学习基线之上实现了改进。代码已开源。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "交通事故导致全球每年造成数百万人员伤亡，其中相当一部分发生在十字路口。交通信号控制（TSC）是提高这些城市路口安全性的有效策略。尽管强化学习（RL）方法在优化TSC方面越来越受欢迎，但这些方法通常优先考虑驾驶效率而非安全性，因此未能解决这两个方面之间的关键平衡。此外，这些方法通常缺乏可解释性。反事实（CF）学习是各种因果分析领域中一种很有前景的方法。在本研究中，我们引入了一种新颖的框架，以改进RL在TSC中的安全性。该框架引入了一种基于CF学习的新方法来解决以下问题：“如果当不安全事件发生时，我们回溯以执行替代行动，那么在随后的时间段内是否仍会发生此不安全事件？”为了回答这个问题，我们提出了一种新的结构因果模型来预测执行不同行动后的结果，并提出了一个新的CF模块，该模块与额外的“X”模块集成，以促进安全的RL实践。我们的新算法CFLight，源自该框架，有效地解决了具有挑战性的安全事件，并通过近零碰撞控制策略显著提高了路口的安全性。通过在真实世界和合成数据集上进行的大量数值实验，我们证明了与传统的RL方法和最新的安全RL模型相比，CFLight减少了碰撞并提高了整体交通性能。此外，我们的方法代表了一种通用的安全RL方法框架，为在其他领域的应用开辟了可能性。数据和代码可在github上获取：https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning."
    },
    {
        "title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization",
        "summary": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.",
        "url": "http://arxiv.org/abs/2512.09608v1",
        "published_date": "2025-12-10T12:55:05+00:00",
        "updated_date": "2025-12-10T12:55:05+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhiheng Li",
            "Weihua Wang",
            "Qiang Shen",
            "Yichen Zhao",
            "Zheng Fang"
        ],
        "tldr": "The paper introduces Super4DR, a novel 4D radar-centric SLAM framework using self-supervised learning for odometry and Gaussian-based map optimization, demonstrating improved performance in challenging environments.",
        "tldr_zh": "该论文介绍了 Super4DR，一个新颖的以 4D 雷达为中心的 SLAM 框架，使用自监督学习进行里程计和基于高斯的地图优化，展示了在具有挑战性的环境中的改进性能。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "基于学习的4D雷达里程计和高斯优化框架Super4DR：\n\n传统的利用视觉或激光雷达数据的SLAM系统在光照不佳和恶劣天气下通常表现不佳。虽然4D雷达适用于此类环境，但其稀疏且充满噪声的点云阻碍了精确的里程计估计，同时雷达地图也存在结构模糊和不完整的问题。因此，我们提出了Super4DR，一个以4D雷达为中心的学习型里程计估计和基于高斯的地图优化框架。首先，我们设计了一个聚类感知的里程计网络，该网络结合了来自聚类雷达点的对象级线索用于帧间匹配，并结合了分层自监督机制，通过时空一致性、知识迁移和特征对比来克服异常值。其次，我们提出使用3D高斯作为一种中间表示，并结合雷达特定的增长策略、选择性分离和多视角正则化，以恢复模糊的地图区域以及基于图像纹理未检测到的区域。实验表明，Super4DR比先前的自监督方法性能提升了67%，几乎与监督里程计相当，缩小了与激光雷达的地图质量差距，同时实现了多模态图像渲染。"
    },
    {
        "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM",
        "summary": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.",
        "url": "http://arxiv.org/abs/2512.09411v1",
        "published_date": "2025-12-10T08:12:29+00:00",
        "updated_date": "2025-12-10T08:12:29+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Siting Zhu",
            "Yuxiang Huang",
            "Wenhua Wu",
            "Chaokang Jiang",
            "Yongbo Chen",
            "I-Ming Chen",
            "Hesheng Wang"
        ],
        "tldr": "The paper introduces D$^2$GSLAM, a novel dense SLAM system that utilizes Gaussian representation to simultaneously perform dynamic reconstruction and robust tracking in dynamic environments by integrating static 3D Gaussians with dynamic 4D Gaussians.",
        "tldr_zh": "该论文介绍了D$^2$GSLAM，一种新颖的密集SLAM系统，它利用高斯表示来同时执行动态重建和在动态环境中的鲁棒跟踪，通过整合静态3D高斯和动态4D高斯。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "近来，稠密同步定位与建图（SLAM）在静态环境中表现出卓越的性能。然而，动态环境下的稠密SLAM仍然面临挑战。大多数方法直接移除动态物体，仅关注静态场景重建，忽略了这些动态物体中包含的运动信息。本文提出了一种新的动态SLAM系统D$^2$GSLAM，该系统利用高斯表示，能够同时在动态环境中进行精确的动态重建和鲁棒的跟踪。我们的系统由四个关键组成部分构成：（i）我们提出了一种基于几何提示的动态分离方法，用于区分场景中的静态和动态元素。该方法利用高斯表示和场景几何的几何一致性来获得粗糙的动态区域。这些区域随后作为提示，引导粗糙掩码的精细化，从而获得精确的运动掩码。（ii）为了促进动态场景的精确高效建图，我们引入了动态-静态复合表示，该表示将静态3D高斯与动态4D高斯相结合。这种表示允许对场景中物体的静态和动态状态之间的转换进行建模，从而实现复合建图和优化。（iii）我们采用渐进式位姿优化策略，该策略利用静态场景几何的多视图一致性和动态物体的运动信息来实现精确的相机跟踪。(iv) 我们引入了一种运动一致性损失，利用物体运动的时间连续性来实现精确的动态建模。我们的D$^2$GSLAM在动态场景中表现出卓越的建图和跟踪精度，同时也展示了精确的动态建模能力。"
    },
    {
        "title": "Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane",
        "summary": "QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.",
        "url": "http://arxiv.org/abs/2512.09343v2",
        "published_date": "2025-12-10T06:02:25+00:00",
        "updated_date": "2025-12-11T04:44:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ashik E Rasul",
            "Humaira Tasnim",
            "Ji Yu Kim",
            "Young Hyun Lim",
            "Scott Schmitz",
            "Bruce W. Jo",
            "Hyung-Jin Yoon"
        ],
        "tldr": "This paper presents a lightweight QuadPlane system for vision-based autonomous landing and visual-inertial odometry in GPS-denied environments, addressing challenges like limited edge AI resources and high inertia, making it suitable for long-range aerial monitoring.",
        "tldr_zh": "本文介绍了一种轻量级的四旋翼飞机系统，用于在GPS拒止环境中的基于视觉的自主着陆和视觉惯性里程计。该系统解决了诸如有限的边缘AI资源和高惯性等挑战，使其适用于远程空中监测。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "QuadPlane结合了固定翼飞机的航程效率和多旋翼平台的机动性，适用于远程自主任务。在GPS受限或复杂的城市环境中，基于感知的着陆对于可靠运行至关重要。与结构化着陆区不同，真实世界的场地是非结构化且高度可变的，这要求感知系统具有强大的泛化能力。深度神经网络（DNN）为学习各种视觉和环境条件下的着陆点特征提供了一种可扩展的解决方案。虽然感知驱动的着陆已经在模拟中得到验证，但现实世界的部署带来了重大挑战。有效载荷和体积限制限制了诸如NVIDIA Jetson Orin Nano之类的高性能边缘AI设备的使用，而这些设备对于实时检测和控制至关重要。下降期间的精确姿态估计是必要的，尤其是在没有GPS的情况下，并且依赖于可靠的视觉惯性里程计。在有限的边缘AI资源下实现这一点，需要对整个部署框架进行仔细优化。大型QuadPlane的飞行特性进一步使问题复杂化。这些飞机表现出高惯性、推力矢量控制降低以及缓慢的响应时间，进一步复杂化了稳定的着陆操作。本文介绍了一种用于高效的基于视觉的自主着陆和视觉惯性里程计的轻量级QuadPlane系统，专门为诸如空中监测之类的远程QuadPlane操作而开发。它描述了为满足苛刻的实时性和物理约束而设计的硬件平台、传感器配置和嵌入式计算架构。这为在动态、非结构化、GPS受限环境中部署自主着陆奠定了基础。"
    },
    {
        "title": "Characterizing Human Feedback-Based Control in Naturalistic Driving Interactions via Gaussian Process Regression with Linear Feedback",
        "summary": "Understanding driver interactions is critical to designing autonomous vehicles to interoperate safely with human-driven cars. We consider the impact of these interactions on the policies drivers employ when navigating unsigned intersections in a driving simulator. The simulator allows the collection of naturalistic decision-making and behavior data in a controlled environment. Using these data, we model the human driver responses as state-based feedback controllers learned via Gaussian Process regression methods. We compute the feedback gain of the controller using a weighted combination of linear and nonlinear priors. We then analyze how the individual gains are reflected in driver behavior. We also assess differences in these controllers across populations of drivers. Our work in data-driven analyses of how drivers determine their policies can facilitate future work in the design of socially responsive autonomy for vehicles.",
        "url": "http://arxiv.org/abs/2512.09097v1",
        "published_date": "2025-12-09T20:21:22+00:00",
        "updated_date": "2025-12-09T20:21:22+00:00",
        "categories": [
            "eess.SY",
            "cs.RO"
        ],
        "authors": [
            "Rachel DiPirro",
            "Rosalyn Devonport",
            "Dan Calderone",
            "Chishang \"Mario'' Yang",
            "Wendy Ju",
            "Meeko Oishi"
        ],
        "tldr": "This paper uses Gaussian Process regression to model human driver behavior at intersections as feedback controllers, analyzing how different drivers respond to uncertain environments. This data-driven approach aims to inform the design of socially responsive autonomous vehicles.",
        "tldr_zh": "本文采用高斯过程回归将人类驾驶员在十字路口的驾驶行为建模为反馈控制器，分析不同驾驶员如何应对不确定环境。 这种数据驱动的方法旨在为社交响应型自动驾驶汽车的设计提供信息。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "理解驾驶员交互对于设计能够与人类驾驶车辆安全互操作的自动驾驶车辆至关重要。我们考察了这些交互在驾驶员于驾驶模拟器中导航无信号灯交叉路口时所采用策略的影响。该模拟器允许在受控环境中收集自然决策和行为数据。利用这些数据，我们将人类驾驶员的反应建模为基于状态的反馈控制器，该控制器通过高斯过程回归方法进行学习。我们使用线性和非线性先验的加权组合来计算控制器的反馈增益。然后，我们分析各个增益如何反映在驾驶员行为中。我们还评估了不同驾驶员群体之间这些控制器的差异。我们对驾驶员如何确定其策略的数据驱动分析工作，能够促进未来在车辆的社会响应性自主设计方面的研究。"
    },
    {
        "title": "Inferring Operator Emotions from a Motion-Controlled Robotic Arm",
        "summary": "A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.",
        "url": "http://arxiv.org/abs/2512.09086v1",
        "published_date": "2025-12-09T19:57:43+00:00",
        "updated_date": "2025-12-09T19:57:43+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Xinyu Qi",
            "Zeyu Deng",
            "Shaun Alexander Macdonald",
            "Liying Li",
            "Chen Wang",
            "Muhammad Ali Imran",
            "Philip G. Zhao"
        ],
        "tldr": "This paper explores using robot movements to infer the operator's emotional state with 83.3% accuracy, offering a non-intrusive alternative to traditional emotion recognition methods in remote robot control.",
        "tldr_zh": "本文探讨了利用机器人运动来推断操作员的情绪状态，准确率达到83.3%，为远程机器人控制中的传统情绪识别方法提供了一种非侵入性的替代方案。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "远程机器人操作员的情感状态会显著影响机器人的运动，导致意想不到的后果，即使操作员遵循协议并执行允许的任务。然而，在远程机器人控制场景中识别操作员的情感状态仍未得到充分探索。目前的情感识别方法依赖于读取用户的生命体征或肢体语言，但这些措施所需的设备和用户参与会给远程机器人控制带来限制。我们证明了可以通过机器学习系统，利用远程控制机器人化身的功能性运动（并非设计用于情感表达）来推断人类操作员的情感状态。具体而言，我们的系统通过机器人的运动，识别用户手部动作表达的情感状态，达到了83.3%的准确率。我们讨论了该系统对当前和未来重要的远程机器人操作和情感机器人环境的影响。"
    },
    {
        "title": "Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics",
        "summary": "Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.",
        "url": "http://arxiv.org/abs/2512.09071v1",
        "published_date": "2025-12-09T19:34:43+00:00",
        "updated_date": "2025-12-09T19:34:43+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Nick Trinh",
            "Damian Lyons"
        ],
        "tldr": "This paper introduces an adaptive thresholding method for visual place recognition (VPR) based on negative Gaussian mixture statistics, aiming to automate threshold selection for robust VPR performance across diverse visual environments.",
        "tldr_zh": "本文提出了一种基于负高斯混合统计的视觉场景识别(VPR)的自适应阈值方法，旨在自动选择阈值，以实现跨不同视觉环境的稳健VPR性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "视觉地点识别 (VPR) 是基于相机的地图构建和导航应用的重要组成技术。这是一个具有挑战性的问题，因为同一地点的图像由于季节变化、天气光照、环境结构变化以及瞬时行人或车辆交通等原因可能看起来迥然不同。专注于为 VPR 生成图像描述符的论文通常使用诸如 recall@K 和 ROC 曲线等指标来报告其结果。然而，对于机器人实现来说，判定哪些匹配足够好通常会简化为手动设置一个阈值。而且，手动选择一个适用于各种视觉场景的阈值是很困难的。本文通过研究一个地点的“负”高斯混合统计量——即指示非该地点的图像统计量，来解决自动选择 VPR 阈值的问题。我们证明，该方法可以用于选择适用于各种图像数据库和图像描述符的阈值。"
    },
    {
        "title": "LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception",
        "summary": "Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.",
        "url": "http://arxiv.org/abs/2512.08912v1",
        "published_date": "2025-12-09T18:47:56+00:00",
        "updated_date": "2025-12-09T18:47:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Simon de Moreau",
            "Andrei Bursuc",
            "Hafid El-Idrissi",
            "Fabien Moutarde"
        ],
        "tldr": "LiDAS is a closed-loop active illumination system that dynamically optimizes headlight projection to improve nighttime perception using existing visual perception models, resulting in performance gains and energy savings.",
        "tldr_zh": "LiDAS是一个闭环主动照明系统，它通过动态优化车头灯的投射来改善夜间感知，使用现有的视觉感知模型，从而提高性能并节省能源。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "夜间环境为基于相机的感知带来了重大挑战，因为现有方法被动地依赖于场景照明。我们引入了照明驱动的动态主动感知（LiDAS），这是一种闭环主动照明系统，它结合了现成的视觉感知模型和高清前照灯。LiDAS并非均匀地照亮场景，而是动态地预测一个优化照明场，以最大化下游感知性能，即减少空旷区域的光照，将其重新分配到物体区域。LiDAS通过自适应照明控制，实现了白天训练模型在夜间的零样本泛化。LiDAS在合成数据上进行训练，并在真实世界的闭环驾驶场景中进行零样本部署，在相同功率下，相对于标准近光灯，实现了+18.7%的mAP50和+5.0%的mIoU。在保持性能的同时，它还能减少40%的能源消耗。LiDAS补充了领域泛化方法，进一步增强了鲁棒性且无需重新训练。通过将现成的前照灯转换为主动视觉致动器，LiDAS为鲁棒的夜间感知提供了一种经济高效的解决方案。"
    },
    {
        "title": "IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams",
        "summary": "Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.",
        "url": "http://arxiv.org/abs/2512.08877v1",
        "published_date": "2025-12-09T18:10:17+00:00",
        "updated_date": "2025-12-09T18:10:17+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Ryan LeRoy",
            "Jack Kolb"
        ],
        "tldr": "The paper investigates generalization capabilities of IPPO in a heterogeneous multi-agent environment, finding that IPPO can generalize to novel teammates without explicit training for team diversity.",
        "tldr_zh": "该论文研究了IPPO在异构多智能体环境中的泛化能力，发现IPPO无需专门训练团队多样性即可泛化到新的队友。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "多智能体强化学习（MARL）通常部署于通过与同质队友进行自博弈训练的环境中，通常采用参数共享和单一策略架构。这引发了一个问题：相比于过度拟合训练伙伴的行为，自博弈PPO智能体在多大程度上学习了扎根于底层游戏的一般协调策略？本文利用异构多智能体挑战赛（HeMAC）环境来研究此问题，该环境包含了具有互补能力的观察者和无人机两种不同的智能体。我们引入了轮换策略训练（RPT），这是一种在训练期间轮换不同学习算法的异构队友策略的方法，以使智能体接触到更广泛的伙伴策略。当与保留的队友策略（DDQN）一起博弈时，我们发现RPT实现了与标准自博弈基线IPPO相似的性能，其中所有智能体都通过共享单一的PPO策略进行训练。这一结果表明，在这种异构多智能体环境中，即使在训练期间没有经历队友多样性，IPPO基线也能泛化到新的队友算法。这表明，一个简单的IPPO基线可能具备达到多样化训练方案所设计的针对新队友的泛化能力。"
    },
    {
        "title": "Analyzing Planner Design Trade-offs for MAPF under Realistic Simulation",
        "summary": "Multi-Agent Path Finding (MAPF) algorithms are increasingly deployed in industrial warehouses and automated manufacturing facilities, where robots must operate reliably under real-world physical constraints. However, existing MAPF evaluation frameworks typically rely on simplified robot models, leaving a substantial gap between algorithmic benchmarks and practical performance. Recent frameworks such as SMART, incorporate kinodynamic modeling and offer the MAPF community a platform for large-scale, realistic evaluation. Building on this capability, this work investigates how key planner design choices influence performance under realistic execution settings. We systematically study three fundamental factors: (1) the relationship between solution optimality and execution performance, (2) the sensitivity of system performance to inaccuracies in kinodynamic modeling, and (3) the interaction between model accuracy and plan optimality. Empirically, we examine these factors to understand how these design choices affect performance in realistic scenarios. We highlight open challenges and research directions to steer the community toward practical, real-world deployment.",
        "url": "http://arxiv.org/abs/2512.09736v1",
        "published_date": "2025-12-10T15:15:26+00:00",
        "updated_date": "2025-12-10T15:15:26+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Jingtian Yan",
            "Zhifei Li",
            "William Kang",
            "Stephen F. Smith",
            "Jiaoyang Li"
        ],
        "tldr": "This paper analyzes the impact of planner design choices (optimality, kinodynamic model accuracy) on Multi-Agent Path Finding (MAPF) performance in realistic robot simulation environments, aiming to bridge the gap between algorithmic benchmarks and real-world deployments.",
        "tldr_zh": "本文分析了在真实的机器人仿真环境中，规划器设计选择（最优性、运动学模型准确性）对多智能体路径规划（MAPF）性能的影响，旨在弥合算法基准和实际部署之间的差距。",
        "relevance_score": 6,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "多智能体路径规划（MAPF）算法正日益广泛地应用于工业仓库和自动化制造设施中，在这些场景中，机器人必须在真实物理约束下可靠运行。然而，现有的MAPF评估框架通常依赖于简化的机器人模型，导致算法基准与实际性能之间存在巨大差距。诸如SMART等最新框架，融入了运动学建模，为MAPF社区提供了一个大规模、真实评估的平台。在此基础上，本工作研究了关键规划器设计选择如何在现实执行环境下的性能产生影响。我们系统地研究了三个基本要素：（1）解决方案最优性与执行性能之间的关系，（2）系统性能对运动学建模不准确性的敏感度，以及（3）模型精度与规划最优性之间的相互作用。通过实证研究，我们检验这些要素，以理解这些设计选择如何在真实场景中影响性能。我们强调了当前面临的挑战和研究方向，以引导社区走向实际、真实世界的部署。"
    },
    {
        "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions",
        "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.",
        "url": "http://arxiv.org/abs/2512.09727v1",
        "published_date": "2025-12-10T15:09:06+00:00",
        "updated_date": "2025-12-10T15:09:06+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Junlin Xiao",
            "Victor-Alexandru Darvariu",
            "Bruno Lacerda",
            "Nick Hawes"
        ],
        "tldr": "This paper introduces a Gaussian Process Regression-based aggregation method for root-parallel Monte Carlo Tree Search (MCTS) in continuous action space environments, demonstrating improved performance over existing methods across several domains.",
        "tldr_zh": "该论文提出了一种基于高斯过程回归的聚合方法，用于连续动作空间环境下的根并行蒙特卡洛树搜索(MCTS)。实验表明，该方法在多个领域优于现有的聚合策略。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "蒙特卡洛树搜索是在线规划的基石算法，而其根并行变体在时间有限但希望获得最佳性能时被广泛使用。在具有连续动作空间的环境中，如何最好地聚合来自不同线程的统计信息是一个重要但尚未充分探索的问题。在这项工作中，我们提出了一种使用高斯过程回归来获取未在环境中尝试过的有希望动作的价值估计的方法。我们对 6 个不同的领域进行了系统评估，表明我们的方法优于现有的聚合策略，同时只需要适度增加推理时间。"
    },
    {
        "title": "Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies",
        "summary": "This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.",
        "url": "http://arxiv.org/abs/2512.09682v1",
        "published_date": "2025-12-10T14:29:04+00:00",
        "updated_date": "2025-12-10T14:29:04+00:00",
        "categories": [
            "eess.SY",
            "cs.AI",
            "cs.GT",
            "cs.MA"
        ],
        "authors": [
            "Mika Persson",
            "Jonas Lidman",
            "Jacob Ljungberg",
            "Samuel Sandelius",
            "Adam Andersson"
        ],
        "tldr": "This paper explores using MARL to control UAV swarms for critical data delivery, identifying scalability issues with off-the-shelf algorithms as the swarm size increases and proposing a Dijkstra-based baseline.",
        "tldr_zh": "本文探讨了使用多智能体强化学习（MARL）控制无人机群进行关键数据传输，发现随着群体规模的增加，现有算法存在可扩展性问题，并提出了一种基于Dijkstra算法的基线方法。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "本研究提出了一种关于应用多智能体强化学习(MARL)进行无人机分布式控制的概念性研究，旨在将一个关键数据包中继到已知位置。为此，引入了一系列确定性博弈，专为MARL的缩放研究而设计。提出了一种鲁棒的基线策略，该策略基于限制智能体运动范围并应用Dijkstra算法。实验结果表明，对于少量智能体而言，两种现成的MARL算法的性能与基线算法具有竞争力，但随着智能体数量的增加，可扩展性问题开始显现。"
    },
    {
        "title": "Benchmarking SAM2-based Trackers on FMOX",
        "summary": "Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.",
        "url": "http://arxiv.org/abs/2512.09633v1",
        "published_date": "2025-12-10T13:21:09+00:00",
        "updated_date": "2025-12-10T13:21:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Senem Aktas",
            "Charles Markham",
            "John McDonald",
            "Rozenn Dahyot"
        ],
        "tldr": "This paper benchmarks several SAM2-based object trackers on a dataset of fast-moving objects (FMOX), identifying limitations and highlighting the superior performance of DAM4SAM and SAMURAI in challenging sequences.",
        "tldr_zh": "该论文对几种基于SAM2的目标跟踪器在快速移动对象数据集(FMOX)上进行了基准测试，发现了局限性，并强调了DAM4SAM和SAMURAI在具有挑战性的序列中的卓越性能。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "过去一年中，已经出现了几种扩展Segment Anything Model 2 (SAM2)的目标跟踪流程，这些方法通常通过在初始化帧上由用户提供的单个示例模板来跟踪和分割目标。我们建议在包含快速移动对象 (FMO) 的数据集上对这些高性能跟踪器（SAM2、EfficientTAM、DAM4SAM 和 SAMURAI）进行基准测试，这些数据集专门设计用于挑战跟踪方法。其目的是通过提供对这些跟踪器行为的更详细洞察力，来更好地理解当前最先进跟踪器的局限性。我们表明，总体而言，跟踪器DAM4SAM和SAMURAI在更具挑战性的序列上表现良好。"
    },
    {
        "title": "Generative Point Cloud Registration",
        "summary": "In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2512.09407v1",
        "published_date": "2025-12-10T08:01:20+00:00",
        "updated_date": "2025-12-10T08:01:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haobo Jiang",
            "Jin Xie",
            "Jian Yang",
            "Liang Yu",
            "Jianmin Zheng"
        ],
        "tldr": "This paper introduces Generative Point Cloud Registration, a novel 3D registration paradigm leveraging 2D generative models (specifically, a modified ControlNet called Match-ControlNet) to generate cross-view consistent images for enhanced feature fusion and robust matching.",
        "tldr_zh": "本文提出了一种新的3D配准范式，即生成式点云配准。该方法利用2D生成模型（特别是名为Match-ControlNet的修改版ControlNet）来生成跨视角的图像，以增强特征融合和稳健匹配。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "本文提出了一种新颖的3D配准范式——生成式点云配准，它将先进的2D生成模型与3D匹配任务相结合，从而提升配准性能。我们的核心思想是生成与源点云和目标点云良好对齐的跨视角一致图像对，实现几何-颜色特征融合，从而促进稳健匹配。为了确保高质量匹配，生成的图像对应具有2D-3D几何一致性和跨视角纹理一致性。为此，我们引入了Match-ControlNet，一种特定于匹配的可控2D生成模型。具体来说，它利用ControlNet的深度条件生成能力来生成与从点云导出的深度图在几何上对齐的图像，从而确保2D-3D几何一致性。此外，通过结合耦合条件去噪方案和耦合提示指导，Match-ControlNet进一步促进跨视角特征交互，引导纹理一致性生成。我们的生成式3D配准范式具有通用性，可以无缝集成到各种配准方法中，以提高它们的性能。在3DMatch和ScanNet数据集上的大量实验验证了我们方法的有效性。"
    },
    {
        "title": "FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement",
        "summary": "Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.",
        "url": "http://arxiv.org/abs/2512.09373v1",
        "published_date": "2025-12-10T07:11:22+00:00",
        "updated_date": "2025-12-10T07:11:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haobo Jiang",
            "Jin Xie",
            "Jian Yang",
            "Liang Yu",
            "Jianmin Zheng"
        ],
        "tldr": "The paper introduces FUSER, a feed-forward transformer-based approach for multiview 3D registration that directly predicts global poses without pairwise estimation, and FUSER-DF, a diffusion refinement framework for further pose correction.",
        "tldr_zh": "该论文介绍了FUSER，一种基于前馈Transformer的多视图3D配准方法，可以直接预测全局姿态而无需成对估计，以及FUSER-DF，一种用于进一步姿态校正的扩散细化框架。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "多视角点云配准传统上依赖于大量的两两匹配来构建位姿图，以进行全局同步，这在计算上是昂贵的，并且在没有整体几何约束的情况下本质上是不适定的。本文提出FUSER，这是第一个前馈多视角配准Transformer，它在一个统一的、紧凑的潜在空间中联合处理所有扫描，以直接预测全局位姿，而无需任何两两估计。为了保持可处理性，FUSER通过保留绝对平移线索的稀疏3D CNN将每个扫描编码为低分辨率的超点特征，并通过几何交替注意力模块执行高效的内扫描和跨扫描推理。 特别是，我们从现成的基础模型中迁移2D注意力先验，以增强3D特征交互和几何一致性。在FUSER的基础上，我们进一步引入了FUSER-DF，一个SE(3)$^N$扩散细化框架，通过在联合SE(3)$^N$空间中去噪来校正FUSER的估计。FUSER充当代理多视角配准模型来构建去噪器，并且推导出一个先验条件SE(3)$^N$变分下界用于去噪监督。在3DMatch、ScanNet和ArkitScenes上的大量实验表明，我们的方法实现了卓越的配准精度和出色的计算效率。"
    },
    {
        "title": "Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime",
        "summary": "We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.\n  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.",
        "url": "http://arxiv.org/abs/2512.09850v1",
        "published_date": "2025-12-10T17:34:55+00:00",
        "updated_date": "2025-12-10T17:34:55+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Simone Cuonzo",
            "Nina Deliu"
        ],
        "tldr": "The paper introduces Conformal Bandits, a novel framework that integrates Conformal Prediction into bandit problems to provide statistical guarantees and improve reward efficiency in small-gap regimes, demonstrating its effectiveness in portfolio allocation with regime-switching models.",
        "tldr_zh": "该论文介绍了Conformal Bandits，这是一个将Conformal Prediction集成到bandit问题中的新框架，旨在提供统计保证并提高小间隙场景中的奖励效率。论文通过将该框架应用于投资组合分配，并结合状态切换模型，展示了其有效性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "我们引入了共形强盗算法，这是一个将共形预测（CP）整合到强盗问题中的新颖框架，强盗问题是具有不确定性的序贯决策的经典范式。传统的后悔最小化强盗策略，如汤普森抽样和置信上限法（UCB），通常依赖于分布假设或渐近保证；此外，它们主要关注后悔值，而忽略了它们的统计特性。我们旨在弥补这一不足。通过采用共形预测，我们将决策强盗策略的后悔最小化潜力与有限时间的预测覆盖率形式的统计保证联系起来。\n\n我们通过模拟研究以及在投资组合配置中的应用，展示了共形强盗算法的潜力，这是一个典型的小间隔场景，即臂奖励的差异太小，以致于经典策略无法在有限样本中实现最佳后悔界。基于此，我们展示了我们的框架在小间隔设置中在后悔方面的实际优势，以及其在实现名义覆盖保证方面的附加价值，而经典UCB策略则无法实现。着重于我们感兴趣的应用，我们进一步说明了如何整合隐马尔可夫模型来捕捉金融市场的状态切换行为，从而增强探索-利用的权衡，并转化为更高的风险调整后悔效率回报，同时保持覆盖保证。"
    },
    {
        "title": "Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks",
        "summary": "Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.",
        "url": "http://arxiv.org/abs/2512.09621v1",
        "published_date": "2025-12-10T13:08:05+00:00",
        "updated_date": "2025-12-10T13:08:05+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Jingbo Zhang",
            "Maoxin Ji",
            "Qiong Wu",
            "Pingyi Fan",
            "Kezhi Wang",
            "Wen Chen"
        ],
        "tldr": "The paper proposes a Tripartite Cooperative Semantic Communication framework (TCSC) for vehicular networks, optimizing task offloading with a novel multi-agent proximal policy optimization method (MAPPO-PDN) and linear programming to minimize latency and semantic symbol usage.",
        "tldr_zh": "该论文提出了一种用于车辆网络的的三方协作语义通信框架（TCSC），通过一种新颖的多智能体近端策略优化方法（MAPPO-PDN）和线性规划来优化任务卸载，以最小化延迟和语义符号使用量。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "语义通信（SC）与车载边缘计算（VEC）相结合，为车联网（IoV）提供了一种高效的边缘任务处理范式。本文针对高速公路场景，提出了一种三方协作语义通信（TCSC）框架，该框架使车辆用户（VU）能够通过车对基础设施（V2I）和车对车（V2V）通信执行语义任务卸载。考虑到任务延迟和语义符号的数量，该框架构建了一个混合整数非线性规划（MINLP）问题，并将其转化为两个子问题。首先，我们创新性地提出了一种基于参数化分布噪声的多智能体近端策略优化任务卸载优化方法（MAPPO-PDN），以解决语义符号数量的优化问题；其次，采用线性规划（LP）来解决卸载比例的问题。仿真结果表明，该方案的性能优于其他算法。"
    },
    {
        "title": "Goal inference with Rao-Blackwellized Particle Filters",
        "summary": "Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.",
        "url": "http://arxiv.org/abs/2512.09269v1",
        "published_date": "2025-12-10T02:48:55+00:00",
        "updated_date": "2025-12-10T02:48:55+00:00",
        "categories": [
            "cs.LG",
            "cs.IR"
        ],
        "authors": [
            "Yixuan Wang",
            "Dan P. Guralnik",
            "Warren E. Dixon"
        ],
        "tldr": "This paper presents a Rao-Blackwellized Particle Filter (RBPF) approach for inferring the goal of a mobile agent from noisy trajectory observations, using information-theoretic metrics to quantify intent recovery and estimator performance.",
        "tldr_zh": "本文提出了一种Rao-Blackwellized粒子滤波（RBPF）方法，用于从嘈杂的轨迹观测中推断移动智能体的目标，并使用信息论指标来量化意图恢复和估计器性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "从移动智能体轨迹的噪声观测中推断其最终目标是一个基础的估计问题。我们启动对此类意图推断的研究，采用一种Rao-Blackwellized粒子滤波(RBPF)的变体，前提是智能体的意图通过具有最先进的可证明的实用稳定性特性的闭环行为来体现。利用假定的闭式智能体动力学，RBPF解析地边缘化线性高斯子结构，并且仅更新粒子权重，从而提高相比于标准粒子滤波的采样效率。我们引入两个差分估计器：一个使用RBPF权重的高斯混合模型和一个将混合限制在有效样本的简化版本。我们使用信息论泄露指标量化了对抗者恢复智能体意图的能力，并通过高斯混合KL界限，提供了真实意图分布和RBPF估计之间Kullback-Leibler(KL)散度的可计算下界。我们还提供了两个估计器之间性能差异的界限，突出了简化估计器的性能几乎与完整估计器一样好的事实。实验结果表明，对于顺从的智能体，能够快速准确地恢复意图，这激励了未来在设计意图混淆控制器方面的工作。"
    },
    {
        "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle",
        "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.",
        "url": "http://arxiv.org/abs/2512.09798v1",
        "published_date": "2025-12-10T16:12:59+00:00",
        "updated_date": "2025-12-10T16:12:59+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Misael Mamani",
            "Mariel Fernandez",
            "Grace Luna",
            "Steffani Limachi",
            "Leonel Apaza",
            "Carolina Montes-Dávalos",
            "Marcelo Herrera",
            "Edwin Salcedo"
        ],
        "tldr": "This paper presents a solar-powered autonomous surface vehicle (USV) with a novel water sampling system capable of collecting 72 discrete samples, demonstrating reliable autonomous navigation and physicochemical measurements in field trials.",
        "tldr_zh": "本文介绍了一种太阳能自主水面航行器 (USV)，配备新型水采样系统，能够收集 72 个离散样本，并在现场试验中展示了可靠的自主导航和物理化学测量。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "准确的水质评估需要空间分辨率的采样，然而大多数无人水面船（USV）只能采集有限数量的样本，或者依赖代表性差的单点传感器。本研究提出了一种太阳能驱动、全自主的USV，其特色是以新型注射器为基础的采样架构，每次任务能够采集72个独立的、污染最小化的水样。该车辆集成了ROS 2自主堆栈，包括GPS-RTK导航系统、激光雷达和立体视觉障碍物检测系统、基于Nav2的任务规划系统以及远距离LoRa监控系统，从而能够在非结构化环境中可靠地执行采样路线。该平台集成了从Nav2改编的行为树自主架构，实现了任务级别的推理和感知感知的导航。由分布式micro-ROS节点控制的模块化6x12采样系统提供确定性的驱动、故障隔离和快速的模块更换，实现了超越先前报道的基于USV采样器的空间覆盖范围。在阿乔科拉泻湖（玻利维亚拉巴斯）进行的现场试验表明，航点精度达到87%，自主导航稳定，并且物理化学测量结果（温度、pH值、电导率、总溶解固体）与手动采集的参考值相当。这些结果表明，该平台能够实现可靠的高分辨率采样和自主任务执行，为偏远水域环境的监测提供了一种可扩展的解决方案。"
    },
    {
        "title": "CS3D: An Efficient Facial Expression Recognition via Event Vision",
        "summary": "Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\\% of the original C3D required on the same device.",
        "url": "http://arxiv.org/abs/2512.09592v1",
        "published_date": "2025-12-10T12:42:28+00:00",
        "updated_date": "2025-12-10T12:42:28+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhe Wang",
            "Qijin Song",
            "Yucen Peng",
            "Weibang Bai"
        ],
        "tldr": "The paper introduces CS3D, an efficient event-based facial expression recognition framework using a decomposed Convolutional 3D method with soft spiking neurons and spatial-temporal attention to reduce computational complexity and improve accuracy, demonstrating lower energy consumption and higher accuracy compared to other architectures.",
        "tldr_zh": "该论文介绍了CS3D，一个高效的基于事件的表情识别框架，它使用分解的卷积3D方法，结合软脉冲神经元和时空注意力机制来降低计算复杂性并提高准确性，实验表明其能耗更低，精度高于其他架构。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "响应迅速且精准的面部表情识别对于日常服务机器人的人机交互至关重要。如今，事件相机正变得越来越普及，因为它们在捕获面部表情变化方面超越了RGB相机，这得益于它们高的时间分辨率、低延迟、计算效率以及在低光照条件下的鲁棒性。尽管具有这些优势，但基于事件的方法仍然面临实际挑战，尤其是在采用主流深度学习模型方面。传统的面部表情分析深度学习方法是能源密集型的，使得它们难以部署在边缘计算设备上，从而增加了成本，特别是对于高频、动态、基于事件视觉的方法。为了解决这个具有挑战性的问题，我们提出了一种CS3D框架，通过分解卷积3D方法来降低计算复杂度和能耗。此外，通过利用软脉冲神经元和时空注意力机制，增强了信息保持能力，从而提高了面部表情检测的准确性。实验结果表明，与RNN、Transformer和C3D等架构相比，我们提出的CS3D方法在多个数据集上获得了更高的准确率，同时CS3D方法的能耗仅为同一设备上原始C3D所需的21.97%。"
    },
    {
        "title": "On Mobile Ad Hoc Networks for Coverage of Partially Observable Worlds",
        "summary": "This paper addresses the movement and placement of mobile agents to establish a communication network in initially unknown environments. We cast the problem in a computational-geometric framework by relating the coverage problem and line-of-sight constraints to the Cooperative Guard Art Gallery Problem, and introduce its partially observable variant, the Partially Observable Cooperative Guard Art Gallery Problem (POCGAGP). We then present two algorithms that solve POCGAGP: CADENCE, a centralized planner that incrementally selects 270 degree corners at which to deploy agents, and DADENCE, a decentralized scheme that coordinates agents using local information and lightweight messaging. Both approaches operate under partial observability and target simultaneous coverage and connectivity. We evaluate the methods in simulation across 1,500 test cases of varied size and structure, demonstrating consistent success in forming connected networks while covering and exploring unknown space. These results highlight the value of geometric abstractions for communication-driven exploration and show that decentralized policies are competitive with centralized performance while retaining scalability.",
        "url": "http://arxiv.org/abs/2512.09495v1",
        "published_date": "2025-12-10T10:19:34+00:00",
        "updated_date": "2025-12-10T10:19:34+00:00",
        "categories": [
            "cs.RO",
            "cs.CG",
            "cs.MA"
        ],
        "authors": [
            "Edwin Meriaux",
            "Shuo Wen",
            "Louis-Roy Langevin",
            "Doina Precup",
            "Antonio Loría",
            "Gregory Dudek"
        ],
        "tldr": "This paper proposes centralized (CADENCE) and decentralized (DADENCE) algorithms for deploying mobile agents to establish communication networks in initially unknown environments, focusing on coverage, connectivity, and partial observability, and validating them through simulations.",
        "tldr_zh": "本文提出了集中式 (CADENCE) 和分散式 (DADENCE) 算法，用于在初始未知环境中部署移动代理以建立通信网络，重点关注覆盖范围、连通性和部分可观测性，并通过仿真进行了验证。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "本文探讨了移动代理的移动和部署，以在最初未知的环境中建立通信网络。 我们通过将覆盖问题和视线约束与协作守卫美术馆问题联系起来，并将问题置于计算几何框架中，并引入了它的部分可观测变体，即部分可观测协作守卫美术馆问题（POCGAGP）。 之后，我们提出了两种解决POCGAGP的算法：CADENCE，一种集中式规划器，可逐步选择270度角以部署代理；DADENCE，一种分散式方案，它使用本地信息和轻量级消息传递来协调代理。 两种方法都在部分可观测性下运行，并针对同步覆盖和连通性。 我们在各种规模和结构的1500个测试用例中的模拟环境中评估了这些方法，证明了在形成连接网络的同时成功覆盖和探索未知空间。 这些结果突出了几何抽象对于通信驱动探索的价值，并表明分散式策略在保持可扩展性的同时，与集中式性能相比具有竞争力。"
    },
    {
        "title": "Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing",
        "summary": "In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.",
        "url": "http://arxiv.org/abs/2512.09462v1",
        "published_date": "2025-12-10T09:30:36+00:00",
        "updated_date": "2025-12-10T09:30:36+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jayant Unde",
            "Takumi Inden",
            "Yuki Wakayama",
            "Jacinto Colan",
            "Yaonan Zhu",
            "Tadayoshi Aoyama",
            "Yasuhisa Hasegawa"
        ],
        "tldr": "This paper presents a compliant gripper system integrated with a robotic manipulator to assist elderly or hemiplegic individuals in dressing and undressing trousers, demonstrating a high success rate in experiments.",
        "tldr_zh": "本文介绍了一种兼容型夹持器系统，集成了机器人操纵器，旨在帮助老年人或偏瘫患者穿脱裤子，实验表明该系统具有很高的成功率。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "近年来，包括日本在内的许多国家人口迅速老龄化，使得维护老年人的生活质量成为一个重要的关注点。对于行动不便的老年人来说，如厕辅助是最重要的问题之一。本文详细介绍了夹持器系统的设计、开发、实验评估和潜在应用，重点关注辅助老年人或偏瘫患者穿脱裤子时所涉及的独特需求和障碍。我们提出的夹持器旨在找到柔顺性和抓握力之间的平衡点，确保精确操作的同时，保持与用户的安全和柔顺的交互。夹持器集成到定制的机器人机械臂系统中，为辅助偏瘫患者完成穿脱裤子任务提供了一个综合解决方案。 实验评估以及与现有研究的比较表明，该夹持器能够在狭小空间内以高成功率地辅助穿脱裤子。这项研究有助于辅助机器人技术的发展，从而帮助老年人和肢体障碍人士保持独立性并提高他们的生活质量。"
    },
    {
        "title": "Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments",
        "summary": "We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.",
        "url": "http://arxiv.org/abs/2512.09447v1",
        "published_date": "2025-12-10T09:20:09+00:00",
        "updated_date": "2025-12-10T09:20:09+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jaehyun Kim",
            "Seungwon Choi",
            "Tae-Wan Kim"
        ],
        "tldr": "This paper introduces a descriptor-agnostic LiDAR loop closure verification method based on sequential probability ratio testing (SPRT), designed to reduce false positives in repetitive environments by accumulating evidence from multiple frames.",
        "tldr_zh": "本文提出了一种基于序贯概率比检验 (SPRT) 的、与描述符无关的激光雷达回环闭合验证方法。该方法通过从多个帧累积证据，旨在减少重复环境中的误报。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "我们提出了一种与描述符无关的多帧回环闭合验证方法，该方法将激光雷达回环闭合问题建模为截断序列概率比检验（SPRT）。该验证器不是依赖于单个描述符的比较或使用后期迭代最近点（ICP）校正的固定阈值，而是累积查询帧与每个候选帧之间的一段短时间序列的描述符相似度。一旦观察到足够的多帧证据，它会根据用户指定的Type-I/II错误设计目标，自适应地发出接受/拒绝决策。这种精度优先的策略旨在抑制结构重复的室内环境中的假阳性。我们使用固定的检索前端和几种具有代表性的激光雷达全局描述符，在一个五序列库数据集上评估了该验证器。性能通过分割级别的K-hit精度-召回率，以及姿态图优化后的绝对轨迹误差（ATE）和相对姿态误差（RPE）进行评估。与单帧和启发式多帧基线相比，该序列验证器在各种描述符中均能持续提高精度并减少混叠回环的影响。我们的实现和数据集将在以下网址发布：https://github.com/wanderingcar/snu_library_dataset。"
    },
    {
        "title": "Cognitive Trust in HRI: \"Pay Attention to Me and I'll Trust You Even if You are Wrong\"",
        "summary": "Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.",
        "url": "http://arxiv.org/abs/2512.09105v1",
        "published_date": "2025-12-09T20:41:29+00:00",
        "updated_date": "2025-12-09T20:41:29+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Adi Manor",
            "Dan Cohen",
            "Ziv Keidar",
            "Avi Parush",
            "Hadas Erel"
        ],
        "tldr": "This paper investigates how attentiveness can compensate for low competence in robots to build cognitive trust in human-robot interaction, revealing an affective compensatory mechanism.",
        "tldr_zh": "本文研究了在人机交互中，机器人的专注度如何弥补能力不足，从而建立认知信任，揭示了一种情感补偿机制。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "认知信任以及机器人能够准确执行任务的信念，被认为是促进高质量人机交互的核心要素。机器人胜任力和可靠性等性能因素能够塑造认知信任，这一点已得到充分证实。近期研究表明，诸如机器人专注度等情感因素也在建立认知信任方面发挥作用。本研究探讨了塑造认知信任的这两个因素之间的相互作用。具体而言，我们评估了机器人胜任度和专注度的不同组合是否会引入一种补偿机制，即一个因素可以弥补另一个因素的不足。在实验中，参与者与一只机器狗一起完成了一项搜索任务，采用 2x2 的实验设计，包含两个因素：胜任力（高或低）和专注度（高或低）。结果表明，高专注度可以弥补低胜任力。与高度关注但表现不佳的机器人合作的参与者，其信任度与那些与高度胜任的机器人合作的参与者相当。当机器人没有表现出专注度时，低胜任力会导致认知信任大幅下降。研究结果表明，人机交互中认知信任的建立可能比以前认为的更为复杂，涉及通常被忽视的情感过程。我们强调了一种情感补偿机制，它增加了一个需要考虑的层面，与传统的基于胜任力的认知信任模型并存。"
    },
    {
        "title": "Accelerated Rotation-Invariant Convolution for UAV Image Segmentation",
        "summary": "Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.\n  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\\(\\times\\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\\(\\times\\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.",
        "url": "http://arxiv.org/abs/2512.08888v1",
        "published_date": "2025-12-09T18:30:00+00:00",
        "updated_date": "2025-12-09T18:30:00+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Manduhu Manduhu",
            "Alexander Dow",
            "Gerard Dooly",
            "James Riordan"
        ],
        "tldr": "This paper introduces a GPU-optimized rotation-invariant convolution framework for UAV image segmentation that achieves faster training and lower energy consumption compared to existing methods, while maintaining comparable accuracy.",
        "tldr_zh": "本文介绍了一种针对无人机图像分割的GPU优化旋转不变卷积框架，与现有方法相比，该框架实现了更快的训练速度和更低的能耗，同时保持了相当的准确率。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "在无人机航拍图像中，精确的物体级分割至关重要，而旋转不变性是其关键，因为目标可能具有任意方向并呈现精细尺度细节。 传统的分割架构（如 U-Net）依赖于不具备旋转不变性的卷积算子，导致在不同视角下分割精度降低。可以通过扩展跨多个方向的滤波器组来实现旋转不变性；然而，这将显著增加计算成本和内存流量。 在本文中，我们介绍了一种 GPU 优化的旋转不变卷积框架，该框架消除了基于矩阵乘法的卷积所需的传统数据缩减 (im2col) 步骤。 通过利用对称旋转滤波器之间的结构化数据共享，我们的方法以大大减少的内存流量和计算冗余实现了多方向卷积。 我们进一步推广了该方法，以加速任意（非对称）旋转角度的卷积。 在广泛的基准测试中，所提出的卷积比 CUDNN 实现了 20-55% 更快的训练速度和 15-45% 更低的能耗，同时保持了与最先进的旋转不变方法相当的精度。 在八方向设置中，我们的方法在 256\\(\\times\\)256 的输入上实现了高达 45% 的加速和 41% 的节能，在 1024\\(\\times\\)1024 的输入上实现了 32% 的加速和 23% 的节能。 集成到 U-Net 分割模型中，该框架比非旋转感知的基线模型精度提高了高达 6%。 这些结果表明，所提出的方法为现有的旋转不变 CNN 框架提供了一种有效且高效的替代方案。"
    },
    {
        "title": "Understanding Mental States in Active and Autonomous Driving with EEG",
        "summary": "Understanding how driver mental states differ between active and autonomous driving is critical for designing safe human-vehicle interfaces. This paper presents the first EEG-based comparison of cognitive load, fatigue, valence, and arousal across the two driving modes. Using data from 31 participants performing identical tasks in both scenarios of three different complexity levels, we analyze temporal patterns, task-complexity effects, and channel-wise activation differences. Our findings show that although both modes evoke similar trends across complexity levels, the intensity of mental states and the underlying neural activation differ substantially, indicating a clear distribution shift between active and autonomous driving. Transfer-learning experiments confirm that models trained on active driving data generalize poorly to autonomous driving and vice versa. We attribute this distribution shift primarily to differences in motor engagement and attentional demands between the two driving modes, which lead to distinct spatial and temporal EEG activation patterns. Although autonomous driving results in lower overall cortical activation, participants continue to exhibit measurable fluctuations in cognitive load, fatigue, valence, and arousal associated with readiness to intervene, task-evoked emotional responses, and monotony-related passive fatigue. These results emphasize the need for scenario-specific data and models when developing next-generation driver monitoring systems for autonomous vehicles.",
        "url": "http://arxiv.org/abs/2512.09190v1",
        "published_date": "2025-12-09T23:30:52+00:00",
        "updated_date": "2025-12-09T23:30:52+00:00",
        "categories": [
            "cs.HC",
            "cs.AI"
        ],
        "authors": [
            "Prithila Angkan",
            "Paul Hungler",
            "Ali Etemad"
        ],
        "tldr": "This study investigates differences in driver mental states (cognitive load, fatigue, valence, arousal) using EEG data between active and autonomous driving, finding significant distribution shifts and emphasizing the need for scenario-specific driver monitoring systems. It shows that even in autonomous driving, measurable fluctuations happen.",
        "tldr_zh": "本研究使用脑电图（EEG）数据调查了主动驾驶和自动驾驶中驾驶员精神状态（认知负荷、疲劳、效价、唤醒）的差异，发现显著的分布差异，并强调了针对特定场景的驾驶员监控系统的必要性。它表明即使在自动驾驶中，也会发生可测量的波动。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "理解主动驾驶和自动驾驶之间驾驶员精神状态的差异对于设计安全的人机交互界面至关重要。本文首次基于脑电图比较了两种驾驶模式下的认知负荷、疲劳度、效价和唤醒度。我们利用31名参与者在三种不同复杂程度场景下执行相同任务的数据，分析了时间模式、任务复杂度的影响以及通道维度的激活差异。我们的结果表明，尽管两种模式在不同复杂程度下都呈现相似的趋势，但精神状态的强度和潜在的神经激活存在显著差异，表明主动驾驶和自动驾驶之间存在明显的分布转移。迁移学习实验证实，在主动驾驶数据上训练的模型对自动驾驶的泛化性能较差，反之亦然。我们将这种分布转移主要归因于两种驾驶模式下运动参与和注意力需求的不同，这导致了不同的空间和时间脑电激活模式。尽管自动驾驶导致较低的总体皮层激活，但参与者仍然表现出可测量的认知负荷、疲劳度、效价和唤醒度的波动，这些波动与干预准备、任务引发的情绪反应以及与单调性相关的被动疲劳相关。这些结果强调了在为自动驾驶汽车开发下一代驾驶员监控系统时，需要使用特定场景的数据和模型。"
    },
    {
        "title": "Neuromorphic Eye Tracking for Low-Latency Pupil Detection",
        "summary": "Eye tracking for wearable systems demands low latency and milliwatt-level power, but conventional frame-based pipelines struggle with motion blur, high compute cost, and limited temporal resolution. Such capabilities are vital for enabling seamless and responsive interaction in emerging technologies like augmented reality (AR) and virtual reality (VR), where understanding user gaze is key to immersion and interface design. Neuromorphic sensors and spiking neural networks (SNNs) offer a promising alternative, yet existing SNN approaches are either too specialized or fall short of the performance of modern ANN architectures. This paper presents a neuromorphic version of top-performing event-based eye-tracking models, replacing their recurrent and attention modules with lightweight LIF layers and exploiting depth-wise separable convolutions to reduce model complexity. Our models obtain 3.7-4.1px mean error, approaching the accuracy of the application-specific neuromorphic system, Retina (3.24px), while reducing model size by 20x and theoretical compute by 850x, compared to the closest ANN variant of the proposed model. These efficient variants are projected to operate at an estimated 3.9-4.9 mW with 3 ms latency at 1 kHz. The present results indicate that high-performing event-based eye-tracking architectures can be redesigned as SNNs with substantial efficiency gains, while retaining accuracy suitable for real-time wearable deployment.",
        "url": "http://arxiv.org/abs/2512.09969v1",
        "published_date": "2025-12-10T11:30:21+00:00",
        "updated_date": "2025-12-10T11:30:21+00:00",
        "categories": [
            "cs.CV",
            "cs.NE"
        ],
        "authors": [
            "Paul Hueber",
            "Luca Peres",
            "Florian Pitters",
            "Alejandro Gloriani",
            "Oliver Rhodes"
        ],
        "tldr": "This paper presents a neuromorphic eye-tracking system using spiking neural networks (SNNs) that achieves comparable accuracy to existing systems with significantly reduced model size, compute, power consumption, and latency, making it suitable for wearable AR/VR applications.",
        "tldr_zh": "本文提出了一种使用脉冲神经网络（SNNs）的神经形态眼动追踪系统，该系统在实现与现有系统相当的精度的同时，显著降低了模型尺寸、计算量、功耗和延迟，使其适用于可穿戴AR/VR应用。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 5,
        "summary_zh": "可穿戴系统的眼动追踪需要低延迟和毫瓦级的功耗，但传统的基于帧的流程难以应对运动模糊、高计算成本和有限的时间分辨率。 这些能力对于实现增强现实 (AR) 和虚拟现实 (VR) 等新兴技术中的无缝和响应式交互至关重要，在这些技术中，理解用户视线是沉浸感和界面设计的关键。神经形态传感器和脉冲神经网络 (SNN) 提供了一种有前景的替代方案，但现有的 SNN 方法要么过于专业化，要么达不到现代 ANN 架构的性能。 本文提出了一种高性能的基于事件的眼动追踪模型的神经形态版本，用轻量级 LIF 层替换了它们的循环和注意力模块，并利用深度可分离卷积来降低模型复杂度。 我们的模型获得了 3.7-4.1 像素的平均误差，接近于特定应用神经形态系统 Retina（3.24 像素）的精度，同时与所提出模型的最近似 ANN 变体相比，模型尺寸减少了 20 倍，理论计算量减少了 850 倍。 这些高效的变体预计以 1 kHz 的频率运行在估计的 3.9-4.9 mW 功耗和 3 ms 延迟下。 目前的结果表明，高性能的基于事件的眼动追踪架构可以被重新设计为 SNN，从而获得显著的效率提升，同时保持适用于实时可穿戴部署的精度。"
    },
    {
        "title": "UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking",
        "summary": "Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.",
        "url": "http://arxiv.org/abs/2512.09327v1",
        "published_date": "2025-12-10T05:25:58+00:00",
        "updated_date": "2025-12-10T05:25:58+00:00",
        "categories": [
            "cs.CV",
            "cs.SD"
        ],
        "authors": [
            "Xuangeng Chu",
            "Ruicong Liu",
            "Yifei Huang",
            "Yun Liu",
            "Yichen Peng",
            "Bo Zheng"
        ],
        "tldr": "UniLS is a novel end-to-end framework for generating realistic speaking and listening facial expressions for avatars, using dual-track audio as input. It addresses the challenge of generating natural listener behavior by using a two-stage training process that first learns an internal motion prior then modulates it with audio cues.",
        "tldr_zh": "UniLS是一个新颖的端到端框架，用于生成逼真说话和听取面部表情的虚拟化身，使用双音轨音频作为输入。它通过使用两阶段训练过程来解决生成自然听众行为的挑战，该过程首先学习内部运动先验，然后使用音频提示来调节它。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "生成栩栩如生的对话化身不仅需要对孤立的说话者进行建模，还需要对说话和倾听之间的动态、互惠互动进行建模。然而，对倾听者进行建模极具挑战性：直接的音频驱动训练会失败，产生僵硬、静态的倾听动作。 这种失败源于一个根本的不平衡：说话者的动作很大程度上由语音音频驱动，而倾听者的动作主要遵循内部运动先验，并且仅受到外部语音的松散引导。 这一挑战导致大多数方法侧重于仅生成说话动作。 唯一尝试联合生成的方法依赖于额外的说话者动作来生成倾听者动作。 这种设计不是端到端的，从而妨碍了实时应用性。 为了解决这一局限性，我们提出了 UniLS，这是第一个用于生成统一的说话-倾听表情的端到端框架，仅由双声道音频驱动。 我们的方法引入了一种新颖的两阶段训练范式。 阶段 1 首先通过训练一个无音频的自回归生成器来学习内部运动先验，从而捕捉自然面部运动的自发动态。 阶段 2 随后引入双声道音频，微调生成器以根据外部语音线索调节学习到的运动先验。 大量的评估表明，UniLS 实现了最先进的说话准确率。 更重要的是，它在倾听指标方面提高了高达 44.1%，生成了更加多样化和自然的倾听表情。 这有效地缓解了僵硬问题，并为交互式数字人提供了一种实用、高保真的音频驱动解决方案。"
    },
    {
        "title": "Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving",
        "summary": "This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.",
        "url": "http://arxiv.org/abs/2512.09296v1",
        "published_date": "2025-12-10T03:46:57+00:00",
        "updated_date": "2025-12-10T03:46:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Songhan Wu"
        ],
        "tldr": "The paper proposes an improved YOLOv8n model (YOLOv8n-SPTS) for small target detection in autonomous driving, achieving state-of-the-art results on the VisDrone2019-DET dataset by incorporating space-to-depth convolution, a modified SPP module, and a triple-stage feature pyramid.",
        "tldr_zh": "该论文提出了一种改进的YOLOv8n模型（YOLOv8n-SPTS），用于自动驾驶中的小目标检测。通过结合空间到深度卷积，改进的SPP模块和三阶段特征金字塔，在VisDrone2019-DET数据集上取得了最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文聚焦于自动驾驶中的关键问题：动态感知中的小目标识别。现有算法因小目标信息缺失、尺度不平衡以及遮挡等问题，检测性能较差。我们提出了一种改进的YOLOv8n-SPTS模型，通过三个关键创新，增强了对小型交通目标的检测精度：首先，优化特征提取模块。在YOLOv8n的Backbone Bottleneck结构中，使用空间到深度卷积（SPD-Conv）模块替代了4个传统卷积模块。该模块通过空间到深度转换保留细粒度信息，减少信息损失，并增强捕获低分辨率小目标特征的能力。其次，增强特征融合能力。引入空间金字塔池化-快速跨阶段局部连接（SPPFCSPC）模块来替代原始SPPF模块，整合了空间金字塔池化（SPP）的多尺度特征提取和跨阶段局部连接（CSP）的特征融合机制，从而提升模型对复杂场景的上下文理解和多尺度特征表达能力。第三，为小目标设计专用检测结构。提出了一种三阶段特征金字塔（TSFP）结构，在原始检测头的基础上增加了一个160*160的小目标检测头，以充分利用浅层的高分辨率特征；同时，移除冗余的大目标检测头，以平衡计算效率。在VisDrone2019-DET数据集上的对比实验表明，YOLOv8n-SPTS模型在精度（61.9%）、召回率（48.3%）、mAP@0.5（52.6%）和mAP@0.5:0.95（32.6%）方面均排名第一。可视化结果验证了在遮挡和密集场景下，对行人、自行车等小目标的漏检率显著降低。"
    },
    {
        "title": "LoGoColor: Local-Global 3D Colorization for 360° Scenes",
        "summary": "Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.",
        "url": "http://arxiv.org/abs/2512.09278v1",
        "published_date": "2025-12-10T03:03:38+00:00",
        "updated_date": "2025-12-10T03:03:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yeonjin Chang",
            "Juhwan Cho",
            "Seunghyeon Seo",
            "Wonsik Shin",
            "Nojun Kwak"
        ],
        "tldr": "The paper introduces LoGoColor, a local-global approach using a fine-tuned multi-view diffusion model for 3D colorization of 360° scenes, aiming to preserve color diversity and multi-view consistency, outperforming existing methods.",
        "tldr_zh": "该论文介绍了LoGoColor，一种使用微调的多视角扩散模型的局部-全局方法，用于360°场景的3D着色，旨在保持颜色多样性和多视角一致性，优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "单通道3D重建广泛应用于机器人和医疗成像等领域。尽管该研究方向擅长重建3D几何结构，但输出结果并非彩色3D模型，因此需要进行3D着色以进行可视化。近期的3D着色研究通过提炼2D图像着色模型来解决这个问题。然而，这些方法存在2D图像模型的固有不一致性。这导致训练过程中颜色被平均化，从而产生单调且过度简化的结果，尤其是在复杂的360°场景中。与此相反，我们的目标是通过生成一组一致着色的新训练视角来保持颜色多样性，从而绕过平均过程。然而，消除平均过程引入了一个新的挑战：确保这些着色视角之间严格的多视角一致性。为了实现这一目标，我们提出了LoGoColor，一种旨在通过“局部-全局”方法，消除这种指导性平均过程，从而保持颜色多样性的流程：我们将场景划分为子场景，并使用微调的多视角扩散模型显式地处理子场景间和子场景内的颜色一致性。我们证明，与现有方法相比，我们的方法可以在复杂的360°场景中实现定量和定性上更一致和更合理的3D着色，并通过一种新颖的颜色多样性指数验证了其卓越的颜色多样性。"
    },
    {
        "title": "TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers",
        "summary": "Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyDéjàVu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyDéjàVu as open source, and we perform reproducible benchmarks on hardware. We show that TinyDéjàVu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.",
        "url": "http://arxiv.org/abs/2512.09786v1",
        "published_date": "2025-12-10T16:07:17+00:00",
        "updated_date": "2025-12-10T16:07:17+00:00",
        "categories": [
            "cs.LG",
            "cs.PF",
            "cs.SD",
            "eess.AS",
            "eess.SP"
        ],
        "authors": [
            "Zhaolan Huang",
            "Emmanuel Baccelli"
        ],
        "tldr": "TinyDéjàVu is a framework optimizing data flows in tiny ML models on microcontrollers, achieving significant RAM reduction and compute elimination for sensor data streams.",
        "tldr_zh": "TinyDéjàVu是一个优化微控制器上小型机器学习模型数据流的框架，可在使用传感器数据流时显著减少RAM使用量并消除计算冗余。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "摘要：\n常时开启传感器正日益被期望搭载各种微型神经网络，并对其感知到的数据时间序列进行持续推理。为了满足电池供电环境下的寿命和能耗要求，此类硬件通常采用具有极小内存预算（例如128kB RAM）的微控制器（MCU）。在此背景下，优化跨神经网络层的数据流变得至关重要。在本文中，我们介绍TinyDéjàVu，这是一个新的框架和我们设计的创新算法，旨在大幅减少在典型微控制器硬件上使用各种微型机器学习模型对传感器数据时间序列进行推理所需的RAM占用空间。我们将TinyDéjàVu的实现开源发布，并对硬件进行了可复现的基准测试。结果表明，TinyDéjàVu可以节省超过60%的RAM使用量，并消除高达90%的重叠滑动窗口输入上的冗余计算。"
    },
    {
        "title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration",
        "summary": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.",
        "url": "http://arxiv.org/abs/2512.09833v1",
        "published_date": "2025-12-10T17:13:24+00:00",
        "updated_date": "2025-12-10T17:13:24+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Elias Krantz",
            "Ngai Nam Chan",
            "Gunnar Tibert",
            "Huina Mao",
            "Christer Fuglesang"
        ],
        "tldr": "This paper introduces a ROS 2 bridge for the Basilisk astrodynamics simulator, enabling real-time data exchange and hardware-in-the-loop testing for spacecraft autonomy, demonstrated with a formation flying scenario.",
        "tldr_zh": "该论文介绍了一个用于 Basilisk 天体动力学模拟器的 ROS 2 桥，实现了实时数据交换和航天器自主的硬件在环测试，并通过编队飞行场景进行了演示。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "将高保真航天器仿真器与模块化机器人框架集成仍然是自主开发面临的挑战。本文提出了一种轻量级、开源的通信桥梁，连接了Basilisk天体动力学仿真器和机器人操作系统2（ROS 2），实现了用于航天器控制的实时双向数据交换。该桥梁无需修改Basilisk的核心，并能与ROS 2节点无缝集成。我们通过一个领队-跟随者编队飞行场景来展示它的应用，该场景使用了非线性模型预测控制，并在仿真和ATMOS平面微重力测试平台上进行了相同的部署。该设置支持快速开发、硬件在环测试，以及从仿真到硬件的无缝过渡。该桥梁为模块化航天器自主性和可重复研究工作流程提供了一个灵活且可扩展的平台。"
    },
    {
        "title": "Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances",
        "summary": "Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\\mathbb{R}^3)^2\\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.",
        "url": "http://arxiv.org/abs/2512.09377v1",
        "published_date": "2025-12-10T07:17:04+00:00",
        "updated_date": "2025-12-10T07:17:04+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Lidan Xu",
            "Dadong Fan",
            "Junhong Wang",
            "Wenshuo Li",
            "Hao Lu",
            "Jianzhong Qiao"
        ],
        "tldr": "This paper proves the observability of a two-drone-bar system under multi-source disturbances using only drone odometry, and develops a composite disturbance filtering scheme for state and disturbance estimation.",
        "tldr_zh": "该论文证明了双无人机悬挂杆系统在多源干扰下仅使用无人机里程计即可实现可观测性，并开发了一种用于状态和干扰估计的复合干扰滤波方案。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "合作式悬挂空中运输极易受到诸如气动效应和推力不确定性等多种来源的干扰。为了实现精确的负载操控，现有方法通常依赖额外的传感器来测量缆绳方向或负载的姿态，这增加了系统成本和复杂性。一个根本性的问题依然存在：仅使用无人机的里程计信息，在多源干扰下负载的姿态是否可观测？为了解答这个问题，本文聚焦于双无人机-杆系统，并通过使用可观测性秩判据证明，当仅存在两种或两种以下类型的集总干扰时，整个系统是可观测的。据我们所知，我们是第一个提出这一结论的人，这一结果为通过最小化传感器套件来实现更具成本效益且更鲁棒的系统铺平了道路。接下来，为了验证该分析，我们考虑仅在无人机上施加干扰的情况，并开发了一种复合干扰滤波方案。设计了一个基于干扰观测器的误差状态扩展卡尔曼滤波器，用于状态和干扰估计，这提高了整个系统在流形$(\\mathbb{R}^3)^2\\times(TS^2)^3$上演化时的估计性能。我们的仿真和实验测试验证了仅使用无人机的里程计信息完全估计系统的状态和干扰是可行的。"
    },
    {
        "title": "MPC for momentum counter-balanced and zero-impulse contact with a free-spinning satellite",
        "summary": "In on-orbit robotics, a servicer satellite's ability to make contact with a free-spinning target satellite is essential to completing most on-orbit servicing (OOS) tasks. This manuscript develops a nonlinear model predictive control (MPC) framework that generates feasible controls for a servicer satellite to achieve zero-impulse contact with a free-spinning target satellite. The overall maneuver requires coordination between two separately actuated modules of the servicer satellite: (1) a moment generation module and (2) a manipulation module. We apply MPC to control both modules by explicitly modeling the cross-coupling dynamics between them. We demonstrate that the MPC controller can enforce actuation and state constraints that prior control approaches could not account for. We evaluate the performance of the MPC controller by simulating zero-impulse contact scenarios with a free-spinning target satellite via numerical Monte Carlo (MC) trials and comparing the simulation results with prior control approaches. Our simulation results validate the effectiveness of the MPC controller in maintaining spin synchronization and zero-impulse contact under operation constraints, moving contact location, and observation and actuation noise.",
        "url": "http://arxiv.org/abs/2512.09213v1",
        "published_date": "2025-12-10T00:53:10+00:00",
        "updated_date": "2025-12-10T00:53:10+00:00",
        "categories": [
            "eess.SY",
            "cs.RO"
        ],
        "authors": [
            "Theofania Karampela",
            "Rishie Seshadri",
            "Florian Dörfler",
            "Sarah H. Q. Li"
        ],
        "tldr": "This paper presents a nonlinear Model Predictive Control (MPC) framework for a servicer satellite to achieve zero-impulse contact with a free-spinning target satellite, demonstrating its effectiveness through simulations and comparisons with prior approaches.",
        "tldr_zh": "本文提出了一种非线性模型预测控制（MPC）框架，用于服务卫星实现与自由旋转目标卫星的零冲量接触，并通过仿真和与先前方法的比较证明了其有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "在轨机器人领域中，服务卫星与自由旋转的目标卫星建立接触的能力是完成大多数在轨服务 (OOS) 任务的关键。本文开发了一种非线性模型预测控制 (MPC) 框架，该框架为服务卫星生成可行的控制，以实现与自由旋转目标卫星的零冲量接触。整个机动要求服务卫星的两个独立驱动模块之间进行协调：（1）力矩生成模块和（2）操作模块。我们应用 MPC 来控制这两个模块，并显式建模它们之间的交叉耦合动力学。我们证明了 MPC 控制器可以强制执行先前控制方法无法考虑的驱动和状态约束。我们通过数值蒙特卡洛 (MC) 试验来模拟与自由旋转目标卫星的零冲量接触场景，并与先前的控制方法比较了仿真结果，从而评估了 MPC 控制器的性能。我们的仿真结果验证了 MPC 控制器在操作约束、移动接触位置以及观察和驱动噪声下，维持自旋同步和零冲量接触的有效性。"
    },
    {
        "title": "Ethics Readiness of Artificial Intelligence: A Practical Evaluation Method",
        "summary": "We present Ethics Readiness Levels (ERLs), a four-level, iterative method to track how ethical reflection is implemented in the design of AI systems. ERLs bridge high-level ethical principles and everyday engineering by turning ethical values into concrete prompts, checks, and controls within real use cases. The evaluation is conducted using a dynamic, tree-like questionnaire built from context-specific indicators, ensuring relevance to the technology and application domain. Beyond being a managerial tool, ERLs help facilitate a structured dialogue between ethics experts and technical teams, while our scoring system helps track progress over time. We demonstrate the methodology through two case studies: an AI facial sketch generator for law enforcement and a collaborative industrial robot. The ERL tool effectively catalyzes concrete design changes and promotes a shift from narrow technological solutionism to a more reflective, ethics-by-design mindset.",
        "url": "http://arxiv.org/abs/2512.09729v1",
        "published_date": "2025-12-10T15:10:42+00:00",
        "updated_date": "2025-12-10T15:10:42+00:00",
        "categories": [
            "cs.CY",
            "cs.AI"
        ],
        "authors": [
            "Laurynas Adomaitis",
            "Vincent Israel-Jost",
            "Alexei Grinbaum"
        ],
        "tldr": "This paper introduces Ethics Readiness Levels (ERLs), a four-level method to integrate ethical considerations into AI system design, demonstrated through case studies. It seems targeted towards practical application of ethics in AI rather than developing novel AI models.",
        "tldr_zh": "本文介绍了一种名为伦理准备度级别（ERLs）的四级方法，旨在将伦理考量融入人工智能系统设计中，并通过案例研究进行了演示。它似乎侧重于伦理在人工智能中的实际应用，而不是开发新的人工智能模型。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "我们提出了伦理准备度等级（ERLs），这是一种四级迭代方法，用于跟踪伦理反思如何在人工智能系统设计中实施。ERLs通过将伦理价值观转化为实际用例中的具体提示、检查和控制，从而连接了高层次的伦理原则和日常工程实践。评估采用基于上下文特定指标构建的动态树状问卷进行，确保与技术和应用领域的相关性。除了作为一种管理工具之外，ERLs还有助于促进伦理专家和技术团队之间结构化的对话，同时我们的评分系统有助于跟踪随时间推移的进展。我们通过两个案例研究展示了该方法：一个用于执法的AI人脸素描生成器和一个协作型工业机器人。ERL工具有效地催化了具体的设计变更，并促进了一种从狭隘的技术解决方案至上主义向更具反思性的、伦理融入设计的思维模式转变。"
    },
    {
        "title": "Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation",
        "summary": "Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.",
        "url": "http://arxiv.org/abs/2512.09134v1",
        "published_date": "2025-12-09T21:26:45+00:00",
        "updated_date": "2025-12-09T21:26:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Georgy Kopanitsa",
            "Oleg Metsker",
            "Alexey Yakovlev"
        ],
        "tldr": "The paper presents AngioAI-QFR, an automated pipeline for coronary angiography that integrates deep learning, functional profiling, and virtual PCI, achieving strong correlation with invasive FFR and showing promise for near real-time clinical use.",
        "tldr_zh": "该论文介绍了一种名为 AngioAI-QFR 的冠状动脉血管造影自动化流程，该流程集成了深度学习、功能分析和虚拟 PCI，与侵入性 FFR 具有很强的相关性，并展现了近实时临床应用的潜力。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "冠状动脉造影是评估冠状动脉疾病的主要手段，但视觉评估狭窄程度存在差异，且与缺血的相关性仅为中等。基于导丝的血流储备分数(FFR)可提高病变选择的准确性，但未得到系统应用。诸如定量血流比率(QFR)等基于血管造影的指标提供了无导丝生理学评估，然而许多工具工作流程繁琐，且与自动解剖分析和虚拟PCI规划相分离。我们开发了AngioAI-QFR，这是一个端到端的仅血管造影流程，它结合了深度学习狭窄检测、管腔分割、中心线和直径提取、每毫米相对血流容量(RFC)剖析以及虚拟支架植入，并自动重新计算基于血管造影的QFR。该系统在100条连续血管中进行了评估，以侵入性FFR作为参考。主要终点是与FFR的一致性（相关性、平均绝对误差）和FFR <= 0.80的诊断性能。在预留帧上，狭窄检测的精确度达到0.97，管腔分割的Dice系数为0.78。在100条血管中，AngioAI-QFR与FFR高度相关（r = 0.89，MAE 0.045）。用于检测FFR <= 0.80的AUC为0.93，灵敏度为0.88，特异性为0.86。该流程在93%的血管中完全自动完成，结果中位时间为41秒。RFC剖析区分了局灶性和弥漫性容量损失，虚拟支架植入预测局灶性疾病的QFR增益大于弥漫性疾病。AngioAI-QFR提供了一个实用、近实时的流程，它统一了计算机视觉、功能剖析和虚拟PCI与自动血管造影衍生的生理学信息。"
    },
    {
        "title": "SynthPix: A lightspeed PIV images generator",
        "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.",
        "url": "http://arxiv.org/abs/2512.09664v1",
        "published_date": "2025-12-10T14:08:42+00:00",
        "updated_date": "2025-12-10T14:08:42+00:00",
        "categories": [
            "cs.DC",
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Antonio Terpin",
            "Alan Bonomi",
            "Francesco Banelli",
            "Raffaello D'Andrea"
        ],
        "tldr": "SynthPix is a JAX-based synthetic PIV image generator that's significantly faster than existing tools, enabling faster development and training of RL-based flow estimation methods which might indirectly be useful in some RL robotics applications.",
        "tldr_zh": "SynthPix是一个基于JAX的合成粒子图像测速（PIV）图像生成器，速度远快于现有工具，从而能够更快地开发和训练基于强化学习的流体估计方法，这可能间接有助于某些强化学习机器人应用。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "我们描述了SynthPix，一个用于粒子图像测速（PIV）的合成图像生成器，重点关注在加速器上的性能和并行性，采用JAX实现。 SynthPix支持与现有工具相同的配置参数，但在每秒图像对生成方面的吞吐量提高了几个数量级。 开发SynthPix是为了能够训练数据密集型的强化学习方法，用于流动估计，并缩短在开发快速流动估计方法时的迭代时间，这些方法用于最近的具有实时PIV反馈的主动流体控制研究。 我们相信SynthPix对流体动力学界是有用的，并且在本文中，我们描述了这个软件包背后的主要思想。"
    },
    {
        "title": "Masked Registration and Autoencoding of CT Images for Predictive Tibia Reconstruction",
        "summary": "Surgical planning for complex tibial fractures can be challenging for surgeons, as the 3D structure of the later desirable bone alignment may be diffi- cult to imagine. To assist in such planning, we address the challenge of predicting a patient-specific reconstruction target from a CT of the fractured tibia. Our ap- proach combines neural registration and autoencoder models. Specifically, we first train a modified spatial transformer network (STN) to register a raw CT to a standardized coordinate system of a jointly trained tibia prototype. Subsequently, various autoencoder (AE) architectures are trained to model healthy tibial varia- tions. Both the STN and AE models are further designed to be robust to masked input, allowing us to apply them to fractured CTs and decode to a prediction of the patient-specific healthy bone in standard coordinates. Our contributions include: i) a 3D-adapted STN for global spatial registration, ii) a comparative analysis of AEs for bone CT modeling, and iii) the extension of both to handle masked inputs for predictive generation of healthy bone structures. Project page: https://github.com/HongyouZhou/repair",
        "url": "http://arxiv.org/abs/2512.09525v1",
        "published_date": "2025-12-10T11:04:28+00:00",
        "updated_date": "2025-12-10T11:04:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongyou Zhou",
            "Cederic Aßmann",
            "Alaa Bejaoui",
            "Heiko Tzschätzsch",
            "Mark Heyland",
            "Julian Zierke",
            "Niklas Tuttle",
            "Sebastian Hölzl",
            "Timo Auer",
            "David A. Back",
            "Marc Toussaint"
        ],
        "tldr": "The paper presents a method to predict patient-specific healthy tibia structure from fractured CT scans using a masked spatial transformer network for registration and autoencoders for modeling healthy bone variations.",
        "tldr_zh": "该论文提出了一种方法，利用Masked空间变换网络进行配准，利用自编码器对健康骨骼变异进行建模，从而从骨折CT扫描中预测特定患者的健康胫骨结构。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "复杂胫骨骨折的手术规划对外科医生来说可能具有挑战性，因为后期期望的骨骼对齐的三维结构难以想象。为了辅助此类规划，我们致力于解决从骨折胫骨的CT图像中预测患者特定重建目标这一难题。我们的方法结合了神经配准和自编码器模型。具体来说，我们首先训练一个改进的空间变换网络(STN)，将原始CT图像配准到一个联合训练的胫骨原型的标准化坐标系中。随后，训练各种自编码器(AE)架构来建模健康的胫骨变异。STN和AE模型都被进一步设计为对掩蔽输入具有鲁棒性，允许我们将其应用于骨折的CT图像，并解码为标准坐标系中患者特定健康骨骼的预测结果。我们的贡献包括：i) 一个用于全局空间配准的3D自适应STN，ii) 用于骨CT建模的AE的比较分析，以及iii) 两者到处理掩蔽输入的扩展，用于健康骨骼结构的预测性生成。项目主页：https://github.com/HongyouZhou/repair"
    },
    {
        "title": "Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video",
        "summary": "Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.",
        "url": "http://arxiv.org/abs/2512.09335v2",
        "published_date": "2025-12-10T05:51:59+00:00",
        "updated_date": "2025-12-11T04:18:41+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Seonghwa Choi",
            "Moonkyeong Choi",
            "Mingyu Jang",
            "Jaekyung Kim",
            "Jianfei Cai",
            "Wen-Huang Cheng",
            "Sanghoon Lee"
        ],
        "tldr": "The paper introduces RnD-Avatar, a 3D Gaussian Splatting framework for creating relightable and dynamic human avatars from monocular video, achieving SOTA results in novel view synthesis, pose rendering, and relighting.",
        "tldr_zh": "该论文介绍了 RnD-Avatar，一个基于 3D 高斯溅射的框架，用于从单目视频创建可重新光照和动态的人类化身，并在新视角合成、姿势渲染和重新光照方面取得了 SOTA 结果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "摘要：\n从单目视频中建模可重光照且可动画的人类化身是一个长期存在的挑战性任务。近年来，神经辐射场 (NeRF) 和 3D 高斯溅射 (3DGS) 方法已被用于重建化身。然而，由于与身体运动相关的不足几何细节，例如服装褶皱，它们通常产生令人不满意的照片级真实感效果。在本文中，我们提出了一种基于 3DGS 的人类化身建模框架，称为可重光照和动态高斯化身 (RnD-Avatar)，该框架可以呈现精确的姿势变体形变，从而实现高保真度的几何细节。为了实现这一点，我们引入了动态蒙皮权重，该权重定义了基于姿势的人类化身关节运动，同时学习了由身体运动引起的额外形变。我们还引入了一种新的正则化方法，以在稀疏视觉线索下捕捉精细的几何细节。此外，我们提出了一个新的多视图数据集，其中包含不同的光照条件，用于评估重光照效果。我们的框架能够对新姿势和视图进行逼真的渲染，同时支持任意光照条件下的照片级真实感光照效果。我们的方法在新视角合成、新姿势渲染和重光照方面达到了最先进的性能。"
    },
    {
        "title": "GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars",
        "summary": "Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.",
        "url": "http://arxiv.org/abs/2512.09162v1",
        "published_date": "2025-12-09T22:19:28+00:00",
        "updated_date": "2025-12-09T22:19:28+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Kelian Baert",
            "Mae Younes",
            "Francois Bourel",
            "Marc Christie",
            "Adnane Boukhayma"
        ],
        "tldr": "The paper introduces GTAvatar, a method combining Gaussian Splatting and UV texture mapping for creating relightable and editable head avatars from monocular video, offering improved editability compared to pure Gaussian Splatting approaches.",
        "tldr_zh": "该论文介绍了 GTAvatar，一种结合高斯溅射和 UV 纹理映射的方法，用于从单目视频创建可重新照明和编辑的头部头像，与纯高斯溅射方法相比，提高了可编辑性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "高斯溅射技术的最新进展使得越来越精确的逼真头部头像重建成为可能，为视觉特效、视频会议和虚拟现实等众多应用打开了大门。然而，这也带来了一个问题，即缺乏传统基于三角形网格的方法所提供的直观可编辑性。与此相反，我们提出了一种将二维高斯溅射的准确性和保真度与UV纹理映射的直观性相结合的方法。通过将每个规范高斯图元的局部坐标系以计算高效的方式嵌入到模板网格的UV空间中的一个贴片中，我们从单个单目视频中在常规UV域上重建连续可编辑的材质头部纹理。此外，我们利用高效的基于物理的反射模型来实现这些固有材质贴图的重新光照和编辑。通过与最先进的方法进行广泛的比较，我们证明了我们重建的准确性、重新光照结果的质量，以及通过纹理映射提供直观控制以修改头像外观和几何形状的能力，而无需额外的优化。"
    },
    {
        "title": "Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks",
        "summary": "The rise of space AI is reshaping government and industry through applications such as disaster detection, border surveillance, and climate monitoring, powered by massive data from commercial and governmental low Earth orbit (LEO) satellites. Federated satellite learning (FSL) enables joint model training without sharing raw data, but suffers from slow convergence due to intermittent connectivity and introduces critical trust challenges--where biased or falsified updates can arise across satellite constellations, including those injected through cyberattacks on inter-satellite or satellite-ground communication links. We propose OrbitChain, a blockchain-backed framework that empowers trustworthy multi-vendor collaboration in LEO networks. OrbitChain (i) offloads consensus to high-altitude platforms (HAPs) with greater computational capacity, (ii) ensures transparent, auditable provenance of model updates from different orbits owned by different vendors, and (iii) prevents manipulated or incomplete contributions from affecting global FSL model aggregation. Extensive simulations show that OrbitChain reduces computational and communication overhead while improving privacy, security, and global model accuracy. Its permissioned proof-of-authority ledger finalizes over 1000 blocks with sub-second latency (0.16,s, 0.26,s, 0.35,s for 1-of-5, 3-of-5, and 5-of-5 quorums). Moreover, OrbitChain reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor, demonstrating its effectiveness for real-time, multi-vendor learning. Our code is available at https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git",
        "url": "http://arxiv.org/abs/2512.08882v1",
        "published_date": "2025-12-09T18:16:34+00:00",
        "updated_date": "2025-12-09T18:16:34+00:00",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Mohamed Elmahallawy",
            "Asma Jodeiri Akbarfam"
        ],
        "tldr": "The paper proposes OrbitChain, a blockchain-based framework for trustworthy federated learning across multi-vendor LEO satellite networks, leveraging high-altitude platforms for consensus and demonstrating improved convergence time and security.",
        "tldr_zh": "该论文提出了OrbitChain，一个基于区块链的框架，用于在多供应商的低地球轨道卫星网络中进行可信的联邦学习，利用高空平台进行共识，并展示了改进的收敛时间和安全性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "太空人工智能的兴起正在重塑政府和行业，通过灾害检测、边境监控和气候监测等应用，而这些应用由商业和政府低地球轨道（LEO）卫星产生的大量数据驱动。联邦卫星学习（FSL）无需共享原始数据即可实现联合模型训练，但由于间歇性连接而导致收敛缓慢，并引入了关键的信任挑战——偏差或伪造的更新可能会出现在卫星星座中，包括通过对星间或星地通信链路的网络攻击注入的更新。我们提出了OrbitChain，一种基于区块链的框架，旨在赋能LEO网络中值得信赖的多供应商协作。OrbitChain（i）将共识计算卸载到具有更强计算能力的高空平台（HAPs），（ii）确保来自不同供应商拥有的不同轨道模型的更新具有透明、可审计的出处，以及（iii）防止被操纵或不完整的贡献影响全局FSL模型聚合。大量仿真表明，OrbitChain在提高隐私性、安全性和全局模型准确性的同时，降低了计算和通信开销。其许可型权威证明账本可以在亚秒级延迟内完成1000多个区块的最终确认（1/5、3/5和5/5法定人数分别为0.16秒、0.26秒和0.35秒）。此外，与单供应商相比，OrbitChain在真实卫星数据集上将收敛时间缩短了多达30个小时，证明了其在实时、多供应商学习方面的有效性。我们的代码可在https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git上获取。"
    }
]