[
    {
        "title": "Dexterous World Models",
        "summary": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.\n  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.\n  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.",
        "url": "http://arxiv.org/abs/2512.17907v1",
        "published_date": "2025-12-19T18:59:51+00:00",
        "updated_date": "2025-12-19T18:59:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Byungjun Kim",
            "Taeksoo Kim",
            "Junyoung Lee",
            "Hanbyul Joo"
        ],
        "tldr": "The paper introduces Dexterous World Model (DWM), a video diffusion framework conditioned on static 3D scenes and egocentric hand motions to generate realistic videos of human-scene interactions, enabling embodied simulation in digital twins.",
        "tldr_zh": "这篇论文介绍了灵巧世界模型（DWM），一个以静态3D场景和以自我为中心的手部动作为条件的视频扩散框架，用于生成逼真的人与场景交互视频，从而在数字孪生中实现具身模拟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "3D重建技术的最新进展使得从日常环境中创建逼真的数字孪生变得容易。然而，当前的数字孪生在很大程度上仍然是静态的，并且仅限于导航和视图合成，缺乏具身交互性。为了弥合这一差距，我们引入了灵巧世界模型（DWM），这是一个场景-动作条件视频Diffusion框架，可以对灵巧的人类动作如何在静态3D场景中引起动态变化进行建模。\n\n  给定一个静态3D场景渲染图和一个以自我为中心的手部运动序列，DWM可以生成时间上连贯的视频，描绘合理的人类-场景交互。我们的方法将视频生成基于：（1）遵循指定相机轨迹的静态场景渲染，以确保空间一致性；以及（2）以自我为中心的手部网格渲染，该渲染编码了几何形状和运动线索，以直接模拟动作条件下的动力学。为了训练DWM，我们构建了一个混合交互视频数据集。合成的以自我为中心的交互为联合的运动和操作学习提供了完全对齐的监督，而固定相机的真实世界视频则贡献了多样且逼真的物体动力学。\n\n  实验表明，DWM能够实现逼真且符合物理规律的交互，例如抓取、打开和移动物体，同时保持相机和场景的一致性。该框架代表了基于视频Diffusion的交互式数字孪生的第一步，并且能够从以自我为中心的动作中进行具身模拟。"
    },
    {
        "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
        "summary": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/",
        "url": "http://arxiv.org/abs/2512.17900v1",
        "published_date": "2025-12-19T18:59:02+00:00",
        "updated_date": "2025-12-19T18:59:02+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Vongani H. Maluleke",
            "Kie Horiuchi",
            "Lea Wilken",
            "Evonne Ng",
            "Jitendra Malik",
            "Angjoo Kanazawa"
        ],
        "tldr": "The paper introduces MAGNet, a diffusion-based framework for generating multi-agent interaction sequences, achieving strong performance across various interaction types and group sizes.",
        "tldr_zh": "该论文介绍了MAGNet，一个基于扩散模型的框架，用于生成多智能体交互序列，在各种交互类型和群体规模上都表现出色。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "理解和生成多人互动是机器人技术和社会计算领域的一项基础性挑战，具有广泛的意义。虽然人类在群体中自然地进行协调，但由于时间跨度长、主体间依赖性强以及群体规模可变，对这种互动进行建模仍然很困难。现有的运动生成方法大多是针对特定任务的，不能推广到灵活的多主体生成。我们提出了MAGNet（多主体扩散强制Transformer），这是一个统一的自回归扩散框架，用于多主体运动生成，通过灵活的条件设定和采样，支持广泛的互动任务。MAGNet在单个模型中执行二元预测、伙伴补全和完整的多主体运动生成，并且可以自回归地生成跨越数百帧的超长序列。在扩散强制的基础上，我们引入了关键的修改，从而在自回归去噪过程中显式地对主体间的耦合进行建模，从而实现主体之间的连贯协同。因此，MAGNet能够捕捉到紧密同步的活动（如跳舞、拳击）和松散结构的社交互动。我们的方法在二元基准测试中与专门方法相当，同时自然地扩展到涉及三个或更多互动人员的多元情景，这得益于一个与主体数量无关的可扩展架构。我们建议读者观看补充视频，在那里，生成的互动的时间动态和空间协调能够得到最佳的欣赏。项目页面: https://von31.github.io/MAGNet/"
    },
    {
        "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
        "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
        "url": "http://arxiv.org/abs/2512.17897v1",
        "published_date": "2025-12-19T18:57:33+00:00",
        "updated_date": "2025-12-19T18:57:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Tomer Borreda",
            "Fangqiang Ding",
            "Sanja Fidler",
            "Shengyu Huang",
            "Or Litany"
        ],
        "tldr": "RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera imagery, incorporating visual cues to guide the generation process.",
        "tldr_zh": "RadarGen是一个扩散模型，可以从多视角的相机图像中生成逼真的汽车雷达点云，并结合视觉线索来指导生成过程。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们提出了RadarGen，一种用于从多视角相机图像合成逼真车载雷达点云的扩散模型。RadarGen通过将雷达测量结果表示为鸟瞰图形式，使其能够将高效的图像潜在扩散适配到雷达领域，该鸟瞰图编码了空间结构以及雷达横截面（RCS）和多普勒属性。一个轻量级的恢复步骤从生成的地图中重建点云。为了更好地将生成与视觉场景对齐，RadarGen结合了从预训练基础模型中提取的BEV对齐的深度、语义和运动线索，这些线索引导随机生成过程产生物理上合理的雷达模式。原则上，以图像为条件使得该方法与现有的视觉数据集和仿真框架广泛兼容，为跨模态生成式仿真的可扩展方向提供了思路。在大型驾驶数据上的评估表明，RadarGen捕获了雷达测量分布的特征，并缩小了与在真实数据上训练的感知模型之间的差距，标志着朝着跨传感模态的统一生成式仿真迈进了一步。"
    },
    {
        "title": "Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy",
        "summary": "Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \\ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \\textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.",
        "url": "http://arxiv.org/abs/2512.17899v1",
        "published_date": "2025-12-19T18:58:11+00:00",
        "updated_date": "2025-12-19T18:58:11+00:00",
        "categories": [
            "eess.SY",
            "cs.LG"
        ],
        "authors": [
            "Aditya Gahlawat",
            "Ahmed Aboudonia",
            "Sandeep Banik",
            "Naira Hovakimyan",
            "Nikolai Matni",
            "Aaron D. Ames",
            "Gioele Zardini",
            "Alberto Speranzon"
        ],
        "tldr": "This paper introduces Distributionally Robust Imitation Policy (DRIP), a layered control architecture combining Taylor Series Imitation Learning (TaSIL) and $\\mathcal{L}_1$-Distributionally Robust Adaptive Control (\\ellonedrac) for certifiable autonomous systems by mitigating distribution shifts in imitation learning.",
        "tldr_zh": "本文介绍了分布鲁棒模仿策略(DRIP)，一种分层控制架构，结合了泰勒级数模仿学习(TaSIL)和$\\mathcal{L}_1$-分布鲁棒自适应控制(\\ellonedrac)，通过减少模仿学习中的分布偏移，实现可验证的自主系统。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "模仿学习（IL）通过学习专家演示来实现自主行为。相比于诸如强化学习等其他替代方法，模仿学习具有更高的样本效率，但其对由分布偏移引起的累积误差非常敏感。在使用基于模仿学习的反馈律作用于系统时，存在两个显著的分布偏移来源：由策略误差引起的分布偏移，以及由于外生扰动和因缺乏学习而产生的内生模型误差引起的分布偏移。我们之前开发的泰勒级数模仿学习（TaSIL）和 $\\mathcal{L}_1$ -分布鲁棒自适应控制（\\ellonedrac）以互补的方式解决了分布偏移的挑战。TaSIL提供了对策略误差引起的分布偏移的鲁棒性，而\\ellonedrac 提供了对由偶然不确定性和认知不确定性引起的分布偏移的鲁棒性。为了实现针对已学习和/或不确定动力系统的可认证模仿学习，我们构建了\\textit{分布鲁棒模仿策略（DRIP）}架构，一种集成了TaSIL和 \\ellonedrac 的分层控制架构（LCA）。通过谨慎设计以各层为中心输入和输出要求，我们展示了如何保证整个控制流程的可认证性。我们的解决方案为设计完全可认证的自主系统流程铺平了道路，通过提出的分层控制架构方法，将基于学习的组件（如感知）与可认证的基于模型的决策相结合。"
    }
]