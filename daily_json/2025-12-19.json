[
    {
        "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future",
        "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.",
        "url": "http://arxiv.org/abs/2512.16760v1",
        "published_date": "2025-12-18T16:57:44+00:00",
        "updated_date": "2025-12-18T16:57:44+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Tianshuai Hu",
            "Xiaolu Liu",
            "Song Wang",
            "Yiyao Zhu",
            "Ao Liang",
            "Lingdong Kong",
            "Guoyang Zhao",
            "Zeying Gong",
            "Jun Cen",
            "Zhiyu Huang",
            "Xiaoshuai Hao",
            "Linfeng Li",
            "Hang Song",
            "Xiangtai Li",
            "Jun Ma",
            "Shaojie Shen",
            "Jianke Zhu",
            "Dacheng Tao",
            "Ziwei Liu",
            "Junwei Liang"
        ],
        "tldr": "This paper provides a structured overview of Vision-Language-Action (VLA) frameworks for autonomous driving, categorizing existing methods and highlighting future research directions. It traces the evolution from Vision-Action models to modern VLA approaches and identifies key challenges.",
        "tldr_zh": "本文综述了用于自动驾驶的视觉-语言-动作 (VLA) 框架，对现有方法进行了分类，并强调了未来的研究方向。它追溯了从视觉-动作模型到现代 VLA 方法的演变，并确定了关键挑战。",
        "relevance_score": 10,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "自动驾驶长期以来依赖于模块化的“感知-决策-行动”流水线，但手工设计的接口和基于规则的组件在复杂或长尾场景中常常失效。其级联设计进一步传播感知误差，从而降低下游规划和控制的性能。视觉-行动（VA）模型通过学习从视觉输入到行动的直接映射来解决一些局限性，但它们仍然不透明，对分布偏移敏感，并且缺乏结构化推理或指令遵循能力。大型语言模型（LLM）和多模态学习的最新进展推动了视觉-语言-行动（VLA）框架的出现，该框架将感知与基于语言的决策相结合。通过统一视觉理解、语言推理和可执行的输出，VLA为更具可解释性、通用性和符合人类习惯的驾驶策略提供了一条途径。本文对新兴的自动驾驶VLA领域进行了结构化的描述。我们追溯了从早期VA方法到现代VLA框架的演变，并将现有方法组织成两种主要范式：端到端VLA，它将感知、推理和规划集成在一个模型中；双系统VLA，它将缓慢的深思熟虑（通过VLM）与快速、安全关键的执行（通过规划器）分离。在这些范式中，我们进一步区分了诸如文本与数值行动生成器以及显式与隐式指导机制等子类。我们还总结了用于评估基于VLA的驾驶系统的代表性数据集和基准，并强调了关键挑战和开放方向，包括鲁棒性、可解释性和指令保真度。总的来说，这项工作旨在为推进人机兼容的自动驾驶系统奠定连贯的基础。"
    },
    {
        "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning",
        "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.",
        "url": "http://arxiv.org/abs/2512.16461v1",
        "published_date": "2025-12-18T12:27:06+00:00",
        "updated_date": "2025-12-18T12:27:06+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Tin Stribor Sohn",
            "Maximilian Dillitzer",
            "Jason J. Corso",
            "Eric Sax"
        ],
        "tldr": "SNOW is a training-free framework that unifies VLM-derived semantics with 3D point cloud geometry and temporal consistency to create a 4D scene graph for improved embodied reasoning and autonomous robotics.",
        "tldr_zh": "SNOW是一个无需训练的框架，它将VLM衍生的语义与3D点云几何和时间连贯性相统一，从而创建一个4D场景图，以改进具身推理和自主机器人技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "自主机器人系统需要对动态环境进行时空理解，以确保可靠的导航和交互。虽然视觉-语言模型（VLM）提供了开放世界的语义先验，但它们缺乏在3D几何和时间动态方面的基础。相反，几何感知能够捕捉结构和运动，但仍存在语义稀疏的问题。我们提出了SNOW（利用开放世界知识的场景理解），这是一个无需训练且与骨干网络无关的框架，用于统一的4D场景理解，它将VLM衍生的语义与点云几何和时间一致性相结合。SNOW处理同步的RGB图像和3D点云，使用HDBSCAN聚类生成对象级别的提议，以指导基于SAM2的分割。每个分割区域通过我们提出的时空令牌化块编码（STEP）进行编码，生成捕获局部语义、几何和时间属性的多模态令牌。这些令牌被逐步整合到4D场景图（4DSG）中，4DSG作为下游推理的4D先验。一个轻量级的SLAM后端在环境中对所有STEP令牌进行空间锚定，提供全局参考对齐，并确保随时间推移无歧义的空间基础。由此产生的4DSG形成了一个可查询的、统一的世界模型，VLM可以通过它直接解释空间场景结构和时间动态。在多样化基准数据集上的实验表明，SNOW能够实现精确的4D场景理解和空间基础推理，从而在多个设置中实现了最新的性能，突出了结构化4D先验对于具身推理和自主机器人的重要性。"
    },
    {
        "title": "INTELLECT-3: Technical Report",
        "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
        "url": "http://arxiv.org/abs/2512.16144v1",
        "published_date": "2025-12-18T03:57:01+00:00",
        "updated_date": "2025-12-18T03:57:01+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Prime Intellect Team",
            "Mika Senghaas",
            "Fares Obeid",
            "Sami Jaghouar",
            "William Brown",
            "Jack Min Ong",
            "Daniel Auras",
            "Matej Sirovatka",
            "Jannik Straube",
            "Andrew Baker",
            "Sebastian Müller",
            "Justus Mattern",
            "Manveer Basra",
            "Aiman Ismail",
            "Dominik Scherm",
            "Cooper Miller",
            "Ameen Patel",
            "Simon Kirsten",
            "Mario Sieg",
            "Christian Reetz",
            "Kemal Erdem",
            "Vincent Weisser",
            "Johannes Hagemann"
        ],
        "tldr": "INTELLECT-3 is a new 106B-parameter Mixture-of-Experts model trained with large-scale reinforcement learning, achieving SOTA performance on various benchmarks and releasing its infrastructure stack, including a new RL framework called prime-rl.",
        "tldr_zh": "INTELLECT-3是一个新的106B参数的混合专家模型，通过大规模强化学习训练，在多个基准测试上实现了SOTA性能，并发布了其基础设施堆栈，包括名为prime-rl的新型强化学习框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "我们提出 INTELLECT-3，一个具有 1060 亿参数的混合专家模型（激活 120 亿），它在我们端到端的强化学习基础设施堆栈上，通过大规模强化学习进行训练。INTELLECT-3 在数学、代码、科学和推理基准测试中，针对其模型的尺寸，实现了最先进的性能，超越了许多更大的前沿模型。我们将开源该模型以及用于创建它的完整基础设施堆栈，包括 RL 框架、完整配方以及大量环境，这些环境由 verifiers 库构建，用于在我们 Environments Hub 社区平台上进行训练和评估。为了这项工作，我们引入了 prime-rl，这是一个用于大规模异步强化学习的开放框架，可以从单个节点无缝扩展到数千个 GPU，并且专为具有一流的多轮交互和工具使用代理强化学习而定制。使用此堆栈，我们在 GLM-4.5-Air-Base 模型之上运行 SFT 和 RL 训练，将 RL 训练扩展到高达 512 个 H200，并具有很高的训练效率。"
    },
    {
        "title": "Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models",
        "summary": "Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.",
        "url": "http://arxiv.org/abs/2512.15957v1",
        "published_date": "2025-12-17T20:44:32+00:00",
        "updated_date": "2025-12-17T20:44:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Utsav Panchal",
            "Yuchen Liu",
            "Luigi Palmieri",
            "Ilche Georgievski",
            "Marco Aiello"
        ],
        "tldr": "This paper introduces CAMP-VLM, a VLM-based framework for predicting multi-human behaviors in scenes from a third-person perspective, leveraging contextual features and scene graphs, and demonstrating improved prediction accuracy through fine-tuning with synthetic data and DPO.",
        "tldr_zh": "本文介绍了一种基于视觉语言模型（VLM）的框架CAMP-VLM，用于从第三人称视角预测场景中的多人行为，利用上下文特征和场景图，并通过使用合成数据和直接偏好优化（DPO）进行微调，展示了更高的预测精度。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "准确预测人类行为对于在人群环境中运行的移动机器人至关重要。虽然先前的研究主要集中于从自我中心视角预测单人场景中的动作，但一些机器人应用需要从第三人称视角理解多个人类行为。为此，我们提出了CAMP-VLM（情境感知的多人行为预测）：一个基于视觉语言模型（VLM）的框架，该框架结合了来自视觉输入的情境特征和来自场景图的空间感知能力，从而增强对人类-场景交互的预测。由于缺少适合于从观察者视角进行多人行为预测的数据集，我们使用由逼真模拟器生成的合成人类行为数据对CAMP-VLM进行微调，并在合成和真实世界的序列上评估生成的模型，以评估其泛化能力。通过利用监督微调（SFT）和直接偏好优化（DPO），CAMP-VLM在预测准确率方面优于表现最佳的基线模型，提升幅度高达66.9%。"
    },
    {
        "title": "Kling-Omni Technical Report",
        "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
        "url": "http://arxiv.org/abs/2512.16776v1",
        "published_date": "2025-12-18T17:08:12+00:00",
        "updated_date": "2025-12-18T17:08:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kling Team",
            "Jialu Chen",
            "Yuanzheng Ci",
            "Xiangyu Du",
            "Zipeng Feng",
            "Kun Gai",
            "Sainan Guo",
            "Feng Han",
            "Jingbin He",
            "Kang He",
            "Xiao Hu",
            "Xiaohua Hu",
            "Boyuan Jiang",
            "Fangyuan Kong",
            "Hang Li",
            "Jie Li",
            "Qingyu Li",
            "Shen Li",
            "Xiaohan Li",
            "Yan Li",
            "Jiajun Liang",
            "Borui Liao",
            "Yiqiao Liao",
            "Weihong Lin",
            "Quande Liu",
            "Xiaokun Liu",
            "Yilun Liu",
            "Yuliang Liu",
            "Shun Lu",
            "Hangyu Mao",
            "Yunyao Mao",
            "Haodong Ouyang",
            "Wenyu Qin",
            "Wanqi Shi",
            "Xiaoyu Shi",
            "Lianghao Su",
            "Haozhi Sun",
            "Peiqin Sun",
            "Pengfei Wan",
            "Chao Wang",
            "Chenyu Wang",
            "Meng Wang",
            "Qiulin Wang",
            "Runqi Wang",
            "Xintao Wang",
            "Xuebo Wang",
            "Zekun Wang",
            "Min Wei",
            "Tiancheng Wen",
            "Guohao Wu",
            "Xiaoshi Wu",
            "Zhenhua Wu",
            "Da Xie",
            "Yingtong Xiong",
            "Yulong Xu",
            "Sile Yang",
            "Zikang Yang",
            "Weicai Ye",
            "Ziyang Yuan",
            "Shenglong Zhang",
            "Shuaiyu Zhang",
            "Yuanxing Zhang",
            "Yufan Zhang",
            "Wenzheng Zhao",
            "Ruiliang Zhou",
            "Yan Zhou",
            "Guosheng Zhu",
            "Yongjie Zhu"
        ],
        "tldr": "Kling-Omni is a generalist generative framework for high-fidelity video synthesis from multimodal inputs (text, image, video), enabling video generation, editing, and reasoning, with the goal of building multimodal world simulators.",
        "tldr_zh": "Kling-Omni是一个通用生成框架，可以通过多模态输入（文本、图像、视频）合成高质量视频，实现视频生成、编辑和推理，目标是构建多模态世界模拟器。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "我们提出了 Kling-Omni，一个通用生成框架，旨在直接从多模态视觉语言输入合成高保真视频。Kling-Omni 采用端到端视角，弥合了各种视频生成、编辑和智能推理任务之间的功能分离，并将它们整合到一个整体系统中。不同于不连贯的流水线方法，Kling-Omni 支持多种用户输入，包括文本指令、参考图像和视频上下文，并将它们处理成统一的多模态表示，以提供电影级质量和高度智能的视频内容创作。为了支持这些能力，我们构建了一个全面的数据系统，作为多模态视频创作的基础。该框架还通过高效的大规模预训练策略和推理基础设施优化得到进一步增强。全面的评估表明，Kling-Omni 在上下文生成、基于推理的编辑和多模态指令遵循方面表现出卓越的能力。我们认为 Kling-Omni 不仅是一个内容创作工具，也是朝着多模态世界模拟器的关键进步，能够感知、推理、生成并与动态且复杂的世界互动。"
    },
    {
        "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
        "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
        "url": "http://arxiv.org/abs/2512.15713v1",
        "published_date": "2025-12-17T18:59:55+00:00",
        "updated_date": "2025-12-17T18:59:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lunbin Zeng",
            "Jingfeng Yao",
            "Bencheng Liao",
            "Hongyuan Tao",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces DiffusionVL, a method for converting autoregressive (AR) vision-language models into diffusion-based VLMs, achieving significant performance improvements and inference speedups with less training data.",
        "tldr_zh": "该论文介绍了DiffusionVL，一种将自回归（AR）视觉语言模型转换为基于扩散的VLM的方法，以更少的训练数据实现了显著的性能提升和推理加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "近年来，在多模态研究中，扩散范式凭借其独特的解码优势，已成为一种有希望替代自回归范式 (AR) 的方法。然而，由于基础扩散语言模型的能力限制，扩散视觉语言模型 (dVLM) 的性能仍然显著落后于主流模型。这引出了一个简单而根本的问题：是否有可能基于现有的强大 AR 模型构建 dVLM？针对这个问题，我们提出了 DiffusionVL，一个可以从任何强大 AR 模型转换而来的 dVLM 家族。通过简单的微调，我们成功地将 AR 预训练模型适配到扩散范式中。这种方法产生了两个关键的观察结果：(1) 从基于 AR 的多模态模型到扩散的范式转变非常有效。(2) 将 AR 语言模型直接转换为 dVLM 也是可行的，从而实现与 LLaVA 风格的视觉指令微调相媲美的性能。 此外，我们在 dVLM 中引入了一种块解码设计，该设计支持任意长度的生成和 KV 缓存重用，从而显著加速推理。 我们进行了大量的实验。尽管训练使用的数据量不到先前方法所需的 5%，DiffusionVL 实现了全面的性能提升——在 MMMU-Pro（视觉）基准测试上提升了 34.4%，在 MME（认知）基准测试上提升了 37.5%，同时实现了 2 倍的推理速度提升。 模型和代码已发布在 https://github.com/hustvl/DiffusionVL。"
    },
    {
        "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation",
        "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.",
        "url": "http://arxiv.org/abs/2512.16811v1",
        "published_date": "2025-12-18T17:51:42+00:00",
        "updated_date": "2025-12-18T17:51:42+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jingjing Qian",
            "Boyao Han",
            "Chen Shi",
            "Lei Xiao",
            "Long Yang",
            "Shaoshuai Shi",
            "Li Jiang"
        ],
        "tldr": "GeoPredict enhances Vision-Language-Action models with kinematic and geometric priors, utilizing trajectory and 3D Gaussian geometry predictions for improved precision in robotic manipulation tasks, especially in geometry-intensive scenarios, and demonstrates superior performance over existing baselines.",
        "tldr_zh": "GeoPredict通过结合运动学和几何先验知识增强了视觉-语言-动作模型，利用轨迹和3D高斯几何预测来提高机器人操作任务的精确性，尤其是在几何密集型场景中，并展示了优于现有基线的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉-语言-动作（VLA）模型在机器人操作中实现了强大的泛化能力，但仍然很大程度上是反应式的和以2D为中心的，这使得它们在需要精确3D推理的任务中不可靠。我们提出GeoPredict，一种几何感知VLA框架，它通过预测性的运动学和几何先验来增强一个连续动作策略。GeoPredict引入了一个轨迹级别的模块，该模块对运动历史进行编码，并预测机器人手臂的多步3D关键点轨迹；以及一个预测性的3D高斯几何模块，该模块预测工作空间的几何形状，并通过跟踪引导的细化沿着未来的关键点轨迹进行。这些预测模块仅作为训练时的监督信号，通过基于深度的渲染来实现，而推理时只需要轻量级的额外查询令牌，无需调用任何3D解码。在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线，尤其是在几何密集型和空间要求苛刻的场景中。"
    },
    {
        "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
        "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.",
        "url": "http://arxiv.org/abs/2512.16793v1",
        "published_date": "2025-12-18T17:27:03+00:00",
        "updated_date": "2025-12-18T17:27:03+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Xiaopeng Lin",
            "Shijie Lian",
            "Bin Yu",
            "Ruoqi Yang",
            "Changti Wu",
            "Yuzhuo Miao",
            "Yurun Jin",
            "Yukun Shi",
            "Cong Huang",
            "Bojun Cheng",
            "Kai Chen"
        ],
        "tldr": "The paper introduces PhysBrain, a method for training VLMs on human egocentric videos to improve their understanding of physical interactions and enable better transfer learning to robot control tasks. They create a large-scale dataset (E2E-3M) and demonstrate improved performance on egocentric understanding and downstream robot control.",
        "tldr_zh": "该论文介绍了PhysBrain，一种利用人类第一视角视频训练视觉语言模型的方法，以提高模型对物理交互的理解，并实现更好的迁移学习到机器人控制任务。 他们创建了一个大规模数据集 (E2E-3M)，并展示了在第一视角理解和下游机器人控制方面性能的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "机器人泛化依赖于物理智能：在以自我为中心的感知和动作下，对状态变化、富含接触的交互以及长时程规划进行推理的能力。然而，大多数视觉语言模型（VLMs）主要基于第三人称数据进行训练，这为人形机器人造成了根本性的视点不匹配。扩大机器人以自我为中心的数据收集规模仍然不切实际，因为成本高昂且多样性有限，而大规模的人类以自我为中心视频提供了一种可扩展的替代方案，可以自然地捕获丰富的交互上下文和因果结构。关键挑战是将原始以自我为中心的视频转换为结构化且可靠的具身训练监督。因此，我们提出了一种“以自我为中心到具身”（Egocentric2Embodiment）的翻译流程，该流程将第一人称视频转换为多层次、模式驱动的VQA监督，并强制执行证据 grounding 和时间一致性，从而能够大规模构建“以自我为中心到具身”数据集（E2E-3M）。 通过在 E2E-3M 数据集上训练，我们得到一个以自我为中心感知的具身大脑，称为PhysBrain。 PhysBrain 在以自我为中心的理解方面表现出显著提升，尤其是在 EgoThink 上的规划方面。它提供了一个以自我为中心感知的初始化，从而实现更具样本效率的视觉语言动作（VLA）微调和更高的 SimplerEnv 成功率（53.9%），这证明了从人类以自我为中心的监督到下游机器人控制的有效迁移。"
    },
    {
        "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation",
        "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .",
        "url": "http://arxiv.org/abs/2512.16724v1",
        "published_date": "2025-12-18T16:26:17+00:00",
        "updated_date": "2025-12-18T16:26:17+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yixiang Chen",
            "Yan Huang",
            "Keji He",
            "Peiyan Li",
            "Liang Wang"
        ],
        "tldr": "The paper introduces VERM, a method that leverages foundation models to create a virtual task-adaptive view from 3D point clouds for efficient robotic manipulation, achieving significant speedups in training and inference.",
        "tldr_zh": "该论文介绍了VERM，一种利用基础模型从3D点云创建虚拟任务自适应视图的方法，用于高效的机器人操作，并在训练和推理方面实现了显著加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在执行3D操作任务时，机器人必须基于来自多个固定摄像头的感知信息执行动作规划。多摄像头设置引入了大量的冗余和无关信息，增加了计算成本，并迫使模型花费额外的训练时间来提取关键的任务相关细节。为了过滤掉冗余信息并准确提取任务相关的特征，我们提出了一种 VERM（用于机器人操作的虚拟眼）方法，该方法利用基础模型中的知识，从构建的3D点云中想象出一个虚拟的、任务自适应的视角，从而有效地捕获必要的信息并减轻遮挡。为了促进3D动作规划和精细操作，我们进一步设计了一个深度感知模块和一个动态的由粗到精的过程。在仿真基准RLBench和真实世界评估中的大量实验结果表明了我们方法的有效性，超越了之前的最先进方法，同时在训练时间上实现了1.89倍的加速，在推理速度上实现了1.54倍的加速。更多结果可以在我们的项目网站上找到：https://verm-ral.github.io 。"
    },
    {
        "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion",
        "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.",
        "url": "http://arxiv.org/abs/2512.16446v1",
        "published_date": "2025-12-18T12:08:24+00:00",
        "updated_date": "2025-12-18T12:08:24+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Enis Yalcin",
            "Joshua O'Hara",
            "Maria Stamatopoulou",
            "Chengxu Zhou",
            "Dimitrios Kanoulas"
        ],
        "tldr": "The paper introduces E-SDS, a framework integrating vision-language models with real-time terrain sensors for automated reward design in humanoid locomotion, enabling more robust and capable policies on complex terrains compared to manual or non-perceptive automated approaches.",
        "tldr_zh": "本文介绍了一个名为E-SDS的框架，该框架将视觉-语言模型与实时地形传感器相结合，用于人形机器人运动中的自动化奖励设计。与手动或非感知自动化方法相比，E-SDS能够在复杂地形上实现更强大和更有能力的策略。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉语言模型（VLMs）在自动化人形机器人步态的奖励函数设计中展现出前景，有望消除繁琐的人工工程设计需求。然而，当前的基于VLM的方法本质上是“盲目的”，因为它们缺乏在复杂地形中导航所需的环境感知能力。我们提出了E-SDS（环境感知-观察，执行，排序），一个弥合这一感知差距的框架。E-SDS将VLMs与实时地形传感器分析相结合，以自动生成奖励函数，促进训练鲁棒的基于示例视频的有感知的步态策略。在优傲机器人G1人形机器人上，对四种不同的地形（简单、间隙、障碍物、楼梯）进行评估，E-SDS独特地实现了成功的楼梯下降，而使用人工设计的奖励或非感知的自动化基线训练的策略则无法完成此任务。在所有地形中，E-SDS还降低了51.9%-82.6%的速度跟踪误差。我们的框架将奖励函数设计的人工工作量从几天缩短到不到两小时，同时产生了更鲁棒和更有能力的步态策略。"
    },
    {
        "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation",
        "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.",
        "url": "http://arxiv.org/abs/2512.16302v1",
        "published_date": "2025-12-18T08:39:34+00:00",
        "updated_date": "2025-12-18T08:39:34+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zixuan Chen",
            "Chongkai Gao",
            "Lin Shao",
            "Jieqi Shi",
            "Jing Huo",
            "Yang Gao"
        ],
        "tldr": "The paper introduces ManiLong-Shot, a framework for one-shot imitation learning of long-horizon manipulation tasks by decomposing them into interaction-aware primitives, leveraging VLMs or rule-based heuristics for sequencing.",
        "tldr_zh": "该论文介绍了一种名为 ManiLong-Shot 的框架，它通过将长时程操作任务分解为交互感知原语，并利用 VLM 或基于规则的启发式方法进行排序，从而实现长时程操作任务的单样本模仿学习。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "单样本模仿学习（OSIL）为无需大规模数据采集即可教会机器人新技能提供了一条有前景的途径。然而，当前的单样本模仿学习方法主要局限于短时域任务，从而限制了其在复杂、长时域操作中的适用性。为了解决这一局限性，我们提出了 ManiLong-Shot，一种新型框架，能够为长时域抓取操作任务实现有效的单样本模仿学习。ManiLong-Shot 将长时域任务构建为围绕物理交互事件展开，将问题重新定义为交互感知原语的序列化，而不是直接模仿连续轨迹。这种原语分解可以通过视觉-语言模型 (VLM) 的高层推理驱动，也可以通过从机器人状态变化中获得的基于规则的启发式方法驱动。对于每个原语，ManiLong-Shot 预测对交互至关重要的不变区域，建立演示与当前观察之间的对应关系，并计算目标末端执行器姿态，从而实现有效的任务执行。大量的仿真实验表明，ManiLong-Shot 在仅接受 10 个短时域任务的训练后，通过单样本模仿泛化到跨三个难度级别的 20 个未见过的长时域任务，与最先进技术相比实现了 22.8% 的相对改进。此外，真实的机器人实验验证了 ManiLong-Shot 通过单样本模仿稳健地执行三个长时域操作任务的能力，证实了它的实际应用性。"
    },
    {
        "title": "Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving",
        "summary": "Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.",
        "url": "http://arxiv.org/abs/2512.16055v1",
        "published_date": "2025-12-18T00:41:31+00:00",
        "updated_date": "2025-12-18T00:41:31+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jiaheng Geng",
            "Jiatong Du",
            "Xinyu Zhang",
            "Ye Li",
            "Panqu Wang",
            "Yanjun Huang"
        ],
        "tldr": "The paper introduces a real-world adversarial closed-loop evaluation platform for end-to-end autonomous driving, which uses a flow-matching based image generator and an adversarial traffic policy to identify corner cases and evaluate model robustness.",
        "tldr_zh": "本文介绍了一个端到端自动驾驶的真实世界对抗性闭环评估平台，该平台使用基于流匹配的图像生成器和对抗性交通策略来识别极端情况并评估模型的鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在真实世界中难以收集的安全关键极端场景，对于评估端到端自动驾驶至关重要。对抗性交互是生成此类安全关键极端场景的有效方法。虽然现有的对抗性评估方法是为在简化仿真环境中运行的模型构建的，但针对真实世界端到端自动驾驶的对抗性评估研究甚少。为了应对这一挑战，我们提出了一种端到端自动驾驶的闭环评估平台，该平台可以在真实世界场景中生成对抗性交互。在我们的平台中，真实世界图像生成器与对抗性交通策略协同工作，以评估在真实世界数据上训练的各种端到端模型。基于流匹配的生成器，根据交通环境信息高效且稳定地生成真实世界图像。高效的对抗性周围车辆策略旨在模拟具有挑战性的交互，并创建当前自动驾驶系统难以处理的极端场景。实验结果表明，该平台可以高效地生成逼真的驾驶图像。通过评估 UniAD 和 VAD 等端到端模型，我们证明了在对抗性策略的基础上，我们的平台可以评估被测模型在极端场景中的性能下降。这一结果表明，该平台可以有效地检测模型的潜在问题，这将有助于端到端自动驾驶的安全性和鲁棒性。"
    },
    {
        "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments",
        "summary": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.",
        "url": "http://arxiv.org/abs/2512.16027v1",
        "published_date": "2025-12-17T23:19:06+00:00",
        "updated_date": "2025-12-17T23:19:06+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Shuaidong Ji",
            "Mahdi Bamdad",
            "Francisco Cruz"
        ],
        "tldr": "This paper introduces SWIFT-Nav, a TD3-based UAV navigation framework enhanced with fuzzy logic for safety and prioritized experience replay for faster, more stable learning in cluttered environments, demonstrating improved performance in simulation.",
        "tldr_zh": "本文介绍了一种基于TD3的无人机导航框架SWIFT-Nav，该框架结合了模糊逻辑以确保安全性，并采用优先经验回放来实现在杂乱环境中更快、更稳定的学习，并在仿真中表现出更好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "在杂乱和动态环境中实现高效可靠的无人机导航仍然具有挑战性。我们提出了SWIFT-Nav：用于导航的模糊仲裁和TD3的稳定性感知航路点级集成，这是一种基于TD3的导航框架，可实现快速、稳定地收敛到具有避障意识的路径。该系统将传感器驱动的感知前端与TD3航路点策略相结合：感知模块将激光雷达测距数据转换为置信度加权的安全地图和目标提示，而TD3策略则通过优先级经验回放进行训练，以专注于高误差转换，并采用衰减的ε-贪心探索调度，逐步从探索转向利用。一个轻量级的模糊逻辑层根据径向测量值和附近障碍物计算安全评分，控制模式切换并限制不安全动作；同时，与任务对齐的奖励塑造结合了目标进度、间隙和切换经济性项，提供了密集、良好缩放的反馈，从而加速学习。我们在Webots中通过基于邻近度的碰撞检测实现此方法，其在轨迹平滑性和对未见布局的泛化方面始终优于基线方法，同时保持了实时响应性。这些结果表明，将TD3与回放优先级、校准探索和模糊安全规则相结合，可以为杂乱场景中的无人机导航提供稳健且可部署的解决方案。"
    },
    {
        "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
        "summary": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.",
        "url": "http://arxiv.org/abs/2512.16019v1",
        "published_date": "2025-12-17T23:06:36+00:00",
        "updated_date": "2025-12-17T23:06:36+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Qiping Zhang",
            "Nathan Tsoi",
            "Mofeed Nagib",
            "Hao-Tien Lewis Chiang",
            "Marynel Vázquez"
        ],
        "tldr": "This paper explores using few-shot learning with LLMs to predict human perceptions of robot performance in social navigation, achieving comparable or better results than supervised learning with significantly less data.",
        "tldr_zh": "本文探讨了使用LLM的少样本学习来预测人类对机器人社交导航性能的感知，与监督学习相比，在数据量显著减少的情况下，实现了相当甚至更好的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "理解人类在人机交互过程中如何评估机器人行为，对于开发能够依据人类期望行动的、具有社会意识的机器人至关重要。虽然获取这些评估的传统方法是进行用户研究，但最近的研究提出利用机器学习来替代。然而，现有的数据驱动方法需要大量的标记数据，这限制了它们在实践中的应用。为了解决这个缺口，我们提出利用大型语言模型（LLMs）的少样本学习能力，来提升机器人预测用户对其性能感知的准确度，并在社交导航任务中对这一想法进行了实验研究。为此，我们扩展了SEAN TOGETHER数据集，增加了更多真实世界的人机导航片段和参与者反馈。利用这个增强的数据集，我们评估了几种LLM在少量上下文示例的基础上，基于观察到的机器人和周围人体运动的时空线索，预测人类对机器人性能感知的能力。我们的结果表明，LLM可以达到或超过传统监督学习模型的性能，同时只需要少一个数量级的标记实例。我们进一步表明，预测性能可以随着更多上下文示例的增加而提高，证实了我们方法的可扩展性。此外，我们通过对性能预测所考虑的输入特征进行消融研究，来调查LLM依赖哪些基于传感器的信息来进行这些推断。最后，我们探索了上下文学习中个性化示例的新颖应用，即从被评估的同一用户处提取示例，发现它们进一步提高了预测准确性。这项工作为以可扩展的方式通过以用户为中心的反馈来改善机器人行为铺平了道路。"
    },
    {
        "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
        "summary": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.",
        "url": "http://arxiv.org/abs/2512.15940v1",
        "published_date": "2025-12-17T20:08:32+00:00",
        "updated_date": "2025-12-17T20:08:32+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Tin Stribor Sohn",
            "Maximilian Dillitzer",
            "Jason J. Corso",
            "Eric Sax"
        ],
        "tldr": "R4 is a training-free, retrieval-augmented framework for VLMs that builds a 4D knowledge database to improve reasoning in dynamic, embodied environments by recalling past events and inferring states.",
        "tldr_zh": "R4是一个免训练的检索增强框架，通过构建4D知识库来改进VLM在动态、具身环境中的推理能力，从而回忆过去事件并推断状态。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人类通过构建持久的、结构化的内部表征来编码语义信息、空间布局和时间动态，从而在四个维度上感知和推理他们的周围环境。这些多模态记忆使他们能够回忆过去的事件、推断未观察到的状态，并将新的信息整合到依赖于上下文的推理中。受这种能力的启发，我们提出了R4，一个无需训练的框架，用于在4D时空空间中进行检索增强推理，该框架为视觉语言模型（VLM）配备了结构化的、终身记忆。R4通过将对象级别的语义描述锚定在度量空间和时间中，持续构建一个4D知识数据库，从而产生一个可以跨智能体共享的持久世界模型。在推理过程中，自然语言查询被分解为语义、空间和时间的关键信息，以检索相关的观察结果，这些观察结果被整合到VLM的推理中。与经典的检索增强生成方法不同，R4中的检索直接在4D空间中进行，从而实现无需训练的情景式和协同推理。在具身问答和导航基准测试上的实验表明，与基线相比，R4显著提高了时空信息的检索和推理能力，从而推进了动态环境中具身4D推理的新范式。"
    },
    {
        "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
        "summary": "Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \\model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.",
        "url": "http://arxiv.org/abs/2512.15692v1",
        "published_date": "2025-12-17T18:47:31+00:00",
        "updated_date": "2025-12-17T18:47:31+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jonas Pai",
            "Liam Achenbach",
            "Victoriano Montesinos",
            "Benedek Forrai",
            "Oier Mees",
            "Elvis Nava"
        ],
        "tldr": "The paper introduces mimic-video, a Video-Action Model (VAM) that uses a pretrained Internet-scale video model with a flow matching-based action decoder to improve sample efficiency and convergence speed in robotic manipulation compared to Vision-Language-Action models.",
        "tldr_zh": "该论文介绍了一种视频-动作模型 (VAM), mimic-video。该模型利用预训练的互联网规模视频模型和一个基于流匹配的动作解码器，与视觉-语言-动作模型相比，提高了机器人操作的样本效率和收敛速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "当前机器人操作的视觉-语言-动作模型（VLA）构建于大规模但不连贯的静态网络数据上预训练的视觉-语言骨干网络之上。因此，尽管语义泛化能力有所提高，策略仍必须仅从机器人轨迹中隐式地推断出复杂的物理动力学和时间依赖性。这种依赖性造成了不可持续的数据负担，需要持续、大规模的专家数据收集，以弥补先天缺乏物理理解的不足。我们认为，虽然视觉-语言预训练有效地捕捉了语义先验，但它仍然对物理因果关系视而不见。更有效的范例是利用视频在预训练期间联合捕捉语义和视觉动态，从而隔离剩余的低层控制任务。为此，我们引入了\\model，一种新颖的视频-动作模型（VAM），它将预训练的互联网规模视频模型与基于流匹配的动作解码器配对，该解码器以其潜在表示为条件。该解码器充当逆动力学模型（IDM），从视频空间动作计划的潜在表示中生成低层机器人动作。我们的大量评估表明，与传统的VLA架构相比，我们的方法在模拟和真实世界的机器人操作任务上都取得了最先进的性能，并将样本效率提高了10倍，收敛速度提高了2倍。"
    },
    {
        "title": "Large Video Planner Enables Generalizable Robot Control",
        "summary": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.",
        "url": "http://arxiv.org/abs/2512.15840v1",
        "published_date": "2025-12-17T18:35:54+00:00",
        "updated_date": "2025-12-17T18:35:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Boyuan Chen",
            "Tianyuan Zhang",
            "Haoran Geng",
            "Kiwhan Song",
            "Caiyi Zhang",
            "Peihao Li",
            "William T. Freeman",
            "Jitendra Malik",
            "Pieter Abbeel",
            "Russ Tedrake",
            "Vincent Sitzmann",
            "Yilun Du"
        ],
        "tldr": "This paper introduces a large video model trained on internet-scale human activity videos for zero-shot robotic planning, demonstrating strong generalization and real-world feasibility. They also release the model and dataset.",
        "tldr_zh": "本文介绍了一个大型视频模型，该模型在互联网规模的人类活动视频上进行训练，用于零样本机器人规划，展示了强大的泛化能力和现实可行性。他们还发布了模型和数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "通用机器人需要能够泛化到各种任务和环境的决策模型。 近期研究通过将多模态大型语言模型 (MLLM) 扩展到动作输出，构建机器人基础模型，从而创建了视觉-语言-动作 (VLA) 系统。 这些努力的动力来自于这样一种直觉：MLLM的大规模语言和图像预训练可以有效地迁移到动作输出模态。 在这项工作中，我们探索了一种替代范例，即使用大规模视频预训练作为构建机器人基础模型的主要模态。 与静态图像和语言不同，视频捕捉了物理世界中状态和动作的时空序列，这些序列与机器人行为自然对齐。 我们整理了一个互联网规模的人类活动和任务演示视频数据集，并首次以基础模型的规模训练一个用于生成式机器人规划的开放视频模型。 该模型为新的场景和任务生成零样本视频计划，我们对这些计划进行后处理以提取可执行的机器人动作。 我们通过第三方选择的实际任务和真实的机器人实验来评估任务层面的泛化能力，证明了成功的物理执行。 总之，这些结果表明了强大的指令遵循能力、强大的泛化能力和现实世界的可行性。 我们发布模型和数据集，以支持开放的、可复现的基于视频的机器人学习。 我们的网站位于 https://www.boyuan.space/large-video-planner/。"
    },
    {
        "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?",
        "summary": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.",
        "url": "http://arxiv.org/abs/2512.16755v1",
        "published_date": "2025-12-18T16:53:12+00:00",
        "updated_date": "2025-12-18T16:53:12+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Siqi Wang",
            "Chao Liang",
            "Yunfan Gao",
            "Erxin Yu",
            "Sen Li",
            "Yushi Li",
            "Jing Li",
            "Haofen Wang"
        ],
        "tldr": "The paper introduces CitySeeker, a new benchmark for evaluating VLMs in embodied urban navigation based on implicit human needs, revealing performance bottlenecks and actionable insights for improvement. They find that current VLMs poorly address embodied urban navigation when considering implicit needs.",
        "tldr_zh": "该论文介绍了CitySeeker，一个新的基准，用于评估VLMs在基于隐式人类需求的具身城市导航中的表现，揭示了性能瓶颈和改进的可行见解。他们发现，当前的VLMs在考虑隐式需求时，在具身城市导航方面的表现很差。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉-语言模型（VLMs）在基于显式指令的导航方面取得了显著进展；然而，它们在动态城市环境中解释隐式人类需求（例如，“我口渴了”）的能力仍未得到充分探索。本文介绍 CitySeeker，这是一个新颖的基准，旨在评估 VLM 在具身城市导航中探索隐式需求的空间推理和决策能力。CitySeeker 包含 8 个城市中 6,440 条轨迹，捕捉了 7 个目标驱动场景中不同的视觉特征和隐式需求。大量实验表明，即使是表现最好的模型（例如，Qwen2.5-VL-32B-Instruct）也仅能实现 21.1% 的任务完成率。我们发现，长程推理中的误差累积、空间认知不足和经验回忆欠缺是关键瓶颈。为了进一步分析它们，我们研究了一系列探索性策略——回溯机制、丰富空间认知和基于记忆的检索 (BCR)，这些策略的灵感来源于人类认知地图对迭代观察-推理循环和自适应路径优化的强调。我们的分析为开发具有强大空间智能的 VLM 提供了可行的见解，以应对“最后一英里”的导航挑战。"
    },
    {
        "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities",
        "summary": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.",
        "url": "http://arxiv.org/abs/2512.16733v1",
        "published_date": "2025-12-18T16:32:06+00:00",
        "updated_date": "2025-12-18T16:32:06+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Daniel Bramblett",
            "Rushang Karia",
            "Adrian Ciotinga",
            "Ruthvick Suresh",
            "Pulkit Verma",
            "YooJung Choi",
            "Siddharth Srivastava"
        ],
        "tldr": "This paper presents a method to learn probabilistic, PDDL-style symbolic models of black-box AI systems' capabilities using Monte-Carlo tree search, demonstrating soundness, completeness, and convergence. The learned models can represent the conditions, actions, and probabilistic outcomes of BBAI systems.",
        "tldr_zh": "本文提出了一种使用蒙特卡洛树搜索学习黑盒人工智能系统能力的概率性PDDL风格符号模型的方法，并展示了模型的可靠性、完整性和收敛性。学习到的模型可以表示BBAI系统的条件、动作和概率性结果。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "黑盒人工智能（BBAI）系统，例如基础模型，正越来越多地被用于序贯决策。为了确保此类系统安全运行和部署，迫切需要开发能够提供对BBAI能力进行可靠且可解释表示的有效方法。本文表明，PDDL风格的表示可以有效地学习和建模输入BBAI的规划能力。它使用蒙特卡洛树搜索范式来系统地创建测试任务、获取数据并修剪可能符号模型的假设空间。学习到的模型描述了BBAI的能力、执行这些能力的条件以及执行这些能力可能产生的结果及其相关概率。理论结果表明了所学模型的可靠性、完备性和收敛性。对多个BBAI系统进行的实证结果说明了所提出方法范围、效率和准确性。"
    },
    {
        "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
        "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
        "url": "http://arxiv.org/abs/2512.16453v1",
        "published_date": "2025-12-18T12:15:52+00:00",
        "updated_date": "2025-12-18T12:15:52+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Jiayang Yang",
            "Chunhui Zhao",
            "Martin Guay",
            "Zhixing Cao"
        ],
        "tldr": "The paper introduces TimeSeries2Report (TS2R), a prompting framework that translates raw battery time-series data into semantically enriched reports, enabling LLMs to effectively manage battery energy storage systems with improved accuracy, robustness, and explainability.",
        "tldr_zh": "该论文介绍了TimeSeries2Report (TS2R)，一个提示框架，可以将原始电池时间序列数据转换为语义丰富的报告，从而使大型语言模型能够有效地管理电池储能系统，并提高准确性、稳健性和可解释性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLM）在解读多元时间序列数据方面展现出前景广阔的能力，但其在真实世界电池储能系统（BESS）运行和维护中的应用仍有待探索。在此，我们提出TimeSeries2Report（TS2R），一种提示框架，可将原始锂离子电池运行时间序列数据转换为结构化、语义增强的报告，从而使LLM能够在BESS管理场景中进行推理、预测和决策。TS2R通过分割、语义抽象和基于规则的解释相结合的方式，将短期时间动态编码为自然语言，有效地将低层次传感器信号与高层次上下文信息连接起来。我们在实验室规模和真实世界数据集上对TS2R进行了基准测试，评估了报告质量以及在异常检测、荷电状态预测和充放电管理等下游任务中的性能。与基于视觉、嵌入和文本的提示基线相比，通过TS2R进行的基于报告的提示，在准确性、鲁棒性和可解释性指标方面，始终能提高LLM的性能。值得注意的是，集成TS2R的LLM无需重新训练或架构修改，即可实现专家级的决策质量和预测一致性，为自适应的、LLM驱动的电池智能建立了一条切实可行的路径。"
    },
    {
        "title": "Hypernetworks That Evolve Themselves",
        "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.",
        "url": "http://arxiv.org/abs/2512.16406v1",
        "published_date": "2025-12-18T11:05:34+00:00",
        "updated_date": "2025-12-18T11:05:34+00:00",
        "categories": [
            "cs.NE",
            "cs.AI"
        ],
        "authors": [
            "Joachim Winther Pedersen",
            "Erwan Plantec",
            "Eleni Nisioti",
            "Marcello Barylli",
            "Milton Montero",
            "Kathrin Korte",
            "Sebastian Risi"
        ],
        "tldr": "This paper introduces Self-Referential Graph HyperNetworks (SR-GHNs), a novel architecture where neural networks evolve themselves without external optimizers, demonstrating adaptation in RL benchmarks and gait evolution in locomotion.",
        "tldr_zh": "该论文介绍了自引用图超网络（SR-GHNs），一种新型架构，其中神经网络无需外部优化器即可自行进化，并在强化学习基准测试中展示了适应性，并在运动中展示了步态进化。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "如何让神经网络在不依赖外部优化器的情况下自我进化？我们提出了自引用图超网络（Self-Referential Graph HyperNetworks, Self-Referential GHNs），这是一种将变异和遗传的机制嵌入到网络内部的系统。通过结合超网络、随机参数生成和基于图的表示，自引用GHN能够变异和评估自身，同时将变异率作为可选择的性状进行调整。通过在具有环境变化的新强化学习基准测试（CartPoleSwitch、LunarLander-Switch）中进行实验，自引用GHN展现出快速、可靠的适应能力和涌现的人口动态。在Ant-v5的运动基准测试中，它们进化出连贯的步态，并通过自主降低种群变异来集中于有希望的解决方案，展现出很有前景的微调能力。我们的发现支持了这样一种观点：即可进化性本身可以从神经自我引用中涌现。自引用GHN代表了朝着更接近生物进化的合成系统迈出的一步，为自主的、开放式的学习代理提供了工具。"
    },
    {
        "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models",
        "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.",
        "url": "http://arxiv.org/abs/2512.16295v1",
        "published_date": "2025-12-18T08:29:50+00:00",
        "updated_date": "2025-12-18T08:29:50+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Zhenyu Wu",
            "Jingjing Xie",
            "Zehao Li",
            "Bowen Yang",
            "Qiushi Sun",
            "Zhaoyang Liu",
            "Zhoumianze Liu",
            "Yu Qiao",
            "Xiangyu Yue",
            "Zun Wang",
            "Zichen Ding"
        ],
        "tldr": "The paper introduces OS-Oracle, a framework for developing cross-platform GUI critic models using a scalable data pipeline, a two-stage training paradigm, and a benchmark (OS-Critic Bench). The resulting model, OS-Oracle-7B, achieves state-of-the-art performance and improves GUI agent performance as a pre-critic.",
        "tldr_zh": "该论文介绍了 OS-Oracle，一个用于开发跨平台 GUI 评论模型的框架，包括一个可扩展的数据管道、一个两阶段训练范式和一个基准 (OS-Critic Bench)。 所得模型 OS-Oracle-7B 达到了最先进的性能，并作为预评论器提高了 GUI 代理的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "随着由VLM驱动的计算机使用智能体（CUA）在图形用户界面(GUI)导航和操作方面能力日益增强，可靠的步进式决策已经成为实际部署的关键瓶颈。在长时程工作流程中，错误会迅速累积，且不可逆操作可能导致意想不到的后果，因此需要评估每次执行动作的评论模型。尽管评论模型提供了一个有希望的解决方案，但其有效性受到缺乏多样化、高质量的GUI反馈数据以及用于计算机使用中步进式评估的公共评论基准测试的限制。为了弥合这些差距，我们推出了OS-Oracle，它做出了三个核心贡献：（1）一个用于合成跨平台GUI评论数据的可扩展数据管道；（2）一个结合了监督式微调(SFT)和一致性保持的组相对策略优化(CP-GRPO)的两阶段训练范式；（3）OS-Critic Bench，一个用于评估评论模型在移动、Web和桌面平台上的性能的整体基准测试。借助这个框架，我们整理了一个包含31万个评论样本的高质量数据集。由此产生的评论模型OS-Oracle-7B，在OS-Critic Bench上实现了开源VLM中的最先进性能，并在移动领域超越了专有模型。此外，当作为预评论模型时，OS-Oracle-7B提高了原生GUI智能体（如OSWorld和AndroidWorld环境中的UI-TARS-1.5-7B）的性能。代码已在https://github.com/numbmelon/OS-Oracle上开源。"
    },
    {
        "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems",
        "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.",
        "url": "http://arxiv.org/abs/2512.16279v1",
        "published_date": "2025-12-18T07:58:40+00:00",
        "updated_date": "2025-12-18T07:58:40+00:00",
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yiliu Yang",
            "Yilei Jiang",
            "Qunzhong Wang",
            "Yingshui Tan",
            "Xiaoyong Zhu",
            "Sherman S. M. Chow",
            "Bo Zheng",
            "Xiangyu Yue"
        ],
        "tldr": "The paper introduces QuadSentinel, a four-agent system for enforcing safety policies in multi-agent systems using machine-checkable rules derived from sequents, demonstrating improved guardrail accuracy and safety compared to single-agent baselines.",
        "tldr_zh": "该论文介绍了 QuadSentinel，一个四代理系统，用于在多代理系统中执行安全策略，使用从序列推导出的机器可检查规则，与单代理基线相比，展示了改进的护栏准确性和安全性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "基于大型语言模型的智能体在利用工具、多步计划和智能体间消息解决复杂任务时，会产生安全风险。然而，部署者用自然语言编写的安全策略具有歧义性和上下文依赖性，因此它们难以映射到机器可检查的规则，并且运行时执行不可靠。我们将安全策略表示为序列，提出了\\textsc{QuadSentinel}，一个四智能体防护系统（状态追踪器、策略验证器、威胁监视器和裁判），它将这些策略编译成由可观察状态上的谓词构建的机器可检查规则，并在运行时强制执行。裁判逻辑加上高效的top-$k$谓词更新器，通过优先检查和分层解决冲突来降低成本。在ST-WebAgentBench (ICML CUA~'25) 和 AgentHarm (ICLR~'25) 上的测量表明，\\textsc{QuadSentinel}提高了护栏的准确性和规则召回率，同时减少了假阳性。与诸如ShieldAgent (ICML~'25) 等单智能体基线相比，它产生了更好的整体安全控制。通过保持策略独立和机器可检查，近期部署可以在不修改核心智能体的情况下采用这种模式。我们的代码将在https://github.com/yyiliu/QuadSentinel上公开。"
    },
    {
        "title": "Learning to Wait: Synchronizing Agents with the Physical World",
        "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
        "url": "http://arxiv.org/abs/2512.16262v1",
        "published_date": "2025-12-18T07:24:44+00:00",
        "updated_date": "2025-12-18T07:24:44+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yifei She",
            "Ping Zhang",
            "He Liu",
            "Yanmin Jia",
            "Yang Jing",
            "Zijun Liu",
            "Peng Sun",
            "Xiangbin Li",
            "Xiaohe Hu"
        ],
        "tldr": "This paper introduces an agent-side approach for LLMs to learn to synchronize with asynchronous real-world environments by predicting waiting durations, improving efficiency in tasks with variable latencies.",
        "tldr_zh": "本文介绍了一种代理侧方法，允许LLM通过预测等待时间来学习与异步现实世界环境同步，从而提高具有可变延迟的任务的效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "真实世界的主动代理任务，与同步马尔可夫决策过程（MDP）不同，通常涉及具有可变延迟的非阻塞动作，从而在动作发起和完成之间产生根本性的\\textit{时间间隔}。现有环境侧解决方案，例如阻塞包装器或频繁轮询，要么限制了可扩展性，要么用冗余观察稀释了代理的上下文窗口。在这项工作中，我们提出一种\\textbf{代理侧方法}，赋予大型语言模型（LLM）积极地将其\\textit{认知时间线}与物理世界对齐的能力。通过将代码即动作范式扩展到时间域，代理利用语义先验和上下文学习（ICL）来预测精确的等待时长（\\texttt{time.sleep(t)}），从而有效地与异步环境同步，而无需进行详尽的检查。在模拟的Kubernetes集群中的实验表明，代理可以精确地校准其内部时钟，以最大限度地减少查询开销和执行延迟，从而验证了时间感知是在开放式环境中实现自主演化的关键可学习能力。"
    },
    {
        "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
        "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
        "url": "http://arxiv.org/abs/2512.16250v1",
        "published_date": "2025-12-18T07:01:47+00:00",
        "updated_date": "2025-12-18T07:01:47+00:00",
        "categories": [
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Sanjoy Chowdhury",
            "Karren D. Yang",
            "Xudong Liu",
            "Fartash Faghri",
            "Pavan Kumar Anasosalu Vasu",
            "Oncel Tuzel",
            "Dinesh Manocha",
            "Chun-Liang Li",
            "Raviteja Vemulapalli"
        ],
        "tldr": "The paper introduces AMUSE, a benchmark for evaluating agentic reasoning in multimodal large language models in multi-speaker audio-visual scenarios, and RAFT, an alignment framework that achieves significant improvements on the benchmark.",
        "tldr_zh": "本文提出了AMUSE，一个用于评估多说话者音视频场景中多模态大型语言模型主体推理能力的基准，以及RAFT，一个在该基准上实现了显著改进的对齐框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "诸如 GPT-4o 和 Qwen3-Omni 等最新的多模态大型语言模型 (MLLMs) 在感知方面表现出色，但在多说话人、以对话为中心的场景中表现不佳，这些场景需要自主代理式推理，例如跟踪谁在说话、维持角色以及基于时间推移进行事件定位。这些场景对于多模态音视频理解至关重要，在这种理解中，模型必须联合推理音频和视频流，应用于诸如对话式视频助手和会议分析等领域。我们引入 AMUSE，这是一个围绕本质上是自主代理的任务设计的基准，它要求模型将复杂的音视频交互分解为计划、定位和反思步骤。它评估了 MLLMs 的三种模式：零样本、引导式和自主代理式，以及六个任务系列，包括时空说话人定位和多模态对话摘要。在所有模式下，当前模型都表现出较弱的多说话人推理能力，并且在非代理式和代理式评估下都表现出不一致的行为。受到这些任务的本质自主代理性质以及 LLM 代理的最新进展的启发，我们提出了 RAFT，一种数据高效的代理式对齐框架，它将奖励优化与内在的多模态自我评估（作为奖励）以及选择性参数调整相结合，以实现数据和参数高效的更新。使用 RAFT，我们在我们的基准测试中实现了高达 39.52% 的相对准确率提升。总而言之，AMUSE 和 RAFT 提供了一个实用的平台，用于检查多模态模型中的自主代理式推理并提高它们的能力。"
    },
    {
        "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis",
        "summary": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.",
        "url": "http://arxiv.org/abs/2512.16237v1",
        "published_date": "2025-12-18T06:30:08+00:00",
        "updated_date": "2025-12-18T06:30:08+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Zhi Helu",
            "Huang Jingjing",
            "Xu Wang",
            "Xu Yangbin",
            "Zhang Wanyue",
            "Jiang Baoyang",
            "Deng Shirui",
            "Zhu Liang",
            "Li Fangfang",
            "Zhao Tiejun",
            "Lin Yankai",
            "Yao Yuan"
        ],
        "tldr": "The paper introduces SPRITE, a framework using LLMs and simulators to programmatically generate a large, diverse, and precise spatial reasoning dataset for training VLMs, achieving significant performance gains on spatial benchmarks.",
        "tldr_zh": "该论文介绍了SPRITE，一个利用LLM和模拟器以程序化方式生成的大规模、多样化且精确的空间推理数据集，用于训练VLM，在空间基准测试中取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "具身智能是人工智能领域的一项重大挑战，它从根本上受到当前模型有限的空间理解和推理能力的制约。目前通过增强视觉-语言模型（VLMs）来解决这一问题的努力陷入两难境地：基于模板的数据集具有可扩展性，但结构僵化；而人工标注在语言上具有多样性，但不可扩展，且尤其在计算上不够精确。我们引入SPRITE，一种新颖的框架，它通过利用模拟器和大型模型来程序化地合成可扩展、多样化和高质量的空间推理数据，从而克服了这个两难困境。SPRITE的核心创新是将真值生成重构为一个代码生成任务。我们利用大型语言模型（LLMs）将复杂的空间问题编译成可执行的程序，然后根据从模拟器中提取的高精度场景元信息对这些程序进行验证。这确保了我们的真值在计算上精确且可验证，而LLMs的生成能力提供了巨大的语言多样性。利用此流程，我们策划了一个数据集，包含3个模拟器、11,000多个场景和30万多个图像/视频指令式微调对。我们证明，在我们的数据上训练的VLM在多个空间基准测试中取得了显著的性能提升，并且优于其他同等规模的开源数据集。此外，可扩展性分析证实了我们的假设，即克服传统模板方法低多样性的本质对于构建强大、可泛化的空间智能至关重要。我们将公开SPRITE框架代码和完整的30多万数据集，以促进未来在空间智能领域的研究。"
    },
    {
        "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation",
        "summary": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.",
        "url": "http://arxiv.org/abs/2512.16145v1",
        "published_date": "2025-12-18T03:57:55+00:00",
        "updated_date": "2025-12-18T03:57:55+00:00",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "authors": [
            "Pengyu Wang",
            "Shuchang Ye",
            "Usman Naseem",
            "Jinman Kim"
        ],
        "tldr": "The paper presents a semantic-driven reinforcement learning (SRL) method, MRG-R1, for medical report generation using a large vision-language model, achieving state-of-the-art clinical efficacy by optimizing a report-level reward based on clinical label agreement.",
        "tldr_zh": "该论文提出了一种语义驱动的强化学习(SRL)方法MRG-R1，用于使用大型视觉语言模型生成医疗报告，通过优化基于临床标签一致性的报告级别奖励，实现了最先进的临床疗效。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "医学报告生成(MRG)旨在从医学图像中自动生成放射学风格的报告，以辅助临床决策。然而，现有方法通常生成模仿放射科医生语言风格的文本，但无法保证临床正确性，因为它们是在词符级别的目标上训练的，这些目标侧重于词语选择和句子结构，而不是实际的医学准确性。我们提出了一种基于语义驱动的强化学习(SRL)医学报告生成方法，应用于大型视觉-语言模型(LVLM)。 SRL采用组相对策略优化(GRPO)，鼓励在模仿语言风格之外的临床正确性引导学习。具体来说，我们优化了一个报告级别的奖励：基于裕度的余弦相似度(MCCS)，该相似度是在从生成报告和参考报告中提取的关键放射学发现之间计算的，从而直接对齐临床标签一致性并提高语义正确性。一种轻量级的推理格式约束进一步引导模型生成结构化的“思维报告”输出。我们使用临床疗效(CE)指标，在IU X-Ray和MIMIC-CXR两个数据集上评估了使用语义驱动强化学习的医学报告生成（MRG-R1）。MRG-R1取得了最先进的性能，在IU X-Ray上CE-F1值为51.88，在MIMIC-CXR上为40.39。我们发现标签语义强化比传统的词符级别监督更好。这些结果表明，优化一个基于临床的、报告级别的奖励，而不是词符重叠，可以显著提高临床正确性。这项工作是探索在医学大型视觉-语言模型(Med-LVLM)训练中进行语义强化的先前工作，以监督医学正确性。"
    },
    {
        "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
        "summary": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
        "url": "http://arxiv.org/abs/2512.15688v1",
        "published_date": "2025-12-17T18:45:25+00:00",
        "updated_date": "2025-12-17T18:45:25+00:00",
        "categories": [
            "cs.CR",
            "cs.AI"
        ],
        "authors": [
            "Adam Kaufman",
            "James Lucassen",
            "Tyler Tracy",
            "Cody Rushing",
            "Aryan Bhatt"
        ],
        "tldr": "The paper introduces BashArena, a realistic environment for evaluating AI control techniques, where LLMs are assessed on their ability to complete tasks and perform sabotageundetected. Evaluating frontier LLMs reveals vulnerabilities, highlighting needs for robust AI control mechanisms.",
        "tldr_zh": "该论文介绍了BashArena，一个用于评估AI控制技术的真实环境，其中LLM被评估在完成任务和执行未被察觉的破坏的能力。对前沿LLM的评估揭示了漏洞，突出了对强大的AI控制机制的需求。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "未来人工智能体可能会以提升的权限自主运行。如果这些智能体未对齐，它们可能会滥用这些权限造成严重损害。人工智能控制领域致力于开发相关技术，以降低未对齐人工智能造成此类损害的可能性，同时保留其效用。我们引入了BashArena，这是一个用于研究安全关键环境中人工智能控制技术的环境。BashArena包含637个复杂的、真实环境下的Linux系统管理和基础设施工程任务，以及四个破坏目标（执行恶意软件、窃取机密、提升权限和禁用防火墙），供红队进行攻击。我们评估了多个前沿大型语言模型完成任务、在不被检测的情况下执行破坏以及检测破坏尝试的能力。Claude Sonnet 4.5 成功执行破坏并在26%的时间内逃避了GPT-4.1 mini的监控，同时轨迹层面的假阳性率仅为4%。我们的研究结果为在BashArena中设计更有效的控制协议提供了基线。我们将该数据集作为ControlArena环境发布，并分享我们的任务生成流程。"
    },
    {
        "title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
        "summary": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.",
        "url": "http://arxiv.org/abs/2512.15687v1",
        "published_date": "2025-12-17T18:44:45+00:00",
        "updated_date": "2025-12-17T18:44:45+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Zhenwen Liang",
            "Sidi Lu",
            "Wenhao Yu",
            "Kishan Panaganti",
            "Yujun Zhou",
            "Haitao Mi",
            "Dong Yu"
        ],
        "tldr": "This paper introduces Gradient-Guided Reinforcement Learning (G2RL), a novel RL framework for LLMs that leverages the model's own gradient geometry to guide exploration, showing improved reasoning performance across various benchmarks compared to existing exploration methods.",
        "tldr_zh": "该论文介绍了一种名为梯度引导强化学习 (G2RL) 的新型LLM强化学习框架，它利用模型自身的梯度几何来引导探索，与现有的探索方法相比，在各种基准测试中表现出改进的推理性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "强化学习已成为增强大型语言模型推理能力的关键，但当前的探索机制与这些模型实际学习方式仍存在根本性的错位。熵奖励和外部语义比较器鼓励表面的变化，但不能保证采样轨迹在塑造优化的更新方向上有所不同。我们提出了G2RL，一种梯度引导的强化学习框架，其中探索不是由外部启发式方法驱动，而是由模型自身的一阶更新几何结构驱动。对于每个响应，G2RL从模型的最终层敏感性构建一个序列层面的特征，它可以通过标准的正向传播以极低的成本获得，并通过比较采样组内的这些特征来衡量每个轨迹将如何重塑策略。引入新梯度方向的轨迹会获得有界的乘法奖励缩放，而冗余或脱离流形的更新会被弱化，从而产生一种自引用式的探索信号，该信号与PPO风格的稳定性和KL控制自然对齐。在Qwen3基础模型的1.7B和4B参数规模上，针对数学和通用推理基准测试（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro），G2RL在pass@1、maj@16 和 pass@k 方面始终优于基于熵的GRPO和外部嵌入方法。分析所产生的几何结构，我们发现G2RL将探索扩展到更多正交且通常相反的梯度方向，同时保持语义连贯性，这表明策略自身的更新空间为指导大型语言模型强化学习中的探索提供了一个更忠实和有效的依据。"
    },
    {
        "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
        "summary": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.",
        "url": "http://arxiv.org/abs/2512.15834v1",
        "published_date": "2025-12-17T18:22:44+00:00",
        "updated_date": "2025-12-17T18:22:44+00:00",
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.DC",
            "cs.PF",
            "cs.SE"
        ],
        "authors": [
            "Daniel Nichols",
            "Prajwal Singhania",
            "Charles Jekel",
            "Abhinav Bhatele",
            "Harshitha Menon"
        ],
        "tldr": "This paper introduces systems optimizations for LM agents that speculate tool calls and keep sequences resident in the inference engine, leading to throughput improvements. They also propose a 'tool cache' API.",
        "tldr_zh": "该论文介绍了LM代理的系统优化方法，通过推测工具调用和保持序列驻留在推理引擎中，从而提高吞吐量。他们还提出了一个“工具缓存”API。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "语言模型（LMs）正变得越来越依赖于外部工具。基于LM的智能体框架经常通过这些工具与其环境交互，例如搜索文件、运行代码、调用API等。此外，现代基于推理的LMs使用诸如网页搜索和Python代码执行等工具来增强其推理能力。尽管工具大大提高了LMs的能力，但它们也在推理过程中引入了性能瓶颈。在本文中，我们引入了新颖的系统优化方法，通过推测工具调用和强制序列驻留在推理引擎中以最小化开销，从而解决这些性能瓶颈。我们的优化使得在为LM智能体托管推理时，吞吐量可以提高每秒数百个token。我们提供了算法的理论分析，以深入了解可以产生最佳性能的推测配置。此外，我们推荐一种新的“工具缓存”API端点，以使LM提供商能够轻松采用这些优化。"
    },
    {
        "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
        "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.",
        "url": "http://arxiv.org/abs/2512.16561v1",
        "published_date": "2025-12-18T14:03:44+00:00",
        "updated_date": "2025-12-18T14:03:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxin Wang",
            "Lei Ke",
            "Boqiang Zhang",
            "Tianyuan Qu",
            "Hanxun Yu",
            "Zhenpeng Huang",
            "Meng Yu",
            "Dan Xu",
            "Dong Yu"
        ],
        "tldr": "The paper introduces N3D-VLM, a framework that integrates native 3D object perception into vision-language models, enabling accurate 3D grounding and spatial reasoning by leveraging a novel data construction pipeline to generate large-scale 3D object grounding and spatial question-answering datasets.",
        "tldr_zh": "该论文介绍了N3D-VLM，一个将原生3D物体感知集成到视觉语言模型中的框架，通过利用新的数据构建流程生成大规模的3D物体定位和空间问答数据集，从而实现精确的3D定位和空间推理。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "当前的多模态模型虽然可以回答基于 2D 图像的问题，但它们缺乏内在的 3D 对象感知能力，从而限制了它们理解 3D 场景中的空间关系和深度线索的能力。在这项工作中，我们提出了一种新颖的统一框架 N3D-VLM，它将原生 3D 对象感知与具有 3D 感知的视觉推理无缝集成，从而能够实现精确的 3D 定位和可解释的空间理解。与直接从 RGB/RGB-D 输入预测答案的传统端到端模型不同，我们的方法赋予模型原生的 3D 对象感知能力，使其能够基于文本描述直接在 3D 空间中定位对象。在精确的 3D 对象定位的基础上，该模型进一步在 3D 中执行显式推理，从而实现更具可解释性和结构化的空间理解。为了支持对这些能力的稳健训练，我们开发了一个可扩展的数据构建流程，该流程利用深度估计将大规模 2D 标注提升到 3D 空间，从而显著提高了 3D 对象定位数据的多样性和覆盖率，产生的规模是现有最大单图像 3D 检测数据集的六倍以上。此外，该流程还生成了针对 3D 中思维链 (CoT) 推理的空间问答数据集，从而有助于对 3D 对象定位和 3D 空间推理进行联合训练。实验结果表明，我们的统一框架不仅在 3D 定位任务上取得了最先进的性能，而且在视觉-语言模型中的 3D 空间推理方面也始终超越了现有方法。"
    },
    {
        "title": "Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation",
        "summary": "Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.",
        "url": "http://arxiv.org/abs/2512.16201v1",
        "published_date": "2025-12-18T05:48:21+00:00",
        "updated_date": "2025-12-18T05:48:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sarosij Bose",
            "Ravi K. Rajendran",
            "Biplob Debnath",
            "Konstantinos Karydis",
            "Amit K. Roy-Chowdhury",
            "Srimat Chakradhar"
        ],
        "tldr": "The paper introduces VALOR, a reinforcement learning-based framework to improve the visual grounding and clinical accuracy of medical vision-language models for radiology report generation, achieving significant performance gains over state-of-the-art methods.",
        "tldr_zh": "该论文介绍了VALOR，一个基于强化学习的框架，用于提高医学视觉-语言模型在放射学报告生成中的视觉基础和临床准确性，并在性能上显著优于最先进的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "放射学报告生成（RRG）是自动化医疗工作流程、促进精准患者评估以及减轻医疗专业人员工作负担的关键步骤。 尽管大型医学视觉-语言模型（Med-VLMs）取得了近期进展，但生成既具有视觉依据又具有临床准确性的放射学报告仍然是一项重大挑战。现有方法通常依赖于用于预训练的大型标记语料库、昂贵的任务特定偏好数据或基于检索的方法。然而，这些策略未能充分缓解因视觉和语言表征之间不良跨模态对齐而产生的幻觉问题。为解决这些局限性，我们提出了VALOR：用于放射学报告生成的医学视觉-语言模型的视觉对齐。我们的方法引入了一个基于强化学习的后对齐框架，该框架利用了组相对邻近策略优化（GRPO）。训练分两个阶段进行：（1）利用文本奖励改进Med-VLM，以鼓励临床上精确的术语；（2）将文本接地的模型视觉投影模块与疾病发现对齐，从而引导注意力朝向与诊断任务最相关的图像区域。在多个基准测试上的大量实验表明，VALOR 显著提高了事实准确性和视觉接地性，与最先进的报告生成方法相比，实现了显著的性能提升。"
    },
    {
        "title": "Auto-Vocabulary 3D Object Detection",
        "summary": "Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.",
        "url": "http://arxiv.org/abs/2512.16077v1",
        "published_date": "2025-12-18T01:53:40+00:00",
        "updated_date": "2025-12-18T01:53:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haomeng Zhang",
            "Kuan-Chuan Peng",
            "Suhas Lohit",
            "Raymond A. Yeh"
        ],
        "tldr": "The paper introduces Auto-Vocabulary 3D Object Detection (AV3DOD), a novel framework that automatically generates class names for detected 3D objects using 2D vision-language models, achieving state-of-the-art performance on ScanNetV2 and SUNRGB-D datasets.",
        "tldr_zh": "该论文介绍了自动词汇三维物体检测（AV3DOD），这是一种新型框架，它使用二维视觉语言模型自动生成检测到的三维物体的类别名称，并在ScanNetV2和SUNRGB-D数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "开放词汇3D物体检测方法能够定位训练期间未见类别的3D框。尽管名义上如此，现有方法在训练和推理阶段都依赖于用户指定的类别。我们提出研究自动词汇3D物体检测 (AV3DOD)，其中类别是为检测到的物体自动生成的，无需任何用户输入。为此，我们引入语义分数 (SS) 来评估生成的类别名称的质量。然后，我们开发了一个新的框架，AV3DOD，该框架利用2D视觉-语言模型 (VLMs)，通过图像描述、伪3D框生成和特征空间语义扩展来生成丰富的语义候选。AV3DOD 在 ScanNetV2 和 SUNRGB-D 数据集上实现了最先进 (SOTA) 的定位 (mAP) 和语义质量 (SS) 性能。值得注意的是，它超越了最先进的 CoDA，在 ScanNetV2 上整体 mAP 提升了 3.48，并且在 SS 方面实现了 24.5% 的相对改进。"
    },
    {
        "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion",
        "summary": "We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.",
        "url": "http://arxiv.org/abs/2512.16023v1",
        "published_date": "2025-12-17T23:16:02+00:00",
        "updated_date": "2025-12-17T23:16:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liudi Yang",
            "Yang Bai",
            "George Eskandar",
            "Fengyi Shen",
            "Mohammad Altillawi",
            "Dong Chen",
            "Ziyuan Liu",
            "Abhinav Valada"
        ],
        "tldr": "The paper introduces CoVAR, a multi-modal diffusion model that co-generates videos and robot actions from text instructions and initial states, enabling robotic policy learning by leveraging unlabeled video data and outperforming existing methods.",
        "tldr_zh": "该论文介绍了CoVAR，一种多模态扩散模型，可以根据文本指令和初始状态共同生成视频和机器人动作，通过利用未标记的视频数据实现机器人策略学习，并且优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出一种生成遵循文本指令的视频-动作对的方法，该方法从初始图像观测和机器人的关节状态出发。我们的方法能够自动为视频扩散模型提供动作标签，克服了动作标注普遍缺失的问题，并使其能被充分利用于机器人策略学习。现有方法要么采用限制紧密耦合跨模态信息共享的两阶段流程，要么依赖于调整单模态扩散模型以适应联合分布，而这无法充分利用预训练的视频知识。为了克服这些限制，我们 (1) 使用一个并行的、专用的动作扩散模型扩展了预训练的视频扩散模型，以此保留预训练知识，(2) 引入了桥接注意力机制以实现有效的跨模态交互，(3) 设计了一个动作细化模块，用于将粗略动作转化为低分辨率数据集上的精确控制。在多个公共基准测试和真实世界数据集上的大量评估表明，我们的方法能够生成更高质量的视频和更准确的动作，并显著优于现有的基线方法，为利用大规模视频数据进行机器人学习提供了一个可扩展的框架。"
    },
    {
        "title": "From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection",
        "summary": "Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.",
        "url": "http://arxiv.org/abs/2512.15971v1",
        "published_date": "2025-12-17T21:06:36+00:00",
        "updated_date": "2025-12-17T21:06:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manuel Nkegoum",
            "Minh-Tan Pham",
            "Élisa Fromont",
            "Bruno Avignon",
            "Sébastien Lefèvre"
        ],
        "tldr": "This paper explores using Vision-Language Models (VLMs) for few-shot multispectral object detection, adapting Grounding DINO and YOLO-World to handle multispectral data and showing significant improvements over specialized models in data-scarce scenarios.",
        "tldr_zh": "这篇论文探索了使用视觉语言模型 (VLM) 进行小样本多光谱目标检测，通过调整 Grounding DINO 和 YOLO-World 以处理多光谱数据，并在数据稀缺情况下显著优于专用模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "多光谱目标检测对于安全敏感型应用至关重要，例如自动驾驶和监控，在这些应用中，在各种光照条件下保持稳健的感知是必不可少的。然而，带标注的多光谱数据的有限可用性严重限制了深度检测器的训练。在这种数据匮乏的情况下，文本类信息可以作为一种有价值的语义监督来源。受到视觉-语言模型（VLMs）在计算机视觉领域取得的最新成功的启发，我们探索了它们在少样本多光谱目标检测中的潜力。具体来说，我们调整了两个具有代表性的基于VLM的检测器，Grounding DINO和YOLO-World，使其能够处理多光谱输入，并提出了一种有效的机制来整合文本、可见光和热红外模态。通过在两个流行的多光谱图像基准测试集FLIR和M3FD上进行的大量实验，我们证明了基于VLM的检测器不仅在少样本情况下表现出色，显著优于使用可比数据训练的专用多光谱模型，而且在完全监督的设置下也取得了有竞争力的或更优异的结果。我们的研究结果表明，大规模VLM学习到的语义先验能够有效地迁移到未见过的光谱模态，为数据高效的多光谱感知提供了一条强大的途径。"
    },
    {
        "title": "City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs",
        "summary": "Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/",
        "url": "http://arxiv.org/abs/2512.15933v1",
        "published_date": "2025-12-17T19:59:31+00:00",
        "updated_date": "2025-12-17T19:59:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dwip Dalal",
            "Utkarsh Mishra",
            "Narendra Ahuja",
            "Nebojsa Jojic"
        ],
        "tldr": "The paper introduces CityNav, a challenging benchmark for evaluating MLLM-driven embodied agents in real-world city navigation, and proposes a novel method called Verbalization of Path (VoP) to improve performance.",
        "tldr_zh": "该论文介绍了CityNav，这是一个用于评估MLLM驱动的具身智能体在现实城市导航中能力的具有挑战性的基准，并提出了一种名为路径文本化（VoP）的新方法来提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "利用多模态大型语言模型 (MLLM) 开发具身智能体为解决复杂的现实世界任务提供了巨大的潜力。然而，目前的评估基准仍然主要以语言为中心，或严重依赖于模拟环境，很少探究实际现实场景所需的那种细致入微的、知识密集型的推理。为了弥合这一关键差距，我们提出了稀疏接地视觉导航任务，该任务专门用于评估 MLLM 在具有挑战性的知识密集型现实世界环境中进行序列决策的能力。我们通过 CityNav 将此任务付诸实践，CityNav 是一个综合的基准，包含四个不同的全球城市，专门用于评估基于原始 MLLM 驱动的智能体在城市导航中的表现。智能体需要仅依靠视觉输入和内部多模态推理，在没有额外环境标注或专门的架构修改的情况下，依次导航 50 多个决策点。至关重要的是，智能体必须通过解释特定城市的线索和识别地标来自主实现定位，执行空间推理，并战略性地规划和执行到达目的地的路线。通过广泛的评估，我们证明了当前最先进的 MLLM 和标准推理技术（例如，思维链、反思）在这个具有挑战性的环境中表现明显不佳。为了解决这个问题，我们提出了路径言语化 (VoP)，它通过从 MLLM 中探测一个显式的认知地图（关键地标和朝向目的地的方向）来显式地扎根智能体的内部推理，从而显著提高导航成功率。项目网页：https://dwipddalal.github.io/AgentNav/"
    },
    {
        "title": "Multi-Modal Semantic Communication",
        "summary": "Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.",
        "url": "http://arxiv.org/abs/2512.15691v1",
        "published_date": "2025-12-17T18:47:22+00:00",
        "updated_date": "2025-12-17T18:47:22+00:00",
        "categories": [
            "cs.LG",
            "cs.IT",
            "eess.SP",
            "eess.SY"
        ],
        "authors": [
            "Matin Mortaheb",
            "Erciyes Karakaya",
            "Sennur Ulukus"
        ],
        "tldr": "This paper introduces a multi-modal semantic communication framework leveraging text-based queries to guide information extraction from images, improving efficiency in bandwidth-constrained complex scenes.",
        "tldr_zh": "本文介绍了一种多模态语义通信框架，该框架利用基于文本的查询来指导从图像中提取信息，从而提高了带宽受限的复杂场景中的效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "语义通信旨在传输与任务最相关的信息，而非原始数据，从而为远程呈现、增强现实和遥感等应用提供显著的通信效率增益。近期基于Transformer的方法使用自注意力图来识别图像内的信息区域，但在包含多个物体的复杂场景中，自注意力缺乏明确的任务指导，表现往往不佳。为了解决这个问题，我们提出了一种新型多模态语义通信框架，它集成了基于文本的用户查询来引导信息提取过程。我们提出的系统采用了一种跨模态注意力机制，将视觉特征与语言嵌入融合，以生成视觉数据的软相关性分数。基于这些分数和瞬时信道带宽，我们使用一种算法，利用独立训练的编码器-解码器对，以自适应分辨率传输图像块，其总比特率与信道容量相匹配。在接收端，重建并组合这些图像块，以保留任务关键信息。这种灵活且以目标驱动的设计能够在复杂和带宽受限的环境中实现高效的语义通信。"
    },
    {
        "title": "Olaf: Bringing an Animated Character to Life in the Physical World",
        "summary": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.",
        "url": "http://arxiv.org/abs/2512.16705v1",
        "published_date": "2025-12-18T16:10:18+00:00",
        "updated_date": "2025-12-18T16:10:18+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "David Müller",
            "Espen Knoop",
            "Dario Mylonopoulos",
            "Agon Serifi",
            "Michael A. Hopkins",
            "Ruben Grandia",
            "Moritz Bächer"
        ],
        "tldr": "This paper presents a reinforcement learning approach to control a robotic Olaf, focusing on mechanical design, stylized motion, noise reduction, and thermal management to achieve believable character animation in hardware.",
        "tldr_zh": "本文提出了一种使用强化学习控制机器人Olaf的方法，重点关注机械设计、风格化运动、降噪和热管理，以在硬件中实现可信的角色动画。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7,
        "summary_zh": "动画角色通常以非物理的方式移动，并且其比例远非典型的步行机器人可比。这为机械设计和风格化运动控制领域的创新提供了一个理想的平台。在本文中，我们将奥拉夫带入物理世界，依靠由动画参考引导的强化学习进行控制。为了创造奥拉夫的脚沿身体移动的错觉，我们在柔软的泡沫裙下隐藏了两条不对称的腿。为了将驱动器装入角色内部，我们在手臂、嘴巴和眼睛中使用了球面和平面连杆机构。由于行走循环会产生刺耳的接触声音，我们引入了额外的奖励，以显著降低冲击噪声。由角色纤细的脖颈中的小型驱动器带动的大脑袋，带来了过热的风险，这种风险又被服装放大。为了防止驱动器过热，我们将温度值作为额外的输入提供给策略，引入新的奖励以将其保持在界限内。我们在仿真和硬件上验证了我们建模的有效性，展示了装扮的机器人角色前所未有的可信度水平。"
    },
    {
        "title": "Single-View Shape Completion for Robotic Grasping in Clutter",
        "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.",
        "url": "http://arxiv.org/abs/2512.16449v1",
        "published_date": "2025-12-18T12:11:05+00:00",
        "updated_date": "2025-12-18T12:11:05+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Abhishek Kashyap",
            "Yuxuan Yang",
            "Henrik Andreasson",
            "Todor Stoyanov"
        ],
        "tldr": "This paper uses diffusion models for single-view 3D shape completion in cluttered scenes to improve robotic grasping of household objects, achieving better grasp success rates than baselines.",
        "tldr_zh": "该论文利用扩散模型在杂乱场景中进行单视图3D形状补全，从而提高机器人对家用物体的抓取能力，并实现了比基线方法更高的抓取成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在基于视觉的机器人操作中，单个相机视角通常只能捕捉到目标物体的一侧，而在杂乱场景中，额外的遮挡进一步限制了可见性。因此，观测到的几何信息是不完整的，抓取估计算法的性能也会受到影响。为了解决这一局限性，我们利用扩散模型，从单视角获取的部分深度观测中进行类别级别的3D形状补全，从而重建完整的物体几何形状，为抓取规划提供更丰富的上下文信息。我们的方法聚焦于具有多样几何形状的常见家用物品，生成完整的3D形状，并将其作为下游抓取推理网络的输入。与以往主要考虑孤立物体或最小程度杂乱的工作不同，我们在具有家用物体的真实杂乱场景中评估形状补全和抓取。在杂乱场景的初步评估中，我们的方法始终比不进行形状补全的简单基线方法提高了23%的抓取成功率，并且超过了最新的形状补全方法19%。我们的代码可以在https://amm.aass.oru.se/shape-completion-grasping/上找到。"
    },
    {
        "title": "Towards Closing the Domain Gap with Event Cameras",
        "summary": "Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.",
        "url": "http://arxiv.org/abs/2512.16178v1",
        "published_date": "2025-12-18T04:57:32+00:00",
        "updated_date": "2025-12-18T04:57:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "M. Oltan Sevinc",
            "Liao Wu",
            "Francisco Cruz"
        ],
        "tldr": "This paper explores using event cameras to mitigate the domain gap caused by day-night lighting differences in autonomous driving, demonstrating more consistent performance compared to traditional grayscale cameras.",
        "tldr_zh": "本文探讨了使用事件相机来缓解自动驾驶中因昼夜光照差异引起的领域差距，与传统灰度相机相比，其表现出更一致的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "尽管传统相机是端到端驾驶的主要传感器，但当训练数据条件与部署环境不匹配时，它们的性能会显著下降，这个问题被称为领域鸿沟。在这项工作中，我们考虑昼夜光照差异的领域鸿沟。我们提出使用事件相机作为潜在的替代方案，而不是传统相机，它可以保持在光照条件领域鸿沟中的性能，而无需额外的调整。我们的结果表明，事件相机在不同的光照条件下保持了更一致的性能，表现出的领域迁移惩罚通常可与灰度帧相媲美或更小，并且在跨领域场景中提供了优越的基线性能。"
    },
    {
        "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators",
        "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.",
        "url": "http://arxiv.org/abs/2512.16069v1",
        "published_date": "2025-12-18T01:27:34+00:00",
        "updated_date": "2025-12-18T01:27:34+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Maolin Lei",
            "Edoardo Romiti",
            "Arturo Laurenzi",
            "Rui Dai",
            "Matteo Dalle Vedove",
            "Jiatao Ding",
            "Daniele Fontanelli",
            "Nikos Tsagarakis"
        ],
        "tldr": "This paper presents a task-driven computational framework for designing modular manipulators, optimizing morphology and mounted pose alongside trajectory planning, and introducing a novel bi-branch morphology to offload torque and extend workspace.",
        "tldr_zh": "该论文提出了一个任务驱动的计算框架，用于设计模块化机械臂，优化形态和安装姿态以及轨迹规划，并引入了一种新型的双分支形态来卸载扭矩并扩展工作空间。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "模块化机械臂由预制且可互换的模块组成，可在各种任务中提供高度的适应性。然而，其部署需要在运动学、动力学和物理约束下，联合优化形态和安装姿态，并生成可行的运动轨迹。此外，传统的单分支设计通常通过增加连杆长度来扩展工作范围，这很容易导致基座关节的扭矩限制被超出。为了应对这些挑战，我们提出了一个统一的、任务驱动的计算框架，集成了跨不同形态的轨迹规划以及形态和安装姿态的协同优化。在该框架内，开发了一种分层模型预测控制（HMPC）策略，以实现冗余和非冗余机械臂的运动规划。对于设计优化，采用 CMA-ES 算法来有效探索由离散形态配置和连续安装姿态组成的混合搜索空间。同时，引入了一种虚拟模块抽象方法来启用双分支形态，允许辅助分支从主分支卸载扭矩，从而扩展可实现的工作空间，而无需增加单个关节模块的容量。在抛光、钻孔和抓取放置任务上的大量仿真和硬件实验证明了所提出的框架的有效性。结果表明：1) 该框架可以生成多个满足运动学和动力学约束的可行设计，同时避免给定任务的环境碰撞；2) 通过定制成本函数，可以实现灵活的设计目标，例如最大化可操作性、最小化关节力矩或减少模块数量；3) 可以实现能够在大型工作空间中操作的双分支形态，而无需更强大的基本模块。"
    },
    {
        "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
        "summary": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
        "url": "http://arxiv.org/abs/2512.16814v1",
        "published_date": "2025-12-18T17:55:15+00:00",
        "updated_date": "2025-12-18T17:55:15+00:00",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "authors": [
            "William English",
            "Dominic Simon",
            "Sumit Kumar Jha",
            "Rickard Ewetz"
        ],
        "tldr": "This paper introduces Grammar Forced Translation (GraFT), a novel framework that improves the accuracy and efficiency of translating natural language to temporal logic by constraining the output token space at each step using LLMs and demonstrates improved performance on standard benchmarks.",
        "tldr_zh": "这篇论文介绍了语法强制翻译 (GraFT)，这是一种新颖的框架，通过使用 LLM 在每一步约束输出标记空间，提高了自然语言到时序逻辑的翻译的准确性和效率，并在标准基准上展示了改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "将自然语言(NL)翻译成形式化语言（如时序逻辑(TL)）对于人与机器人和自主系统之间的通信至关重要。当前最先进的方法将此任务分解为原子命题(AP)提升阶段和翻译阶段。然而，现有方法在准确提升、共指现象的存在以及从有限数据中学习方面存在困难。在本文中，我们提出了一种名为语法强制翻译(GraFT)的NL到TL翻译框架。该框架基于如下观察：先前的工作通过让语言模型迭代地从其完整的词汇表中预测token来解决提升和翻译步骤。相比之下，GraFT通过将每步有效的输出token集合从完整的词汇表限制为少数几个，从而降低了这两个任务的复杂性。这种解空间缩减是通过利用每个问题的独特属性来实现的。我们还提供了一个理论依据，解释为什么解空间缩减能够带来更有效的学习。我们使用CW、GLTL和Navi基准来评估GraFT的有效性。与最先进的翻译方法相比，可以观察到GraFT的端到端翻译准确率平均提高了5.49%，跨领域翻译准确率平均提高了14.06%。"
    },
    {
        "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
        "summary": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
        "url": "http://arxiv.org/abs/2512.16813v1",
        "published_date": "2025-12-18T17:54:20+00:00",
        "updated_date": "2025-12-18T17:54:20+00:00",
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Bahman Abolhassani",
            "Tugba Erpek",
            "Kemal Davaslioglu",
            "Yalin E. Sagduyu",
            "Sastry Kompella"
        ],
        "tldr": "This paper presents a QMIX-based MARL framework to enhance swarm communication resilience against reactive jamming by optimizing channel selection and power allocation, achieving near-optimal performance compared to a genie-aided policy and outperforming baseline methods.",
        "tldr_zh": "本文提出了一种基于QMIX的MARL框架，通过优化信道选择和功率分配来增强集群通信对抗反应式干扰的弹性。与拥有先验知识的策略相比，该方法实现了接近最优的性能，并且优于基线方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "反应式干扰器通过有选择地中断代理间通信，破坏队形结构的完整性和任务的成功，对机器人集群网络构成严重的安全性威胁。传统的对抗措施，如固定功率控制或静态信道跳频，在很大程度上对这种自适应对手无效。本文提出了一种基于QMIX算法的多智能体强化学习(MARL)框架，以提高集群通信在反应式干扰下的韧性。我们考虑一个由多个发射机-接收机对共享信道的网络，同时一个具有马尔可夫阈值动态的反应式干扰器感知总功率并做出相应反应。每个智能体联合选择发射频率（信道）和功率，QMIX学习一个集中式但可分解的动作价值函数，从而实现协调但分散的执行。我们在一个没有信道重用设置中，将QMIX与一个拥有预知信息的优化策略进行基准测试；并在一个更普遍的启用信道重用的衰落环境中，将其与局部置信上限(UCB)和一个无状态反应策略进行基准测试。仿真结果表明，QMIX能够快速收敛到几乎与预知信息优化策略相匹配的合作策略，同时比基线方法实现了更高的吞吐量和更低的干扰发生率，从而证明了MARL在保障自主集群在竞争环境中安全方面的有效性。"
    },
    {
        "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning",
        "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver",
        "url": "http://arxiv.org/abs/2512.16698v1",
        "published_date": "2025-12-18T16:00:47+00:00",
        "updated_date": "2025-12-18T16:00:47+00:00",
        "categories": [
            "cs.AI",
            "cs.CG"
        ],
        "authors": [
            "Mahbub E Sobhani",
            "Md. Faiyaz Abdullah Sayeedi",
            "Mohammad Nehad Alam",
            "Proma Hossain Progga",
            "Swakkhar Shatabda"
        ],
        "tldr": "This paper compares single-agent and multi-agent frameworks for diagram-grounded geometry problem solving using LLMs, finding that multi-agent approaches generally benefit open-source models and newer benchmarks, but not necessarily closed-source models or classic benchmarks.",
        "tldr_zh": "该论文比较了单智能体和多智能体框架在使用LLM解决基于图表的几何问题时的效果，发现多智能体方法通常有利于开源模型和较新的基准，但对闭源模型或经典基准不一定如此。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于图表的几何问题求解是多模态大型语言模型（MLLMs）的关键基准，但多智能体设计相对于单智能体设计的优势仍不明确。我们系统地比较了单智能体和多智能体流程在四个视觉数学基准测试上的表现：Geometry3K、MathVerse、OlympiadBench 和 We-Math。对于开源模型，多智能体始终提高性能。例如，Qwen-2.5-VL (7B) 在 Geometry3K 上获得了 +6.8 分的提升，而 Qwen-2.5-VL (32B) 获得了 +3.3 分的提升，并且两个 Qwen-2.5-VL 变体在 OlympiadBench 和 We-Math 上都有进一步的提升。相比之下，对于经典基准测试，闭源的 Gemini-2.0-Flash 通常在单智能体模式下表现更好，而多智能体仅在较新的 We-Math 数据集上产生适度改进。 这些发现表明，多智能体流程为开源模型提供了明显的优势，并且可以在较新的、不太熟悉的基准测试上辅助强大的专有系统，但智能体分解并非普遍最优。所有代码、数据和推理文件均可在 https://github.com/faiyazabdullah/Interpreter-Solver 获取。"
    },
    {
        "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery",
        "summary": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.",
        "url": "http://arxiv.org/abs/2512.16468v1",
        "published_date": "2025-12-18T12:39:13+00:00",
        "updated_date": "2025-12-18T12:39:13+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Danial Safaei",
            "Siddartha Khastgir",
            "Mohsen Alirezaei",
            "Jeroen Ploeg",
            "Son Tong",
            "Xingyu Zhao"
        ],
        "tldr": "This paper introduces Decisive Feature Fidelity (DFF), a new metric to evaluate the agreement in causal evidence used by autonomous systems between real and synthetic data, and proposes a DFF-guided calibration scheme for simulators.",
        "tldr_zh": "本文介绍了一种新的指标，即决定性特征保真度 (DFF)，用于评估自动驾驶系统在真实和合成数据之间使用的因果证据的一致性，并提出了一种 DFF 指导的模拟器校准方案。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "使用合成数据进行的虚拟测试已成为自动驾驶汽车 (AV) 安全保障的基石。尽管通过先进的模拟器和生成式人工智能在提高视觉真实感方面取得了进展，但近期的研究表明，仅像素级保真度并不能确保从模拟到现实世界的可靠迁移。真正重要的是，被测系统 (SUT) 在现实环境和模拟环境中是否都基于相同的因果证据做出决策——而不仅仅是图像对人类来说是否“看起来真实”。本文针对缺乏这种基于行为的保真度指标的问题，引入了决定性特征保真度 (DFF)，这是一种新的 SUT 特有指标，它扩展了现有的保真度范围，以捕捉机制对等性——即跨领域支持 SUT 决策的潜在因果证据的一致性。DFF 利用可解释人工智能 (XAI) 方法来识别和比较驱动匹配的真实-合成对的 SUT 输出的决定性特征。我们进一步提出了基于反事实解释的实用估计器，以及一种 DFF 引导的校准方案，以增强模拟器的保真度。在 2126 对匹配的 KITTI-VirtualKITTI2 对上的实验表明，DFF 揭示了传统输出值保真度所忽略的差异。 此外，结果表明，DFF 引导的校准可以在不牺牲各种 SUT 的输出值保真度的情况下，提高决定性特征和输入级别的保真度。"
    },
    {
        "title": "StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm",
        "summary": "Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \\href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.",
        "url": "http://arxiv.org/abs/2512.16444v1",
        "published_date": "2025-12-18T11:58:10+00:00",
        "updated_date": "2025-12-18T11:58:10+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yadong Li",
            "Tong Zhang",
            "Bo Huang",
            "Zhen Cui"
        ],
        "tldr": "This paper introduces SC2BA, a new StarCraft II environment designed for benchmarking multi-agent reinforcement learning algorithms in an adversarial setting, addressing limitations of the existing SMAC benchmark.",
        "tldr_zh": "本文介绍了SC2BA，一个新的星际争霸II环境，旨在用于在对抗环境中基准测试多智能体强化学习算法，从而解决现有SMAC基准的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "深度多智能体强化学习(MARL)算法在协同智能领域蓬勃发展，而星际争霸多智能体挑战(SMAC)被广泛用作其中的基准。然而，MARL算法的假想对手实际上是以固定的内置AI模式进行配置和控制的，这导致算法评估的多样性和通用性不足。为了解决这个问题，在这项工作中，我们建立了一个多智能体算法对算法的环境，名为星际争霸II 竞技场（SC2BA），以在对抗范式中刷新MARL算法的基准。SC2BA环境以星际争霸为基础设施，专门为算法间的对抗而创建，并考虑了公平性、可用性和可定制性，同时开发了一个具有易于使用的接口/模块的对抗PyMARL（APyMARL）库。基于SC2BA，我们在两种对抗模式下对那些经典的MARL算法进行基准测试：双算法配对对抗和多算法混合对抗，前者进行成对算法的对抗，而后者侧重于对来自一组算法的多种行为的对抗。大量的基准实验在这些已完成算法的有效性、敏感性和可扩展性方面展示了一些引人深思的观察/问题。SC2BA环境以及复现的实验已在\\href{https://github.com/dooliu/SC2BA}{Github}上发布，我们相信这项工作将在未来几年标志着MARL领域的新一步。"
    },
    {
        "title": "Spatia: Video Generation with Updatable Spatial Memory",
        "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
        "url": "http://arxiv.org/abs/2512.15716v1",
        "published_date": "2025-12-17T18:59:59+00:00",
        "updated_date": "2025-12-17T18:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinjing Zhao",
            "Fangyun Wei",
            "Zhening Liu",
            "Hongyang Zhang",
            "Chang Xu",
            "Yan Lu"
        ],
        "tldr": "Spatia is a novel video generation framework that uses a 3D point cloud as spatial memory, updated via visual SLAM, to improve long-term spatio-temporal consistency and enable 3D-aware editing.",
        "tldr_zh": "Spatia 是一种新的视频生成框架，它使用 3D 点云作为空间记忆，并通过视觉 SLAM 进行更新，以提高长期时空一致性并实现 3D 感知的编辑。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "现有视频生成模型难以维持长期的空间和时间一致性，这是由于视频信号的密集性和高维度所致。为了克服这一局限，我们提出 Spatia，一个空间记忆感知的视频生成框架，它将一个 3D 场景点云显式地保存为持久的空间记忆。Spatia 迭代地生成以该空间记忆为条件的视频片段，并通过视觉 SLAM 持续更新该空间记忆。这种动态-静态解耦设计增强了整个生成过程中的空间一致性，同时保留了模型生成逼真动态实体的能力。此外，Spatia 还实现了诸如显式相机控制和 3D 感知交互式编辑等应用，为可扩展的、记忆驱动的视频生成提供了一个几何基础框架。"
    },
    {
        "title": "DenseBEV: Transforming BEV Grid Cells into 3D Objects",
        "summary": "In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.",
        "url": "http://arxiv.org/abs/2512.16818v1",
        "published_date": "2025-12-18T17:59:22+00:00",
        "updated_date": "2025-12-18T17:59:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marius Dähling",
            "Sebastian Krebs",
            "J. Marius Zöllner"
        ],
        "tldr": "The paper introduces DenseBEV, a novel two-stage anchor generation method for multi-camera 3D object detection that uses BEV feature cells directly as anchors, achieving state-of-the-art performance on nuScenes and Waymo datasets.",
        "tldr_zh": "该论文介绍了DenseBEV，一种新颖的两阶段anchor生成方法，用于多摄像头3D目标检测，该方法直接使用BEV特征单元作为anchor，并在nuScenes和Waymo数据集上实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "当前研究中，基于鸟瞰图（BEV）的Transformer正被越来越多地应用于多摄像头3D目标检测。传统模型通常采用随机查询作为锚点，并对其进行迭代优化。最近的进展是对这些随机查询进行补充，或用辅助网络的检测结果取代它们。我们提出了一种更加直观和高效的方法，直接使用BEV特征单元格作为锚点。这种端到端方法利用BEV查询的密集网格，将每个单元格视为最终检测任务的潜在目标。因此，我们提出了一种专门为多摄像头3D目标检测设计的新型两阶段锚点生成方法。为了解决大量查询带来的注意力机制扩展问题，我们应用了基于BEV的非极大值抑制（Non-Maximum Suppression），只允许梯度流经未被抑制的目标。这确保了高效的训练，而无需后处理。通过直接使用来自BEVFormer等编码器的BEV特征作为目标查询，时间BEV信息被固有地嵌入。在已经嵌入在我们的目标查询中的时间BEV信息的基础上，我们引入了一种混合时间建模方法，通过整合先前的检测结果来进一步提高检测性能。在nuScenes数据集上的评估表明，即使使用更稀疏的BEV网格和更少的初始锚点，我们的方法在NDS和mAP指标上也能实现持续且显著的改进。它对于小目标尤其有效，在nuScenes上将行人检测的mAP提高了3.8%，在Waymo上将LET-mAP提高了8%。将我们的方法（命名为DenseBEV）应用于具有挑战性的Waymo开放数据集，产生了最先进的性能，实现了60.7%的LET-mAP，超过了之前的最佳结果5.4%。代码已在https://github.com/mdaehl/DenseBEV上提供。"
    },
    {
        "title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories",
        "summary": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.",
        "url": "http://arxiv.org/abs/2512.16784v1",
        "published_date": "2025-12-18T17:18:45+00:00",
        "updated_date": "2025-12-18T17:18:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Simone Teglia",
            "Claudia Melis Tonti",
            "Francesco Pro",
            "Leonardo Russo",
            "Andrea Alfarano",
            "Leonardo Pentassuglia",
            "Irene Amerini"
        ],
        "tldr": "The paper introduces R3ST, a synthetic 3D dataset for traffic analysis that integrates realistic vehicle trajectories derived from real-world drone footage, addressing the common limitation of synthetic datasets lacking realistic motion.",
        "tldr_zh": "本文介绍了一个名为R3ST的合成3D数据集，用于交通分析。该数据集结合了从真实无人机拍摄的镜头中提取的真实车辆轨迹，解决了合成数据集普遍缺乏真实运动的问题。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "数据集对于训练和评估用于交通分析和增强道路安全的计算机视觉模型至关重要。现有的真实数据集符合真实场景，捕捉了真实的道路物体行为，但通常缺乏精确的地面真实标注。相比之下，合成数据集发挥着关键作用，可以在没有额外成本或额外时间的情况下标注大量帧。然而，合成数据集的一个普遍缺点是缺乏真实的车辆运动，因为轨迹是使用人工智能模型或基于规则的系统生成的。在这项工作中，我们介绍了R3ST（真实3D合成轨迹），这是一个通过生成合成3D环境并整合来自SinD的真实轨迹来克服此限制的合成数据集，SinD是一个从无人机航拍记录的鸟瞰数据集。所提出的数据集弥合了合成数据和真实轨迹之间的差距，推进了道路车辆轨迹预测的研究，提供了准确的多模态地面真实标注和真实的人工驾驶车辆轨迹。"
    },
    {
        "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach",
        "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.",
        "url": "http://arxiv.org/abs/2512.16456v1",
        "published_date": "2025-12-18T12:21:17+00:00",
        "updated_date": "2025-12-18T12:21:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masashi Hatano",
            "Saptarshi Sinha",
            "Jacob Chalk",
            "Wei-Hong Li",
            "Hideo Saito",
            "Dima Damen"
        ],
        "tldr": "This paper introduces a new dataset of gaze-primed human reaching motions and trains a diffusion model to generate realistic reaching motions conditioned on text and goal location, focusing on both reach and prime success.",
        "tldr_zh": "该论文介绍了一个新的凝视启动的人类抓取运动数据集，并训练了一个扩散模型来生成真实的、以文本和目标位置为条件的抓取运动，重点关注抓取成功和启动成功。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "人体运动生成是一个具有挑战性的任务，旨在创建模仿自然人类行为的逼真运动。我们专注于对接物体/位置进行拾取或放置的良好研究的行为——即，从远处发现物体/位置，称为注视启动，然后是接近并到达目标位置的运动。为此，我们首次整理了来自五个公开数据集（即HD-EPIC、MoGaze、HOT3D、ADT和GIMO）的2.37万个用于到达目标物体位置的注视启动的人体运动序列。我们预训练了一个文本条件扩散的运动生成模型，然后在我们整理的序列上，以目标姿势或位置为条件对其进行微调。重要的是，我们通过包括“到达成功率”和新引入的“启动成功率”在内的几个指标，评估了生成的运动模仿自然人类运动的能力。在最大的数据集HD-EPIC上，当以目标物体位置为条件时，我们的模型达到了60%的启动成功率和89%的到达成功率。"
    },
    {
        "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation",
        "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.",
        "url": "http://arxiv.org/abs/2512.16199v1",
        "published_date": "2025-12-18T05:46:15+00:00",
        "updated_date": "2025-12-18T05:46:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jerrin Bright",
            "Zhibo Wang",
            "Dmytro Klepachevskyi",
            "Yuhao Chen",
            "Sirisha Rambhatla",
            "David Clausi",
            "John Zelek"
        ],
        "tldr": "Avatar4D is a pipeline for generating customizable, domain-specific synthetic human motion datasets without manual annotations, validated through the creation of Syn2Sport, a synthetic sports dataset for training and benchmarking pose estimation models.",
        "tldr_zh": "Avatar4D是一个用于生成可定制的、特定领域的合成人体运动数据集的流程，无需手动标注。通过创建Syn2Sport（一个用于训练和评估姿态估计模型的合成体育数据集）来验证其有效性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们提出了Avatar4D，一个真实世界可迁移的流程，用于生成可定制的合成人体运动数据集，专门针对特定领域的应用。与以往专注于通用、日常运动且灵活性有限的研究不同，我们的方法提供了对身体姿态、外观、相机视角和环境背景的细粒度控制，并且不需要任何手动标注。为了验证Avatar4D的影响，我们专注于体育运动，其中特定领域的人体动作和运动模式对运动理解提出了独特的挑战。在此背景下，我们推出了Syn2Sport，一个涵盖多种运动项目（包括棒球和冰球）的大规模合成数据集。Avatar4D的特点是拥有高保真的4D（随时间变化的3D几何）人体运动序列，具有不同的运动员外观，并在不同的环境中渲染。我们在Syn2Sport上对几种最先进的姿态估计模型进行了基准测试，并证明了它们在监督学习、零样本迁移到真实世界数据以及跨运动项目的泛化方面的有效性。此外，我们还评估了生成的合成数据在特征空间中与真实数据集的接近程度。我们的结果突显了这种系统在生成可扩展、可控制且可迁移的人体数据集方面的潜力，这些数据集适用于各种特定领域的任务，而无需依赖特定领域的真实数据。"
    },
    {
        "title": "Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings",
        "summary": "This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.",
        "url": "http://arxiv.org/abs/2512.15993v1",
        "published_date": "2025-12-17T21:55:50+00:00",
        "updated_date": "2025-12-17T21:55:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lars Beckers",
            "Arno Waes",
            "Aaron Van Campenhout",
            "Toon Goedemé"
        ],
        "tldr": "This paper presents a robotic mower that uses deep learning to identify and preserve visually diverse vegetation patches, increasing garden biodiversity without species-level supervision. It demonstrates a correlation between visual diversity and ecological richness.",
        "tldr_zh": "本文介绍了一种机器人割草机，它使用深度学习来识别和保护视觉上多样化的植被区域，从而在没有物种级别监督的情况下提高花园生物多样性。该研究展示了视觉多样性与生态丰富度之间的相关性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文提出了一种机器人割草框架，该框架通过视觉感知和自适应决策主动提升花园生物多样性。与被动重野化方法不同，该系统采用深度特征空间分析来识别并保护相机图像中视觉上多样化的植被斑块，通过选择性地禁用割草机刀片实现。在PlantNet300K上预训练的ResNet50网络提供具有生态学意义的嵌入，从中导出的全局偏差指标无需物种层面监督即可估计生物多样性。这些估计驱动一种选择性割草算法，该算法动态地在割草和保护行为之间切换。该系统在一个经过修改的商用机器人割草机上实现，并在受控的模拟草坪和真实的园林数据集上进行了验证。结果表明，嵌入空间离散度和专家生物多样性评估之间存在很强的相关性，证实了深度视觉多样性作为生态丰富度代理的可行性，以及所提出的割草决策方法的有效性。此类系统的广泛应用将把生态上毫无价值的单一栽培草坪转变为充满活力、有价值的生物群落，从而促进城市生物多样性。"
    },
    {
        "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
        "summary": "At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed \"Pixio\", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.",
        "url": "http://arxiv.org/abs/2512.15715v1",
        "published_date": "2025-12-17T18:59:58+00:00",
        "updated_date": "2025-12-17T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lihe Yang",
            "Shang-Wen Li",
            "Yang Li",
            "Xinjie Lei",
            "Dong Wang",
            "Abdelrahman Mohamed",
            "Hengshuang Zhao",
            "Hu Xu"
        ],
        "tldr": "This paper introduces Pixio, an enhanced masked autoencoder for self-supervised visual pre-training on 2B web images, demonstrating competitive performance on various downstream tasks compared to latent-space approaches like DINOv3.",
        "tldr_zh": "本文介绍了Pixio，一种增强的掩码自编码器，用于在20亿网络图像上进行自监督视觉预训练。结果表明，与DINOv3等潜在空间方法相比，Pixio在各种下游任务中表现出具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在最基本的层面上，像素是我们感知世界视觉信息的来源。像素包含各个层面的信息，从低层属性到高层概念。自编码器代表了一种经典且长期存在的范式，用于从像素或其他原始输入中学习表征。在这项工作中，我们证明了基于自编码器的自监督学习在今天仍然具有竞争力，并且可以为下游任务产生强大的表征，同时保持简单、稳定和高效。我们的模型，代号为“Pixio”，是一种增强的掩码自编码器(MAE)，具有更具挑战性的预训练任务和更强大的架构。该模型使用自策展策略，在20亿张网络爬取图像上进行训练，且人工策展程度极低。Pixio在各种实际的下游任务中表现出竞争力，包括单目深度估计（例如，Depth Anything）、前馈3D重建（即，MapAnything）、语义分割和机器人学习，其表现优于或与以相似规模训练的DINOv3相媲美。我们的结果表明，像素空间自监督学习可以作为一种有希望的替代方案和对潜在空间方法的补充。"
    },
    {
        "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
        "summary": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
        "url": "http://arxiv.org/abs/2512.15702v1",
        "published_date": "2025-12-17T18:53:29+00:00",
        "updated_date": "2025-12-17T18:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuwei Guo",
            "Ceyuan Yang",
            "Hao He",
            "Yang Zhao",
            "Meng Wei",
            "Zhenheng Yang",
            "Weilin Huang",
            "Dahua Lin"
        ],
        "tldr": "This paper introduces a novel self-resampling training framework for autoregressive video diffusion models, aiming to mitigate exposure bias and improve temporal consistency in generated videos, especially over long horizons.",
        "tldr_zh": "本文提出了一种新的自重采样训练框架，用于自回归视频扩散模型，旨在减轻暴露偏差，并提高生成的视频中的时间一致性，尤其是在长程视频中。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自回归视频扩散模型在世界模拟方面具有潜力，但容易受到源于训练-测试不匹配的曝光偏差的影响。虽然最近的研究通过后训练解决这个问题，但它们通常依赖于双向教师模型或在线判别器。为了实现端到端解决方案，我们引入了重采样强制（Resampling Forcing），这是一个无教师框架，可以从头开始大规模地训练自回归视频模型。我们方法的核心是一个自重采样方案，该方案在训练期间模拟推理时历史帧上的模型误差。以这些降级的历史为条件，一个稀疏因果掩码强制执行时间因果关系，同时允许使用帧级扩散损失进行并行训练。为了方便高效的长程生成，我们进一步引入了历史路由，这是一种无需参数的机制，可动态检索每个查询的前k个最相关的历史帧。实验表明，我们的方法实现了与基于蒸馏的基线相当的性能，同时由于原生长度训练，在较长视频上表现出卓越的时间一致性。"
    },
    {
        "title": "Non-Asymptotic Global Convergence of PPO-Clip",
        "summary": "Reinforcement learning (RL) has gained attention for aligning large language models (LLMs) via reinforcement learning from human feedback (RLHF). The actor-only variants of Proximal Policy Optimization (PPO) are widely applied for their efficiency. These algorithms incorporate a clipping mechanism to improve stability. Besides, a regularization term, such as the reverse KL-divergence or a more general \\(f\\)-divergence, is introduced to prevent policy drift. Despite their empirical success, a rigorous theoretical understanding of the problem and the algorithm's properties is limited. This paper advances the theoretical foundations of the PPO-Clip algorithm by analyzing a deterministic actor-only PPO algorithm within the general RL setting with \\(f\\)-divergence regularization under the softmax policy parameterization. We derive a non-uniform Lipschitz smoothness condition and a Łojasiewicz inequality for the considered problem. Based on these, a non-asymptotic linear convergence rate to the globally optimal policy is established for the forward KL-regularizer. Furthermore, stationary convergence and local linear convergence are derived for the reverse KL-regularizer.",
        "url": "http://arxiv.org/abs/2512.16565v1",
        "published_date": "2025-12-18T14:06:37+00:00",
        "updated_date": "2025-12-18T14:06:37+00:00",
        "categories": [
            "math.OC",
            "cs.LG"
        ],
        "authors": [
            "Yin Liu",
            "Qiming Dai",
            "Junyu Zhang",
            "Zaiwen Wen"
        ],
        "tldr": "This paper provides a theoretical analysis of the PPO-Clip algorithm with f-divergence regularization, proving non-asymptotic global convergence under certain conditions and offering insights into its stability and convergence properties.",
        "tldr_zh": "本文对带有f-散度正则化的PPO-Clip算法进行了理论分析，证明了在特定条件下的非渐近全局收敛性，并深入了解了其稳定性和收敛特性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "强化学习 (RL) 正因其通过人类反馈强化学习 (RLHF) 对齐大型语言模型 (LLM) 的能力而备受关注。近端策略优化 (PPO) 的仅有行动者变体因其效率而被广泛应用。这些算法包含一个裁剪机制以提高稳定性。此外，还会引入诸如逆向 KL 散度或更一般的 \\(f\\)-散度等正则化项来防止策略漂移。尽管它们在经验上取得了成功，但对该问题和算法性质的严格理论理解是有限的。本文通过分析具有 \\(f\\)-散度正则化的确定性仅有行动者 PPO 算法，并在 softmax 策略参数化下的一般 RL 设置中，推进了 PPO-Clip 算法的理论基础。我们推导出了所考虑问题的非均匀 Lipschitz 光滑性条件和 Łojasiewicz 不等式。在此基础上，我们为前向 KL 正则化器建立了到全局最优策略的非渐近线性收敛速度。 此外，我们还推导出了逆向 KL 正则化器的平稳收敛和局部线性收敛。"
    },
    {
        "title": "A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level",
        "summary": "Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.",
        "url": "http://arxiv.org/abs/2512.16076v1",
        "published_date": "2025-12-18T01:51:54+00:00",
        "updated_date": "2025-12-18T01:51:54+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jia Hu",
            "Junqi Li",
            "Xuerun Yan",
            "Jintao Lai",
            "Lianhua An"
        ],
        "tldr": "This paper presents a fully automated simulation platform calibration method for evaluating automated vehicles, ensuring high accuracy at both the vehicle and traffic flow levels, with significant improvements in accuracy and efficiency compared to existing methods.",
        "tldr_zh": "本文提出了一种全自动的仿真平台校准方法，用于评估自动驾驶车辆，确保在车辆和交通流量层面都具有高精度，与现有方法相比，在准确性和效率方面都有显著提高。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "仿真测试是评估自动驾驶车辆 (AV) 的基本方法。为确保其可靠性，准确复现自动驾驶车辆与背景交通之间的交互至关重要，而这需要有效的校准。然而，现有的校准方法通常无法充分实现这一目标。为了弥补这一差距，本研究引入了一种仿真平台校准方法，以确保车辆和交通流层面上的高精度。该方法有以下几个关键特性：(1) 具备车辆间交互的校准能力；(2) 具有精度保障；(3) 具有更高的效率；(4) 具备流水线式校准能力。该方法与未校准的基线方法和一种最先进的校准方法进行了基准测试。结果表明，该方法将交互复现的准确率提高了83.53%，并将校准效率提高了76.75%。此外，它在车辆层面和交通流层面的指标上都保持了准确性，提升幅度为51.9%。值得注意的是，整个校准过程是完全自动化的，无需人工干预。"
    },
    {
        "title": "SORS: A Modular, High-Fidelity Simulator for Soft Robots",
        "summary": "The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.",
        "url": "http://arxiv.org/abs/2512.15994v1",
        "published_date": "2025-12-17T21:58:46+00:00",
        "updated_date": "2025-12-17T21:58:46+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Manuel Mekkattu",
            "Mike Y. Michelis",
            "Robert K. Katzschmann"
        ],
        "tldr": "The paper introduces SORS, a modular high-fidelity simulator for soft robots, validated through real-world experiments and control optimization simulations. It claims to bridge the sim-to-real gap with better extensibility, fidelity, and usability within the soft robotics research area.",
        "tldr_zh": "该论文介绍了一个名为SORS的模块化、高保真软体机器人模拟器，通过真实世界的实验和控制优化模拟进行了验证。它声称通过更好的可扩展性、保真度和可用性来弥合模拟与现实之间的差距，适用于软体机器人研究领域。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "复杂软机器人在多物理场环境中的部署需要先进的仿真框架，不仅要捕捉不同类型材料之间的相互作用，还要准确地转化为实际性能。软机器人因其大的非线性变形、材料不可压缩性以及接触相互作用而带来了独特的建模挑战，这些都使数值稳定性和物理精度复杂化。尽管最近取得了进展，但机器人仿真器通常难以以可扩展且与应用相关的方式对这些现象进行建模。我们提出了SORS(软体-刚体仿真器)，这是一种通用、高保真的仿真器，旨在处理软机器人应用的这些复杂性。我们基于有限元法的能量框架允许模块化扩展，从而能够包含定制设计的材料和驱动模型。为了确保物理上一致的接触处理，我们集成了一种基于序列二次规划的约束非线性优化方法，从而可以对接触现象进行稳定而精确的建模。我们通过一系列真实的实验验证了我们的仿真器，包括悬臂梁挠度、软机器人手臂的压力驱动以及来自PokeFlex数据集的接触相互作用。此外，我们展示了我们的框架在软机器人腿控制优化方面的潜力。这些测试证实，我们的仿真器可以高物理保真度地捕捉基本材料行为和复杂的驱动动力学。通过弥合这些具有挑战性的领域中的模拟到现实差距，我们的方法提供了一种经过验证的工具，用于原型设计下一代软机器人，填补了软机器人生态系统中可扩展性、保真度和可用性的空白。"
    },
    {
        "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals",
        "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/",
        "url": "http://arxiv.org/abs/2512.16791v1",
        "published_date": "2025-12-18T17:25:47+00:00",
        "updated_date": "2025-12-18T17:25:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shuting Zhao",
            "Zeyu Xiao",
            "Xinrong Chen"
        ],
        "tldr": "The paper introduces KineST, a kinematics-guided state space model for accurate and temporally coherent full-body motion tracking from sparse signals, particularly relevant in AR/VR scenarios, using bidirectional kinematic scanning and mixed spatiotemporal representation learning.",
        "tldr_zh": "该论文提出了KineST，一种基于运动学引导的状态空间模型，用于从稀疏信号中进行准确且时间一致的全身运动跟踪，尤其适用于AR/VR场景。该模型采用双向运动学扫描和混合时空表示学习。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "全身运动追踪在增强现实/虚拟现实（AR/VR）应用中发挥着至关重要的作用，它连接了物理交互和虚拟交互。然而，基于头戴式显示器所获取的稀疏信号（头戴式显示器是AR/VR场景中的主要设备）来重建逼真且多样化的全身姿态极具挑战性。现有的姿态重建方法通常会产生高昂的计算成本，或者依赖于分别建模空间和时间依赖性，使其难以在准确性、时间连贯性和效率之间取得平衡。为了解决这个问题，我们提出了 KineST，一种新颖的运动学引导的状态空间模型，它有效地提取时空依赖性，同时整合局部和全局姿态感知。创新之处在于两个核心思想。首先，为了更好地捕捉复杂的关节关系，我们将状态空间对偶框架内的扫描策略重新定义为运动学引导的双向扫描，从而嵌入了运动学先验。其次，采用混合时空表示学习方法，紧密耦合空间和时间上下文，平衡了准确性和平滑性。此外，引入了几何角速度损失，对旋转变化施加具有物理意义的约束，以进一步提高运动稳定性。大量实验表明，KineST在轻量级框架内，在准确性和时间一致性方面都具有卓越的性能。项目主页：https://kaka-1314.github.io/KineST/"
    },
    {
        "title": "YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images",
        "summary": "The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.",
        "url": "http://arxiv.org/abs/2512.16493v1",
        "published_date": "2025-12-18T13:00:05+00:00",
        "updated_date": "2025-12-18T13:00:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huma Hafeez",
            "Matthew Garratt",
            "Jo Plested",
            "Sankaran Iyer",
            "Arcot Sowmya"
        ],
        "tldr": "The paper introduces YOLO11-4K, a modified YOLO architecture with a novel detection head and GhostConv backbone, optimized for real-time object detection in high-resolution (4K) panoramic images, showing improved speed and accuracy compared to YOLO11. They also contribute a new annotated dataset.",
        "tldr_zh": "该论文介绍了YOLO11-4K，一种修改过的YOLO架构，具有新颖的检测头和基于GhostConv的主干网络，专门为高分辨率（4K）全景图像的实时目标检测而优化，与YOLO11相比，速度和准确性均有所提高。他们还贡献了一个新的注释数据集。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "全景360度图像的处理对目标检测提出了严峻挑战，原因在于其固有的空间扭曲、宽广的视野以及超高分辨率的输入。诸如YOLO的传统检测器针对标准图像尺寸（例如640x640像素）进行了优化，并且常常难以应对360度视觉中典型的4K或更高分辨率图像所带来的计算需求。为了解决这些局限性，我们引入了YOLO11-4K，这是一个专为4K全景图像定制的高效实时检测框架。该架构结合了一种新颖的多尺度检测头，它包含一个P2层，以提高对粗略尺度下经常遗漏的小目标的敏感度，以及一个基于GhostConv的主干网络，以在不牺牲表征能力的情况下降低计算复杂度。为了实现评估，我们手动标注了CVIP360数据集，生成了6876个帧级别边界框，并创建了一个公开可用的、可用于检测的4K全景场景基准。YOLO11-4K在0.50 IoU下实现了0.95 mAP，每帧推断耗时28.3毫秒，与YOLO11（112.3毫秒）相比延迟降低了75%，同时还提高了精度（0.50 IoU下的mAP为0.95，而YOLO11为0.908）。这种效率和精度的平衡使得在广阔的360度环境中进行鲁棒的目标检测成为可能，从而使该框架适用于实际场景中的高分辨率全景应用。虽然这项工作侧重于4K全向图像，但该方法广泛适用于自主导航、监控和增强现实中的高分辨率检测任务。"
    },
    {
        "title": "ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation",
        "summary": "3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.",
        "url": "http://arxiv.org/abs/2512.16234v1",
        "published_date": "2025-12-18T06:28:42+00:00",
        "updated_date": "2025-12-18T06:28:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zichen Geng",
            "Zeeshan Hayder",
            "Wei Liu",
            "Hesheng Wang",
            "Ajmal Mian"
        ],
        "tldr": "The paper introduces ARMFlow, a MeanFlow-based autoregressive framework for online 3D human reaction generation, addressing challenges in motion fidelity, real-time inference, and autoregressive adaptability by using Bootstrap Contextual Encoding (BSCE) to reduce error accumulation.",
        "tldr_zh": "该论文介绍了ARMFlow，一个基于MeanFlow的自回归框架，用于在线生成3D人体反应。它通过使用Bootstrap Contextual Encoding (BSCE) 来减少误差累积，从而解决在运动保真度、实时推理和自回归适应性方面的挑战。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "3D人体反应生成面临三个主要挑战：(1) 高动作保真度，(2) 实时推理，以及 (3) 在线场景下的自回归适应性。现有方法无法同时满足这三者。我们提出了ARMFlow，一种基于MeanFlow的自回归框架，用于建模演员和反应者动作之间的时间依赖关系。它由一个因果上下文编码器和一个基于MLP的速度预测器组成。我们在训练中引入了引导上下文编码（BSCE），编码生成的历史轨迹而非真实轨迹，以减轻自回归生成中的误差累积。我们进一步引入了ARMFlow的离线变体ReMFlow，在离线方法中以最快的推理速度实现了最先进的性能。我们的ARMFlow通过以下方式解决了在线设置的关键限制：(1) 通过全局上下文编码器增强语义对齐；(2) 在单步推理中实现高精度和低延迟；以及 (3) 通过BSCE减少累积误差。我们的单步在线生成在InterHuman和InterX数据集上，在FID指标上超越现有在线方法超过40%，同时在使用部分序列条件的情况下，性能与离线最先进水平相当。"
    },
    {
        "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation",
        "summary": "This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.",
        "url": "http://arxiv.org/abs/2512.16143v1",
        "published_date": "2025-12-18T03:55:17+00:00",
        "updated_date": "2025-12-18T03:55:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yueyang Hu",
            "Haiyong Jiang",
            "Haoxuan Song",
            "Jun Xiao",
            "Hao Pan"
        ],
        "tldr": "The paper introduces SegGraph, a novel method leveraging SAM segments and graph neural networks for improved few-shot 3D part segmentation by explicitly learning geometric features and enforcing intra-segment consistency.",
        "tldr_zh": "该论文介绍了一种名为SegGraph的新方法，它利用SAM分割和图神经网络，通过显式学习几何特征和强制执行段内一致性来改进少样本3D部件分割。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "本文提出了一种用于少样本3D零件分割的新颖框架。最新的研究进展表明，2D基础模型在少样本3D零件分割方面具有巨大的潜力。然而，如何有效地将来自基础模型的2D知识聚合到3D仍然是一个悬而未决的问题。现有方法要么忽略3D特征学习的几何结构，要么忽略SAM提供的高质量分组线索，导致分割不足和零件标签不一致。我们设计了一种新型的基于SAM分割图的传播方法，名为SegGraph，以显式地学习编码在SAM分割掩码中的几何特征。我们的方法通过建模段之间的相互重叠和邻接关系来编码几何特征，同时保持段内语义一致性。我们构建了一个分割图，概念上类似于地图集，其中节点代表分割部分，边捕获它们的空间关系（重叠/邻接）。每个节点自适应地调整2D基础模型特征，然后通过图神经网络传播，以学习全局几何结构。为了强制段内语义一致性，我们将分割特征映射到3D点，并采用一种新的视图方向加权融合方法，衰减来自低质量分割部分的贡献。在PartNet-E上的大量实验表明，我们的方法比所有竞争基线至少高出6.9个百分点的mIoU。进一步的分析表明，SegGraph在小组件和零件边界上表现尤为出色，证明了其卓越的几何理解能力。代码可在以下网址获取：https://github.com/YueyangHu2000/SegGraph。"
    },
    {
        "title": "A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots",
        "summary": "In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots",
        "url": "http://arxiv.org/abs/2512.16555v1",
        "published_date": "2025-12-18T13:58:17+00:00",
        "updated_date": "2025-12-18T13:58:17+00:00",
        "categories": [
            "cs.RO",
            "cs.FL",
            "cs.MA",
            "eess.SY"
        ],
        "authors": [
            "Marcelo Rosa",
            "José E. R. Cury",
            "Fabio L. Baldissera"
        ],
        "tldr": "This paper presents a Supervisory Control Theory-based approach for coordinating multiple robots in the autonomous construction of 3D structures, synthesizing a correct-by-construction controller replicated across robots.",
        "tldr_zh": "本文提出了一种基于监管控制理论的方法，用于协调多个机器人自主构建3D结构。该方法合成了一个基于正确性构造的控制器，并在多个机器人上复制使用。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "本文研究了协调多个机器人构建三维结构的问题。该问题涉及一组移动机器人相互作用，从而自主构建预定义的三维结构。我们的方法基于监督控制理论，它允许我们从表示单个机器人和目标结构的模型的合成中，产生出一个按构造正确的反应式控制器，称为监督器。当此监督器被复制到其他机器人上时，所有机器人即可完成目标结构的构建。"
    },
    {
        "title": "AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems",
        "summary": "As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable \"user\". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.",
        "url": "http://arxiv.org/abs/2512.16454v1",
        "published_date": "2025-12-18T12:18:59+00:00",
        "updated_date": "2025-12-18T12:18:59+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Tianhao Shao",
            "Kaixing Zhao",
            "Feng Liu",
            "Lixin Yang",
            "Bin Guo"
        ],
        "tldr": "The paper introduces MPBS, a framework for efficiently recruiting UAVs and UGVs for time-sensitive tasks using behavior-aware classification, mobility prediction, and dynamic priority scheduling, demonstrated on the GeoLife dataset.",
        "tldr_zh": "该论文介绍了MPBS，一个用于高效招募无人机和无人地面车辆来执行时间敏感型任务的框架。该框架结合了行为感知分类、移动性预测和动态优先级调度，并在GeoLife数据集上进行了验证。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "随着无人飞行器（UAV）和无人地面车辆（UGV）等无人系统在城市感知和应急响应等应用中变得越来越重要，如何高效地招募这些自主设备来执行时间敏感型任务已成为一项关键挑战。本文提出了MPBS（移动性感知预测和基于行为的调度），这是一个可扩展的任务招募框架，将每个设备视为一个可招募的“用户”。MPBS集成了三个关键模块：一个行为感知KNN分类器，一个用于预测设备移动性的时变马尔可夫预测模型，以及一个考虑任务紧急性和基站性能的动态优先级调度机制。通过将行为分类与时空预测相结合，MPBS可以实时地将任务自适应地分配给最合适的设备。在真实世界GeoLife数据集上的实验评估表明，MPBS显著提高了任务完成效率和资源利用率。所提出的框架为无人系统中的智能和协同调度提供了一种预测性的、行为感知的解决方案。"
    },
    {
        "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion",
        "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.",
        "url": "http://arxiv.org/abs/2512.16367v1",
        "published_date": "2025-12-18T10:07:06+00:00",
        "updated_date": "2025-12-18T10:07:06+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Sijia Chen",
            "Wei Dong"
        ],
        "tldr": "This paper presents a ground-aerial collaborative localization system (A2VISR) that uses active vision, single-ranging, inertial odometry, and optical flow to improve the robustness of flying robot localization in challenging environments. The system demonstrates robust online localization in various conditions, achieving an RMSE of approximately 0.09m.",
        "tldr_zh": "本文提出了一种地空协作定位系统（A2VISR），该系统利用主动视觉、单点测距、惯性里程计和光流来提高飞行机器人在复杂环境中定位的鲁棒性。该系统在各种条件下展示了强大的在线定位能力，实现了约0.09米的均方根误差。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文提出了一种实用的地面-空中协同系统方法，旨在增强飞行机器人在杂乱环境中定位的鲁棒性，尤其是在视觉传感器性能下降时。传统方法利用固定摄像头观察预先附着的标志物来估计飞行机器人的位置，但这种方法可能受到有限距离的约束并且容易发生捕获失败。为了解决这个问题，我们以更全面的方式改进了地面-空中定位框架，该框架集成了主动视觉、单点测距、惯性里程计和光流。首先，设计的安装在地面车辆上的主动视觉子系统可以动态旋转，以检测和跟踪空中机器人上的红外标志物，利用单个摄像头来扩大视野并提高目标识别能力。同时，单点测距的引入扩展了可行距离，并增强了在视觉退化情况下的重新捕获能力。在估计过程中，一种降维估计器基于多项式逼近和扩展滑动窗口融合多源测量数据，从而平衡了计算效率和冗余度。考虑到不同的传感器保真度，我们实现了一种自适应滑动置信度评估算法来评估测量质量，并根据移动方差动态调整权重参数。最后，在烟雾干扰、光照变化、障碍物遮挡、长时间视觉丢失和扩展操作范围等条件下的大量实验表明，所提出的方法实现了稳健的在线定位，平均均方根误差约为0.09米，同时保持了对捕获丢失和传感器故障的弹性。"
    },
    {
        "title": "Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception",
        "summary": "Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.",
        "url": "http://arxiv.org/abs/2512.16265v1",
        "published_date": "2025-12-18T07:27:09+00:00",
        "updated_date": "2025-12-18T07:27:09+00:00",
        "categories": [
            "cs.NI",
            "cs.RO"
        ],
        "authors": [
            "Bangya Liu",
            "Chengpo Yan",
            "Chenghao Jiang",
            "Suman Banerjee",
            "Akarsh Prabhakara"
        ],
        "tldr": "This paper addresses privacy concerns in cooperative perception systems that share raw sensor data and proposes a framework called SHARP to minimize privacy leakage, calling for collaborative research.",
        "tldr_zh": "该论文探讨了共享原始传感器数据的协同感知系统中的隐私问题，并提出了名为SHARP的框架来最大限度地减少隐私泄露，同时呼吁开展合作研究。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "车辆间协同感知有望提供稳健可靠的场景理解。 近年来，我们见证了实验性系统研究构建测试平台，共享原始空间传感器数据用于协同感知。 尽管精度有了显著提高，且这是自然的发展方向，但我们在此停下脚步，思考这种方法在最终被汽车制造商采用时可能面临的问题。 在本文中，我们首先论证了新的隐私问题将会出现，并阻碍利益相关者共享原始传感器数据。 接着，我们提出了SHARP，一个旨在最小化隐私泄露的研究框架，以推动利益相关者朝着基于原始数据的协同感知的宏伟目标前进。 最后，我们讨论了在实现我们提出的框架过程中，网络系统、移动计算、感知研究人员、行业和政府面临的开放性问题。"
    },
    {
        "title": "dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection",
        "summary": "Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/",
        "url": "http://arxiv.org/abs/2512.16011v1",
        "published_date": "2025-12-17T22:40:05+00:00",
        "updated_date": "2025-12-17T22:40:05+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jack Naylor",
            "Raghav Mishra",
            "Nicholas H. Barbara",
            "Donald G. Dansereau"
        ],
        "tldr": "The paper introduces $\\partial$LITE, a differentiable simulation pipeline for optimizing on-orbit inspection trajectories to improve the quality of visual sensor data, addressing challenges like specular reflection and dynamic lighting in LEO.",
        "tldr_zh": "该论文介绍了$\\partial$LITE，一个用于优化在轨检查轨迹的可微分模拟管线，旨在提高视觉传感器数据的质量，并解决了在低地球轨道（LEO）中诸如镜面反射和动态光照等挑战。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "对在轨航天器进行目视检查，对于希望规划维护、评估损伤并延长高价值在轨卫星寿命的航天器运营商来说，越来越受到关注。低地球轨道 (LEO) 的环境在规划最大化可见性、信息量和数据质量的检查操作时，带来了独特的挑战。来自航天器表面的阳光镜面反射、自身遮蔽以及 LEO 的动态光照，显著影响了整个轨道上捕获的数据质量。航天器之间的相对运动加剧了这种情况，这在检查期间引入了可变的成像距离和姿态。借助仿真规划检查轨迹是一种常见的方法。然而，专门设计和优化检查轨迹以提高近距离操作中最终图像质量的能力，在很大程度上仍未得到探索。在这项工作中，我们提出了 $\\partial$LITE，一个用于在轨检查操作的端到端可微仿真流程。我们利用最先进的可微渲染工具和定制的轨道传播器，实现基于视觉传感器数据的轨道参数的端到端优化。$\\partial$LITE 使我们能够自动设计非显而易见的轨迹，从而显著提高所获得数据的质量和效用。据我们所知，我们的可微检查规划流程是同类首创，并为航天器任务规划的现代计算方法提供了新的见解。项目页面：https://appearance-aware.github.io/dlite/"
    },
    {
        "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
        "summary": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
        "url": "http://arxiv.org/abs/2512.16770v1",
        "published_date": "2025-12-18T17:03:07+00:00",
        "updated_date": "2025-12-18T17:03:07+00:00",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "authors": [
            "William English",
            "Chase Walker",
            "Dominic Simon",
            "Rickard Ewetz"
        ],
        "tldr": "The paper introduces GinSign, a framework for grounding natural language into system signatures for temporal logic translation, improving grounded logical-equivalence scores for specifying and verifying system behaviors in autonomous systems.",
        "tldr_zh": "该论文介绍了一种名为 GinSign 的框架，用于将自然语言转换为系统签名，以进行时序逻辑翻译，从而提高在自主系统中指定和验证系统行为的接地元逻辑等效性分数。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "自然语言到时序逻辑（NL-to-TL）的翻译使工程师能够指定、验证和实施系统行为，而无需手动编写形式化规范，这对于构建可信赖的自主系统至关重要。虽然现有的 NL-to-TL 翻译框架已经展示了令人鼓舞的初步结果，但这些系统要么明确地假设可以访问准确的原子谓词定义，要么遭受低的已定义的翻译精度。在本文中，我们提出了一个名为 GinSign 的框架，用于将自然语言定义到系统签名中以进行时序逻辑翻译。该框架引入了一个定义模型，该模型学习将 NL 段落映射到给定系统签名的抽象任务：给定一个提升的 NL 规范和一个系统签名 $\\mathcal{S}$，分类器必须将每个提升的原子命题分配给签名定义的原子集合 $\\mathcal{P}$ 中的一个元素。我们将定义任务分层分解——首先预测谓词标签，然后选择适当类型的常量参数。 将此任务从自由形式的生成问题分解为结构化的分类问题，可以使用更小的掩码语言模型，并消除了对昂贵的大型语言模型的依赖。跨多个领域的实验表明，省略定义的框架倾向于产生句法上正确的提升 LTL，但在语义上与已定义的的目标表达式不等价，而我们的框架支持下游模型检查，并实现了 95.5% 的已定义的逻辑等价性分数，比 SOTA 提高了 1.4 倍。"
    },
    {
        "title": "Evaluation of Generative Models for Emotional 3D Animation Generation in VR",
        "summary": "Social interactions incorporate nonverbal signals to convey emotions alongside speech, including facial expressions and body gestures. Generative models have demonstrated promising results in creating full-body nonverbal animations synchronized with speech; however, evaluations using statistical metrics in 2D settings fail to fully capture user-perceived emotions, limiting our understanding of model effectiveness. To address this, we evaluate emotional 3D animation generative models within a Virtual Reality (VR) environment, emphasizing user-centric metrics emotional arousal realism, naturalness, enjoyment, diversity, and interaction quality in a real-time human-agent interaction scenario. Through a user study (N=48), we examine perceived emotional quality for three state of the art speech-driven 3D animation methods across two emotions happiness (high arousal) and neutral (mid arousal). Additionally, we compare these generative models against real human expressions obtained via a reconstruction-based method to assess both their strengths and limitations and how closely they replicate real human facial and body expressions. Our results demonstrate that methods explicitly modeling emotions lead to higher recognition accuracy compared to those focusing solely on speech-driven synchrony. Users rated the realism and naturalness of happy animations significantly higher than those of neutral animations, highlighting the limitations of current generative models in handling subtle emotional states. Generative models underperformed compared to reconstruction-based methods in facial expression quality, and all methods received relatively low ratings for animation enjoyment and interaction quality, emphasizing the importance of incorporating user-centric evaluations into generative model development. Finally, participants positively recognized animation diversity across all generative models.",
        "url": "http://arxiv.org/abs/2512.16081v1",
        "published_date": "2025-12-18T01:56:22+00:00",
        "updated_date": "2025-12-18T01:56:22+00:00",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Kiran Chhatre",
            "Renan Guarese",
            "Andrii Matviienko",
            "Christopher Peters"
        ],
        "tldr": "This paper evaluates speech-driven 3D animation generative models in a VR environment using user-centric metrics, revealing limitations in current approaches for handling subtle emotions and highlighting the need for improved realism and interaction quality.",
        "tldr_zh": "本文在VR环境中，采用以用户为中心的指标评估了语音驱动的3D动画生成模型，揭示了当前方法在处理微妙情感方面的局限性，并强调了改进现实感和交互质量的必要性。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "社交互动中会通过非语言信号（如面部表情和肢体动作）来传递情感，与言语相辅相成。生成模型在创建与语音同步的全身体非语言动画方面已展现出有前景的结果；然而，在二维环境中使用的统计指标评估未能充分捕捉用户感知到的情感，从而限制了我们对模型有效性的理解。为了解决这个问题，我们在虚拟现实(VR)环境中评估情感3D动画生成模型，着重于以用户为中心的指标：情感唤醒真实感、自然度、愉悦度、多样性以及在实时人机交互场景中的交互质量。通过一项用户研究（N=48），我们考察了三种最先进的语音驱动3D动画方法在两种情感（快乐（高唤醒）和中性（中等唤醒））下的感知情感质量。此外，我们将这些生成模型与通过基于重建的方法获得的真实人类表情进行比较，以评估它们的优势和局限性，以及它们在多大程度上复制了真实的人类面部和身体表情。我们的结果表明，明确对情感进行建模的方法比那些仅关注语音驱动同步的方法具有更高的识别准确率。用户对快乐动画的真实感和自然度评分明显高于中性动画，突显了当前生成模型在处理微妙情感状态方面的局限性。在面部表情质量方面，生成模型表现不如基于重建的方法，并且所有方法在动画愉悦度和交互质量方面的评分都相对较低，这强调了将以用户为中心的评估纳入生成模型开发的重要性。最后，参与者对所有生成模型的动画多样性都给予了积极评价。"
    },
    {
        "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation",
        "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.",
        "url": "http://arxiv.org/abs/2512.16767v1",
        "published_date": "2025-12-18T17:01:44+00:00",
        "updated_date": "2025-12-18T17:01:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyang Guo",
            "Ori Zhang",
            "Jax Xiang",
            "Alan Zhao",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "tldr": "The paper proposes Make-It-Poseable, a feed-forward framework for 3D humanoid character animation that manipulates a character's latent representation based on skeletal motion, offering improved posing quality and extending to 3D editing applications.",
        "tldr_zh": "该论文提出了Make-It-Poseable，一个用于3D人形角色动画的前馈框架，通过基于骨骼运动操纵角色的潜在表示，从而提供更高的姿势质量并扩展到3D编辑应用。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "三维角色姿态调整是计算机图形学和视觉领域的一项基本任务。然而，现有的方法，如自动绑定和姿态条件下的生成，经常面临诸如不准确的蒙皮权重预测、拓扑缺陷和较差的姿态一致性等挑战，限制了它们的鲁棒性和泛化能力。为了克服这些局限性，我们提出一种新颖的前馈框架Make-It-Poseable，它将角色姿态调整重新定义为潜在空间的变换问题。与传统流程中变形网格顶点不同，我们的方法通过直接操纵角色的潜在表示来重建新的姿态。我们方法的核心是一个潜在姿态Transformer，它基于骨骼运动来操纵形状token。密集姿态表示有助于实现精确的控制。为了确保高保真几何体并适应拓扑变化，我们还引入了一种潜在空间监督策略和一个自适应补全模块。我们的方法在姿态调整质量方面展现了卓越的性能。同时，它自然地扩展到三维编辑应用，如部件替换和细化。"
    },
    {
        "title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition",
        "summary": "Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/",
        "url": "http://arxiv.org/abs/2512.16727v1",
        "published_date": "2025-12-18T16:27:31+00:00",
        "updated_date": "2025-12-18T16:27:31+00:00",
        "categories": [
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Haochen Chang",
            "Pengfei Ren",
            "Buyuan Zhang",
            "Da Li",
            "Tianhao Han",
            "Haoyang Zhang",
            "Liang Xie",
            "Hongbo Chen",
            "Erwei Yin"
        ],
        "tldr": "The paper introduces OMG-Bench, a new large-scale benchmark dataset for skeleton-based online micro hand gesture recognition, and proposes a novel Hierarchical Memory-Augmented Transformer (HMATr) model for this task.",
        "tldr_zh": "该论文介绍了一个新的大规模手势识别基准数据集OMG-Bench，用于基于骨骼的在线微手势识别，并提出了一种新的分层记忆增强Transformer（HMATr）模型。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "基于手部骨骼的在线微手势识别对于VR/AR交互至关重要，但由于公开数据集的匮乏和任务相关的特定算法面临诸多挑战。微手势涉及微妙的运动模式，这使得构建具有精确骨骼和帧级别标注的数据集变得困难。为此，我们开发了一个多视角自监督流程，以自动生成骨骼数据，并辅以启发式规则和专家精炼进行半自动标注。基于此流程，我们推出了OMG-Bench，这是首个大规模的、基于骨骼的在线微手势识别公共基准数据集。它包含40个细粒度的手势类别，共计13,948个实例，分布在1,272个序列中，其特征为微妙的运动、快速的动态变化和连续的执行。为了应对这些挑战，我们提出了一种分层记忆增强Transformer (HMATr)，这是一个端到端的框架，它通过利用分层记忆库统一了手势检测和分类，这些记忆库存储了帧级别细节和窗口级别语义，以保持历史上下文。此外，它采用了从记忆初始化的可学习的位置感知查询，以隐式地编码手势位置和语义。实验表明，HMATr在检测率上比最先进的方法提高了7.6%，为在线微手势识别建立了一个强大的基线。项目页面：https://omg-bench.github.io/"
    },
    {
        "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering",
        "summary": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.",
        "url": "http://arxiv.org/abs/2512.16670v1",
        "published_date": "2025-12-18T15:41:08+00:00",
        "updated_date": "2025-12-18T15:41:08+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Ole Beisswenger",
            "Jan-Niklas Dihlmann",
            "Hendrik P. A. Lensch"
        ],
        "tldr": "FrameDiffuser introduces an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames conditioned on G-buffer data and its own previous output, enabling interactive applications with realistic lighting and shadows.",
        "tldr_zh": "FrameDiffuser 提出了一种自回归神经渲染框架，通过 G-buffer 数据和自身先前的输出为条件，生成时间上连贯、照片般逼真的帧，从而实现具有逼真光照和阴影的交互式应用。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "以下是翻译结果：\n\n用于交互式应用的神经渲染需要在逐帧基础上将几何和材质属性（G-buffer）转换为具有逼真光照的逼真图像。虽然最近基于扩散的方法在G-buffer条件下的图像合成方面显示出潜力，但它们面临着关键的局限性：像RGBX这样的单图像模型独立生成帧，缺乏时间一致性；而像DiffusionRenderer这样的视频模型对大多数消费级游戏设备来说计算成本过高，并且需要预先完整的序列，这使得它们不适合未来帧依赖于用户输入的交互式应用。我们介绍 FrameDiffuser，一个自回归神经渲染框架，它通过对G-buffer数据和模型自身之前的输出进行条件化，生成时间上一致的、逼真的帧。在初始帧之后，FrameDiffuser纯粹在传入的 G-buffer 数据上运行，包括几何体、材质和表面属性，同时使用其先前生成的帧进行时间引导，从而在数百到数千帧上保持稳定且在时间上一致的生成。我们的双重条件架构结合了用于结构引导的 ControlNet 和用于时间连贯性的 ControlLoRA。一个三阶段的训练策略实现了稳定的自回归生成。我们将我们的模型专门用于单个环境，优先考虑一致性和推理速度，而不是广泛的泛化能力，这表明与广义方法相比，特定于环境的训练能够以精确的光照、阴影和反射实现卓越的逼真质量。"
    },
    {
        "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction",
        "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.",
        "url": "http://arxiv.org/abs/2512.16564v1",
        "published_date": "2025-12-18T14:06:15+00:00",
        "updated_date": "2025-12-18T14:06:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kirill Mazur",
            "Marwan Taher",
            "Andrew J. Davison"
        ],
        "tldr": "The paper presents a dynamic 3D reconstruction system from monocular RGB video by decomposing the scene into moving 3D primitives and inferring their rigid motion over time, enabling persistent 4D scene reconstruction and outperforming existing methods.",
        "tldr_zh": "该论文提出了一种动态3D重建系统，该系统通过将场景分解为移动的3D图元并推断它们随时间的刚性运动，从而从单目RGB视频重建场景，实现持久的4D场景重建， 并且性能优于现有方法。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "我们提出了一种动态重建系统，该系统接收随意拍摄的单目RGB视频作为输入，并输出场景的完整且持久的重建结果。换句话说，我们不仅重建场景当前可见的部分，还重建所有先前观察到的部分，从而能够在所有时间步长上回放完整的重建结果。\n\n我们的方法将场景分解为一组刚性的3D基元，假设这些基元在场景中运动。利用估计的稠密2D对应关系，我们通过一个优化流程联合推断这些基元的刚性运动，从而生成一个场景的4D重建，即提供随时间动态运动的3D几何结构。为了实现这一点，我们还引入了一种机制来外推不可见物体的运动，利用运动分组技术来保持连续性。\n\n由此产生的系统实现了4D时空感知，提供了诸如可回放的铰接物体随时间的3D重建、多物体扫描和物体永久性等功能。在物体扫描和多物体数据集上，我们的系统在定量和定性方面均显著优于现有方法。"
    },
    {
        "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization",
        "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.",
        "url": "http://arxiv.org/abs/2512.16504v1",
        "published_date": "2025-12-18T13:15:52+00:00",
        "updated_date": "2025-12-18T13:15:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiushuo Cheng",
            "Jingjing Liu",
            "Catherine Morgan",
            "Alan Whone",
            "Majid Mirmehdi"
        ],
        "tldr": "This paper introduces a self-supervised contrastive learning approach for skeleton-based temporal action localization using snippet discrimination and multiscale feature fusion, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "本文提出了一种自监督对比学习方法，用于基于骨骼的动作时间定位，使用片段判别和多尺度特征融合，并在基准数据集上取得了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "自监督预训练范式已在使用对比学习的基于骨骼的动作识别中学习3D动作表示方面取得了巨大成功。然而，为基于骨骼的时序动作定位学习有效的表示仍然具有挑战性且未被充分探索。与视频级别的{动作}识别不同，检测动作边界需要时间敏感的特征，这些特征能够捕捉标签改变的相邻帧之间的细微差异。为此，我们制定了一个片段判别预训练任务进行自监督预训练，该任务将骨骼序列密集投影到非重叠片段中，并通过对比学习来促进在不同视频中区分它们的特征。此外，我们基于强大的基于骨骼的动作识别模型主干，通过融合中间特征和一个U型模块来增强帧级别定位的特征分辨率。我们的方法在BABEL上针对不同子集和评估协议，始终如一地改进了现有的基于骨骼的对比学习动作定位方法。通过在NTU RGB+D和BABEL上进行预训练，我们在PKUMMD上也实现了最先进的迁移学习性能。"
    },
    {
        "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation",
        "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.",
        "url": "http://arxiv.org/abs/2512.16360v1",
        "published_date": "2025-12-18T09:55:14+00:00",
        "updated_date": "2025-12-18T09:55:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotian Ling",
            "Zequn Chen",
            "Qiuying Chen",
            "Donglin Di",
            "Yongjia Ma",
            "Hao Li",
            "Chen Wei",
            "Zhulin Tao",
            "Xun Yang"
        ],
        "tldr": "The paper introduces EverybodyDance, a method for consistent multi-character animation that addresses the challenge of maintaining correct identity correspondence (IC) using a bipartite graph-based approach. It introduces an identity correspondence evaluation benchmark and shows improved performance over SOTA methods.",
        "tldr_zh": "该论文介绍了EverybodyDance，一种用于一致的多角色动画的方法，通过基于二分图的方法来解决保持正确的身份对应(IC)的挑战。它引入了一个身份对应评估基准，并显示出优于现有最佳方法的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "在单角色场景下，一致的姿态驱动角色动画已经取得了显著进展。然而，将这些进展扩展到多角色场景并非易事，尤其是在涉及到位置交换时。除了简单的缩放之外，核心挑战在于强制参考帧和生成帧中角色之间正确的身份对应关系（IC）。为了解决这个问题，我们引入了EverybodyDance，这是一个针对多角色动画中IC正确性的系统性解决方案。EverybodyDance构建于身份匹配图（IMG）之上，该图将生成帧和参考帧中的角色建模为加权完全二分图中的两个节点集合。边缘权重通过我们提出的掩码查询注意力（MQA）计算，量化了每对角色之间的亲和力。我们的关键洞见是将IC正确性形式化为一种图结构指标，并在训练期间对其进行优化。我们还提出了一系列专门为多角色动画量身定制的策略，包括身份嵌入引导、多尺度匹配策略和预分类采样，这些策略协同工作。最后，为了评估IC性能，我们整理了身份对应关系评估基准，专门用于多角色IC正确性。大量的实验表明，在IC和视觉保真度方面，EverybodyDance都显著优于最先进的基线方法。"
    },
    {
        "title": "Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs",
        "summary": "Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.",
        "url": "http://arxiv.org/abs/2512.16314v1",
        "published_date": "2025-12-18T08:54:24+00:00",
        "updated_date": "2025-12-18T08:54:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huayu Huang",
            "Chen Chen",
            "Banglei Guan",
            "Ze Tan",
            "Yang Shang",
            "Zhang Li",
            "Qifeng Yu"
        ],
        "tldr": "This paper proposes a ridge estimation-based fusion localization method for UAVs using vision and laser ranging to improve accuracy and robustness, especially under limited observation conditions where multicollinearity is a problem.",
        "tldr_zh": "本文提出了一种基于岭估计的无人机视觉和激光测距融合定位方法，旨在提高精度和鲁棒性，特别是在多重共线性问题突出的有限观测条件下。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "使用无人机搭载的多种传感器来追踪和测量目标是快速准确地定位目标的有效手段。本文提出一种基于岭估计的融合定位方法，结合了连续图像丰富的场景信息和激光测距高精度的优点，以提高定位精度。在长距离、小交叉角和大倾斜角等受限条件下，使用最小二乘估计算法时，设计矩阵的列向量会存在严重的多重共线性。多重共线性会导致病态问题，从而导致显著的不稳定性和较低的鲁棒性。引入岭估计是为了缓解受限观测条件下严重的共线性问题。实验结果表明，与基于单一信息的地面定位算法相比，我们的方法实现了更高的定位精度。此外，岭估计的引入有效地提高了鲁棒性，尤其是在受限观测条件下。"
    },
    {
        "title": "Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space",
        "summary": "This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.",
        "url": "http://arxiv.org/abs/2512.16133v1",
        "published_date": "2025-12-18T03:42:54+00:00",
        "updated_date": "2025-12-18T03:42:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ren Nakagawa",
            "Yang Yang",
            "Risa Shinoda",
            "Hiroaki Santo",
            "Kenji Oyama",
            "Fumio Okura",
            "Takenao Ohkawa"
        ],
        "tldr": "The paper introduces CattleAct, a data-efficient method for detecting cattle interactions in images by jointly learning action and interaction latent spaces. It demonstrates accurate interaction detection using a practical system integrating video and GPS inputs on a commercial-scale pasture.",
        "tldr_zh": "该论文介绍了CattleAct，一种数据高效的方法，用于通过联合学习动作和交互潜在空间来检测图像中的牛的交互。它展示了一个实用的系统，该系统集成了视频和GPS输入，并在商业规模的牧场上实现了准确的交互检测。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文介绍了一种从单张图像中自动检测放牧牛行为交互的方法及应用，这对于牛产业中的智能畜牧管理（例如发情检测）至关重要。虽然人类交互检测已得到积极研究，但牛交互检测面临着一个重要的挑战，即缺乏包含交互行为的综合性行为数据集，因为放牧牛的交互行为是罕见事件。因此，我们提出CattleAct，一种数据高效的交互检测方法，该方法将交互分解为单个牛动作的组合。具体而言，我们首先从大规模牛动作数据集中学习一个动作潜在空间。然后，我们通过使用对比学习对预训练的潜在空间进行微调来嵌入罕见交互，从而构建一个统一的动作和交互潜在空间。在所提出的方法之上，我们开发了一个集成了视频和GPS输入的实用工作系统。在商业规模的牧场上的实验表明，与基线方法相比，我们的方法实现了准确的交互检测。我们的实现可在 https://github.com/rakawanegan/CattleAct 找到。"
    },
    {
        "title": "Flexible Camera Calibration using a Collimator System",
        "summary": "Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration",
        "url": "http://arxiv.org/abs/2512.16113v1",
        "published_date": "2025-12-18T03:06:50+00:00",
        "updated_date": "2025-12-18T03:06:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shunkun Liang",
            "Banglei Guan",
            "Zhenbao Yu",
            "Dongcai Tan",
            "Pengju Sun",
            "Zibin Liu",
            "Qifeng Yu",
            "Yang Shang"
        ],
        "tldr": "This paper introduces a novel camera calibration method utilizing a collimator system, leveraging angle invariance and spherical motion constraints to simplify the calibration process and enable single-image calibration.",
        "tldr_zh": "本文介绍了一种利用准直器系统的新型相机标定方法，利用角度不变性和球面运动约束简化标定过程，并实现单图像标定。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "相机标定是摄影测量和三维视觉应用中的一个关键步骤。本文介绍了一种使用设计的平行光管系统的新型相机标定方法。我们的平行光管系统为相机提供了一个可靠且可控的标定环境。利用我们平行光管系统独特的几何光学特性，我们引入了一个角度不变约束，并进一步证明了标定目标和相机之间的相对运动符合球面运动模型。该约束将目标和相机之间原本的 6 自由度相对运动减少到 3 自由度的纯旋转运动。利用球面运动约束，我们提出了多图像的闭合形式线性求解器以及两图像的最小求解器用于相机标定。此外，我们还提出了一种基于角度不变约束的单平行光管图像标定算法。该算法消除了对相机运动的要求，为灵活、快速的标定提供了一种新颖的解决方案。我们的方法在合成和真实世界的实验中都进行了评估，验证了使用平行光管系统进行标定的可行性，并表明我们的方法优于现有的基线方法。演示代码可在 https://github.com/LiangSK98/CollimatorCalibration 获取。"
    },
    {
        "title": "Information theory and discriminative sampling for model discovery",
        "summary": "Fisher information and Shannon entropy are fundamental tools for understanding and analyzing dynamical systems from complementary perspectives. They can characterize unknown parameters by quantifying the information contained in variables, or measure how different initial trajectories or temporal segments of a trajectory contribute to learning or inferring system dynamics. In this work, we leverage the Fisher Information Matrix (FIM) within the data-driven framework of {\\em sparse identification of nonlinear dynamics} (SINDy). We visualize information patterns in chaotic and non-chaotic systems for both single trajectories and multiple initial conditions, demonstrating how information-based analysis can improve sampling efficiency and enhance model performance by prioritizing more informative data. The benefits of statistical bagging are further elucidated through spectral analysis of the FIM. We also illustrate how Fisher information and entropy metrics can promote data efficiency in three scenarios: when only a single trajectory is available, when a tunable control parameter exists, and when multiple trajectories can be freely initialized. As data-driven model discovery continues to gain prominence, principled sampling strategies guided by quantifiable information metrics offer a powerful approach for improving learning efficiency and reducing data requirements.",
        "url": "http://arxiv.org/abs/2512.16000v1",
        "published_date": "2025-12-17T22:08:21+00:00",
        "updated_date": "2025-12-17T22:08:21+00:00",
        "categories": [
            "cs.IT",
            "cs.LG",
            "math.AP",
            "math.DS"
        ],
        "authors": [
            "Yuxuan Bao",
            "J. Nathan Kutz"
        ],
        "tldr": "This paper explores using Fisher Information and Shannon Entropy with SINDy for more efficient data sampling in dynamical system modeling, demonstrating improvements in learning and data requirements across several scenarios.",
        "tldr_zh": "本文探讨了将费舍尔信息和香农熵与SINDy结合使用，以提高动态系统建模中的数据采样效率，并在多个场景中展示了学习和数据需求的改进。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "费希尔信息和香农熵是从互补角度理解和分析动力系统的基本工具。它们可以通过量化变量中包含的信息来表征未知参数，或测量不同初始轨迹或轨迹的时间片段对学习或推断系统动力学的贡献程度。在这项工作中，我们利用费希尔信息矩阵（FIM）在{\\em 非线性动力学稀疏辨识}（SINDy）的数据驱动框架中。我们可视化混沌和非混沌系统中单条轨迹和多个初始条件下的信息模式，展示了基于信息的分析如何通过优先考虑更具信息量的数据来提高采样效率和增强模型性能。通过对FIM进行谱分析，进一步阐明了统计装袋的优势。我们还展示了费希尔信息和熵度量如何在三种情况下提高数据效率：当仅可获得单条轨迹时，当存在可调控制参数时，以及当可以自由初始化多条轨迹时。随着数据驱动模型发现的持续普及，由可量化的信息度量指导的原则性采样策略为提高学习效率和减少数据需求提供了一种强大的方法。"
    },
    {
        "title": "Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface",
        "summary": "In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.",
        "url": "http://arxiv.org/abs/2512.16024v1",
        "published_date": "2025-12-17T23:16:51+00:00",
        "updated_date": "2025-12-17T23:16:51+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Rishabh Dev Yadav",
            "Shrey Agrawal",
            "Kamalakar Karlapalem"
        ],
        "tldr": "This paper presents a multi-robot system with linear actuators and a combined open/closed-loop controller to maintain the orientation of a payload on unknown, uneven terrains. The system's efficacy is demonstrated through simulations.",
        "tldr_zh": "本文介绍了一种多机器人负载运输系统，该系统利用线性致动器和一个结合了开环/闭环的控制器，在未知、不平坦的地形上保持负载的方向。通过仿真验证了该系统的有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 4,
        "overall_priority_score": 4,
        "summary_zh": "本文介绍了一种多机器人有效载荷运输系统，该系统能够在未知和不均匀倾斜的环境中运输有效载荷，同时保持有效载荷的期望姿态。为此，我们使用了定制的机器人，每个机器人的顶部都安装了一个线性致动器（活塞）。该系统持续监测有效载荷的姿态，并计算每个机器人所需的活塞高度，以维持有效载荷的期望姿态。在这项研究中，我们提出了一种开环控制器与闭环PID控制器相结合的方法来实现这一目标。由于我们的建模对地形类型不做任何假设，该系统可以在任何未知和不均匀的地形和倾斜度上工作。我们通过在各种具有不同且复杂地形的模拟环境中测试该控制器，展示了我们提出的控制器的有效性。"
    },
    {
        "title": "GFLAN: Generative Functional Layouts",
        "summary": "Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.",
        "url": "http://arxiv.org/abs/2512.16275v1",
        "published_date": "2025-12-18T07:52:47+00:00",
        "updated_date": "2025-12-18T07:52:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mohamed Abouagour",
            "Eleftherios Garyfallidis"
        ],
        "tldr": "The paper introduces GFLAN, a two-stage generative framework for automated floor plan generation that separates topological planning from geometric realization using specialized convolutional and graph neural networks. It aims to improve architectural reasoning in floor plan synthesis.",
        "tldr_zh": "该论文介绍了一种名为GFLAN的两阶段生成框架，用于自动生成楼层平面图，该框架利用专门的卷积和图神经网络将拓扑规划与几何实现分离。旨在提升楼层平面图合成中的架构推理能力。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "自动化平面图生成位于组合搜索、几何约束满足和功能设计需求的交叉点——这种汇集历来难以进行统一的计算处理。虽然最近的深度学习方法提高了现有技术的水平，但它们通常难以捕捉建筑推理：拓扑关系优先于几何实例化，功能约束通过邻接网络传播，以及循环模式从局部连接决策中涌现。为了应对这些根本挑战，本文介绍了一种生成式框架GFLAN，它通过将平面图合成显式分解为拓扑规划和几何实现来重构平面图合成过程。给定一个外部边界和一个前门位置，我们的方法摒弃了直接的像素到像素或墙体追踪生成，而倾向于一种原则性的两阶段分解。阶段A采用具有双编码器的专用卷积架构——将不变的空间上下文与不断演变的布局状态分离——通过可行位置上的离散概率图，在建筑包络内按顺序分配房间质心。阶段B构建一个将房间节点链接到边界顶点的异构图，然后应用一个由Transformer增强的图神经网络(GNN)，联合回归房间边界。"
    },
    {
        "title": "Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection",
        "summary": "Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.",
        "url": "http://arxiv.org/abs/2512.16123v1",
        "published_date": "2025-12-18T03:19:40+00:00",
        "updated_date": "2025-12-18T03:19:40+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Min Geun Song",
            "Gang Min Kim",
            "Woonmin Kim",
            "Yongsik Kim",
            "Jeonghyun Sim",
            "Sangbeom Park",
            "Huy Kang Kim"
        ],
        "tldr": "This paper explores using a convolutional autoencoder for denoising to defend against adversarial attacks on YOLOv5 object detection, showing a modest improvement in mAP after applying the defense.",
        "tldr_zh": "本文探讨了使用卷积自编码器进行去噪，以防御对 YOLOv5 目标检测的对抗攻击，结果表明应用防御后 mAP 有小幅提升。",
        "relevance_score": 3,
        "novelty_claim_score": 4,
        "clarity_score": 9,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "基于深度学习的目标检测模型在自动驾驶和安全监控系统等现实应用中发挥着关键作用，但它们仍然容易受到对抗样本的攻击。在这项工作中，我们提出一种基于自编码器的去噪防御方法，以恢复因对抗扰动而降低的目标检测性能。我们使用Perlin噪声对COCO数据集中的车辆相关图像进行对抗攻击，应用单层卷积自编码器来去除扰动，并使用YOLOv5评估检测性能。我们的实验表明，对抗攻击将bbox mAP从0.2890降低到0.1640，代表了43.3%的性能下降。应用我们提出的自编码器防御后，bbox mAP提高到0.1700（恢复3.7%），bbox mAP@50从0.2780增加到0.3080（提升10.8%）。这些结果表明，基于自编码器的去噪可以在无需模型重训练的情况下，提供针对对抗攻击的部分防御能力。"
    },
    {
        "title": "QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing",
        "summary": "This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.",
        "url": "http://arxiv.org/abs/2512.16325v1",
        "published_date": "2025-12-18T09:06:35+00:00",
        "updated_date": "2025-12-18T09:06:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nan Zhou",
            "Zuxin Li",
            "Fanhang Man",
            "Xuecheng Chen",
            "Susu Xu",
            "Fan Dang",
            "Chaopeng Hong",
            "Yunhao Liu",
            "Xiao-Ping Zhang",
            "Xinlei Chen"
        ],
        "tldr": "The paper introduces QUIDS, a multi-agent dispatching system for mobile crowdsensing that optimizes sensing coverage and reliability by using incentives and a novel metric called Aggregated Sensing Quality (ASQ). It achieved significant improvements in ASQ and map reconstruction accuracy in real-world experiments.",
        "tldr_zh": "该论文介绍了一种名为QUIDS的多智能体调度系统，用于移动众包感知，通过使用激励和一种名为聚合感知质量 (ASQ) 的新指标来优化感知覆盖率和可靠性。 在真实世界的实验中，它在 ASQ 和地图重建精度方面取得了显着改进。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "本文旨在解决非专用车辆移动群智感知（NVMCS）系统中实现最优信息质量（QoI）的挑战。主要障碍包括相互关联的感知覆盖范围、感知可靠性和车辆的动态参与问题。为此，我们提出了 QUIDS，即一种质量感知的激励驱动多智能体调度系统，该系统在预算约束下确保高感知覆盖范围和可靠性。 QUIDS 引入了一种新颖的度量标准，即聚合感知质量（ASQ），通过整合覆盖范围和可靠性来量化捕获 QoI。我们还开发了一种互助信任感知的车辆调度算法，该算法在不确定性下估算感知可靠性并分配激励，从而进一步提高 ASQ。使用来自城域 NVMCS 部署的真实世界数据进行的评估表明，QUIDS 比非调度场景下的 ASQ 提高了 38%，比最先进的方法提高了 10%。它还将所有算法的重建地图误差降低了 39-74%。通过质量感知的激励机制联合优化覆盖范围和可靠性，QUIDS 能够实现低成本、高质量的城市监控，而无需专用基础设施，适用于交通和环境感知等智慧城市场景。"
    },
    {
        "title": "Collimator-assisted high-precision calibration method for event cameras",
        "summary": "Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.",
        "url": "http://arxiv.org/abs/2512.16092v1",
        "published_date": "2025-12-18T02:16:22+00:00",
        "updated_date": "2025-12-18T02:16:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zibin Liu",
            "Shunkun Liang",
            "Banglei Guan",
            "Dongcai Tan",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "tldr": "This paper introduces a collimator-assisted calibration method for event cameras, designed for high-precision and long-range measurements, demonstrating improved accuracy and reliability compared to existing methods.",
        "tldr_zh": "该论文介绍了一种用于事件相机的准直器辅助校准方法，旨在实现高精度和远距离测量，并证明其相比现有方法具有更高的准确性和可靠性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "事件相机是一种新型的、受大脑启发的视觉传感器，具有高动态范围和高时间分辨率等优势。 事件相机的几何标定，即确定其内参和外参，尤其是在远距离测量场景中，仍然是一个重要的挑战。 为了解决远距离和高精度测量的双重需求，我们提出了一种利用带有闪烁星型图案的平行光管的事件相机标定方法。 该方法首先利用平行光管的球体运动模型线性求解相机参数，然后进行非线性优化以高精度地细化这些参数。 通过在不同条件下进行的全面真实世界实验，我们证明了所提出的方法在精度和可靠性方面始终优于现有的事件相机标定方法。"
    },
    {
        "title": "Tracking Wildfire Assets with Commodity RFID and Gaussian Process Modeling",
        "summary": "This paper presents a novel, cost-effective, and scalable approach to track numerous assets distributed in forested environments using commodity Radio Frequency Identification (RFID) targeting wildfire response applications. Commodity RFID systems suffer from poor tag localization when dispersed in forested environments due to signal attenuation, multi-path effects and environmental variability. Current methods to address this issue via fingerprinting rely on dispersing tags at known locations {\\em a priori}. In this paper, we address the case when it is not possible to tag known locations and show that it is possible to localize tags to accuracies comparable to global positioning systems (GPS) without such a constraint. For this, we propose Gaussian Process to model various environments solely based on RF signal response signatures and without the aid of additional sensors such as global positioning GPS or cameras, and match an unknown RF to the closest match in a model dictionary. We utilize a new weighted log-likelihood method to associate an unknown environment with the closest environment in a dictionary of previously modeled environments, which is a crucial step in being able to use our approach. Our results show that it is possible to achieve localization accuracies of the order of GPS, but with passive commodity RFID, which will allow the tracking of dozens of wildfire assets within the vicinity of mobile readers at-a-time simultaneously, does not require known positions to be tagged {\\em a priori}, and can achieve localization at a fraction of the cost compared to GPS.",
        "url": "http://arxiv.org/abs/2512.15956v1",
        "published_date": "2025-12-17T20:43:17+00:00",
        "updated_date": "2025-12-17T20:43:17+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "John Hateley",
            "Sriram Narasimhan",
            "Omid Abari"
        ],
        "tldr": "The paper introduces a cost-effective RFID-based asset tracking system for wildfire response, using Gaussian Processes to achieve GPS-comparable localization accuracy without requiring prior tag placement information.",
        "tldr_zh": "该论文提出了一种用于野火响应的低成本 RFID 资产追踪系统，使用高斯过程实现与 GPS 相当的定位精度，且无需事先提供标签位置信息。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "本文提出了一种新颖、经济高效且可扩展的方法，利用商用射频识别（RFID）技术在森林环境中跟踪大量资产，目标应用于野火响应。由于信号衰减、多径效应和环境变异性，商用 RFID 系统在分散于森林环境中时，通常存在标签定位精度差的问题。当前通过指纹识别解决此问题的方法依赖于{\\em 事先}在已知位置分散标签。本文研究了无法标记已知位置的情况，并表明可以在没有此约束的情况下，将标签定位到与全球定位系统 (GPS) 相当的精度。为此，我们提出利用高斯过程，仅基于射频信号响应特征对各种环境进行建模，而无需借助全球定位 GPS 或摄像头等额外传感器，并将未知的射频信号与模型字典中最接近的匹配项进行匹配。我们采用了一种新的加权对数似然方法，将未知的环境与先前建模的环境字典中最接近的环境相关联，这是使用我们方法的关键步骤。结果表明，使用无源商用 RFID 可以实现与 GPS 相当的定位精度，从而能够同时跟踪移动读取器附近数十个野火资产，并且不需要{\\em 事先}标记已知位置，与 GPS 相比，定位成本仅为其一小部分。"
    },
    {
        "title": "NDRL: Cotton Irrigation and Nitrogen Application with Nested Dual-Agent Reinforcement Learning",
        "summary": "Effective irrigation and nitrogen fertilization have a significant impact on crop yield. However, existing research faces two limitations: (1) the high complexity of optimizing water-nitrogen combinations during crop growth and poor yield optimization results; and (2) the difficulty in quantifying mild stress signals and the delayed feedback, which results in less precise dynamic regulation of water and nitrogen and lower resource utilization efficiency. To address these issues, we propose a Nested Dual-Agent Reinforcement Learning (NDRL) method. The parent agent in NDRL identifies promising macroscopic irrigation and fertilization actions based on projected cumulative yield benefits, reducing ineffective explorationwhile maintaining alignment between objectives and yield. The child agent's reward function incorporates quantified Water Stress Factor (WSF) and Nitrogen Stress Factor (NSF), and uses a mixed probability distribution to dynamically optimize daily strategies, thereby enhancing both yield and resource efficiency. We used field experiment data from 2023 and 2024 to calibrate and validate the Decision Support System for Agrotechnology Transfer (DSSAT) to simulate real-world conditions and interact with NDRL. Experimental results demonstrate that, compared to the best baseline, the simulated yield increased by 4.7% in both 2023 and 2024, the irrigation water productivity increased by 5.6% and 5.1% respectively, and the nitrogen partial factor productivity increased by 6.3% and 1.0% respectively. Our method advances the development of cotton irrigation and nitrogen fertilization, providing new ideas for addressing the complexity and precision issues in agricultural resource management and for sustainable agricultural development.",
        "url": "http://arxiv.org/abs/2512.16408v1",
        "published_date": "2025-12-18T11:07:35+00:00",
        "updated_date": "2025-12-18T11:07:35+00:00",
        "categories": [
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Ruifeng Xu",
            "Liang He"
        ],
        "tldr": "The paper presents a Nested Dual-Agent Reinforcement Learning (NDRL) method for optimizing cotton irrigation and nitrogen fertilization, demonstrating improved yield and resource efficiency compared to baselines in simulations.",
        "tldr_zh": "该论文提出了一种嵌套双智能体强化学习（NDRL）方法，用于优化棉花灌溉和氮肥施用，并在模拟中证明了与基线相比，产量和资源效率均有所提高。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 3,
        "summary_zh": "有效灌溉和氮肥施用对作物产量具有显著影响。然而，现有研究面临两个局限性：（1）作物生长期间水氮组合优化的高复杂性以及产量优化结果不佳；（2）量化轻度胁迫信号的困难和反馈的延迟，导致水氮动态调节不够精确，资源利用效率较低。为了解决这些问题，我们提出了一种嵌套双智能体强化学习（NDRL）方法。NDRL中的父智能体基于预测的累积产量效益识别有前景的宏观灌溉和施肥行动，减少无效探索，同时保持目标与产量之间的一致性。子智能体的奖励函数纳入了量化的水分胁迫因子（WSF）和氮素胁迫因子（NSF），并使用混合概率分布来动态优化每日策略，从而提高产量和资源效率。 我们使用了2023年和2024年的田间试验数据来校准和验证农业技术转移决策支持系统（DSSAT），以模拟真实世界条件并与NDRL交互。实验结果表明，与最佳基线相比，模拟产量在2023年和2024年均提高了4.7%，灌溉用水生产力分别提高了5.6%和5.1%，氮素偏生产力分别提高了6.3%和1.0%。我们的方法推进了棉花灌溉和氮肥施运的发展，为解决农业资源管理中的复杂性和精度问题以及可持续农业发展提供了新思路。"
    }
]