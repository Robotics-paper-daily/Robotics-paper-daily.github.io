[
    {
        "title": "CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation",
        "summary": "Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH's strong robustness, validating its effectiveness in both simulation and deployment scenarios.",
        "url": "http://arxiv.org/abs/2512.10360v1",
        "published_date": "2025-12-11T07:20:06+00:00",
        "updated_date": "2025-12-11T07:20:06+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Liuyi Wang",
            "Zongtao He",
            "Jinlong Li",
            "Xiaoyan Qi",
            "Mengxian Hu",
            "Chenpeng Yao",
            "Chengju Liu",
            "Qijun Chen"
        ],
        "tldr": "The paper introduces CLASH, a collaborative framework for Vision-and-Language Navigation that combines a reactive small-model planner with a reflective large-model reasoner, achieving state-of-the-art results on the VLN-CE leaderboard and demonstrating robustness in real-world experiments.",
        "tldr_zh": "该论文介绍了CLASH，一种用于视觉-语言导航的协作框架，它结合了反应式小模型规划器和反射式大模型推理器，在VLN-CE排行榜上取得了最先进的成果，并在真实世界实验中展示了鲁棒性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "视觉与语言导航（VLN）要求机器人遵循自然语言指令，并在没有先验地图的情况下导航复杂的环境。虽然最近的视觉-语言大模型展现出强大的推理能力，但在VLN任务中，它们的表现通常不如特定任务的全景小模型。为了解决这个问题，我们提出了CLASH（协作式大小层级），一个VLN-CE框架，该框架集成了反应式小模型规划器（RSMP）和反思式大模型推理器（RLMR）。RSMP采用了一种基于因果学习的双分支架构来增强泛化能力，而RLMR则利用带思维链推理的全景视觉提示来支持可解释的空间理解和导航。我们进一步引入了一种不确定性感知协作机制（UCM），它可以自适应地融合来自两个模型的决策。对于障碍物规避，在模拟中，我们用一个完全可学习的点目标策略替换了基于规则的控制器；在真实世界部署中，我们设计了一个基于激光雷达聚类的模块，用于生成可导航的航路点，并将其与基于在线SLAM的局部控制器配对。CLASH取得了最先进（SoTA）的结果（排名第一），在VLN-CE排行榜上，显著提高了在未见测试集上的SR和SPL，超过了之前的SoTA方法。真实世界的实验证明了CLASH的强大鲁棒性，验证了其在模拟和部署场景中的有效性。"
    },
    {
        "title": "Latent Chain-of-Thought World Modeling for End-to-End Driving",
        "summary": "Recent Vision-Language-Action (VLA) models for autonomous driving explore inference-time reasoning as a way to improve driving performance and safety in challenging scenarios. Most prior work uses natural language to express chain-of-thought (CoT) reasoning before producing driving actions. However, text may not be the most efficient representation for reasoning. In this work, we present Latent-CoT-Drive (LCDrive): a model that expresses CoT in a latent language that captures possible outcomes of the driving actions being considered. Our approach unifies CoT reasoning and decision making by representing both in an action-aligned latent space. Instead of natural language, the model reasons by interleaving (1) action-proposal tokens, which use the same vocabulary as the model's output actions; and (2) world model tokens, which are grounded in a learned latent world model and express future outcomes of these actions. We cold start latent CoT by supervising the model's action proposals and world model tokens based on ground-truth future rollouts of the scene. We then post-train with closed-loop reinforcement learning to strengthen reasoning capabilities. On a large-scale end-to-end driving benchmark, LCDrive achieves faster inference, better trajectory quality, and larger improvements from interactive reinforcement learning compared to both non-reasoning and text-reasoning baselines.",
        "url": "http://arxiv.org/abs/2512.10226v1",
        "published_date": "2025-12-11T02:22:07+00:00",
        "updated_date": "2025-12-11T02:22:07+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Shuhan Tan",
            "Kashyap Chitta",
            "Yuxiao Chen",
            "Ran Tian",
            "Yurong You",
            "Yan Wang",
            "Wenjie Luo",
            "Yulong Cao",
            "Philipp Krahenbuhl",
            "Marco Pavone",
            "Boris Ivanovic"
        ],
        "tldr": "The paper introduces Latent-CoT-Drive (LCDrive), a model that uses a latent space for chain-of-thought reasoning in end-to-end driving, showing improvements in inference speed, trajectory quality, and reinforcement learning gains compared to text-reasoning approaches.",
        "tldr_zh": "该论文介绍了Latent-CoT-Drive (LCDrive)，该模型使用潜在空间进行端到端驾驶中的链式思维推理，与文本推理方法相比，在推理速度、轨迹质量和强化学习收益方面均有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "用于自动驾驶的最新视觉-语言-动作（VLA）模型探索了推理时推理，以此来提高在具有挑战性的场景中的驾驶性能和安全性。大多数先前的工作使用自然语言来表达思维链（CoT）推理，然后再产生驾驶动作。然而，文本可能不是最有效的推理表示形式。 在这项工作中，我们提出了Latent-CoT-Drive（LCDrive）：一种在潜在语言中表达CoT的模型，该语言捕获了所考虑的驾驶动作的可能结果。 我们的方法通过在与动作对齐的潜在空间中表示CoT推理和决策，从而统一了两者。 该模型不使用自然语言，而是通过交错的方式进行推理：（1）动作提议令牌，它使用与模型输出动作相同的词汇表；以及（2）世界模型令牌，它基于学习到的潜在世界模型并表达这些动作的未来结果。我们通过监督模型的动作提议和世界模型令牌，基于场景的真实未来展开，来冷启动潜在的CoT。然后，我们使用闭环强化学习进行后训练，以加强推理能力。 在大规模端到端驾驶基准测试中，与非推理基线和文本推理基线相比，LCDrive实现了更快的推理速度、更好的轨迹质量以及交互式强化学习的更大幅度改进。"
    },
    {
        "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.",
        "url": "http://arxiv.org/abs/2512.09928v1",
        "published_date": "2025-12-10T18:59:32+00:00",
        "updated_date": "2025-12-10T18:59:32+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Minghui Lin",
            "Pengxiang Ding",
            "Shu Wang",
            "Zifeng Zhuang",
            "Yang Liu",
            "Xinyang Tong",
            "Wenxuan Song",
            "Shangke Lyu",
            "Siteng Huang",
            "Donglin Wang"
        ],
        "tldr": "The paper introduces HiF-VLA, a Vision-Language-Action model that uses motion representation for bidirectional temporal reasoning, improving long-horizon manipulation performance in both simulation and real-world robotic tasks.",
        "tldr_zh": "该论文介绍了HiF-VLA，一种视觉-语言-动作模型，它利用运动表征进行双向时间推理，从而提高了在模拟和真实机器人任务中的长期操作性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "视觉-语言-动作（VLA）模型最近实现了通过将视觉和语言线索融入动作从而进行的机器人操作。然而，大多数VLA模型都假设马尔可夫属性，仅依赖于当前的观察结果，因此遭受时间短视，导致长程一致性下降。在这项工作中，我们将运动视为一种更紧凑和信息量更大的时间上下文和世界动力学表示方法，捕捉状态间的变化，同时过滤静态的像素级噪声。基于这个想法，我们提出了HiF-VLA（VLA的回顾、洞察和前瞻），一个统一的框架，利用运动进行双向时间推理。HiF-VLA通过回顾先验编码过去的动力学，通过前瞻推理预测未来的运动，并通过回顾调节的联合专家整合两者，从而为长程操作启用一种“边思考边行动”的范式。结果表明，HiF-VLA在LIBERO-Long和CALVIN ABC-D基准测试中超越了强大的基线，同时几乎没有产生额外的推理延迟。此外，HiF-VLA在现实世界的长程操作任务中取得了显著的改进，证明了其在实际机器人环境中的广泛有效性。"
    },
    {
        "title": "MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence",
        "summary": "Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.",
        "url": "http://arxiv.org/abs/2512.10863v1",
        "published_date": "2025-12-11T17:57:24+00:00",
        "updated_date": "2025-12-11T17:57:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingli Lin",
            "Runsen Xu",
            "Shaohao Zhu",
            "Sihan Yang",
            "Peizhou Cao",
            "Yunlong Ran",
            "Miao Hu",
            "Chenming Zhu",
            "Yiman Xie",
            "Yilin Long",
            "Wenbo Hu",
            "Dahua Lin",
            "Tai Wang",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces MMSI-Video-Bench, a new comprehensive, fully human-annotated benchmark for video-based spatial intelligence in MLLMs, revealing a significant performance gap between AI models and humans and highlighting specific failure modes.",
        "tldr_zh": "本文介绍了MMSI-Video-Bench，这是一个全新的，全面的人工标注的视频空间智能基准测试，用于评估多模态大型语言模型。该基准测试揭示了人工智能模型与人类之间的显著性能差距，并突出了特定的失败模式。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "基于连续视觉输入的空间理解能力对于多模态大型语言模型(MLLMs)演变为物理环境中的通用型助手至关重要。然而，目前仍然缺乏一个全面评估实现此目标进展的基准。在这项工作中，我们推出了MMSI-Video-Bench，一个完全由人工标注的、用于评估MLLMs中基于视频的空间智能的基准。它通过来自25个数据集和内部视频的1,278个片段中的1,106个问题，实现了感知、规划、预测和跨视频推理这四个层级的框架。每个条目都经过3D视觉专家的精心设计和审查，并附有解释性原理，以确保精确且明确的定位。凭借其多样的数据来源和全面的任务覆盖，MMSI-Video-Bench还支持三个面向领域的子基准 (室内场景感知基准、机器人基准和定位基准)，用于有针对性的能力评估。我们评估了25个强大的开源和专有MLLMs，揭示了显著的人工智能差距：许多模型的表现接近随机水平，而最佳的推理模型比人类的表现落后近60%。我们进一步发现，经过空间微调的模型仍然无法在我们的基准上有效泛化。细粒度的误差分析揭示了几何推理、运动定位、长时程预测和跨视频对应中的系统性失败。我们还表明，典型的帧采样策略在我们的推理密集型基准上表现不佳，并且3D空间线索和思维链提示都无法产生有意义的提升。我们期望我们的基准能够为推进基于视频的空间智能建立一个坚实的试验平台。"
    },
    {
        "title": "SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
        "summary": "Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.",
        "url": "http://arxiv.org/abs/2512.10046v1",
        "published_date": "2025-12-10T20:04:08+00:00",
        "updated_date": "2025-12-10T20:04:08+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yan Zhuang",
            "Jiawei Ren",
            "Xiaokang Ye",
            "Jianzhi Shen",
            "Ruixuan Zhang",
            "Tianai Yue",
            "Muhammad Faayez",
            "Xuhong He",
            "Ziqiao Ma",
            "Lianhui Qin",
            "Zhiting Hu",
            "Tianmin Shu"
        ],
        "tldr": "The paper introduces SimWorld-Robotics (SWR), a photorealistic and dynamic urban simulation environment for robotics research, along with two challenging benchmarks for multimodal robot navigation and collaboration, highlighting the limitations of current models in urban environments.",
        "tldr_zh": "该论文介绍了SimWorld-Robotics (SWR)，一个用于机器人研究的光真实且动态的城市模拟环境，以及两个用于多模态机器人导航和协作的具有挑战性的基准，突出了当前模型在城市环境中的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "基于多模态输入的通用机器人技术在开放场景中执行多样化任务方面，基础模型近期进展显示出良好的发展前景。然而，当前的研究主要集中在室内、家庭场景。在这项工作中，我们提出了SimWorld-Robotics~(SWR)，一个用于具身智能的仿真平台，其专注于大规模、照片级真实感的城市环境。SWR构建于虚幻引擎5之上，程序化地生成无限的照片级真实度城市场景，并填充行人、交通系统等动态元素，在真实性、复杂性和可扩展性方面超越了以往的城市仿真。它还支持多机器人控制和通信。借助这些关键特性，我们构建了两个具有挑战性的机器人基准测试：(1) 多模态指令跟随任务，其中机器人必须遵循视觉-语言导航指令，在存在行人和交通的情况下到达目的地；(2) 多智能体搜索任务，其中两个机器人必须通信以协同定位并会合。与现有基准测试不同，这两个新基准测试全面评估了现实场景中广泛的关键机器人能力，包括(1) 多模态指令的接地，(2) 大型环境中的3D空间推理，(3) 在行人与交通中的安全、长距离导航，(4) 多机器人协作，以及(5) 接地的通信。我们的实验结果表明，包括视觉-语言模型 (VLM) 在内的最先进模型，在我们的任务中表现不佳，缺乏城市环境所需的鲁棒感知、推理和规划能力。"
    },
    {
        "title": "Grounding Everything in Tokens for Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) have made significant advancements in vision understanding and reasoning. However, the autoregressive Transformer architecture used by MLLMs requries tokenization on input images, which limits their ability to accurately ground objects within the 2D image space. This raises an important question: how can sequential language tokens be improved to better ground objects in 2D spatial space for MLLMs? To address this, we present a spatial representation method for grounding objects, namely GETok, that integrates a specialized vocabulary of learnable tokens into MLLMs. GETok first uses grid tokens to partition the image plane into structured spatial anchors, and then exploits offset tokens to enable precise and iterative refinement of localization predictions. By embedding spatial relationships directly into tokens, GETok significantly advances MLLMs in native 2D space reasoning without modifying the autoregressive architecture. Extensive experiments demonstrate that GETok achieves superior performance over the state-of-the-art methods across various referring tasks in both supervised fine-tuning and reinforcement learning settings.",
        "url": "http://arxiv.org/abs/2512.10554v1",
        "published_date": "2025-12-11T11:38:50+00:00",
        "updated_date": "2025-12-11T11:38:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangxuan Ren",
            "Zhongdao Wang",
            "Liping Hou",
            "Pin Tang",
            "Guoqing Wang",
            "Chao Ma"
        ],
        "tldr": "The paper introduces GETok, a spatial representation method that integrates learnable tokens into MLLMs to improve object grounding in 2D image space by partitioning the image with grid tokens and refining localization using offset tokens, achieving superior performance in referring tasks.",
        "tldr_zh": "本文介绍了一种名为GETok的空间表示方法，该方法将可学习的tokens集成到MLLM中，通过网格tokens分割图像并使用偏移tokens细化定位，从而提高物体在2D图像空间中的定位精度，在指代表述任务中表现优异。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "多模态大型语言模型（MLLM）在视觉理解和推理方面取得了显著进展。然而，MLLM采用的自回归Transformer架构需要在输入图像上进行令牌化处理，这限制了其在二维图像空间中精确定位对象的能力。这就提出了一个重要问题：如何改进序列语言令牌，以便更好地在二维空间中定位MLLM中的对象？为了解决这个问题，我们提出了一种用于定位对象的空间表示方法，即GETok，它将专门的可学习令牌词汇集成到MLLM中。GETok首先使用网格令牌将图像平面划分为结构化的空间锚点，然后利用偏移令牌来实现对定位预测的精确和迭代细化。通过将空间关系直接嵌入到令牌中，GETok在不修改自回归架构的情况下，显著提升了MLLM在原生二维空间推理方面的能力。大量实验表明，在监督微调和强化学习设置下，GETok在各种指代表达任务中均优于当前最优方法。"
    },
    {
        "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
        "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2512.10310v1",
        "published_date": "2025-12-11T05:57:48+00:00",
        "updated_date": "2025-12-11T05:57:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Duo Zheng",
            "Shijia Huang",
            "Yanyang Li",
            "Liwei Wang"
        ],
        "tldr": "The paper introduces Efficient-VLN, a VLN model that significantly reduces training overhead by using efficient memory mechanisms and a dynamic mixed policy, achieving state-of-the-art performance with much less GPU time.",
        "tldr_zh": "该论文介绍了Efficient-VLN，一种通过高效记忆机制和动态混合策略显著降低训练开销的VLN模型，以更少的GPU时间实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "多模态大型语言模型（MLLMs）在视觉语言导航（VLN）中展现出了极具潜力的应用前景。然而，它们在实际开发中受到了巨大的训练开销的严重阻碍。我们识别出导致这种开销的两个关键问题：（1）处理长程历史观测作为庞大的tokens序列时产生的二次方计算负担，以及（2）DAgger中的探索效率权衡，即收集代理探索轨迹的数据聚合过程。虽然更多的探索会产生有效的错误恢复轨迹，以处理测试时分布偏移，但它会以训练和推理时更长的轨迹长度为代价。为了解决这些挑战，我们提出了Efficient-VLN，一种训练高效的VLN模型。具体而言，为了减轻tokens处理负担，我们设计了两种高效的记忆机制：一种渐进式记忆，动态地为最近的观测分配更多的tokens，以及一种可学习的递归记忆，它利用可学习tokens的键值缓存作为记忆状态。此外，我们引入了一种动态混合策略来平衡探索效率权衡。大量的实验表明，Efficient-VLN在R2R-CE（64.2% SR）和RxR-CE（67.0% SR）上取得了最先进的性能。最重要的是，我们的模型仅消耗282 H800 GPU小时，与最先进的方法相比，训练开销大幅降低。"
    },
    {
        "title": "Multi-Objective Reward and Preference Optimization: Theory and Algorithms",
        "summary": "This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.",
        "url": "http://arxiv.org/abs/2512.10601v1",
        "published_date": "2025-12-11T12:51:21+00:00",
        "updated_date": "2025-12-11T12:51:21+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Akhil Agnihotri"
        ],
        "tldr": "This thesis presents theoretical frameworks and algorithms for constrained RL, preference learning, and LLM alignment, offering theoretical guarantees and practical tools for safe and aligned decision-making, including the MOPO algorithm that scales to multi-billion-parameter language models.",
        "tldr_zh": "该论文提出了约束强化学习、偏好学习和大型语言模型对齐的理论框架和算法，为安全和对齐的决策提供了理论保证和实用工具，包括可扩展到数十亿参数语言模型的 MOPO 算法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "本论文开发了理论框架和算法，旨在推进约束强化学习（RL）在控制、偏好学习和大型语言模型对齐等方面的应用。首先，针对平均成本准则下的约束马尔可夫决策过程（CMDP），提出了平均约束策略优化（ACPO）算法。ACPO集成了敏感性分析和置信域更新，以确保约束处理的稳定性，并在理论保证下实现了最先进的实证性能。随后，约束强化学习扩展到有限视界设置，通过e-COP算法，即首个针对分幕式CMDP的策略优化方法。e-COP基于分幕式策略差分引理，在安全攸关环境中提供可证性能、简易性与可扩展性。接着，本论文研究了基于人类偏好的强化学习。warmPref-PS引入了一种线性bandit的后验采样策略，将来自异构评分者的离线偏好数据整合到在线学习中。对评分者能力的显式建模显著降低了遗憾值，并提高了RLHF的数据收集效率。PSPL算法通过联合采样奖励模型和来自成对轨迹比较的转移动态，进一步提升了基于偏好的强化学习，提供了贝叶斯简单遗憾保证，并能稳健地识别最优策略。最后，本论文将这些方法应用于大规模模型对齐。多目标约束优化视角催生了MOPO算法，这是一种具有闭合形式更新的迭代算法，可以扩展到数十亿参数的语言模型，并在各种对齐设置中保持稳健。总而言之，本论文统一了平均成本、分幕式和偏好驱动范式下的约束强化学习，为安全和对齐的决策提供了理论进展和实用工具。"
    },
    {
        "title": "V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions",
        "summary": "Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.",
        "url": "http://arxiv.org/abs/2512.10822v1",
        "published_date": "2025-12-11T17:14:37+00:00",
        "updated_date": "2025-12-11T17:14:37+00:00",
        "categories": [
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Mumuksh Tayal",
            "Manan Tayal",
            "Aditya Singh",
            "Shishir Kolathaya",
            "Ravi Prakash"
        ],
        "tldr": "The paper introduces V-OCBF, a novel method for learning Control Barrier Functions from offline data to ensure safety in autonomous systems without relying on online interaction or system dynamics knowledge. It uses a value-guided approach and an expectile-based objective to learn a safe controller.",
        "tldr_zh": "本文介绍了 V-OCBF，一种从离线数据中学习控制屏障函数的新方法，旨在确保自主系统的安全性，无需在线交互或系统动力学知识。它采用了一种价值引导方法和基于期望值的目标函数来学习安全控制器。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "确保自动驾驶系统的安全性需要控制器在无需在线交互的情况下满足严格的、与状态相关的约束。现有的安全离线强化学习方法通常强制执行软期望成本约束，但不能保证前向不变性。相反，控制障碍函数（CBF）提供了严格的安全保证，但通常依赖于专家设计的障碍函数或对系统动力学的完整了解。我们引入了基于价值引导的离线控制障碍函数（V-OCBF），这是一个完全从离线演示中学习神经CBF的框架。与之前的方法不同，V-OCBF不假设可以访问动力学模型；相反，它推导出递归的有限差分障碍更新，从而能够对一个障碍进行无模型学习，该障碍可以随时间传播安全信息。此外，V-OCBF结合了一个基于分位数损失的目标，避免在分配之外的动作上查询障碍函数，并将更新限制在数据集支持的动作集内。然后，将学习到的障碍函数与二次规划（QP）公式一起使用，以合成实时的安全控制。在多个案例研究中，V-OCBF产生的安全违例远少于基线方法，同时保持了强大的任务性能，突显了其在无需在线交互或手工设计的障碍函数的情况下，离线合成安全关键控制器的可扩展性。"
    },
    {
        "title": "Evaluating Gemini Robotics Policies in a Veo World Simulator",
        "summary": "Generative world models hold significant potential for simulating interactions with visuomotor policies in varied environments. Frontier video models can enable generation of realistic observations and environment interactions in a scalable and general manner. However, the use of video models in robotics has been limited primarily to in-distribution evaluations, i.e., scenarios that are similar to ones used to train the policy or fine-tune the base video model. In this report, we demonstrate that video models can be used for the entire spectrum of policy evaluation use cases in robotics: from assessing nominal performance to out-of-distribution (OOD) generalization, and probing physical and semantic safety. We introduce a generative evaluation system built upon a frontier video foundation model (Veo). The system is optimized to support robot action conditioning and multi-view consistency, while integrating generative image-editing and multi-view completion to synthesize realistic variations of real-world scenes along multiple axes of generalization. We demonstrate that the system preserves the base capabilities of the video model to enable accurate simulation of scenes that have been edited to include novel interaction objects, novel visual backgrounds, and novel distractor objects. This fidelity enables accurately predicting the relative performance of different policies in both nominal and OOD conditions, determining the relative impact of different axes of generalization on policy performance, and performing red teaming of policies to expose behaviors that violate physical or semantic safety constraints. We validate these capabilities through 1600+ real-world evaluations of eight Gemini Robotics policy checkpoints and five tasks for a bimanual manipulator.",
        "url": "http://arxiv.org/abs/2512.10675v1",
        "published_date": "2025-12-11T14:22:14+00:00",
        "updated_date": "2025-12-11T14:22:14+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Gemini Robotics Team",
            "Coline Devin",
            "Yilun Du",
            "Debidatta Dwibedi",
            "Ruiqi Gao",
            "Abhishek Jindal",
            "Thomas Kipf",
            "Sean Kirmani",
            "Fangchen Liu",
            "Anirudha Majumdar",
            "Andrew Marmon",
            "Carolina Parada",
            "Yulia Rubanova",
            "Dhruv Shah",
            "Vikas Sindhwani",
            "Jie Tan",
            "Fei Xia",
            "Ted Xiao",
            "Sherry Yang",
            "Wenhao Yu",
            "Allan Zhou"
        ],
        "tldr": "This paper introduces a generative evaluation system built on a video foundation model (Veo) for evaluating robotic policies in a variety of scenarios, including out-of-distribution and safety assessments. They demonstrate its effectiveness through extensive real-world evaluations of Gemini Robotics policies.",
        "tldr_zh": "本文介绍了一个建立在视频基础模型（Veo）之上的生成评估系统，用于评估各种场景中的机器人策略，包括分布外和安全性评估。 通过对 Gemini 机器人策略的大量真实世界评估，证明了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "生成式世界模型在模拟与各种环境中的视觉运动策略交互方面具有巨大潜力。前沿视频模型能够以可扩展和通用的方式生成逼真的观察结果和环境交互。然而，视频模型在机器人技术中的应用主要限于同分布评估，即与用于训练策略或微调基础视频模型的场景相似的场景。在本报告中，我们证明了视频模型可用于机器人技术中策略评估的所有用例：从评估标称性能到异分布（OOD）泛化，以及探测物理和语义安全性。我们介绍了一种基于前沿视频基础模型（Veo）构建的生成式评估系统。该系统经过优化，支持机器人动作条件化和多视角一致性，同时集成了生成式图像编辑和多视角补全，以合成真实世界场景在多个泛化轴上的逼真变化。我们证明，该系统保留了视频模型的基本能力，能够准确地模拟经过编辑的场景，包括新的交互对象、新的视觉背景和新的干扰对象。这种保真度能够准确预测不同策略在标称和OOD条件下的相对性能，确定不同泛化轴对策略性能的相对影响，并对策略进行红队测试，以暴露违反物理或语义安全约束的行为。我们通过对双臂机械手的八个Gemini Robotics策略检查点和五个任务的1600多次真实世界评估来验证这些能力。"
    },
    {
        "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
        "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
        "url": "http://arxiv.org/abs/2512.10605v1",
        "published_date": "2025-12-11T12:58:36+00:00",
        "updated_date": "2025-12-11T12:58:36+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Lihuang Chen",
            "Xiangyu Luo",
            "Jun Meng"
        ],
        "tldr": "The paper introduces LEO-RobotAgent, a general-purpose language-driven framework that allows LLMs to control diverse robots in complex tasks with enhanced human-robot collaboration, emphasizing streamlined structure and modularity.",
        "tldr_zh": "该论文介绍了LEO-RobotAgent，一个通用的语言驱动框架，允许LLM控制各种机器人在复杂的任务中，并增强了人机协作，重点是简化结构和模块化设计。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了 LEO-RobotAgent，一个用于机器人的通用语言驱动型智能代理框架。在此框架下，大型语言模型 (LLM) 可以操作不同类型的机器人，以完成各种场景中不可预测的复杂任务。该框架具有强大的泛化性、鲁棒性和效率。围绕其构建的应用级系统可以充分增强双向人机意图理解，并降低人机交互的门槛。在机器人任务规划方面，现有研究大多集中于大型模型在单任务场景和单一机器人类型中的应用上。这些算法通常结构复杂，缺乏泛化能力。因此，我们设计的 LEO-RobotAgent 框架尽可能采用精简的结构，使大型模型能够在这个清晰的框架内独立思考、规划和行动。我们提供了一个模块化且易于注册的工具集，使得大型模型能够灵活地调用各种工具，以满足不同的需求。同时，该框架融入了人机交互机制，使得算法能够像伙伴一样与人类协作。实验验证表明，该框架可以轻松地适配包括无人机 (UAV)、机械臂和轮式机器人在内的主流机器人平台，并高效地执行各种精心设计的、具有不同复杂程度的任务。我们的代码已在 https://github.com/LegendLeoChen/LEO-RobotAgent 上提供。"
    },
    {
        "title": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI",
        "summary": "Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.",
        "url": "http://arxiv.org/abs/2512.10394v1",
        "published_date": "2025-12-11T07:58:19+00:00",
        "updated_date": "2025-12-11T07:58:19+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Weifan Guan",
            "Huasen Xi",
            "Chenxiao Zhang",
            "Aosheng Li",
            "Qinghao Hu",
            "Jian Cheng"
        ],
        "tldr": "RoboNeuron is a modular framework integrating LLMs and VLA models with ROS for embodied AI, aiming to improve adaptability and modularity by using a Model Context Protocol (MCP) and automated ROS-to-MCP translation.",
        "tldr_zh": "RoboNeuron是一个模块化框架，集成了LLM和VLA模型与ROS，用于具身AI，旨在通过使用模型上下文协议（MCP）和自动ROS到MCP的翻译来提高适应性和模块化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "当前具身人工智能系统面临严峻的工程障碍，主要表现为跨场景适应性差、模块间耦合僵硬以及推理加速碎片化。为了克服这些局限性，我们提出了RoboNeuron，一种用于具身智能的通用部署框架。RoboNeuron是首个将大型语言模型（LLM）和视觉-语言-行动（VLA）模型的认知能力与机器人操作系统（ROS）的实时执行骨干网络深度集成的框架。我们利用模型上下文协议（MCP）作为语义桥梁，使LLM能够动态编排底层机器人工具。该框架建立了一个高度模块化的架构，通过利用ROS的统一通信接口严格解耦了感知、推理和控制。至关重要的是，我们引入了一种自动化工具，将ROS消息翻译成可调用的MCP函数，从而显著简化了开发流程。RoboNeuron显著提高了跨场景适应性和组件灵活性，同时建立了一个用于横向性能基准测试的系统平台，为可扩展的真实世界具身应用奠定了坚实的基础。"
    },
    {
        "title": "Push Smarter, Not Harder: Hierarchical RL-Diffusion Policy for Efficient Nonprehensile Manipulation",
        "summary": "Nonprehensile manipulation, such as pushing objects across cluttered environments, presents a challenging control problem due to complex contact dynamics and long-horizon planning requirements. In this work, we propose HeRD, a hierarchical reinforcement learning-diffusion policy that decomposes pushing tasks into two levels: high-level goal selection and low-level trajectory generation. We employ a high-level reinforcement learning (RL) agent to select intermediate spatial goals, and a low-level goal-conditioned diffusion model to generate feasible, efficient trajectories to reach them.\n  This architecture combines the long-term reward maximizing behaviour of RL with the generative capabilities of diffusion models. We evaluate our method in a 2D simulation environment and show that it outperforms the state-of-the-art baseline in success rate, path efficiency, and generalization across multiple environment configurations. Our results suggest that hierarchical control with generative low-level planning is a promising direction for scalable, goal-directed nonprehensile manipulation. Code, documentation, and trained models are available: https://github.com/carosteven/HeRD.",
        "url": "http://arxiv.org/abs/2512.10099v1",
        "published_date": "2025-12-10T21:40:22+00:00",
        "updated_date": "2025-12-10T21:40:22+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Steven Caro",
            "Stephen L. Smith"
        ],
        "tldr": "The paper introduces HeRD, a hierarchical RL-diffusion policy for nonprehensile manipulation, using RL for high-level goal selection and a diffusion model for low-level trajectory generation, demonstrating improved performance in simulation.",
        "tldr_zh": "该论文提出了一种名为HeRD的分层强化学习-扩散策略，用于非抓取操纵。它采用RL进行高层目标选择，扩散模型用于低层轨迹生成，并在模拟中表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "非抓取操作，例如在杂乱环境中推动物体，由于复杂的接触动力学和长程规划要求，提出了一个具有挑战性的控制问题。 在这项工作中，我们提出了HeRD，一种分层强化学习-扩散策略，它将推动任务分解为两个层次：高层目标选择和低层轨迹生成。 我们采用高层强化学习（RL）智能体选择中间空间目标，并使用低层目标条件扩散模型生成可行的、高效的轨迹来实现这些目标。\n  这种架构将RL的长期奖励最大化行为与扩散模型的生成能力相结合。 我们在一个二维仿真环境中评估了我们的方法，结果表明，它在成功率、路径效率以及跨多种环境配置的泛化能力方面优于最先进的基线方法。 我们的结果表明，具有生成式低层规划的分层控制是可扩展的、以目标为导向的非抓取操作的一个有希望的方向。 代码、文档和训练好的模型可在以下网址获取：https://github.com/carosteven/HeRD."
    },
    {
        "title": "Openpi Comet: Competition Solution For 2025 BEHAVIOR Challenge",
        "summary": "The 2025 BEHAVIOR Challenge is designed to rigorously track progress toward solving long-horizon tasks by physical agents in simulated environments. BEHAVIOR-1K focuses on everyday household tasks that people most want robots to assist with and these tasks introduce long-horizon mobile manipulation challenges in realistic settings, bridging the gap between current research and real-world, human-centric applications. This report presents our solution to the 2025 BEHAVIOR Challenge in a very close 2nd place and substantially outperforms the rest of the submissions. Building on $π_{0.5}$, we focus on systematically building our solution by studying the effects of training techniques and data. Through careful ablations, we show the scaling power in pre-training and post-training phases for competitive performance. We summarize our practical lessons and design recommendations that we hope will provide actionable insights for the broader embodied AI community when adapting powerful foundation models to complex embodied scenarios.",
        "url": "http://arxiv.org/abs/2512.10071v1",
        "published_date": "2025-12-10T20:46:40+00:00",
        "updated_date": "2025-12-10T20:46:40+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Junjie Bai",
            "Yu-Wei Chao",
            "Qizhi Chen",
            "Jinwei Gu",
            "Moo Jin Kim",
            "Zhaoshuo Li",
            "Xuan Li",
            "Tsung-Yi Lin",
            "Ming-Yu Liu",
            "Nic Ma",
            "Kaichun Mo",
            "Delin Qu",
            "Shangkun Sun",
            "Hongchi Xia",
            "Fangyin Wei",
            "Xiaohui Zeng"
        ],
        "tldr": "This paper presents a 2nd place solution to the 2025 BEHAVIOR Challenge, focusing on improving long-horizon mobile manipulation tasks in realistic, human-centric settings through pre-training and post-training techniques. The authors provide practical lessons for adapting foundation models to embodied AI.",
        "tldr_zh": "本文介绍了在2025 BEHAVIOR挑战赛中获得第二名的解决方案，重点是通过预训练和后训练技术改进现实的人工环境中长期移动操作任务。作者为将基础模型适配到具身智能（Embodied AI）提供了实践经验。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "2025 BEHAVIOR挑战赛旨在严格追踪物理智能体在模拟环境中解决长周期任务的进展。BEHAVIOR-1K专注于日常家务，这些是人们最希望机器人能够协助完成的任务。这些任务在真实环境中引入了长周期移动操作的挑战，弥合了当前研究与真实世界、以人为中心的应用程序之间的差距。本报告介绍了我们在2025 BEHAVIOR挑战赛中的解决方案，该方案以非常接近的第二名成绩完成比赛，并大幅优于其他提交方案。在$π_{0.5}$的基础上，我们专注于系统地构建解决方案，研究训练技术和数据的影响。通过仔细的消融实验，我们展示了预训练和后训练阶段的可扩展能力对竞争性能的影响。我们总结了实践经验和设计建议，希望能够为更广泛的具身智能社区在将强大的基础模型应用于复杂的具身场景时提供可操作的见解。"
    },
    {
        "title": "Closing the Train-Test Gap in World Models for Gradient-Based Planning",
        "summary": "World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.",
        "url": "http://arxiv.org/abs/2512.09929v1",
        "published_date": "2025-12-10T18:59:45+00:00",
        "updated_date": "2025-12-10T18:59:45+00:00",
        "categories": [
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Arjun Parthasarathy",
            "Nimit Kalra",
            "Rohun Agrawal",
            "Yann LeCun",
            "Oumayma Bounou",
            "Pavel Izmailov",
            "Micah Goldblum"
        ],
        "tldr": "This paper introduces data synthesis techniques to train world models that improve the performance of gradient-based planning, closing the train-test gap by aligning training objectives with test-time action sequence estimation. It shows improvement over CEM in object manipulation and navigation tasks.",
        "tldr_zh": "该论文提出了数据合成技术来训练世界模型，以提高基于梯度的规划性能，通过使训练目标与测试时的动作序列估计对齐来弥合训练-测试差距。结果显示，在物体操作和导航任务中，该方法优于CEM。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "世界模型与模型预测控制（MPC）相结合，可以在大规模专家轨迹数据集上离线训练，并在推理时泛化到广泛的规划任务。与依赖于缓慢搜索算法或迭代求解优化问题的传统MPC流程相比，基于梯度的规划提供了一种计算效率高的替代方案。然而，到目前为止，基于梯度的规划的性能一直落后于其他方法。在本文中，我们提出了改进的世界模型训练方法，以实现高效的基于梯度的规划。我们首先观察到，虽然世界模型是在下一个状态预测目标上训练的，但它在测试时被用来估计一个动作序列。我们工作的目标是缩小这种训练-测试差距。为此，我们提出了训练时数据合成技术，使得使用现有世界模型的基于梯度的规划得到显著改善。在测试时，我们的方法在10%的时间预算下，在一系列物体操作和导航任务中，优于或匹配了经典的无梯度交叉熵方法（CEM）。"
    },
    {
        "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}",
        "url": "http://arxiv.org/abs/2512.09927v1",
        "published_date": "2025-12-10T18:59:24+00:00",
        "updated_date": "2025-12-10T18:59:24+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yifan Ye",
            "Jiaqi Ma",
            "Jun Cen",
            "Zhihe Lu"
        ],
        "tldr": "The paper introduces TEAM-VLA, a training-free token compression method for Vision-Language-Action models that improves inference speed while preserving or improving task performance on robotics benchmarks.",
        "tldr_zh": "该论文介绍了一种名为TEAM-VLA的免训练token压缩方法，用于视觉-语言-动作模型，该方法提高了推理速度，同时保持或提高了在机器人基准测试中的任务性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在大型多模态数据集上预训练的视觉-语言-动作 (VLA) 模型已成为机器人感知和控制的强大基础。然而，它们庞大的规模（通常为数十亿个参数）给实时部署带来了重大挑战，因为在动态环境中，推理在计算上变得昂贵且对延迟敏感。为了解决这个问题，我们提出了一种免训练的token压缩框架 Token Expand-and-Merge-VLA (TEAM-VLA)，它可以在加速VLA推理的同时保持任务性能。TEAM-VLA 引入了一种动态 token 扩展机制，用于识别和采样位于注意力高亮区域空间附近的附加信息丰富的 token，从而增强上下文的完整性。然后，这些扩展的 token 在更深的层中，在动作感知的引导下被选择性地合并，从而有效地减少冗余，同时保持语义连贯性。通过在单个前馈过程中耦合扩展和合并，TEAM-VLA 实现了效率和有效性之间的平衡，无需任何重新训练或参数更新。在LIBERO 基准上的大量实验表明，TEAM-VLA 始终能够提高推理速度，同时保持甚至超过完整 VLA 模型的任务成功率。代码已在 \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA} 上公开。"
    },
    {
        "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating",
        "summary": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/",
        "url": "http://arxiv.org/abs/2512.09920v1",
        "published_date": "2025-12-10T18:54:30+00:00",
        "updated_date": "2025-12-10T18:54:30+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Junting Chen",
            "Yunchuan Li",
            "Panfeng Jiang",
            "Jiacheng Du",
            "Zixuan Chen",
            "Chenrui Tie",
            "Jiajun Deng",
            "Lin Shao"
        ],
        "tldr": "The paper introduces LISN-Bench, a new benchmark for language-instructed social navigation, and proposes Social-Nav-Modulator, a VLM-based hierarchical system that significantly outperforms existing methods in complex social navigation tasks.",
        "tldr_zh": "该论文介绍了 LISN-Bench，一个新的语言指导的社交导航基准，并提出了 Social-Nav-Modulator，一个基于 VLM 的分层系统，在复杂的社交导航任务中显著优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "为了实现人机共存，具备社会意识的导航对于移动机器人至关重要。然而，现有关于该领域的研究主要集中于路径效率和行人避障，这些固然重要，但仅代表社会导航的一小部分。除了这些基本要素之外，机器人还必须遵守用户指令，使其行为与人类表达的任务目标和社会规范相一致。在本工作中，我们提出了LISN-Bench，这是首个基于仿真的语言指导型社会导航基准。它建立在Rosnav-Arena 3.0之上，是首个纳入指令遵循和跨越不同场景的场景理解的标准化社会导航基准。为了解决这项任务，我们进一步提出了Social-Nav-Modulator，一个快-慢分层系统，其中VLM代理调制成本地图和控制器参数。将低级动作生成与较慢的VLM循环解耦，减少了对高频VLM推理的依赖，同时提高了动态避障和感知适应性。我们的方法实现了91.3%的平均成功率，比最具竞争力的基线高出63%以上，并且大部分改进都体现在具有挑战性的任务中，例如在人群中跟随人员以及在严格避免指令禁止区域的情况下进行导航。项目网站位于：https://social-nav.github.io/LISN-project/"
    },
    {
        "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos",
        "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.",
        "url": "http://arxiv.org/abs/2512.09903v1",
        "published_date": "2025-12-10T18:32:38+00:00",
        "updated_date": "2025-12-10T18:32:38+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ryan Meegan",
            "Adam D'Souza",
            "Bryan Bo Cao",
            "Shubham Jain",
            "Kristin Dana"
        ],
        "tldr": "The paper introduces YOPO-Nav, a visual navigation method that uses 3D Gaussian Splatting graphs constructed from one-pass videos to allow robots to retrace explored trajectories, and presents the YOPO-Campus dataset for benchmarking.",
        "tldr_zh": "该论文介绍了YOPO-Nav，一种视觉导航方法，使用从单遍视频构建的3D高斯溅射图来使机器人能够重溯已探索的轨迹，并提出了用于基准测试的YOPO-Campus数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉导航已经成为一种替代传统依赖于详细地图构建和路径规划的机器人导航流程的实用方案。然而，构建和维护3D地图通常计算成本高昂且内存密集。我们研究了当存在一个大型环境的探索视频时的视觉导航问题。这些视频充当视觉参考，允许机器人追溯探索轨迹，而无需依赖度量地图。我们提出的方法 YOPO-Nav (You Only Pass Once) 将环境编码成一个紧凑的空间表示，该空间表示由相互连接的局部 3D 高斯溅射 (3DGS) 模型组成。在导航过程中，该框架将机器人当前的视觉观测与此表示对齐，并预测引导其返回演示轨迹的动作。YOPO-Nav 采用分层设计：视觉地点识别 (VPR) 模块提供粗略的定位，而局部的 3DGS 模型则细化目标和中间姿态，以生成控制动作。为了评估我们的方法，我们引入了 YOPO-Campus 数据集，该数据集包含超过 6 公里的人工遥控机器人轨迹的 4 小时第一人称视角视频和机器人控制器输入。我们使用 Clearpath Jackal 机器人，在 YOPO-Campus 数据集中的轨迹上对最近的视觉导航方法进行了基准测试。实验结果表明，YOPO-Nav 在真实场景中物理机器人的图像目标导航方面表现出色。该数据集和代码将公开发布，用于视觉导航和场景表示的研究。"
    },
    {
        "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
        "summary": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
        "url": "http://arxiv.org/abs/2512.10696v1",
        "published_date": "2025-12-11T14:40:01+00:00",
        "updated_date": "2025-12-11T14:40:01+00:00",
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Zouying Cao",
            "Jiaji Deng",
            "Li Yu",
            "Weikang Zhou",
            "Zhaoyang Liu",
            "Bolin Ding",
            "Hai Zhao"
        ],
        "tldr": "The paper introduces ReMe, a dynamic procedural memory framework for LLM agents that utilizes multi-faceted distillation, context-adaptive reuse, and utility-based refinement to improve performance and memory scalability, outperforming larger memoryless models in benchmarks.",
        "tldr_zh": "该论文介绍了一个名为ReMe的动态程序记忆框架，用于大型语言模型Agent。它通过多方面的提炼、上下文自适应重用和基于效用的优化来提高性能和记忆可扩展性，并在基准测试中优于更大的无记忆模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "程序性记忆使大型语言模型（LLM）智能体能够内化“如何做”的知识，理论上减少冗余的试错。然而，现有的框架主要受限于一种“被动积累”的范式，将记忆视为一个静态的、只追加的档案。为了弥合静态存储和动态推理之间的差距，我们提出了 $\\textbf{ReMe}$ ($\\textit{记住我，精炼我}$)，一个用于经验驱动的智能体演化的综合框架。ReMe通过三种机制在记忆生命周期中进行创新：1）$\\textit{多方面提炼}$，通过识别成功模式、分析失败触发因素并生成比较性见解来提取细粒度的经验；2）$\\textit{上下文自适应重用}$，通过情景感知索引将历史见解定制到新的上下文中；以及3）$\\textit{基于效用的精炼}$，自主添加有效的记忆并修剪过时的记忆，以维持一个紧凑、高质量的经验池。在 BFCL-V3 和 AppWorld 上的大量实验表明，ReMe 在智能体记忆系统中确立了一种新的最先进水平。至关重要的是，我们观察到显著的记忆缩放效应：配备 ReMe 的 Qwen3-8B 胜过更大的、无记忆的 Qwen3-14B，这表明自我进化的记忆为终身学习提供了一种计算高效的途径。我们发布了我们的代码和 $\\texttt{reme.library}$ 数据集，以促进进一步的研究。"
    },
    {
        "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
        "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
        "url": "http://arxiv.org/abs/2512.10534v1",
        "published_date": "2025-12-11T11:05:04+00:00",
        "updated_date": "2025-12-11T11:05:04+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Haiteng Zhao",
            "Junhao Shen",
            "Yiming Zhang",
            "Songyang Gao",
            "Kuikun Liu",
            "Tianyou Ma",
            "Fan Zheng",
            "Dahua Lin",
            "Wenwei Zhang",
            "Kai Chen"
        ],
        "tldr": "The paper introduces InternGeometry, an LLM agent that achieves gold medalist-level performance on IMO geometry problems using a small amount of training data and Complexity-Boosting Reinforcement Learning, surpassing previous expert systems like AlphaGeometry 2.",
        "tldr_zh": "该论文介绍了InternGeometry，一个LLM智能体，通过使用少量训练数据和复杂度提升强化学习，在IMO几何问题上实现了金牌水平的性能，超越了像AlphaGeometry 2 这样的专家系统。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLM）智能体展现出强大的数学问题求解能力，甚至可以在形式化证明系统的辅助下解决国际数学奥林匹克（IMO）水平的问题。然而，由于辅助线作图方面较弱的启发式能力，几何问题求解的AI领域依然被AlphaGeometry 2等专家模型所主导，这些模型在训练和评估中都严重依赖于大规模的数据合成和搜索。在这项工作中，我们首次尝试构建一个奖牌级别的几何问题求解LLM智能体，并提出了InternGeometry。InternGeometry通过迭代地提出命题和辅助线作图方案，利用符号引擎验证这些方案，并根据引擎的反馈进行反思，以指导后续的方案提出，从而克服了几何学中的启发式限制。一种动态记忆机制使得InternGeometry能够就每个问题与符号引擎进行两百多次交互。为了进一步加速学习，我们引入了复杂度提升强化学习（CBRL），该方法在训练阶段逐步提高合成问题的复杂度。基于InternThinker-32B，InternGeometry解决了50道IMO几何问题（2000-2024）中的44道，超过了平均金牌获得者的分数（40.9），并且仅使用了1.3万个训练样本，仅为AlphaGeometry 2所用数据的0.004%，证明了LLM智能体在专家级几何任务中的潜力。InternGeometry还可以为IMO问题提出人类解法中未出现过的新的辅助线作图方案。我们将发布模型、数据和符号引擎以支持未来的研究。"
    },
    {
        "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
        "summary": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
        "url": "http://arxiv.org/abs/2512.10501v1",
        "published_date": "2025-12-11T10:22:02+00:00",
        "updated_date": "2025-12-11T10:22:02+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Lim Chien Her",
            "Ming Yan",
            "Yunshu Bai",
            "Ruihao Li",
            "Hao Zhang"
        ],
        "tldr": "This paper presents a dual-agent architecture leveraging LLMs for zero-shot procedural content generation of 3D maps from natural language instructions, outperforming single-agent baselines without task-specific fine-tuning.",
        "tldr_zh": "本文提出了一种双智能体架构，利用大型语言模型从自然语言指令零样本生成3D地图过程内容，无需针对特定任务进行微调，性能优于单智能体基线。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "程序化内容生成（PCG）为算法创建复杂、可定制的世界提供了可扩展的方法。然而，控制这些流程需要对不透明的技术参数进行精确配置。我们提出了一种无需训练的架构，该架构利用大型语言模型（LLM）代理进行零样本PCG参数配置。虽然大型语言模型（LLM）有望为PCG工具提供自然语言接口，但现成的模型通常无法弥合抽象用户指令和严格参数规范之间的语义鸿沟。我们的系统将一个演员代理与一个评论家代理配对，实现了一个迭代工作流程，其中系统自主地推理工具参数并改进配置，以逐步与人类设计偏好相符。我们在各种3D地图的生成上验证了这种方法，为PCG中的指令遵循建立了一个新的基准。实验表明，我们的方法优于单代理基线，从自然语言描述中生成了多样且结构有效的环境。这些结果表明，现成的LLM可以有效地重新用作任意PCG工具的通用代理。通过将负担从模型训练转移到架构推理，我们的方法提供了一个可扩展的框架，可以在没有特定于任务的微调的情况下掌握复杂的软件。"
    },
    {
        "title": "Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention",
        "summary": "Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL-based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing studies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ignore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the diversity of responses. In this paper, we propose Selective-adversarial Entropy Intervention, namely SaEI, which enhances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the entropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formulates the entropy of sampled responses as an adversarial objective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger answer space during RL sampling. Then, we propose token-selective entropy computation (TsEC) to maximize the effectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.",
        "url": "http://arxiv.org/abs/2512.10414v1",
        "published_date": "2025-12-11T08:27:02+00:00",
        "updated_date": "2025-12-11T08:27:02+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yang Yu",
            "Zhuangzhuang Chen",
            "Siqi Wang",
            "Lanqing Li",
            "Xiaomeng Li"
        ],
        "tldr": "The paper proposes a novel reinforcement learning method, SaEI, to improve the reasoning capabilities of vision-language models by selectively intervening in entropy during RL sampling via adversarial attacks on the visual input, enhancing exploration and diversity of responses.",
        "tldr_zh": "该论文提出了一种新的强化学习方法 SaEI，通过对抗性攻击视觉输入，有选择地干预 RL 采样过程中的熵，从而提高视觉-语言模型的推理能力，增强探索性和响应多样性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "近年来，强化学习 (RL) 已成为增强视觉-语言模型 (VLM) 推理能力的常见选择。考虑到现有的基于强化学习的微调方法，熵干预被证明是一种有效的方式，能够提升探索能力，从而提高策略性能。值得注意的是，大多数现有研究仅通过控制强化学习策略优化期间特定token的更新来进行熵干预，他们忽略了强化学习采样期间的熵干预，而这种干预可以通过提高响应的多样性来提升 GRPO 的性能。在本文中，我们提出了一种选择性对抗熵干预方法，即SaEI，该方法通过利用源自采样响应熵的token选择性对抗目标来扭曲视觉输入，从而增强策略熵。具体来说，我们首先提出熵引导对抗采样 (EgAS)，它将采样响应的熵表述为对抗目标。然后，利用相应的对抗梯度攻击视觉输入以生成对抗样本，使策略模型在强化学习采样期间能够探索更大的答案空间。接下来，我们提出token选择性熵计算 (TsEC)，以在EgAS中最大限度地提高对抗攻击的有效性，同时避免扭曲 VLM 中的事实知识。在领域内和领域外数据集上的大量实验表明，我们提出的方法可以通过熵干预极大地改善策略探索，从而提高推理能力。代码将在论文被接收后发布。"
    },
    {
        "title": "User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation",
        "summary": "Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.",
        "url": "http://arxiv.org/abs/2512.10322v1",
        "published_date": "2025-12-11T06:11:45+00:00",
        "updated_date": "2025-12-11T06:11:45+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yongqiang Yu",
            "Xuhui Li",
            "Hazza Mahmood",
            "Jinxing Zhou",
            "Haodong Hong",
            "Longtao Jiang",
            "Zhiqiang Xu",
            "Qi Wu",
            "Xiaojun Chang"
        ],
        "tldr": "This paper introduces a user-feedback-driven continual adaptation framework for Vision-and-Language Navigation (VLN), improving navigation success and path efficiency by integrating human interactions and a memory-bank warm-start mechanism.",
        "tldr_zh": "本文提出了一种用户反馈驱动的视觉语言导航（VLN）持续适应框架，通过整合人类交互和一个内存库热启动机制，提高导航成功率和路径效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉-语言导航（VLN）要求智能体通过遵循自然语言指令来在复杂环境中导航。针对VLN的通用场景自适应（GSA-VLN）将重点从零样本泛化转移到持续的、特定于环境的自适应，缩小了静态基准与现实世界部署之间的差距。然而，当前的GSA-VLN框架排除了用户反馈，仅依赖于重复环境暴露带来的无监督自适应。在实践中，用户反馈提供了自然且有价值的监督，可以显著提高自适应质量。我们引入了一种用户反馈驱动的自适应框架，该框架通过将人类互动系统地集成到持续学习中来扩展GSA-VLN。我们的方法将用户反馈（导航指令和纠正信号）转化为高质量的、与环境对齐的训练数据，从而实现高效且真实的自适应。一种记忆库热启动机制进一步复用先前获取的环境知识，减轻了冷启动退化并确保了稳定的重新部署。在GSA-R2R基准上的实验表明，我们的方法始终优于诸如GR-DUET之类的强大基线，提高了导航成功率和路径效率。记忆库热启动稳定了早期导航并减少了更新后的性能下降。在持续自适应和混合自适应设置下的结果证实了我们框架的鲁棒性和通用性，表明在各种部署条件下都能持续改进。"
    },
    {
        "title": "CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment",
        "summary": "Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.",
        "url": "http://arxiv.org/abs/2512.10206v1",
        "published_date": "2025-12-11T01:54:55+00:00",
        "updated_date": "2025-12-11T01:54:55+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yakun Zhu",
            "Zhongzhen Huang",
            "Qianhan Feng",
            "Linjie Mu",
            "Yannian Gu",
            "Shaoting Zhang",
            "Qi Dou",
            "Xiaofan Zhang"
        ],
        "tldr": "The paper introduces CP-Env, a controllable hospital environment, for evaluating LLMs on end-to-end clinical pathways, revealing the models' limitations in handling complex scenarios and ethical considerations.",
        "tldr_zh": "该论文介绍了CP-Env，一个可控的医院环境，用于评估LLM在端到端临床路径上的表现，揭示了模型在处理复杂场景和伦理考量方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "医疗保健遵循复杂的临床路径，这些路径超越了孤立的医患互动，强调决策制定和不同阶段之间的过渡。目前关注静态检查或孤立对话的基准测试无法充分评估大型语言模型(LLMs)在动态临床情景中的表现。我们引入了CP-Env，一个可控的、基于代理的医院环境，旨在评估LLMs在端到端临床路径中的表现。CP-Env模拟了一个包含患者和医生代理的医院生态系统，构建了从分诊、专科咨询到诊断测试和多学科团队会议等场景，用于代理之间的互动。遵循真实医院的医疗保健自适应流程，它支持分支和长时程任务执行。我们提出了一个三层评估框架，包括临床疗效、流程能力和职业伦理。结果表明，大多数模型在路径复杂性方面存在困难，表现出幻觉并丢失关键诊断细节。有趣的是，过多的推理步骤有时可能会适得其反，而顶级模型往往通过内化知识来减少工具依赖性。CP-Env通过全面的端到端临床评估，推动了医疗AI代理的发展。我们在https://github.com/SPIRAL-MED/CP-Env上提供了基准和评估工具，以供进一步研究和开发。"
    },
    {
        "title": "SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments",
        "summary": "Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.",
        "url": "http://arxiv.org/abs/2512.09897v1",
        "published_date": "2025-12-10T18:26:14+00:00",
        "updated_date": "2025-12-10T18:26:14+00:00",
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haoye Lu",
            "Pavan Seshadri",
            "Kaheer Suleman"
        ],
        "tldr": "The paper introduces SCOPE, a method for pretraining lightweight agents for text-based hierarchical planning using LLM-generated subgoals at initialization, achieving improved efficiency compared to repeatedly querying LLMs during training and inference.",
        "tldr_zh": "该论文介绍了SCOPE，一种利用LLM生成的子目标在初始化时预训练轻量级代理的方法，用于基于文本的分层规划，与在训练和推理过程中重复查询LLM相比，提高了效率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "在复杂的、基于文本的环境中进行长期规划由于开放式的动作空间、模糊的观测和稀疏的反馈而面临着巨大的挑战。最近的研究表明，大型语言模型（LLMs）编码了关于世界的丰富的语义知识，这对于指导智能体在具身和纯文本设置中的高层次推理和规划非常有用。然而，现有的方法通常严重依赖于在训练和推理过程中查询LLMs，这使得它们在计算上代价高昂且难以高效部署。此外，这些方法通常采用预训练的、未改变的LLM，其参数在整个训练过程中保持固定，没有机会适应目标任务。为了解决这些局限性，我们引入了SCOPE（用于高效规划的子目标条件预训练），这是一种单次分层规划器，仅在初始化时利用LLM生成的子目标来预训练轻量级的学生模型。与之前通过反复提示模型以自适应地生成子目标来提炼LLM知识的方法不同，我们的方法直接从示例轨迹中推导出子目标。这种设计消除了对重复LLM查询的需求，显著提高了效率，但代价是降低了解释性并可能导致次优子目标。尽管存在次优性，但我们在TextCraft环境上的结果表明，LLM生成的子目标仍然可以作为基于文本的规划任务中分层目标分解的强大起点。与基于LLM的分层智能体ADaPT（Prasad等人，2024）达到0.52的成功率相比，我们的方法达到了0.56，并将推理时间从164.4秒减少到仅3.0秒。"
    },
    {
        "title": "IRG-MotionLLM: Interleaving Motion Generation, Assessment and Refinement for Text-to-Motion Generation",
        "summary": "Recent advances in motion-aware large language models have shown remarkable promise for unifying motion understanding and generation tasks. However, these models typically treat understanding and generation separately, limiting the mutual benefits that could arise from interactive feedback between tasks. In this work, we reveal that motion assessment and refinement tasks act as crucial bridges to enable bidirectional knowledge flow between understanding and generation. Leveraging this insight, we propose Interleaved Reasoning for Motion Generation (IRMoGen), a novel paradigm that tightly couples motion generation with assessment and refinement through iterative text-motion dialogue. To realize this, we introduce IRG-MotionLLM, the first model that seamlessly interleaves motion generation, assessment, and refinement to improve generation performance. IRG-MotionLLM is developed progressively with a novel three-stage training scheme, initializing and subsequently enhancing native IRMoGen capabilities. To facilitate this development, we construct an automated data engine to synthesize interleaved reasoning annotations from existing text-motion datasets. Extensive experiments demonstrate that: (i) Assessment and refinement tasks significantly improve text-motion alignment; (ii) Interleaving motion generation, assessment, and refinement steps yields consistent performance gains across training stages; and (iii) IRG-MotionLLM clearly outperforms the baseline model and achieves advanced performance on standard text-to-motion generation benchmarks. Cross-evaluator testing further validates its effectiveness. Code & Data: https://github.com/HumanMLLM/IRG-MotionLLM/tree/main.",
        "url": "http://arxiv.org/abs/2512.10730v1",
        "published_date": "2025-12-11T15:16:06+00:00",
        "updated_date": "2025-12-11T15:16:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuan-Ming Li",
            "Qize Yang",
            "Nan Lei",
            "Shenghao Fu",
            "Ling-An Zeng",
            "Jian-Fang Hu",
            "Xihan Wei",
            "Wei-Shi Zheng"
        ],
        "tldr": "The paper introduces IRG-MotionLLM, a novel text-to-motion generation model that interleaves motion generation, assessment, and refinement through iterative dialogue, achieving improved performance on standard benchmarks.",
        "tldr_zh": "该论文介绍了IRG-MotionLLM，一种新颖的文本到动作生成模型，它通过迭代对话交错进行动作生成、评估和改进，从而在标准基准测试中实现了更高的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "运动感知大语言模型的最新进展在统一运动理解和生成任务方面展现了显著潜力。然而，这些模型通常将理解和生成任务分离处理，限制了任务间交互反馈可能带来的互益。在这项工作中，我们揭示了运动评估和精细化任务是连接理解和生成之间双向知识流的关键桥梁。基于这一洞察，我们提出了一种用于运动生成的交织推理方法 (IRMoGen)，这是一种通过迭代文本-运动对话将运动生成与评估和精细化紧密耦合的新范式。为了实现这一目标，我们引入了 IRG-MotionLLM，这是第一个无缝地交织运动生成、评估和精细化以提高生成性能的模型。IRG-MotionLLM 通过一种新颖的三阶段训练方案逐步开发，初始化并随后增强原生的 IRMoGen 能力。为了促进这一开发，我们构建了一个自动数据引擎，用于从现有的文本-运动数据集中合成交织推理标注。大量实验表明：(i) 评估和精细化任务显著提高了文本-运动的对齐程度；(ii) 交织运动生成、评估和精细化步骤在各个训练阶段都产生了持续的性能提升；以及 (iii) IRG-MotionLLM 明显优于基线模型，并在标准文本到运动生成基准测试中取得了先进的性能。交叉评估测试进一步验证了其有效性。代码和数据：https://github.com/HumanMLLM/IRG-MotionLLM/tree/main。"
    },
    {
        "title": "SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving",
        "summary": "End-to-end autonomous driving methods built on vision language models (VLMs) have undergone rapid development driven by their universal visual understanding and strong reasoning capabilities obtained from the large-scale pretraining. However, we find that current VLMs struggle to understand fine-grained 3D spatial relationships which is a fundamental requirement for systems interacting with the physical world. To address this issue, we propose SpaceDrive, a spatial-aware VLM-based driving framework that treats spatial information as explicit positional encodings (PEs) instead of textual digit tokens, enabling joint reasoning over semantic and spatial representations. SpaceDrive employs a universal positional encoder to all 3D coordinates derived from multi-view depth estimation, historical ego-states, and text prompts. These 3D PEs are first superimposed to augment the corresponding 2D visual tokens. Meanwhile, they serve as a task-agnostic coordinate representation, replacing the digit-wise numerical tokens as both inputs and outputs for the VLM. This mechanism enables the model to better index specific visual semantics in spatial reasoning and directly regress trajectory coordinates rather than generating digit-by-digit, thereby enhancing planning accuracy. Extensive experiments validate that SpaceDrive achieves state-of-the-art open-loop performance on the nuScenes dataset and the second-best Driving Score of 78.02 on the Bench2Drive closed-loop benchmark over existing VLM-based methods.",
        "url": "http://arxiv.org/abs/2512.10719v1",
        "published_date": "2025-12-11T14:59:07+00:00",
        "updated_date": "2025-12-11T14:59:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peizheng Li",
            "Zhenghao Zhang",
            "David Holtz",
            "Hang Yu",
            "Yutong Yang",
            "Yuzhi Lai",
            "Rui Song",
            "Andreas Geiger",
            "Andreas Zell"
        ],
        "tldr": "The paper introduces SpaceDrive, a VLM-based autonomous driving framework that incorporates spatial awareness by using positional encodings for 3D coordinates, improving spatial reasoning and planning accuracy.",
        "tldr_zh": "该论文介绍了SpaceDrive，一个基于VLM的自动驾驶框架，通过使用位置编码来整合空间感知，以此来改善空间推理和规划的准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于视觉语言模型 (VLM) 的端到端自动驾驶方法，凭借其通用的视觉理解能力和从大规模预训练中获得的强大推理能力，经历了快速发展。然而，我们发现目前的 VLM 在理解精细的 3D 空间关系方面存在困难，而这对于与物理世界交互的系统来说是一个基本要求。为了解决这个问题，我们提出了 SpaceDrive，一种基于空间感知的 VLM 驾驶框架，将空间信息视为显式的位置编码 (PEs)，而不是文本数字标记，从而实现语义和空间表示的联合推理。SpaceDrive 采用通用的位置编码器对从多视角深度估计、历史自车状态和文本提示中获得的所有 3D 坐标进行编码。这些 3D PEs 首先被叠加以增强相应的 2D 视觉标记。同时，它们作为任务无关的坐标表示，取代了逐位数字标记，作为 VLM 的输入和输出。这种机制使模型能够更好地在空间推理中索引特定的视觉语义，并直接回归轨迹坐标而不是逐位生成数字，从而提高规划的准确性。大量的实验验证了 SpaceDrive 在 nuScenes 数据集上实现了最先进的开环性能，并在 Bench2Drive 闭环基准测试中获得了 78.02 的第二佳 Driving Score，优于现有的基于 VLM 的方法。"
    },
    {
        "title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching",
        "summary": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.",
        "url": "http://arxiv.org/abs/2512.10674v1",
        "published_date": "2025-12-11T14:20:17+00:00",
        "updated_date": "2025-12-11T14:20:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Javier Villena Toro",
            "Mehdi Tarkian"
        ],
        "tldr": "Geo6DPose introduces a lightweight, training-free pipeline for zero-shot 6D object pose estimation using geometric filtering and foundation model features, achieving competitive performance with better efficiency and no reliance on external networks.",
        "tldr_zh": "Geo6DPose 提出了一种轻量级、免训练的零样本 6D 物体姿态估计流水线，该流水线利用几何滤波和基础模型特征，在实现具有竞争力的性能的同时，具有更高的效率，且不依赖外部网络。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "零样本6D物体姿态估计的最新进展主要由大规模模型和基于云的推理所驱动。然而，这些方法通常引入高延迟、高能耗以及与连接性、成本和数据治理相关的部署风险；这些因素与现实世界机器人的实际约束相悖，在实际应用中，计算资源有限，并且经常需要在设备上进行推理。我们提出了Geo6DPose，一个轻量级、完全本地化且无需训练的零样本6D姿态估计流程，它以几何可靠性来换取模型规模。我们的方法将基础模型视觉特征与几何过滤策略相结合：首先计算载入的模板DINO描述符与场景patches之间的相似度图，然后通过将场景patch中心投影到3D空间并将模板描述符投影到物体模型坐标系中来建立相互对应关系。最后，通过基于对应关系的RANSAC恢复最终姿态，并使用加权几何对齐度量进行排序，该度量同时考虑了重投影一致性和空间支撑，从而提高了对噪声、杂乱和部分遮挡的鲁棒性。Geo6DPose在单个商用GPU上实现了亚秒级的推理速度，同时匹配了显著更大规模的零样本基线的平均召回率（53.7 AR，1.08 FPS）。它无需训练、微调或网络访问，并且与不断发展的基础骨干网络保持兼容，从而推进了机器人部署中实用且完全本地化的6D感知。"
    },
    {
        "title": "XDen-1K: A Density Field Dataset of Real-World Objects",
        "summary": "A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.",
        "url": "http://arxiv.org/abs/2512.10668v1",
        "published_date": "2025-12-11T14:15:42+00:00",
        "updated_date": "2025-12-11T14:15:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingxuan Zhang",
            "Tianqi Yu",
            "Yatu Zhang",
            "Jinze Wu",
            "Kaixin Yao",
            "Jingyang Liu",
            "Yuyao Zhang",
            "Jiayuan Gu",
            "Jingyi Yu"
        ],
        "tldr": "The paper introduces XDen-1K, a large-scale multi-modal dataset of real-world objects with X-ray scans and 3D models to facilitate research on physical property estimation, specifically volumetric density, for robotics and embodied AI.",
        "tldr_zh": "该论文介绍了XDen-1K，一个大规模多模态现实世界物体数据集，包含X射线扫描和3D模型，旨在促进物理属性估计特别是体积密度方面的研究，用于机器人和具身智能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "对物理世界的深刻理解是具身人工智能和逼真模拟的核心目标。虽然目前的模型擅长捕捉物体的表面几何形状和外观，但它们在很大程度上忽略了其内部物理属性。这种疏忽至关重要，因为体积密度等属性是预测物体的质心、稳定性和交互动力学的根本，其应用范围从机器人操作到物理模拟。主要的瓶颈一直是缺乏大规模的真实世界数据。为了弥合这一差距，我们推出了 XDen-1K，这是首个为真实世界物理属性估计而设计的大规模多模态数据集，尤其侧重于体积密度。该数据集的核心包括 1,000 个真实世界物体，涵盖 148 个类别，我们为其提供了全面的多模态数据，包括具有部件级注释的高分辨率 3D 几何模型和一组相应的真实世界双平面 X 射线扫描。在此数据的基础上，我们引入了一种新颖的优化框架，该框架从每件物体的稀疏 X 射线视图中恢复出高保真度的体积密度场。为了证明其在实践中的价值，我们将 X 射线图像作为调节信号添加到现有的分割网络中，并执行体积分割。此外，我们还对下游机器人任务进行了实验。结果表明，利用该数据集可以有效提高质心估计的准确性和机器人操作的成功率。我们相信 XDen-1K 将成为一项基础资源和一项具有挑战性的新基准，从而推动未来在基于物理的视觉推理和具身人工智能方面的研究。"
    },
    {
        "title": "NaviHydra: Controllable Navigation-guided End-to-end Autonomous Driving with Hydra-distillation",
        "summary": "The complexity of autonomous driving scenarios requires robust models that can interpret high-level navigation commands and generate safe trajectories. While traditional rule-based systems can react to these commands, they often struggle in dynamic environments, and end-to-end methods face challenges in complying with explicit navigation commands. To address this, we present NaviHydra, a controllable navigation-guided end-to-end model distilled from an existing rule-based simulator. Our framework accepts high-level navigation commands as control signals, generating trajectories that align with specified intentions. We utilize a Bird's Eye View (BEV) based trajectory gathering method to enhance the trajectory feature extraction. Additionally, we introduce a novel navigation compliance metric to evaluate adherence to intended route, improving controllability and navigation safety. To comprehensively assess our model's controllability, we design a test that evaluates its response to various navigation commands. Our method significantly outperforms baseline models, achieving state-of-the-art results in the NAVSIM benchmark, demonstrating its effectiveness in advancing autonomous driving.",
        "url": "http://arxiv.org/abs/2512.10660v1",
        "published_date": "2025-12-11T14:05:18+00:00",
        "updated_date": "2025-12-11T14:05:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanfeng Wu",
            "Marlon Steiner",
            "Michael Schmidt",
            "Alvaro Marcos-Ramiro",
            "Christoph Stiller"
        ],
        "tldr": "NaviHydra is a navigation-guided end-to-end autonomous driving model distilled from a rule-based simulator, improving controllability and safety by incorporating high-level commands and a novel navigation compliance metric.",
        "tldr_zh": "NaviHydra是一个由规则驱动的模拟器提炼出的导航引导端到端自动驾驶模型，通过整合高级指令和一个新颖的导航符合性指标来提高可控性和安全性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "自动驾驶场景的复杂性要求能够解释高级导航指令并生成安全轨迹的鲁棒模型。虽然传统的基于规则的系统可以对这些指令做出反应，但它们在动态环境中常常显得不足；而端到端方法在遵守明确的导航指令方面面临挑战。为了解决这个问题，我们提出了NaviHydra，一个可控的、导航引导的端到端模型，它从现有的基于规则的模拟器中提炼而来。我们的框架接受高级导航指令作为控制信号，生成符合指定意图的轨迹。我们利用基于鸟瞰图(BEV)的轨迹收集方法来增强轨迹特征提取。此外，我们引入了一种新颖的导航合规性指标来评估对预期路线的遵守情况，从而提高可控性和导航安全性。为了全面评估我们模型的可控性，我们设计了一个测试来评估其对各种导航指令的响应。我们的方法显著优于基线模型，在NAVSIM基准测试中取得了最先进的结果，证明了其在推动自动驾驶方面的有效性。"
    },
    {
        "title": "Lang2Motion: Bridging Language and Motion through Joint Embedding Spaces",
        "summary": "We present Lang2Motion, a framework for language-guided point trajectory generation by aligning motion manifolds with joint embedding spaces. Unlike prior work focusing on human motion or video synthesis, we generate explicit trajectories for arbitrary objects using motion extracted from real-world videos via point tracking. Our transformer-based auto-encoder learns trajectory representations through dual supervision: textual motion descriptions and rendered trajectory visualizations, both mapped through CLIP's frozen encoders. Lang2Motion achieves 34.2% Recall@1 on text-to-trajectory retrieval, outperforming video-based methods by 12.5 points, and improves motion accuracy by 33-52% (12.4 ADE vs 18.3-25.3) compared to video generation baselines. We demonstrate 88.3% Top-1 accuracy on human action recognition despite training only on diverse object motions, showing effective transfer across motion domains. Lang2Motion supports style transfer, semantic interpolation, and latent-space editing through CLIP-aligned trajectory representations.",
        "url": "http://arxiv.org/abs/2512.10617v1",
        "published_date": "2025-12-11T13:14:18+00:00",
        "updated_date": "2025-12-11T13:14:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bishoy Galoaa",
            "Xiangyu Bai",
            "Sarah Ostadabbas"
        ],
        "tldr": "Lang2Motion generates object trajectories from language descriptions by learning a joint embedding space between text and motion, achieving strong results on text-to-trajectory retrieval and transferring well to human action recognition.",
        "tldr_zh": "Lang2Motion通过学习文本和运动之间的联合嵌入空间，从语言描述生成物体轨迹，在文本到轨迹检索方面取得了显著成果，并能很好地转移到人类动作识别。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "我们提出Lang2Motion，一个通过将运动流形与联合嵌入空间对齐，实现语言引导的点轨迹生成的框架。与先前专注于人体运动或视频合成的工作不同，我们利用从真实世界视频中通过点追踪提取的运动，为任意物体生成显式轨迹。我们基于Transformer的自编码器通过双重监督学习轨迹表示：文本运动描述和渲染轨迹可视化，两者都通过CLIP的冻结编码器进行映射。Lang2Motion在文本到轨迹检索方面实现了34.2%的Recall@1，优于基于视频的方法12.5个百分点，并且与视频生成基线相比，运动精度提高了33-52%（12.4 ADE vs 18.3-25.3）。尽管仅在多样化物体运动上进行训练，但我们在人类动作识别方面展示了88.3%的Top-1准确率，表明了有效的跨运动领域迁移。Lang2Motion支持风格迁移、语义插值和潜在空间编辑，这得益于与CLIP对齐的轨迹表示。"
    },
    {
        "title": "Track and Caption Any Motion: Query-Free Motion Discovery and Description in Videos",
        "summary": "We propose Track and Caption Any Motion (TCAM), a motion-centric framework for automatic video understanding that discovers and describes motion patterns without user queries. Understanding videos in challenging conditions like occlusion, camouflage, or rapid movement often depends more on motion dynamics than static appearance. TCAM autonomously observes a video, identifies multiple motion activities, and spatially grounds each natural language description to its corresponding trajectory through a motion-field attention mechanism. Our key insight is that motion patterns, when aligned with contrastive vision-language representations, provide powerful semantic signals for recognizing and describing actions. Through unified training that combines global video-text alignment with fine-grained spatial correspondence, TCAM enables query-free discovery of multiple motion expressions via multi-head cross-attention. On the MeViS benchmark, TCAM achieves 58.4% video-to-text retrieval, 64.9 JF for spatial grounding, and discovers 4.8 relevant expressions per video with 84.7% precision, demonstrating strong cross-task generalization.",
        "url": "http://arxiv.org/abs/2512.10607v1",
        "published_date": "2025-12-11T13:03:03+00:00",
        "updated_date": "2025-12-11T13:03:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bishoy Galoaa",
            "Sarah Ostadabbas"
        ],
        "tldr": "The paper introduces Track and Caption Any Motion (TCAM), a query-free framework for video understanding that discovers and describes motion patterns, achieving strong performance on the MeViS benchmark.",
        "tldr_zh": "该论文介绍了Track and Caption Any Motion (TCAM)，一个无需查询的视频理解框架，用于发现和描述运动模式，并在MeViS基准测试上取得了良好的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了跟踪和描述任意运动（TCAM），这是一个以运动为中心的自动视频理解框架，无需用户查询即可发现和描述运动模式。在诸如遮挡、伪装或快速运动等具有挑战性的情况下理解视频，往往更多地依赖于运动动态，而非静态外观。TCAM自主地观察视频，识别多个运动活动，并通过运动场注意力机制将每个自然语言描述在空间上定位到其对应的轨迹。我们的关键洞察是，当运动模式与对比视觉-语言表征对齐时，能够为识别和描述行为提供强大的语义信号。通过统一训练，将全局视频-文本对齐与细粒度空间对应关系相结合，TCAM通过多头交叉注意力实现对多个运动表达的免查询发现。在MeViS基准测试中，TCAM实现了 58.4% 的视频到文本检索准确率，64.9 JF 的空间定位分数，并以 84.7% 的精度发现了每个视频中 4.8 个相关表达，展现了强大的跨任务泛化能力。"
    },
    {
        "title": "Topology-Agnostic Animal Motion Generation from Text Prompt",
        "summary": "Motion generation is fundamental to computer animation and widely used across entertainment, robotics, and virtual environments. While recent methods achieve impressive results, most rely on fixed skeletal templates, which prevent them from generalizing to skeletons with different or perturbed topologies. We address the core limitation of current motion generation methods - the combined lack of large-scale heterogeneous animal motion data and unified generative frameworks capable of jointly modeling arbitrary skeletal topologies and textual conditions. To this end, we introduce OmniZoo, a large-scale animal motion dataset spanning 140 species and 32,979 sequences, enriched with multimodal annotations. Building on OmniZoo, we propose a generalized autoregressive motion generation framework capable of producing text-driven motions for arbitrary skeletal topologies. Central to our model is a Topology-aware Skeleton Embedding Module that encodes geometric and structural properties of any skeleton into a shared token space, enabling seamless fusion with textual semantics. Given a text prompt and a target skeleton, our method generates temporally coherent, physically plausible, and semantically aligned motions, and further enables cross-species motion style transfer.",
        "url": "http://arxiv.org/abs/2512.10352v1",
        "published_date": "2025-12-11T07:08:29+00:00",
        "updated_date": "2025-12-11T07:08:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keyi Chen",
            "Mingze Sun",
            "Zhenyu Liu",
            "Zhangquan Chen",
            "Ruqi Huang"
        ],
        "tldr": "The paper introduces OmniZoo, a large-scale animal motion dataset, and a topology-agnostic autoregressive motion generation framework that generates text-driven motions for arbitrary skeletal topologies, enabling cross-species motion style transfer.",
        "tldr_zh": "该论文介绍了OmniZoo，一个大型动物运动数据集，以及一个拓扑结构无关的自回归运动生成框架，该框架可以为任意骨骼拓扑生成文本驱动的运动，并实现跨物种的运动风格迁移。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "动作生成是计算机动画的基础，广泛应用于娱乐、机器人和虚拟环境。虽然最近的方法取得了令人瞩目的成果，但大多数方法依赖于固定的骨骼模板，这限制了它们泛化到具有不同或扰动拓扑结构的骨骼的能力。我们解决了当前动作生成方法的核心局限性——缺乏大规模异构动物运动数据以及能够联合建模任意骨骼拓扑结构和文本条件的统一生成框架。为此，我们推出了OmniZoo，一个包含140个物种和32,979个序列的大规模动物运动数据集，并富含多模态注释。在OmniZoo的基础上，我们提出了一个广义自回归运动生成框架，能够为任意骨骼拓扑结构生成文本驱动的运动。我们模型的中心是一个拓扑感知骨骼嵌入模块，它将任何骨骼的几何和结构属性编码到一个共享的令牌空间中，从而能够与文本语义无缝融合。给定一个文本提示和目标骨骼，我们的方法生成时间上连贯、物理上合理且语义上对齐的运动，并进一步实现了跨物种的运动风格迁移。"
    },
    {
        "title": "CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates",
        "summary": "Large-scale Vision-Language Models (VLMs) exhibit impressive complex reasoning capabilities but remain largely unexplored in visual sequential planning, i.e., executing multi-step actions towards a goal. Additionally, practical sequential planning often involves non-optimal (erroneous) steps, challenging VLMs to detect and correct such steps. We propose Corrective Sequential Planning Benchmark (CoSPlan) to evaluate VLMs in error-prone, vision-based sequential planning tasks across 4 domains: maze navigation, block rearrangement, image reconstruction,and object reorganization. CoSPlan assesses two key abilities: Error Detection (identifying non-optimal action) and Step Completion (correcting and completing action sequences to reach the goal). Despite using state-of-the-art reasoning techniques such as Chain-of-Thought and Scene Graphs, VLMs (e.g. Intern-VLM and Qwen2) struggle on CoSPlan, failing to leverage contextual cues to reach goals. Addressing this, we propose a novel training-free method, Scene Graph Incremental updates (SGI), which introduces intermediate reasoning steps between the initial and goal states. SGI helps VLMs reason about sequences, yielding an average performance gain of 5.2%. In addition to enhancing reliability in corrective sequential planning, SGI generalizes to traditional planning tasks such as Plan-Bench and VQA.",
        "url": "http://arxiv.org/abs/2512.10342v1",
        "published_date": "2025-12-11T06:46:51+00:00",
        "updated_date": "2025-12-11T06:46:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shresth Grover",
            "Priyank Pathak",
            "Akash Kumar",
            "Vibhav Vineet",
            "Yogesh S Rawat"
        ],
        "tldr": "The paper introduces CoSPlan, a benchmark for evaluating VLMs in corrective visual sequential planning, and proposes a Scene Graph Incremental updates (SGI) method to improve VLM performance in this domain.",
        "tldr_zh": "该论文介绍了CoSPlan，一个用于评估VLMs在纠错视觉序列规划中的基准，并提出了一种场景图增量更新（SGI）方法，以提高VLM在该领域的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "大型视觉-语言模型（VLMs）展现出了令人印象深刻的复杂推理能力，但在视觉序列规划（即，执行多步动作以达到目标）领域仍未得到充分探索。此外，实际的序列规划通常涉及非最优（错误）步骤，这挑战着VLMs去检测和纠正这些步骤。我们提出了纠错序列规划基准测试（CoSPlan），以评估VLMs在易出错的、基于视觉的序列规划任务中的表现，涵盖4个领域：迷宫导航、积木重排、图像重建和物体重组。CoSPlan评估了两项关键能力：错误检测（识别非最优动作）和步骤完成（纠正和完成动作序列以达到目标）。尽管使用了诸如思维链和场景图等最先进的推理技术，VLMs（例如Intern-VLM和Qwen2）在CoSPlan上表现不佳，无法利用上下文线索来实现目标。为了解决这个问题，我们提出了一种新颖的无训练方法，即场景图增量更新（SGI），它在初始状态和目标状态之间引入中间推理步骤。SGI帮助VLMs推理序列，产生平均5.2%的性能提升。除了增强纠错序列规划的可靠性之外，SGI还推广到传统的规划任务，如Plan-Bench和VQA。"
    },
    {
        "title": "VisualActBench: Can VLMs See and Act like a Human?",
        "summary": "Vision-Language Models (VLMs) have achieved impressive progress in perceiving and describing visual environments. However, their ability to proactively reason and act based solely on visual inputs, without explicit textual prompts, remains underexplored. We introduce a new task, Visual Action Reasoning, and propose VisualActBench, a large-scale benchmark comprising 1,074 videos and 3,733 human-annotated actions across four real-world scenarios. Each action is labeled with an Action Prioritization Level (APL) and a proactive-reactive type to assess models' human-aligned reasoning and value sensitivity. We evaluate 29 VLMs on VisualActBench and find that while frontier models like GPT4o demonstrate relatively strong performance, a significant gap remains compared to human-level reasoning, particularly in generating proactive, high-priority actions. Our results highlight limitations in current VLMs' ability to interpret complex context, anticipate outcomes, and align with human decision-making frameworks. VisualActBench establishes a comprehensive foundation for assessing and improving the real-world readiness of proactive, vision-centric AI agents.",
        "url": "http://arxiv.org/abs/2512.09907v1",
        "published_date": "2025-12-10T18:36:18+00:00",
        "updated_date": "2025-12-10T18:36:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daoan Zhang",
            "Pai Liu",
            "Xiaofei Zhou",
            "Yuan Ge",
            "Guangchen Lan",
            "Jing Bi",
            "Christopher Brinton",
            "Ehsan Hoque",
            "Jiebo Luo"
        ],
        "tldr": "The paper introduces VisualActBench, a benchmark for evaluating VLMs' ability to proactively reason and act based on visual input, revealing a significant performance gap compared to human-level reasoning, especially in proactive, high-priority actions.",
        "tldr_zh": "该论文介绍了VisualActBench，一个用于评估VLMs基于视觉输入进行主动推理和行动能力的基准，揭示了与人类水平推理的显著性能差距，尤其是在主动、高优先级行动方面。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉语言模型（VLMs）在感知和描述视觉环境方面取得了显著进展。然而，它们仅凭视觉输入，无需明确的文本提示，主动进行推理和行动的能力仍有待探索。我们引入了一项新的任务，即视觉行动推理，并提出了VisualActBench，这是一个大规模的基准测试，包含1074个视频和3733个人工标注的跨四个真实世界场景的行动。每个行动都标有行动优先级（APL）和一个主动-被动类型，以评估模型在符合人类思维的推理和价值观敏感度方面的能力。我们在VisualActBench上评估了29个VLMs，发现虽然像GPT4o这样的前沿模型表现出相对较强的性能，但与人类水平的推理相比，仍然存在显著差距，尤其是在生成主动的、高优先级的行动方面。我们的结果凸显了当前VLMs在解释复杂上下文、预测结果以及与人类决策框架对齐方面的局限性。VisualActBench为评估和提高主动的、以视觉为中心的AI智能体在真实世界中的可用性奠定了全面的基础。"
    },
    {
        "title": "AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding",
        "summary": "Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.",
        "url": "http://arxiv.org/abs/2512.10195v1",
        "published_date": "2025-12-11T01:25:36+00:00",
        "updated_date": "2025-12-11T01:25:36+00:00",
        "categories": [
            "cs.CL",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Gyutaek Oh",
            "Sangjoon Park",
            "Byung-Hoon Kim"
        ],
        "tldr": "AutoMedic introduces a multi-agent simulation framework for automated evaluation of LLMs as clinical conversational agents, transforming static QA datasets into realistic patient profiles and employing a multi-faceted CARE metric.",
        "tldr_zh": "AutoMedic 介绍了一个多智能体仿真框架，用于自动评估大型语言模型作为临床对话代理，通过将静态问答数据集转换为真实的患者资料，并采用多方面的 CARE 指标。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "评估大型语言模型（LLM）最近已成为LLM在医疗领域安全可信应用的关键问题。尽管已经提出了各种静态医疗问答（QA）基准，但许多方面仍未得到充分探索，例如LLM在动态、交互式临床多轮对话情境中生成回复的有效性，以及识别超越简单准确性的多方面评估策略。然而，对动态、交互式临床情境进行正式评估受到可能患者状态和交互轨迹的巨大组合空间限制，使得此类情境难以标准化和定量衡量。在此，我们介绍AutoMedic，一种多智能体模拟框架，能够自动评估LLM作为临床对话智能体的能力。AutoMedic将现成的静态QA数据集转化为虚拟患者档案，从而实现LLM智能体之间真实且具有临床依据的多轮临床对话。然后，基于我们的CARE指标评估各种临床对话智能体的性能，该指标提供临床对话的准确性、效率/策略、同理心和稳健性的多方面评估标准。我们的研究结果经过人类专家的验证，证明了AutoMedic作为临床对话智能体自动评估框架的有效性，并为在对话式医疗应用中有效开发LLM提供了实用指南。"
    },
    {
        "title": "Latent Action World Models for Control with Unlabeled Trajectories",
        "summary": "Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.",
        "url": "http://arxiv.org/abs/2512.10016v1",
        "published_date": "2025-12-10T19:09:45+00:00",
        "updated_date": "2025-12-10T19:09:45+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Marvin Alles",
            "Xingyuan Zhang",
            "Patrick van der Smagt",
            "Philip Becker-Ehmck"
        ],
        "tldr": "The paper introduces a latent-action world model that learns from both action-conditioned and action-free data by learning a shared latent action representation, enabling more efficient world model training and bridging offline RL with action-free training.",
        "tldr_zh": "该论文介绍了一种潜在动作世界模型，通过学习共享的潜在动作表示，从动作条件数据和无动作数据中学习，从而更有效地训练世界模型，并将离线强化学习与无动作训练桥接起来。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "受到人类将直接互动与无动作经验（例如，视频）相结合的启发，我们研究了从异构数据中学习的世界模型。标准世界模型通常依赖于动作条件轨迹，这限制了在动作标签稀缺时的有效性。我们引入了一系列潜在动作世界模型，通过学习共享的潜在动作表示来联合利用动作条件和无动作数据。该潜在空间将观察到的控制信号与从被动观察中推断出的动作对齐，从而使得单个动力学模型能够在大规模未标记轨迹上进行训练，同时仅需要少量动作标记的轨迹。我们使用潜在动作世界模型，通过离线强化学习（RL）来学习潜在动作策略，从而桥接了两个传统上分离的领域：离线强化学习（通常依赖于动作条件数据）和无动作训练（很少与后续强化学习一起使用）。在DeepMind控制套件上，我们的方法在使用的动作标记样本数量比纯动作条件基线少大约一个数量级的情况下，实现了强大的性能。这些结果表明，潜在动作使我们能够在被动数据和交互数据上进行训练，这使得世界模型的学习效率更高。"
    },
    {
        "title": "AERMANI-Diffusion: Regime-Conditioned Diffusion for Dynamics Learning in Aerial Manipulators",
        "summary": "Aerial manipulators undergo rapid, configuration-dependent changes in inertial coupling forces and aerodynamic forces, making accurate dynamics modeling a core challenge for reliable control. Analytical models lose fidelity under these nonlinear and nonstationary effects, while standard data-driven methods such as deep neural networks and Gaussian processes cannot represent the diverse residual behaviors that arise across different operating conditions. We propose a regime-conditioned diffusion framework that models the full distribution of residual forces using a conditional diffusion process and a lightweight temporal encoder. The encoder extracts a compact summary of recent motion and configuration, enabling consistent residual predictions even through abrupt transitions or unseen payloads. When combined with an adaptive controller, the framework enables dynamics uncertainty compensation and yields markedly improved tracking accuracy in real-world tests.",
        "url": "http://arxiv.org/abs/2512.10773v1",
        "published_date": "2025-12-11T16:10:32+00:00",
        "updated_date": "2025-12-11T16:10:32+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Samaksh Ujjawal",
            "Shivansh Pratap Singh",
            "Naveen Sudheer Nair",
            "Rishabh Dev Yadav",
            "Wei Pan",
            "Spandan Roy"
        ],
        "tldr": "The paper introduces AERMANI-Diffusion, a regime-conditioned diffusion framework for modeling dynamics in aerial manipulators, addressing the limitations of analytical and standard data-driven methods by modeling the distribution of residual forces conditioned on motion and configuration. It achieves improved tracking accuracy in real-world tests when combined with an adaptive controller.",
        "tldr_zh": "该论文介绍了AERMANI-Diffusion，一种用于建模空中机械臂动力学的状态调节扩散框架。该框架通过对运动和配置进行条件调节来对残余力的分布进行建模，从而克服了分析方法和标准数据驱动方法的局限性。当与自适应控制器结合使用时，它在真实世界的测试中实现了更高的跟踪精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "空中机械臂会经历惯性耦合力和气动力的快速变化，并且这些变化与配置高度相关，这使得精确的动力学建模成为可靠控制的核心挑战。分析模型在这些非线性和非平稳效应下会失去保真度，而诸如深度神经网络和高斯过程之类的标准数据驱动方法无法表示在不同工况下产生的各种残余行为。我们提出了一种状态条件扩散框架，该框架使用条件扩散过程和轻量级时间编码器对残余力的完整分布进行建模。该编码器提取近期运动和配置的紧凑摘要，即使在突然过渡或未见载荷的情况下也能实现一致的残余预测。当与自适应控制器结合使用时，该框架能够进行动力学不确定性补偿，并在实际测试中显著提高跟踪精度。"
    },
    {
        "title": "How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning",
        "summary": "Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.",
        "url": "http://arxiv.org/abs/2512.10698v1",
        "published_date": "2025-12-11T14:40:33+00:00",
        "updated_date": "2025-12-11T14:40:33+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Jianbo Wang",
            "Galina Sidorenko",
            "Johan Thunberg"
        ],
        "tldr": "This paper explores using Deep Reinforcement Learning (DRL) with vehicle-to-vehicle communication for ethical emergency braking in multi-vehicle scenarios, aiming for collective harm reduction and collision avoidance using a hybrid DRL and analytical approach.",
        "tldr_zh": "本文探讨了在多车场景中使用深度强化学习（DRL）和车对车通信的伦理紧急制动，旨在通过混合DRL和分析方法实现集体伤害减少和避免碰撞。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "互联自动驾驶车辆（CAV）具有提升驾驶安全的潜力，例如通过实现安全的车辆跟随和更高效的交通调度。 对于此类未来的部署，应解决安全要求，其中最主要的要求是避免车辆碰撞并在不可避免的情况下大幅减轻伤害。 然而，基于保守的最坏情况的控制策略会以降低灵活性为代价，并可能损害整体性能。 鉴于此，我们研究如何利用深度强化学习（DRL）来提高涉及紧急制动多车跟随场景中的安全性。 具体而言，我们研究在整体或集体的三车伤害减少或碰撞避免，而非单车伤害减少或碰撞避免的情况下，如何使用带有车对车通信的 DRL 来合理选择紧急制动曲线。 作为一种算法，我们提供了一种混合方法，该方法将 DRL 与先前发表的基于恒定减速度优化选择的解析表达式的方法相结合。 通过将 DRL 与先前的方法相结合，所提出的混合方法与独立的 DRL 相比提高了可靠性，同时在整体伤害减少和碰撞避免方面实现了卓越的性能。"
    },
    {
        "title": "Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks",
        "summary": "Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation\". In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM\", is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.",
        "url": "http://arxiv.org/abs/2512.10481v1",
        "published_date": "2025-12-11T09:59:08+00:00",
        "updated_date": "2025-12-11T09:59:08+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Gaozhao Wang",
            "Xing Liu",
            "Zhenduo Ye",
            "Zhengxiong Liu",
            "Panfeng Huang"
        ],
        "tldr": "This paper introduces Contact SLAM, a tactile-based SLAM method with an active exploration policy for robotic blind manipulation, demonstrating its effectiveness in contact-rich tasks like socket assembly and block pushing.",
        "tldr_zh": "本文介绍了一种名为Contact SLAM的触觉SLAM方法，该方法具有主动探索策略，用于机器人盲操作。实验结果表明，该方法在诸如插座组装和推积木等接触丰富的任务中有效。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "富接触操作对机器人来说难以执行，需要对环境有精确的感知。在某些情况下，视觉会被遮挡。机器人因此无法通过视觉反馈获得实时的场景状态信息，这被称为“盲操作”。在本手稿中，提出了一种新颖的物理驱动的接触认知方法，称为“接触SLAM”。它仅使用触觉传感和场景的先验知识来估计环境状态并实现操作。为了最大化探索效率，本手稿还设计了一种主动探索策略。该策略逐步减少操作场景中的不确定性。实验结果表明，所提出的方法在多个富接触任务中具有效果和准确性，包括困难而精细的插座组装任务和块状物体推压任务。"
    },
    {
        "title": "Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots",
        "summary": "In our work we not explicitly hint that it is a misconception to think that humans learn fast. Learning process takes time. Babies start learning to move in the restricted liquid area called placenta. Children often are limited by underdeveloped body. Even adults are not allowed to participate in complex competitions right away. However, with robots, when learning from scratch, we often don't have the privilege of waiting for dozen millions of steps. \"Swaddling\" regularization is responsible for restraining an agent in rapid but unstable development penalizing action strength in a specific way not affecting actions directly. The Symphony, Transitional-policy Deterministic Actor and Critic algorithm, is a concise combination of different ideas for possibility of training humanoid robots from scratch with Sample Efficiency, Sample Proximity and Safety of Actions in mind. It is no secret that continuous increase in Gaussian noise without appropriate smoothing is harmful for motors and gearboxes. Compared to Stochastic algorithms, we set a limited parametric noise and promote a reduced strength of actions, safely increasing entropy, since the actions are kind of immersed in weaker noise. When actions require more extreme values, actions rise above the weak noise. Training becomes empirically much safer for both the environment around and the robot's mechanisms. We use Fading Replay Buffer: using a fixed formula containing the hyperbolic tangent, we adjust the batch sampling probability: the memory contains a recent memory and a long-term memory trail. Fading Replay Buffer allows us to use Temporal Advantage when we improve the current Critic Network prediction compared to the exponential moving average. Temporal Advantage allows us to update Actor and Critic in one pass, as well as combine Actor and Critic in one Object and implement their Losses in one line.",
        "url": "http://arxiv.org/abs/2512.10477v1",
        "published_date": "2025-12-11T09:55:49+00:00",
        "updated_date": "2025-12-11T09:55:49+00:00",
        "categories": [
            "cs.RO",
            "cs.NE"
        ],
        "authors": [
            "Timur Ishuov",
            "Michele Folgheraiter",
            "Madi Nurmanov",
            "Goncalo Gordo",
            "Richárd Farkas",
            "József Dombi"
        ],
        "tldr": "The paper introduces Symphony, a reinforcement learning algorithm tailored for training humanoid robots from scratch with improved sample efficiency, safety, and sample proximity. It utilizes 'swaddling' regularization, a fading replay buffer, and a novel temporal advantage to facilitate efficient and safe learning.",
        "tldr_zh": "该论文介绍了Symphony，一种专为从零开始训练人形机器人设计的强化学习算法，具有更高的样本效率、安全性和样本接近性。它利用“襁褓”正则化、衰减重放缓冲区和一种新的时间优势，以促进高效安全的学习。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 6,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们的工作并非直接暗示人类快速学习是一种误解。学习过程需要时间。婴儿在被称为胎盘的受限液体区域开始学习移动。儿童常常受到未发育成熟身体的限制。即使是成年人也不能立即参加复杂的竞赛。然而，对于机器人，当从零开始学习时，我们通常没有等待数千万步的特权。“襁褓”正则化负责约束智能体在快速但不稳定的发展中，以一种不直接影响动作的方式惩罚动作强度。交响乐算法（Symphony），即过渡策略确定性行动者-评论家算法，是不同思想的简洁组合，旨在实现从零开始训练人形机器人，同时兼顾样本效率、样本邻近性和动作安全性。众所周知，在没有适当平滑的情况下持续增加高斯噪声对电机和齿轮箱是有害的。与随机算法相比，我们设置了有限的参数化噪声，并促进降低的动作强度，安全地增加熵，因为动作某种程度上浸没在较弱的噪声中。当动作需要更极端的值时，动作会超出弱噪声。从经验上讲，训练对于周围环境和机器人的机制都变得更加安全。我们使用衰减回放缓冲区：使用包含双曲正切的固定公式，我们调整批次采样概率：内存包含最近的记忆和长期的记忆轨迹。衰减回放缓冲区允许我们在改进当前评论家网络预测（与指数移动平均相比）时使用时序优势。时序优势允许我们在一次传递中更新行动者和评论家，以及将行动者和评论家组合成一个对象，并在一行中实现它们的损失。"
    },
    {
        "title": "Lies We Can Trust: Quantifying Action Uncertainty with Inaccurate Stochastic Dynamics through Conformalized Nonholonomic Lie Groups",
        "summary": "We propose Conformal Lie-group Action Prediction Sets (CLAPS), a symmetry-aware conformal prediction-based algorithm that constructs, for a given action, a set guaranteed to contain the resulting system configuration at a user-defined probability. Our assurance holds under both aleatoric and epistemic uncertainty, non-asymptotically, and does not require strong assumptions about the true system dynamics, the uncertainty sources, or the quality of the approximate dynamics model. Typically, uncertainty quantification is tackled by making strong assumptions about the error distribution or magnitude, or by relying on uncalibrated uncertainty estimates - i.e., with no link to frequentist probabilities - which are insufficient for safe control. Recently, conformal prediction has emerged as a statistical framework capable of providing distribution-free probabilistic guarantees on test-time prediction accuracy. While current conformal methods treat robots as Euclidean points, many systems have non-Euclidean configurations, e.g., some mobile robots have SE(2). In this work, we rigorously analyze configuration errors using Lie groups, extending previous Euclidean Space theoretical guarantees to SE(2). Our experiments on a simulated JetBot, and on a real MBot, suggest that by considering the configuration space's structure, our symmetry-informed nonconformity score leads to more volume-efficient prediction regions which represent the underlying uncertainty better than existing approaches.",
        "url": "http://arxiv.org/abs/2512.10294v1",
        "published_date": "2025-12-11T05:26:56+00:00",
        "updated_date": "2025-12-11T05:26:56+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Luís Marques",
            "Maani Ghaffari",
            "Dmitry Berenson"
        ],
        "tldr": "The paper introduces Conformal Lie-group Action Prediction Sets (CLAPS) for uncertainty quantification in robotics, providing distribution-free probabilistic guarantees on future system configurations, especially for systems with non-Euclidean configuration spaces like SE(2). It offers a symmetry-informed approach that outperforms existing methods.",
        "tldr_zh": "本文介绍了一种名为Conformal Lie-group Action Prediction Sets (CLAPS) 的方法，用于量化机器人技术中的不确定性，为未来的系统配置提供无分布的概率保证，尤其适用于具有非欧几里得配置空间（如SE(2)）的系统。 它提供了一种对称信息方法，优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们提出共形李群作用预测集(CLAPS)，这是一种对称性感知的基于共形预测的算法，针对给定的动作构建一个集合，保证该集合以用户定义的概率包含最终的系统配置。我们的保证在偶然不确定性和认知不确定性下均成立，具有非渐近性，并且不需要对真实系统动力学、不确定性来源或近似动力学模型的质量做出强假设。通常，不确定性量化是通过对误差分布或大小做出强假设，或者依赖于未校准的不确定性估计（即，与频率概率无关）来解决的，这对于安全控制而言是不够的。最近，共形预测作为一种统计框架出现，能够提供关于测试时预测精度的无分布概率保证。虽然目前的共形方法将机器人视为欧几里得空间中的点，但许多系统具有非欧几里得配置，例如，一些移动机器人具有SE(2) 配置。在这项工作中，我们使用李群严格分析配置误差，将先前欧几里得空间的理论保证扩展到 SE(2)。我们对模拟 JetBot 和真实 MBot 的实验表明，通过考虑配置空间的结构，我们的对称性信息化的不一致性分数可以得到更具体积效率的预测区域，这些区域比现有方法更好地表示了底层的不确定性。"
    },
    {
        "title": "Task-Oriented Grasping Using Reinforcement Learning with a Contextual Reward Machine",
        "summary": "This paper presents a reinforcement learning framework that incorporates a Contextual Reward Machine for task-oriented grasping. The Contextual Reward Machine reduces task complexity by decomposing grasping tasks into manageable sub-tasks. Each sub-task is associated with a stage-specific context, including a reward function, an action space, and a state abstraction function. This contextual information enables efficient intra-stage guidance and improves learning efficiency by reducing the state-action space and guiding exploration within clearly defined boundaries. In addition, transition rewards are introduced to encourage or penalize transitions between stages which guides the model toward desirable stage sequences and further accelerates convergence. When integrated with the Proximal Policy Optimization algorithm, the proposed method achieved a 95% success rate across 1,000 simulated grasping tasks encompassing diverse objects, affordances, and grasp topologies. It outperformed the state-of-the-art methods in both learning speed and success rate. The approach was transferred to a real robot, where it achieved a success rate of 83.3% in 60 grasping tasks over six affordances. These experimental results demonstrate superior accuracy, data efficiency, and learning efficiency. They underscore the model's potential to advance task-oriented grasping in both simulated and real-world settings.",
        "url": "http://arxiv.org/abs/2512.10235v1",
        "published_date": "2025-12-11T02:42:40+00:00",
        "updated_date": "2025-12-11T02:42:40+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Hui Li",
            "Akhlak Uz Zaman",
            "Fujian Yan",
            "Hongsheng He"
        ],
        "tldr": "This paper introduces a reinforcement learning framework for task-oriented grasping using a Contextual Reward Machine to decompose tasks and improve learning efficiency, demonstrating high success rates in simulation and real-world robot experiments.",
        "tldr_zh": "本文提出了一种基于上下文奖励机的强化学习框架，用于任务导向的抓取。该框架通过分解任务并提高学习效率，在仿真和真实机器人实验中都表现出很高的成功率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文提出了一种融合上下文奖励机的强化学习框架，用于面向任务的抓取。上下文奖励机通过将抓取任务分解为可管理的子任务来降低任务复杂度。每个子任务都与一个特定阶段的上下文相关联，包括奖励函数、动作空间和状态抽象函数。这种上下文信息能够实现高效的阶段内指导，并通过缩小状态-动作空间和在明确定义的边界内指导探索来提高学习效率。此外，引入了转移奖励以鼓励或惩罚阶段之间的转移，从而引导模型朝着期望的阶段序列发展，并进一步加速收敛。当与近端策略优化算法集成时，所提出的方法在包含不同对象、可供性和抓取拓扑结构的1000个模拟抓取任务中实现了95%的成功率。在学习速度和成功率方面，该方法均优于现有最先进的方法。该方法被迁移到真实机器人上，在六种可供性上的60个抓取任务中实现了83.3%的成功率。这些实验结果表明了优越的准确性、数据效率和学习效率，并突显了该模型在模拟和真实世界环境中推进面向任务的抓取的潜力。"
    },
    {
        "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
        "summary": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
        "url": "http://arxiv.org/abs/2512.10611v1",
        "published_date": "2025-12-11T13:04:44+00:00",
        "updated_date": "2025-12-11T13:04:44+00:00",
        "categories": [
            "cs.AI",
            "cs.NE"
        ],
        "authors": [
            "Minghao LI",
            "Ruihang Wang",
            "Rui Tan",
            "Yonggang Wen"
        ],
        "tldr": "The paper introduces Phythesis, a framework that uses LLMs and physics-guided evolutionary optimization to design energy-efficient data centers, demonstrating improvements in generation success rate and power usage effectiveness compared to LLM-only approaches.",
        "tldr_zh": "该论文介绍了Phythesis，一个使用LLM和物理引导下的进化优化来设计节能数据中心的框架。与仅使用LLM的方法相比，该框架在生成成功率和电源使用效率方面都有所提高。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "数据中心（DC）基础设施是支撑不断增长的计算需求的中流砥柱。 传统的融合人类专业知识与专用仿真工具的设计方法，难以适应日益增长的系统复杂度。 近期研究采用生成式人工智能来设计看似合理的以人为中心的室内布局。 然而，这些研究并未考虑底层物理规律，因此不适用于设定可量化运营目标和严格物理约束的DC设计。 为弥合这一差距，我们提出了Phythesis，一种新颖的框架，它协同利用大型语言模型（LLM）和物理引导的进化优化，以自动化生成可用于仿真的（SimReady）场景，从而实现节能的DC设计。 Phythesis采用迭代的双层优化架构，其中（i）LLM驱动的优化层生成物理上可行的三维布局并进行自我批判，从而完善场景拓扑结构；（ii）物理信息优化的优化层识别出最佳资产参数并选择最佳资产组合。 在三个生成规模上的实验表明，与基于普通LLM的解决方案相比，Phythesis的生成成功率提高了57.3%，电源使用效率（PUE）提高了11.5%。"
    },
    {
        "title": "Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning",
        "summary": "Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.",
        "url": "http://arxiv.org/abs/2512.10510v1",
        "published_date": "2025-12-11T10:30:04+00:00",
        "updated_date": "2025-12-11T10:30:04+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Chihyeon Song",
            "Jaewoo Lee",
            "Jinkyoo Park"
        ],
        "tldr": "This paper introduces an Adaptive Replay Buffer (ARB) for Offline-to-Online Reinforcement Learning that dynamically prioritizes data sampling based on 'on-policyness', improving both early learning stability and asymptotic performance.",
        "tldr_zh": "本文介绍了一种用于离线到在线强化学习的自适应回放缓冲区（ARB），该缓冲区基于“策略内”动态地优先处理数据采样，从而提高早期学习的稳定性和渐近性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "离线到在线强化学习（O2O RL）面临着一个关键困境，即如何平衡使用固定离线数据集与新收集的在线经验。标准方法通常依赖于固定的数据混合比例，难以管理早期学习稳定性和渐近性能之间的权衡。为了克服这一问题，我们引入了自适应回放缓冲区（ARB），这是一种新颖的方法，它基于一种称为“策略上程度（on-policyness）”的轻量级指标，动态地决定数据采样的优先级。与依赖复杂学习过程或固定比例的先前方法不同，ARB 被设计为无需学习且易于实现，可以无缝集成到现有的 O2O RL 算法中。它评估收集的轨迹与当前策略的行为之间的匹配程度，并为该轨迹内的每个转换分配成比例的采样权重。这种策略有效地利用离线数据来实现初始稳定性，同时逐渐将学习重点放在最相关的、高回报的在线经验上。我们在 D4RL 基准测试上的大量实验表明，ARB 始终能缓解早期性能下降，并显著提高各种 O2O RL 算法的最终性能，突出了自适应、行为感知回放缓冲区设计的重要性。"
    },
    {
        "title": "UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning",
        "summary": "Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.",
        "url": "http://arxiv.org/abs/2512.10492v1",
        "published_date": "2025-12-11T10:14:13+00:00",
        "updated_date": "2025-12-11T10:14:13+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Jiaxi Wu",
            "Tiantian Zhang",
            "Yuxing Wang",
            "Yongzhe Chang",
            "Xueqian Wang"
        ],
        "tldr": "This paper introduces UACER, a novel uncertainty-aware critic ensemble method for robust adversarial reinforcement learning, designed to improve training stability and performance in complex environments by using a diverse critic ensemble and time-varying decay uncertainty mechanism.",
        "tldr_zh": "该论文提出了一种名为UACER的新型不确定性感知评论家集成方法，用于鲁棒对抗强化学习，旨在通过使用多样化的评论家集成和时变衰减不确定性机制来提高复杂环境中的训练稳定性和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "鲁棒对抗强化学习已成为训练智能体以应对真实环境中不确定扰动的一种有效范例，在自主驾驶和机器人控制等序贯决策领域具有关键应用。在该范例中，通常将智能体训练建模为主角和对手之间的零和马尔可夫博弈，以增强策略的鲁棒性。然而，对手的可训练性不可避免地会导致学习动态的非平稳性，进而加剧训练的不稳定性和收敛困难，尤其是在高维复杂环境中。在本文中，我们提出了一种新颖的方法，即不确定性感知评论家集成（UACER），用于鲁棒对抗强化学习。该方法包含两种策略：1) 多样化的评论家集成：并行利用一组不同的K个评论家网络来稳定Q值估计，而非传统的单评论家架构，以实现方差缩减和鲁棒性增强。2) 时变衰减不确定性（TDU）机制：我们超越了简单的线性组合，开发了一种基于方差的Q值聚合策略，该策略显式地结合认知不确定性，以动态调节探索-利用的平衡，同时稳定训练过程。在多个MuJoCo控制问题上进行的综合实验验证了UACER的卓越有效性，在整体性能、稳定性和效率方面均优于最先进的方法。"
    },
    {
        "title": "AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management",
        "summary": "The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.",
        "url": "http://arxiv.org/abs/2512.10371v1",
        "published_date": "2025-12-11T07:37:38+00:00",
        "updated_date": "2025-12-11T07:37:38+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Shizuo Tian",
            "Hao Wen",
            "Yuxuan Chen",
            "Jiacheng Liu",
            "Shanhui Zhao",
            "Guohong Liu",
            "Ju Ren",
            "Yunxin Liu",
            "Yuanchun Li"
        ],
        "tldr": "The paper introduces AgentProg, a program-guided context management method for GUI agents that improves performance on long-horizon tasks by representing interaction history as a program with variables and control flow, combined with a global belief state mechanism.",
        "tldr_zh": "该论文介绍了一种名为AgentProg的程序引导的GUI代理上下文管理方法，通过将交互历史表示为带有变量和控制流的程序，并结合全局置信状态机制，从而提高了在长时程任务中的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "移动GUI代理的快速发展激发了人们对长程任务自动化的日益增长的研究兴趣。然而，构建此类任务的代理面临着一个关键瓶颈：对不断扩展的交互历史的依赖会导致巨大的上下文开销。 现有的上下文管理和压缩技术通常无法保留关键的语义信息，从而导致任务性能下降。 我们提出AgentProg，一种程序引导的代理上下文管理方法，它将交互历史重新构架为一个带有变量和控制流的程序。 通过根据程序的结构来组织信息，这种结构提供了一种有原则的机制来确定哪些信息应该保留，哪些信息可以丢弃。 我们进一步集成了一种受信念MDP框架启发的全局信念状态机制，以处理部分可观测性和适应意外的环境变化。 在AndroidWorld和我们扩展的长程任务套件上的实验表明，AgentProg在这些基准测试中取得了最先进的成功率。 更重要的是，它在长程任务上保持了鲁棒的性能，而基线方法则经历了灾难性的性能下降。 我们的系统已在https://github.com/MobileLLM/AgentProg上开源。"
    },
    {
        "title": "EpiPlanAgent: Agentic Automated Epidemic Response Planning",
        "summary": "Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.",
        "url": "http://arxiv.org/abs/2512.10313v1",
        "published_date": "2025-12-11T06:03:17+00:00",
        "updated_date": "2025-12-11T06:03:17+00:00",
        "categories": [
            "cs.AI",
            "cs.CY"
        ],
        "authors": [
            "Kangkun Mao",
            "Fang Xu",
            "Jinru Ding",
            "Yidong Jiang",
            "Yujun Yao",
            "Yirong Chen",
            "Junming Liu",
            "Xiaoqin Wu",
            "Qian Wu",
            "Xiaoyan Huang",
            "Jie Xu"
        ],
        "tldr": "EpiPlanAgent, an agent-based system using LLMs, automates the generation and validation of epidemic response plans, demonstrating improved completeness, guideline alignment, and development time compared to manual workflows.",
        "tldr_zh": "EpiPlanAgent是一个基于LLM的智能体系统，可自动生成和验证流行病应对计划，与手动工作流程相比，在完整性、指南一致性和开发时间方面均得到了改善。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "疫情应对规划至关重要，但传统上依赖于劳动密集型的手动方法。本研究旨在设计和评估EpiPlanAgent，一个基于代理的系统，该系统使用大型语言模型(LLMs)来自动化数字应急响应计划的生成和验证。该多代理框架集成了任务分解、知识基础和模拟模块。公共卫生专业人员在受控评估中使用真实世界的疫情爆发场景对该系统进行了测试。结果表明，与手动工作流程相比，EpiPlanAgent显著提高了计划的完整性和指南一致性，同时大幅缩短了开发时间。专家评估证实了人工智能生成内容与人工撰写内容之间的高度一致性。用户反馈表明了对其效用的强烈感知。总之，EpiPlanAgent为智能疫情应对规划提供了一种有效且可扩展的解决方案，展示了代理式人工智能在变革公共卫生准备方面的潜力。"
    },
    {
        "title": "InfoCom: Kilobyte-Scale Communication-Efficient Collaborative Perception with Information Bottleneck",
        "summary": "Precise environmental perception is critical for the reliability of autonomous driving systems. While collaborative perception mitigates the limitations of single-agent perception through information sharing, it encounters a fundamental communication-performance trade-off. Existing communication-efficient approaches typically assume MB-level data transmission per collaboration, which may fail due to practical network constraints. To address these issues, we propose InfoCom, an information-aware framework establishing the pioneering theoretical foundation for communication-efficient collaborative perception via extended Information Bottleneck principles. Departing from mainstream feature manipulation, InfoCom introduces a novel information purification paradigm that theoretically optimizes the extraction of minimal sufficient task-critical information under Information Bottleneck constraints. Its core innovations include: i) An Information-Aware Encoding condensing features into minimal messages while preserving perception-relevant information; ii) A Sparse Mask Generation identifying spatial cues with negligible communication cost; and iii) A Multi-Scale Decoding that progressively recovers perceptual information through mask-guided mechanisms rather than simple feature reconstruction. Comprehensive experiments across multiple datasets demonstrate that InfoCom achieves near-lossless perception while reducing communication overhead from megabyte to kilobyte-scale, representing 440-fold and 90-fold reductions per agent compared to Where2comm and ERMVP, respectively.",
        "url": "http://arxiv.org/abs/2512.10305v1",
        "published_date": "2025-12-11T05:51:02+00:00",
        "updated_date": "2025-12-11T05:51:02+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Quanmin Wei",
            "Penglin Dai",
            "Wei Li",
            "Bingyi Liu",
            "Xiao Wu"
        ],
        "tldr": "The paper introduces InfoCom, a novel collaborative perception framework that significantly reduces communication overhead (from MB to KB scale) using information bottleneck principles for autonomous driving.",
        "tldr_zh": "该论文介绍了InfoCom，一种新型协同感知框架，利用信息瓶颈原理，显著降低了通信开销（从MB级到KB级），适用于自动驾驶。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "精确的环境感知对于自动驾驶系统的可靠性至关重要。虽然协同感知通过信息共享缓解了单智能体感知的局限性，但它面临着基本的通信-性能权衡。现有的通信高效方法通常假设每次协作进行MB级的数据传输，这可能会因实际网络约束而失败。为了解决这些问题，我们提出了InfoCom，这是一种信息感知框架，通过扩展的信息瓶颈原理为通信高效的协同感知奠定了开创性的理论基础。与主流的特征操作不同，InfoCom引入了一种新颖的信息净化范式，理论上优化了在信息瓶颈约束下最小充分的任务关键信息的提取。其核心创新包括：i) 一种信息感知编码，将特征压缩成最小的消息，同时保留与感知相关的信息；ii) 一种稀疏掩码生成，以忽略不计的通信成本识别空间线索；以及 iii) 一种多尺度解码，通过掩码引导机制，而不是简单的特征重建，逐步恢复感知信息。在多个数据集上的全面实验表明，InfoCom 实现了接近无损的感知，同时将通信开销从兆字节级降低到千字节级，与 Where2comm 和 ERMVP 相比，每个智能体分别减少了 440 倍和 90 倍。"
    },
    {
        "title": "AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice",
        "summary": "Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.",
        "url": "http://arxiv.org/abs/2512.10114v1",
        "published_date": "2025-12-10T22:06:41+00:00",
        "updated_date": "2025-12-10T22:06:41+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Mesafint Fanuel",
            "Mahmoud Nabil Mahmoud",
            "Crystal Cook Marshal",
            "Vishal Lakhotia",
            "Biswanath Dari",
            "Kaushik Roy",
            "Shaohu Zhang"
        ],
        "tldr": "AgriRegion is a RAG framework specifically designed for region-aware agricultural advice using geospatial metadata injection and region-prioritized re-ranking to reduce hallucinations in LLMs. They also created a new benchmark dataset, AgriRegion-Eval.",
        "tldr_zh": "AgriRegion是一个检索增强生成（RAG）框架，专为区域感知的农业建议而设计，它使用地理空间元数据注入和区域优先的重新排序来减少大型语言模型中的幻觉。他们还创建了一个新的基准数据集AgriRegion-Eval。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "大型语言模型（LLMs）在普及信息获取方面展现出巨大的潜力。然而，在农业领域，通用模型经常遭受上下文幻觉的影响，即提供不符合实际的建议，或者在一个区域科学合理的答案，由于土壤、气候和当地法规的差异，在另一个区域可能是灾难性的。我们介绍AgriRegion，一种专为高保真、区域感知的农业咨询设计的检索增强生成（RAG）框架。与仅依赖语义相似性的标准RAG方法不同，AgriRegion整合了一个地理空间元数据注入层和一个区域优先的重排序机制。通过将知识库限制在经过验证的当地农业推广服务，并在检索过程中强制实施地理空间约束，AgriRegion确保有关种植时间表、病虫害防治和施肥的建议在当地是准确的。我们创建了一个新的基准数据集AgriRegion-Eval，它包含12个农业子领域的160个领域特定问题。实验表明，与最先进的LLM系统相比，AgriRegion将幻觉减少了10-20%，并根据全面的评估显著提高了信任分数。"
    },
    {
        "title": "STACHE: Local Black-Box Explanations for Reinforcement Learning Policies",
        "summary": "Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.",
        "url": "http://arxiv.org/abs/2512.09909v1",
        "published_date": "2025-12-10T18:37:28+00:00",
        "updated_date": "2025-12-10T18:37:28+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Andrew Elashkin",
            "Orna Grumberg"
        ],
        "tldr": "This paper introduces STACHE, a framework for generating local, black-box explanations for reinforcement learning policies by identifying robustness regions and minimal counterfactuals using a search-based algorithm for factored state spaces. It provides actionable insights into agent sensitivity and decision boundaries.",
        "tldr_zh": "本文介绍了STACHE，一个用于生成强化学习策略的局部黑盒解释的框架，通过使用基于搜索的算法来识别因子状态空间的稳健性区域和最小反事实。 它提供了关于agent灵敏度和决策边界的可操作见解。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "强化学习智能体在稀疏奖励或安全关键环境中往往表现出意想不到的行为，因此迫切需要可靠的调试和验证工具。在本文中，我们提出了STACHE，一个全面的框架，用于生成离散马尔可夫游戏中智能体特定动作的局部、黑盒解释。我们的方法产生一个复合解释，包含两个互补的组成部分：（1）鲁棒性区域，即智能体动作保持不变的连通状态邻域；（2）最小反事实，即改变该决策所需的最小状态扰动。通过利用分解状态空间的结构，我们引入了一种精确的、基于搜索的算法，规避了替代模型的保真度差距。在Gymnasium环境中的经验验证表明，我们的框架不仅解释了策略动作，而且有效地捕捉了训练期间策略逻辑的演变——从不稳定的行为到优化的、鲁棒的策略——从而为智能体敏感性和决策边界提供了可操作的见解。"
    },
    {
        "title": "PoseGAM: Robust Unseen Object Pose Estimation via Geometry-Aware Multi-View Reasoning",
        "summary": "6D object pose estimation, which predicts the transformation of an object relative to the camera, remains challenging for unseen objects. Existing approaches typically rely on explicitly constructing feature correspondences between the query image and either the object model or template images. In this work, we propose PoseGAM, a geometry-aware multi-view framework that directly predicts object pose from a query image and multiple template images, eliminating the need for explicit matching. Built upon recent multi-view-based foundation model architectures, the method integrates object geometry information through two complementary mechanisms: explicit point-based geometry and learned features from geometry representation networks. In addition, we construct a large-scale synthetic dataset containing more than 190k objects under diverse environmental conditions to enhance robustness and generalization. Extensive evaluations across multiple benchmarks demonstrate our state-of-the-art performance, yielding an average AR improvement of 5.1% over prior methods and achieving up to 17.6% gains on individual datasets, indicating strong generalization to unseen objects. Project page: https://windvchen.github.io/PoseGAM/ .",
        "url": "http://arxiv.org/abs/2512.10840v1",
        "published_date": "2025-12-11T17:29:25+00:00",
        "updated_date": "2025-12-11T17:29:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianqi Chen",
            "Biao Zhang",
            "Xiangjun Tang",
            "Peter Wonka"
        ],
        "tldr": "PoseGAM is a geometry-aware multi-view framework for 6D object pose estimation of unseen objects, achieving state-of-the-art performance by integrating point-based geometry and learned features, and using a large-scale synthetic dataset for training.",
        "tldr_zh": "PoseGAM是一种几何感知的多视图框架，用于估计未见过的物体的6D姿态。它通过整合基于点的几何和学习到的特征，并使用大规模合成数据集进行训练，实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "6D物体姿态估计，即预测物体相对于相机的变换，对于未见过的物体仍然具有挑战性。现有方法通常依赖于在查询图像和物体模型或模板图像之间显式地构建特征对应关系。在这项工作中，我们提出PoseGAM，一种几何感知多视角框架，可以直接从查询图像和多个模板图像中预测物体姿态，无需显式匹配。该方法基于最近的多视角基础模型架构构建，通过两种互补的机制集成物体几何信息：显式的基于点的几何信息和来自几何表示网络的学习特征。此外，我们构建了一个包含超过19万个物体的大规模合成数据集，其中包含多种多样的环境条件，以增强鲁棒性和泛化能力。在多个基准上的大量评估表明，我们的方法取得了最先进的性能，相较于先前的方法，平均AR值提升了5.1%，在单个数据集上最高提升了17.6%，表明了对未见过物体的强大泛化能力。项目主页：https://windvchen.github.io/PoseGAM/ 。"
    },
    {
        "title": "K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices",
        "summary": "Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.",
        "url": "http://arxiv.org/abs/2512.10628v1",
        "published_date": "2025-12-11T13:26:58+00:00",
        "updated_date": "2025-12-11T13:26:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bishoy Galoaa",
            "Pau Closas",
            "Sarah Ostadabbas"
        ],
        "tldr": "The paper introduces K-Track, a framework that accelerates deep learning based point trackers on edge devices by combining keyframe updates with Kalman filtering, achieving significant speedup with minimal accuracy loss.",
        "tldr_zh": "该论文介绍了K-Track，一个通过结合关键帧更新和卡尔曼滤波来加速边缘设备上基于深度学习的点跟踪器的框架，实现了显著的加速，同时仅略微降低了准确性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "视频序列中的点跟踪是现实世界计算机视觉应用的一项基础能力，包括机器人、自动驾驶系统、增强现实和视频分析。虽然最近基于深度学习的跟踪器在具有挑战性的基准测试中实现了最先进的精度，但它们对每帧GPU推断的依赖构成了在资源受限的边缘设备上部署的主要障碍，因为这些设备上的计算、电力和连接性都受到限制。我们引入了K-Track（Kalman增强跟踪），这是一个通用的、与跟踪器无关的加速框架，旨在弥合这一部署差距。K-Track通过结合稀疏的深度学习关键帧更新和用于中间帧预测的轻量级卡尔曼滤波，来降低推断成本，并使用基于原则的贝叶斯不确定性传播来维持时间一致性。这种混合策略能够实现5-10倍的加速，同时保留原始跟踪器超过85%的精度。我们在多个最先进的点跟踪器上评估了K-Track，并展示了在NVIDIA Jetson Nano和RTX Titan等边缘平台上的实时性能。通过在显著降低计算需求的同时保持精度，K-Track为在真实世界、资源有限的环境中部署高质量点跟踪提供了一条切实可行的途径，缩小了现代跟踪算法和可部署视觉系统之间的差距。"
    },
    {
        "title": "TransLocNet: Cross-Modal Attention for Aerial-Ground Vehicle Localization with Contrastive Learning",
        "summary": "Aerial-ground localization is difficult due to large viewpoint and modality gaps between ground-level LiDAR and overhead imagery. We propose TransLocNet, a cross-modal attention framework that fuses LiDAR geometry with aerial semantic context. LiDAR scans are projected into a bird's-eye-view representation and aligned with aerial features through bidirectional attention, followed by a likelihood map decoder that outputs spatial probability distributions over position and orientation. A contrastive learning module enforces a shared embedding space to improve cross-modal alignment. Experiments on CARLA and KITTI show that TransLocNet outperforms state-of-the-art baselines, reducing localization error by up to 63% and achieving sub-meter, sub-degree accuracy. These results demonstrate that TransLocNet provides robust and generalizable aerial-ground localization in both synthetic and real-world settings.",
        "url": "http://arxiv.org/abs/2512.10419v1",
        "published_date": "2025-12-11T08:34:26+00:00",
        "updated_date": "2025-12-11T08:34:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Phu Pham",
            "Damon Conover",
            "Aniket Bera"
        ],
        "tldr": "TransLocNet uses cross-modal attention and contrastive learning to fuse LiDAR and aerial imagery for accurate aerial-ground vehicle localization, achieving significant error reduction compared to state-of-the-art methods.",
        "tldr_zh": "TransLocNet采用跨模态注意力机制和对比学习，融合LiDAR和航拍图像，实现精确的空地车辆定位，与现有方法相比，显著降低了定位误差。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "空中-地面定位面临挑战，因为地面激光雷达和高空影像之间存在巨大的视角和模态差异。我们提出了TransLocNet，一个融合激光雷达几何信息和高空语义上下文的跨模态注意力框架。激光雷达扫描被投影到鸟瞰图表示中，并通过双向注意力与高空特征对齐，随后通过似然图解码器输出关于位置和方向的空间概率分布。一个对比学习模块强制实施共享的嵌入空间，以改善跨模态对齐。在CARLA和KITTI上的实验表明，TransLocNet优于最先进的基线方法，定位误差最多降低63%，并实现了亚米级和亚度级的精度。这些结果表明，TransLocNet在合成和真实世界环境中都提供了鲁棒且可泛化的空中-地面定位。"
    },
    {
        "title": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings",
        "summary": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.",
        "url": "http://arxiv.org/abs/2512.10293v1",
        "published_date": "2025-12-11T05:20:24+00:00",
        "updated_date": "2025-12-11T05:20:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Karthikeya KV",
            "Narendra Bandaru"
        ],
        "tldr": "The paper introduces Disentangled360, a novel 3D-aware technology that uses disentangled volume rendering to generate 360° views from a single image, outperforming existing methods in image quality and runtime across several datasets.",
        "tldr_zh": "本文介绍了一种名为Disentangled360的新型3D感知技术，该技术利用解耦的体积渲染从单个图像生成360°视图，在图像质量和运行时间方面优于现有方法。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们引入Disentangled360，一项创新的3D感知技术，它整合了方向解耦体渲染和单图像360°独特视角合成的优势，应用于医学成像和自然场景重建。与当前要么过度简化各向异性光行为，要么缺乏跨各种环境的泛化能力的现有技术不同，我们的框架在Gaussian Splatting骨干网络中明确区分了各向同性和各向异性的贡献。我们实现了一个双分支条件框架，一个针对CT强度驱动的体数据散射进行优化，另一个通过归一化相机嵌入用于真实世界的RGB场景。为了解决尺度模糊并保持结构真实感，我们提出了一种混合姿态无关的锚定方法，该方法自适应地采样场景深度和材料过渡，在场景精馏过程中充当稳定的支点。我们的设计将术前放射影像模拟和消费级360°渲染集成到单个推理流程中，从而促进了具有固有方向性的快速、逼真的视角合成。 在Mip-NeRF 360、RealEstate10K和DeepDRR数据集上的评估表明，该方法具有优越的SSIM和LPIPS性能，而运行时间评估证实了其在交互式应用中的可行性。 Disentangled360促进了混合现实医疗监督、机器人感知和沉浸式内容创作，消除了对特定场景微调或昂贵光子模拟的需求。"
    },
    {
        "title": "ShotDirector: Directorially Controllable Multi-Shot Video Generation with Cinematographic Transitions",
        "summary": "Shot transitions play a pivotal role in multi-shot video generation, as they determine the overall narrative expression and the directorial design of visual storytelling. However, recent progress has primarily focused on low-level visual consistency across shots, neglecting how transitions are designed and how cinematographic language contributes to coherent narrative expression. This often leads to mere sequential shot changes without intentional film-editing patterns. To address this limitation, we propose ShotDirector, an efficient framework that integrates parameter-level camera control and hierarchical editing-pattern-aware prompting. Specifically, we adopt a camera control module that incorporates 6-DoF poses and intrinsic settings to enable precise camera information injection. In addition, a shot-aware mask mechanism is employed to introduce hierarchical prompts aware of professional editing patterns, allowing fine-grained control over shot content. Through this design, our framework effectively combines parameter-level conditions with high-level semantic guidance, achieving film-like controllable shot transitions. To facilitate training and evaluation, we construct ShotWeaver40K, a dataset that captures the priors of film-like editing patterns, and develop a set of evaluation metrics for controllable multi-shot video generation. Extensive experiments demonstrate the effectiveness of our framework.",
        "url": "http://arxiv.org/abs/2512.10286v1",
        "published_date": "2025-12-11T05:05:07+00:00",
        "updated_date": "2025-12-11T05:05:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoxue Wu",
            "Xinyuan Chen",
            "Yaohui Wang",
            "Yu Qiao"
        ],
        "tldr": "The paper introduces ShotDirector, a framework for generating multi-shot videos with cinematographic transitions by incorporating camera control and editing-pattern-aware prompting, along with a new dataset and evaluation metrics.",
        "tldr_zh": "本文介绍ShotDirector，一个通过结合相机控制和编辑模式感知提示生成具有电影过渡效果的多镜头视频框架。同时，本文还提出了一个新的数据集和评估指标。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "镜头切换在多镜头视频生成中起着关键作用，因为它决定了整体的叙事表达和视听故事讲述的导演设计。然而，近期的进展主要集中于镜头间的低层视觉一致性，忽略了镜头切换的设计以及电影语言对连贯叙事表达的贡献。这通常导致仅仅是连续的镜头改变，而缺乏有意的电影剪辑模式。为了解决这一局限性，我们提出ShotDirector，一个高效的框架，它集成了参数级的相机控制和层级式的、感知剪辑模式的提示。具体来说，我们采用了一个相机控制模块，该模块包含了6自由度姿态和内在参数设置，以实现精确的相机信息注入。此外，还采用了一种镜头感知的掩码机制，该机制引入感知专业剪辑模式的层级式提示，从而可以对镜头内容进行细粒度控制。通过这种设计，我们的框架有效地结合了参数级别的条件和高层次的语义指导，从而实现电影般的、可控的镜头切换。为了方便训练和评估，我们构建了ShotWeaver40K数据集，该数据集捕捉了电影般剪辑模式的先验知识，并开发了一套用于可控多镜头视频生成的评估指标。大量的实验证明了我们框架的有效性。"
    },
    {
        "title": "VLM-NCD:Novel Class Discovery with Vision-Based Large Language Models",
        "summary": "Novel Class Discovery aims to utilise prior knowledge of known classes to classify and discover unknown classes from unlabelled data. Existing NCD methods for images primarily rely on visual features, which suffer from limitations such as insufficient feature discriminability and the long-tail distribution of data. We propose LLM-NCD, a multimodal framework that breaks this bottleneck by fusing visual-textual semantics and prototype guided clustering. Our key innovation lies in modelling cluster centres and semantic prototypes of known classes by jointly optimising known class image and text features, and a dualphase discovery mechanism that dynamically separates known or novel samples via semantic affinity thresholds and adaptive clustering. Experiments on the CIFAR-100 dataset show that compared to the current methods, this method achieves up to 25.3% improvement in accuracy for unknown classes. Notably, our method shows unique resilience to long tail distributions, a first in NCD literature.",
        "url": "http://arxiv.org/abs/2512.10262v1",
        "published_date": "2025-12-11T03:53:50+00:00",
        "updated_date": "2025-12-11T03:53:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuetong Su",
            "Baoguo Wei",
            "Xinyu Wang",
            "Xu Li",
            "Lixin Li"
        ],
        "tldr": "This paper introduces a Vision-Language Model approach (VLM-NCD) for Novel Class Discovery, utilizing visual-textual semantics and prototype-guided clustering to improve accuracy, especially in handling long-tail distributions, achieving up to 25.3% accuracy improvement on CIFAR-100.",
        "tldr_zh": "该论文提出了一种基于视觉-语言模型（VLM-NCD）的新类别发现方法，利用视觉-文本语义和原型引导的聚类来提高准确性，尤其是在处理长尾分布方面，在CIFAR-100上实现了高达25.3%的准确率提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "新颖类别发现旨在利用已知类别的先验知识来分类和发现来自无标签数据中的未知类别。现有的图像新颖类别发现方法主要依赖于视觉特征，这些特征存在特征区分性不足和数据长尾分布等局限性。我们提出LLM-NCD，一种多模态框架，通过融合视觉-文本语义和原型引导聚类来打破这一瓶颈。我们的主要创新在于通过联合优化已知类别图像和文本特征来建模聚类中心和已知类别的语义原型，以及一种双阶段发现机制，该机制通过语义亲和力阈值和自适应聚类来动态分离已知或新颖样本。在CIFAR-100数据集上的实验表明，与当前方法相比，该方法在未知类别的准确率上提高了高达25.3%。值得注意的是，我们的方法表现出对长尾分布的独特恢复能力，这在新颖类别发现文献中尚属首次。"
    },
    {
        "title": "Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments",
        "summary": "This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.",
        "url": "http://arxiv.org/abs/2512.10835v1",
        "published_date": "2025-12-11T17:26:24+00:00",
        "updated_date": "2025-12-11T17:26:24+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Atahan Cilan",
            "Atay Özgövde"
        ],
        "tldr": "The paper introduces a reinforcement learning framework for generating controllable and diverse player behaviors in multi-agent environments by training agents to match target behavior vectors in a continuous space, achieving greater diversity and control compared to baseline methods.",
        "tldr_zh": "该论文介绍了一种强化学习框架，用于在多智能体环境中生成可控且多样化的玩家行为。通过训练智能体来匹配连续空间中的目标行为向量，与基线方法相比，实现了更大的多样性和控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文介绍了一种强化学习框架，该框架无需依赖人类游戏数据就能实现可控且多样化的玩家行为。 现有方法通常需要大规模玩家轨迹，为不同玩家类型训练独立模型，或者无法在可解释的行为参数和学习到的策略之间提供直接映射，从而限制了其可扩展性和可控性。 我们在N维连续空间中定义玩家行为，并从包含代表真实人类风格的子集区域中均匀采样目标行为向量。 在训练期间，每个智能体都会收到其当前和目标行为向量作为输入，并且奖励基于两者之间距离的归一化减少量。 这使得策略能够学习动作如何影响行为统计，从而实现对攻击性、移动性和合作性等属性的平滑控制。 一个基于PPO的单多智能体策略可以在无需重新训练的情况下复现新的或未见过的游戏风格。 在定制的多人Unity游戏中进行的实验表明，所提出的框架能够产生比仅以胜利为目标的基线方法显著更高的行为多样性，并且能够可靠地匹配不同目标下的指定行为向量。 该方法为自动化游戏测试、游戏平衡、类人行为模拟以及替换在线游戏中掉线玩家提供了一种可扩展的解决方案。"
    },
    {
        "title": "SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation",
        "summary": "In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.",
        "url": "http://arxiv.org/abs/2512.10042v1",
        "published_date": "2025-12-10T19:50:21+00:00",
        "updated_date": "2025-12-10T19:50:21+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Jongmin Lee",
            "Meiqi Sun",
            "Pieter Abbeel"
        ],
        "tldr": "The paper introduces SEMDICE, a novel off-policy reinforcement learning algorithm for unsupervised pre-training. It maximizes state entropy from an arbitrary off-policy dataset, achieving better performance and adaptation efficiency in downstream tasks compared to other SEM-based methods.",
        "tldr_zh": "这篇论文介绍了SEMDICE，一种用于无监督预训练的新型离线强化学习算法。它从任意离线数据集最大化状态熵，与其他的基于状态熵最大化的方法相比，在下游任务中取得了更好的性能和适应效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在强化学习的无监督预训练中，智能体的目标是在不依赖于特定任务奖励函数的情况下，学习用于下游任务的先验策略。我们聚焦于状态熵最大化（SEM），其目标是学习一种策略，该策略最大化状态平稳分布的熵。在本文中，我们介绍SEMDICE，一种有原则的离策略算法，可以从任意离策略数据集中计算SEM策略，从而在平稳分布空间内直接优化策略。 SEMDICE从任意离策略数据集中计算单个、平稳的马尔可夫状态熵最大化策略。实验结果表明，SEMDICE在最大化状态熵方面优于基准算法，并在基于SEM的无监督强化学习预训练方法中，为下游任务实现了最佳的适应效率。"
    },
    {
        "title": "CHyLL: Learning Continuous Neural Representations of Hybrid Systems",
        "summary": "Learning the flows of hybrid systems that have both continuous and discrete time dynamics is challenging. The existing method learns the dynamics in each discrete mode, which suffers from the combination of mode switching and discontinuities in the flows. In this work, we propose CHyLL (Continuous Hybrid System Learning in Latent Space), which learns a continuous neural representation of a hybrid system without trajectory segmentation, event functions, or mode switching. The key insight of CHyLL is that the reset map glues the state space at the guard surface, reformulating the state space as a piecewise smooth quotient manifold where the flow becomes spatially continuous. Building upon these insights and the embedding theorems grounded in differential topology, CHyLL concurrently learns a singularity-free neural embedding in a higher-dimensional space and the continuous flow in it. We showcase that CHyLL can accurately predict the flow of hybrid systems with superior accuracy and identify the topological invariants of the hybrid systems. Finally, we apply CHyLL to the stochastic optimal control problem.",
        "url": "http://arxiv.org/abs/2512.10117v1",
        "published_date": "2025-12-10T22:07:16+00:00",
        "updated_date": "2025-12-10T22:07:16+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO",
            "eess.SP",
            "eess.SY"
        ],
        "authors": [
            "Sangli Teng",
            "Hang Liu",
            "Jingyu Song",
            "Koushil Sreenath"
        ],
        "tldr": "The paper introduces CHyLL, a method for learning continuous neural representations of hybrid systems by learning a singularity-free embedding in a higher-dimensional space, leading to superior accuracy in flow prediction and identification of topological invariants.",
        "tldr_zh": "该论文介绍了CHyLL，一种通过学习高维空间中的无奇异嵌入来学习混合系统连续神经表示的方法，从而在流量预测和拓扑不变量识别方面具有更高的准确性。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "学习具有连续和离散时间混合动力系统的流是具有挑战性的。现有方法学习每个离散模式中的动力学，受到模式切换和流中不连续性的影响。在这项工作中，我们提出了CHyLL（隐空间中连续混合系统学习），它学习混合系统的连续神经表示，无需轨迹分割、事件函数或模式切换。CHyLL的关键洞察在于重置映射在保护面上黏合状态空间，将状态空间重新定义为分段光滑商流形，使得流在空间上是连续的。基于这些洞察和微分拓扑学中的嵌入定理，CHyLL同时在更高维空间中学习无奇点的神经嵌入以及其中的连续流。我们展示了CHyLL能够以卓越的精度准确预测混合系统的流，并识别混合系统的拓扑不变量。最后，我们将CHyLL应用于随机最优控制问题。"
    },
    {
        "title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots",
        "summary": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.",
        "url": "http://arxiv.org/abs/2512.09911v1",
        "published_date": "2025-12-10T18:40:27+00:00",
        "updated_date": "2025-12-10T18:40:27+00:00",
        "categories": [
            "cs.RO",
            "physics.comp-ph"
        ],
        "authors": [
            "Radha Lahoti",
            "Ryan Chaiyakul",
            "M. Khalid Jawed"
        ],
        "tldr": "The paper introduces Py-DiSMech, a Python-based, open-source simulation framework for soft robots using Discrete Differential Geometry, claiming significant performance improvements over existing solutions like Elastica, making it suitable for simulation-driven design and control.",
        "tldr_zh": "该论文介绍了Py-DiSMech，一个基于Python的开源软机器人仿真框架，使用离散微分几何，声称比现有的Elastica等解决方案有显著的性能改进，使其适用于仿真驱动的设计和控制。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "高保真仿真对于软机器人的设计和控制至关重要，在其中大型几何形变和复杂的接触交互对传统建模工具提出了挑战。该领域近期的进展要求仿真框架能够结合物理精度、计算可扩展性和与现代控制和优化流程的无缝集成。在本文中，我们提出了Py-DiSMech，一个基于Python的开源仿真框架，用于建模和控制基于离散微分几何（DDG）原理的软机器人结构。通过直接在网格上离散化曲率和应变等几何量，Py-DiSMech能够以高保真度和降低的计算成本捕捉杆、壳和混合结构的非线性变形。该框架引入了：（i）一个完全向量化的NumPy实现，相比现有基于几何的模拟器实现了数量级的加速；（ii）一个基于罚能量的完全隐式接触模型，支持杆-杆、杆-壳和壳-壳交互；（iii）一个基于自然应变的反馈控制模块，具有比例-积分（PI）控制器，用于形状调节和轨迹跟踪；以及（iv）一个模块化、面向对象的软件设计，允许用户自定义弹性能量、驱动方案，并与机器学习库集成。基准比较表明，Py-DiSMech在保持物理精度的同时，在计算效率方面显著优于最先进的模拟器Elastica。总之，这些特性使Py-DiSMech成为一个可扩展的平台，用于软机器人的仿真驱动设计、控制验证和从仿真到现实的研究。"
    },
    {
        "title": "Visual Heading Prediction for Autonomous Aerial Vehicles",
        "summary": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506° and a root mean squared error of 0.1957°, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration",
        "url": "http://arxiv.org/abs/2512.09898v1",
        "published_date": "2025-12-10T18:27:37+00:00",
        "updated_date": "2025-12-10T18:27:37+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.MA",
            "eess.SY"
        ],
        "authors": [
            "Reza Ahmari",
            "Ahmad Mohammadi",
            "Vahid Hemmati",
            "Mohammed Mynuddin",
            "Parham Kebria",
            "Mahmoud Nabil Mahmoud",
            "Xiaohong Yuan",
            "Abdollah Homaifar"
        ],
        "tldr": "This paper presents a vision-based system using YOLOv5 and a lightweight ANN for real-time UAV-UGV coordination by predicting the UAV's heading angle based on monocular UGV detection, validated in a controlled lab environment.",
        "tldr_zh": "本文提出了一种基于视觉的系统，该系统使用 YOLOv5 和轻量级 ANN 进行实时无人机-地面车辆（UAV-UGV）的协同。它通过单目视觉的 UGV 检测来预测无人机的航向角，并在受控实验环境中进行了验证。",
        "relevance_score": 6,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "无人机（UAV）和无人地面车辆（UGV）的集成在智能自主系统的开发中日益重要，应用于搜索和救援、环境监测和物流等领域。然而，在实时场景下这些平台之间的精确协调，尤其是在GPS或GNSS等外部定位基础设施不可用或性能下降时[1]，面临着重大挑战。本文提出了一种基于视觉的、数据驱动的实时无人机-无人车集成框架，重点在于稳健的无人车检测和航向角预测，以用于导航和协调。该系统采用经过微调的YOLOv5模型来检测无人车并提取边界框特征，然后由一个轻量级人工神经网络（ANN）用于估计无人机所需的航向角。在训练期间，使用VICON运动捕捉系统生成地面真值数据，从而在受控实验室环境中收集了超过13,000张带注释的图像数据集。经过训练的ANN实现了0.1506°的平均绝对误差和0.1957°的均方根误差，仅使用单目摄像头输入即可提供准确的航向角预测。 实验评估表明无人车检测准确率达到95%。这项工作贡献了一种基于视觉的、不依赖基础设施的解决方案，该方案展现出在GPS/GNSS拒止环境中部署的强大潜力，支持在现实动态条件下可靠的多智能体协调。演示视频展示了该系统的实时性能，包括无人车检测、航向角预测以及动态条件下无人机对齐，可在以下网址观看：https://github.com/Kooroshraf/UAV-UGV-Integration"
    },
    {
        "title": "Neuronal Attention Circuit (NAC) for Representation Learning",
        "summary": "Attention improves representation learning over RNNs, but its discrete nature limits continuous-time (CT) modeling. We introduce Neuronal Attention Circuit (NAC), a novel, biologically plausible CT-Attention mechanism that reformulates attention logits computation as the solution to a linear first-order ODE with nonlinear interlinked gates derived from repurposing \\textit{C. elegans} Neuronal Circuit Policies (NCPs) wiring mechanism. NAC replaces dense projections with sparse sensory gates for key-query projections and a sparse backbone network with two heads for computing \\textit{content-target} and \\textit{learnable time-constant} gates, enabling efficient adaptive dynamics. NAC supports three attention logit computation modes: (i) explicit Euler integration, (ii) exact closed-form solution, and (iii) steady-state approximation. To improve memory intensity, we implemented a sparse Top-\\emph{K} pairwise concatenation scheme that selectively curates key-query interactions. We provide rigorous theoretical guarantees, including state stability, bounded approximation errors, and universal approximation. Empirically, we implemented NAC in diverse domains, including irregular time-series classification, lane-keeping for autonomous vehicles, and industrial prognostics. We observed that NAC matches or outperforms competing baselines in accuracy and occupies an intermediate position in runtime and memory efficiency compared with several CT baselines.",
        "url": "http://arxiv.org/abs/2512.10282v1",
        "published_date": "2025-12-11T04:49:44+00:00",
        "updated_date": "2025-12-11T04:49:44+00:00",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Waleed Razzaq",
            "Izis Kankaraway",
            "Yun-Bo Zhao"
        ],
        "tldr": "This paper introduces Neuronal Attention Circuit (NAC), a biologically plausible continuous-time attention mechanism inspired by C. elegans neuronal circuits, offering performance comparable to existing methods with intermediate runtime and memory efficiency across various time-series tasks.",
        "tldr_zh": "本文介绍了一种神经元注意力电路（NAC），这是一种受秀丽隐杆线虫神经元电路启发的、具有生物合理性的连续时间注意力机制，在各种时间序列任务中，其性能与现有方法相当，且在运行时和内存效率方面处于中间水平。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "注意力机制可以改进循环神经网络的表征学习，但其离散特性限制了连续时间（CT）建模。我们引入了神经元注意力电路（NAC），一种新颖的、生物学上合理的CT注意力机制，它将注意力logits的计算重构为线性一阶常微分方程的解，方程具有从重新利用的秀丽隐杆线虫神经回路策略（NCPs）连线机制导出的非线性互连门。NAC用稀疏的感知门替换了密集的投影，用于键-查询投影，并用具有两个头的稀疏主干网络来计算内容-目标门和可学习时间常数门，从而实现高效的自适应动态。NAC支持三种注意力logits计算模式：（i）显式欧拉积分，（ii）精确闭合形式解，以及（iii）稳态近似。为了提高内存强度，我们实施了一种稀疏的Top-\\emph{K}成对连接方案，该方案有选择地管理键-查询交互。我们提供了严格的理论保证，包括状态稳定性、有界近似误差和通用逼近。在实证方面，我们在各种领域中实施了NAC，包括不规则时间序列分类、自动驾驶车辆的车道保持和工业预测。我们观察到，NAC在准确性方面与竞争基线相匹配或优于竞争基线，并且与几种CT基线相比，在运行时和内存效率方面处于中间位置。"
    },
    {
        "title": "An exploration for higher efficiency in multi objective optimisation with reinforcement learning",
        "summary": "Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.",
        "url": "http://arxiv.org/abs/2512.10208v1",
        "published_date": "2025-12-11T01:58:04+00:00",
        "updated_date": "2025-12-11T01:58:04+00:00",
        "categories": [
            "cs.AI",
            "cs.NE"
        ],
        "authors": [
            "Mehmet Emin Aydin"
        ],
        "tldr": "This paper proposes a generalized multi-objective reinforcement learning approach to improve the efficiency of multi-objective optimization algorithms by learning optimal sequences of operators, addressing a gap in the current research landscape.",
        "tldr_zh": "本文提出了一种广义的多目标强化学习方法，旨在通过学习最优的算子序列来提高多目标优化算法的效率，解决当前研究领域的一个空白。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "优化和搜索过程的效率仍然是挑战之一，它影响着优化算法的性能和应用。利用算子池而非单个算子来处理邻域内的移动操作仍然具有潜力，但算子的最优或接近最优序列需要进一步研究。一个有希望的想法是归纳经验并探索如何利用它。尽管围绕这个问题已经进行了大量的单目标优化工作，但多目标情况在这方面尚未得到足够的关注。基于多目标强化学习方法的通用化方法似乎可以弥补这个问题并提供良好的解决方案。本文概述了一种提出的通用化方法，该方法已完成某些阶段，并且还有一些阶段尚未完成，旨在帮助展示使用多目标强化学习的效率。"
    },
    {
        "title": "Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing",
        "summary": "We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.",
        "url": "http://arxiv.org/abs/2512.09882v1",
        "published_date": "2025-12-10T18:12:29+00:00",
        "updated_date": "2025-12-10T18:12:29+00:00",
        "categories": [
            "cs.AI",
            "cs.CR",
            "cs.CY"
        ],
        "authors": [
            "Justin W. Lin",
            "Eliot Krzysztof Jones",
            "Donovan Julian Jasper",
            "Ethan Jun-shen Ho",
            "Anna Wu",
            "Arnold Tianyi Yang",
            "Neil Perry",
            "Andy Zou",
            "Matt Fredrikson",
            "J. Zico Kolter",
            "Percy Liang",
            "Dan Boneh",
            "Daniel E. Ho"
        ],
        "tldr": "This paper introduces ARTEMIS, a novel AI agent scaffold for penetration testing, which outperformed most human cybersecurity professionals in a real-world enterprise environment. It highlights the potential of AI agents in cybersecurity and identifies remaining challenges.",
        "tldr_zh": "本文介绍了一种名为ARTEMIS的新型AI渗透测试框架，该框架在真实的企业环境中优于大多数人类网络安全专业人员。它突出了AI agent在网络安全方面的潜力，并指出了剩余的挑战。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "我们首次对AI智能体在真实企业环境中对抗人类网络安全专家的表现进行了全面评估。我们评估了十名网络安全专家，同时评估了六个现有AI智能体和我们的新型智能体框架ARTEMIS，这些对象在一个由约8,000台主机组成、跨越12个子网的大型大学网络上进行了测试。ARTEMIS是一个多智能体框架，其特点是动态提示生成、任意子智能体以及自动漏洞分诊。在我们的对比研究中，ARTEMIS总体排名第二，发现了9个有效漏洞，有效提交率达到82%，并且优于10名人类参与者中的9名。尽管诸如Codex和CyAgent等现有框架的表现不如大多数人类参与者，但ARTEMIS展示了与最强的参与者相当的技术复杂性和提交质量。我们观察到，AI智能体在系统化枚举、并行利用和成本方面具有优势——某些ARTEMIS变体的成本为18美元/小时，而专业渗透测试人员的成本为60美元/小时。我们还识别出关键的能力差距：AI智能体表现出更高的误报率，并且在基于GUI的任务中表现不佳。"
    },
    {
        "title": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation",
        "summary": "Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/",
        "url": "http://arxiv.org/abs/2512.10860v1",
        "published_date": "2025-12-11T17:54:31+00:00",
        "updated_date": "2025-12-11T17:54:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kehong Gong",
            "Zhengyu Wen",
            "Mingxi Xu",
            "Weixia He",
            "Qi Wang",
            "Ning Zhang",
            "Zhengyu Li",
            "Chenbin Li",
            "Dongze Lian",
            "Wei Zhao",
            "Xiaoyu He",
            "Mingyuan Zhang"
        ],
        "tldr": "The paper introduces SWiT-4D, a sliding-window transformer that leverages image-to-3D diffusion models for parameter-free temporal 4D mesh generation from monocular videos, achieving high fidelity and temporal consistency with minimal 4D supervision.",
        "tldr_zh": "该论文介绍了SWiT-4D，一种滑动窗口Transformer，利用图像到3D扩散模型从单目视频生成无参数的时序4D网格，实现了高保真度和时间一致性，并且只需要极少的4D监督。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "尽管在4D内容生成方面取得了显著进展，但将单目视频转换为具有显式4D网格的高质量动画3D资产仍然极具挑战性。大规模、自然捕获的4D网格数据集的稀缺性进一步限制了以纯数据驱动的方式从零训练泛化性视频到4D模型的能力。与此同时，在大量数据集支持下的图像到3D生成方面的进展，提供了可以利用的强大先验模型。为了更好地利用这些先验，同时最大限度地减少对4D监督的依赖，我们引入了SWiT-4D，一种用于无损、无参数的时间4D网格生成的滑窗Transformer。SWiT-4D与任何基于扩散Transformer (DiT) 的图像到3D生成器无缝集成，在视频帧之间添加了时空建模，同时保留了原始的单图像正向过程，从而能够从任意长度的视频中重建4D网格。为了恢复全局平移，我们进一步引入了一个针对静态相机单目视频定制的基于优化的轨迹模块。SWiT-4D展示了强大的数据效率：只需对单个短视频（<10秒）进行微调，即可实现高保真几何和稳定的时间一致性，表明在极有限的4D监督下具有实际可部署性。对在域动物园测试集和具有挑战性的域外基准（C4D、Objaverse和真实场景视频）的综合实验表明，SWiT-4D在时间平滑度方面始终优于现有基线。项目主页：https://animotionlab.github.io/SWIT4D/"
    },
    {
        "title": "Video Depth Propagation",
        "summary": "Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth",
        "url": "http://arxiv.org/abs/2512.10725v1",
        "published_date": "2025-12-11T15:08:37+00:00",
        "updated_date": "2025-12-11T15:08:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Luigi Piccinelli",
            "Thiemo Wandel",
            "Christos Sakaridis",
            "Wim Abbeloos",
            "Luc Van Gool"
        ],
        "tldr": "The paper introduces VeloDepth, an efficient and temporally consistent online video depth estimation method using flow-based warping and learned residual corrections, achieving state-of-the-art consistency and competitive accuracy with faster inference.",
        "tldr_zh": "该论文介绍了一种名为VeloDepth的高效且时间一致的在线视频深度估计方法，该方法使用基于光流扭曲和学习的残差校正，在实现最先进的时间一致性的同时，具有竞争力的精度和更快的推理速度。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "视频深度估计对于现实应用中的视觉感知至关重要。然而，现有方法要么依赖于简单的逐帧单目模型，导致时间上的不一致性和不准确性，要么使用计算量大的时序建模，不适用于实时应用。这些限制极大地限制了其在实际环境中的通用性和性能。为了解决这个问题，我们提出了 VeloDepth，一种高效且鲁棒的在线视频深度估计流程，它能有效地利用来自先前深度预测的时空先验，并执行深度特征传播。我们的方法引入了一种新颖的传播模块，该模块使用基于光流的扭曲以及学习到的残差校正来细化和传播深度特征和预测结果。此外，我们的设计在结构上强制执行时间一致性，从而在连续帧之间产生稳定的深度预测，并提高了效率。在多个基准上的全面零样本评估表明，VeloDepth具有最先进的时间一致性和具有竞争力的准确性，同时与现有的基于视频的深度估计器相比，其推理速度明显更快。因此，VeloDepth 为实时深度估计提供了一种实用、高效且准确的解决方案，适用于各种感知任务。代码和模型可在https://github.com/lpiccinelli-eth/velodepth获得。"
    },
    {
        "title": "RaLiFlow: Scene Flow Estimation with 4D Radar and LiDAR Point Clouds",
        "summary": "Recent multimodal fusion methods, integrating images with LiDAR point clouds, have shown promise in scene flow estimation. However, the fusion of 4D millimeter wave radar and LiDAR remains unexplored. Unlike LiDAR, radar is cheaper, more robust in various weather conditions and can detect point-wise velocity, making it a valuable complement to LiDAR. However, radar inputs pose challenges due to noise, low resolution, and sparsity. Moreover, there is currently no dataset that combines LiDAR and radar data specifically for scene flow estimation. To address this gap, we construct a Radar-LiDAR scene flow dataset based on a public real-world automotive dataset. We propose an effective preprocessing strategy for radar denoising and scene flow label generation, deriving more reliable flow ground truth for radar points out of the object boundaries. Additionally, we introduce RaLiFlow, the first joint scene flow learning framework for 4D radar and LiDAR, which achieves effective radar-LiDAR fusion through a novel Dynamic-aware Bidirectional Cross-modal Fusion (DBCF) module and a carefully designed set of loss functions. The DBCF module integrates dynamic cues from radar into the local cross-attention mechanism, enabling the propagation of contextual information across modalities. Meanwhile, the proposed loss functions mitigate the adverse effects of unreliable radar data during training and enhance the instance-level consistency in scene flow predictions from both modalities, particularly for dynamic foreground areas. Extensive experiments on the repurposed scene flow dataset demonstrate that our method outperforms existing LiDAR-based and radar-based single-modal methods by a significant margin.",
        "url": "http://arxiv.org/abs/2512.10376v1",
        "published_date": "2025-12-11T07:41:33+00:00",
        "updated_date": "2025-12-11T07:41:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyun Fu",
            "Zhiyu Xiang",
            "Na Zhao"
        ],
        "tldr": "The paper introduces RaLiFlow, a novel scene flow estimation framework that fuses 4D radar and LiDAR data, along with a new Radar-LiDAR scene flow dataset.",
        "tldr_zh": "这篇论文介绍了RaLiFlow，一个融合4D雷达和激光雷达数据的新型场景流估计框架，以及一个新的雷达-激光雷达场景流数据集。",
        "relevance_score": 4,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "最近的多模态融合方法，通过整合图像和激光雷达点云，在场景流估计中展现出潜力。然而，4D毫米波雷达和激光雷达的融合仍未被探索。与激光雷达不同，雷达更廉价，在各种天气条件下更具鲁棒性，并且可以检测点级速度，使其成为激光雷达的宝贵补充。然而，雷达输入由于噪声、低分辨率和稀疏性带来了挑战。此外，目前还没有专门用于场景流估计的、结合激光雷达和雷达数据的数据集。为了解决这个空白，我们基于一个公共的真实世界汽车数据集构建了一个雷达-激光雷达场景流数据集。我们提出了一种有效的雷达去噪和场景流标签生成预处理策略，从物体边界之外的雷达点推导出更可靠的流真值。此外，我们推出了 RaLiFlow，这是第一个用于 4D 雷达和激光雷达的联合场景流学习框架，它通过一种新颖的动态感知双向跨模态融合 (DBCF) 模块和一组精心设计的损失函数，实现了有效的雷达-激光雷达融合。DBCF 模块将来自雷达的动态线索集成到局部跨注意力机制中，从而实现跨模态的上下文信息传播。同时，所提出的损失函数减轻了训练期间不可靠雷达数据的不利影响，并增强了来自两种模态的场景流预测中的实例级一致性，特别是对于动态前景区域。在重新设计的场景流数据集上进行的大量实验表明，我们的方法明显优于现有的基于激光雷达和基于雷达的单模态方法。"
    },
    {
        "title": "THE-Pose: Topological Prior with Hybrid Graph Fusion for Estimating Category-Level 6D Object Pose",
        "summary": "Category-level object pose estimation requires both global context and local structure to ensure robustness against intra-class variations. However, 3D graph convolution (3D-GC) methods only focus on local geometry and depth information, making them vulnerable to complex objects and visual ambiguities. To address this, we present THE-Pose, a novel category-level 6D pose estimation framework that leverages a topological prior via surface embedding and hybrid graph fusion. Specifically, we extract consistent and invariant topological features from the image domain, effectively overcoming the limitations inherent in existing 3D-GC based methods. Our Hybrid Graph Fusion (HGF) module adaptively integrates the topological features with point-cloud features, seamlessly bridging 2D image context and 3D geometric structure. These fused features ensure stability for unseen or complicated objects, even under significant occlusions. Extensive experiments on the REAL275 dataset show that THE-Pose achieves a 35.8% improvement over the 3D-GC baseline (HS-Pose) and surpasses the previous state-of-the-art by 7.2% across all key metrics. The code is avaialbe on https://github.com/EHxxx/THE-Pose",
        "url": "http://arxiv.org/abs/2512.10251v1",
        "published_date": "2025-12-11T03:19:10+00:00",
        "updated_date": "2025-12-11T03:19:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Eunho Lee",
            "Chaehyeon Song",
            "Seunghoon Jeong",
            "Ayoung Kim"
        ],
        "tldr": "THE-Pose introduces a new framework for category-level 6D object pose estimation by fusing topological priors from 2D images with 3D point cloud features using a Hybrid Graph Fusion module, achieving state-of-the-art results on the REAL275 dataset.",
        "tldr_zh": "THE-Pose 提出了一种新的类别级 6D 物体姿态估计框架，通过混合图融合模块将来自 2D 图像的拓扑先验与 3D 点云特征融合，在 REAL275 数据集上实现了最先进的结果。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "类别级物体姿态估计需要全局上下文和局部结构，以确保对类内变化的鲁棒性。然而，3D图卷积（3D-GC）方法仅关注局部几何和深度信息，使其在面对复杂物体和视觉歧义时显得脆弱。为了解决这个问题，我们提出THE-Pose，一种新颖的类别级6D姿态估计框架，它通过表面嵌入和混合图融合来利用拓扑先验。具体来说，我们从图像域中提取一致且不变的拓扑特征，有效地克服了现有基于3D-GC方法的局限性。我们的混合图融合（HGF）模块自适应地将拓扑特征与点云特征集成，无缝地桥接了2D图像上下文和3D几何结构。这些融合后的特征确保了对未见或复杂物体的稳定性，即使在严重遮挡下也是如此。在REAL275数据集上的大量实验表明，THE-Pose比3D-GC基线（HS-Pose）提高了35.8%，并且在所有关键指标上超越了之前的最先进水平7.2%。代码可在https://github.com/EHxxx/THE-Pose上获取."
    },
    {
        "title": "Mr. Virgil: Learning Multi-robot Visual-range Relative Localization",
        "summary": "Ultra-wideband (UWB)-vision fusion localization has achieved extensive applications in the domain of multi-agent relative localization. The challenging matching problem between robots and visual detection renders existing methods highly dependent on identity-encoded hardware or delicate tuning algorithms. Overconfident yet erroneous matches may bring about irreversible damage to the localization system. To address this issue, we introduce Mr. Virgil, an end-to-end learning multi-robot visual-range relative localization framework, consisting of a graph neural network for data association between UWB rangings and visual detections, and a differentiable pose graph optimization (PGO) back-end. The graph-based front-end supplies robust matching results, accurate initial position predictions, and credible uncertainty estimates, which are subsequently integrated into the PGO back-end to elevate the accuracy of the final pose estimation. Additionally, a decentralized system is implemented for real-world applications. Experiments spanning varying robot numbers, simulation and real-world, occlusion and non-occlusion conditions showcase the stability and exactitude under various scenes compared to conventional methods. Our code is available at: https://github.com/HiOnes/Mr-Virgil.",
        "url": "http://arxiv.org/abs/2512.10540v1",
        "published_date": "2025-12-11T11:16:53+00:00",
        "updated_date": "2025-12-11T11:16:53+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Si Wang",
            "Zhehan Li",
            "Jiadong Lu",
            "Rong Xiong",
            "Yanjun Cao",
            "Yue Wang"
        ],
        "tldr": "Mr. Virgil is an end-to-end learning framework for multi-robot relative localization using UWB and visual data, addressing the matching problem with a graph neural network and pose graph optimization.",
        "tldr_zh": "Mr. Virgil 是一个端到端的学习框架，用于多机器人相对定位，使用超宽带和视觉数据，通过图神经网络和姿态图优化解决匹配问题。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "超宽带(UWB)-视觉融合定位已在多智能体相对定位领域得到广泛应用。机器人与视觉检测之间具有挑战性的匹配问题使得现有方法高度依赖于身份编码硬件或精细的调整算法。过度自信但错误的匹配可能对定位系统造成不可逆的损害。为了解决这个问题，我们引入了Mr. Virgil，一个端到端学习的多机器人视觉范围相对定位框架，它由一个用于UWB测距和视觉检测之间数据关联的图神经网络，以及一个可微分的位姿图优化（PGO）后端组成。基于图的前端提供鲁棒的匹配结果、精确的初始位置预测和可信的不确定性估计，这些结果随后被集成到PGO后端，以提高最终位姿估计的精度。此外，还实现了一个去中心化系统用于实际应用。涵盖不同机器人数量、模拟和真实世界、遮挡和非遮挡条件的实验表明，与传统方法相比，该方法在各种场景下都具有稳定性和准确性。我们的代码可在以下网址获取：https://github.com/HiOnes/Mr-Virgil。"
    },
    {
        "title": "Neural Ranging Inertial Odometry",
        "summary": "Ultra-wideband (UWB) has shown promising potential in GPS-denied localization thanks to its lightweight and drift-free characteristics, while the accuracy is limited in real scenarios due to its sensitivity to sensor arrangement and non-Gaussian pattern induced by multi-path or multi-signal interference, which commonly occurs in many typical applications like long tunnels. We introduce a novel neural fusion framework for ranging inertial odometry which involves a graph attention UWB network and a recurrent neural inertial network. Our graph net learns scene-relevant ranging patterns and adapts to any number of anchors or tags, realizing accurate positioning without calibration. Additionally, the integration of least squares and the incorporation of nominal frame enhance overall performance and scalability. The effectiveness and robustness of our methods are validated through extensive experiments on both public and self-collected datasets, spanning indoor, outdoor, and tunnel environments. The results demonstrate the superiority of our proposed IR-ULSG in handling challenging conditions, including scenarios outside the convex envelope and cases where only a single anchor is available.",
        "url": "http://arxiv.org/abs/2512.10531v1",
        "published_date": "2025-12-11T11:03:26+00:00",
        "updated_date": "2025-12-11T11:03:26+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Si Wang",
            "Bingqi Shen",
            "Fei Wang",
            "Yanjun Cao",
            "Rong Xiong",
            "Yue Wang"
        ],
        "tldr": "This paper introduces a novel neural fusion framework combining graph attention networks for UWB ranging and recurrent neural networks for inertial odometry, achieving robust and accurate localization in challenging GPS-denied environments.",
        "tldr_zh": "该论文介绍了一种新颖的神经融合框架，结合了用于超宽带测距的图注意力网络和用于惯性里程计的循环神经网络，从而在具有挑战性的无 GPS 环境中实现了稳健而准确的定位。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "超宽带(UWB)因其轻量级和无漂移特性，在GPS拒止定位中展现出巨大的潜力，但由于其对传感器布置的敏感性以及由多径或多信号干扰引起的非高斯模式（这在诸如长隧道等典型应用中很常见），其精度在实际场景中受到限制。我们提出了一种新颖的神经融合框架，用于测距惯性里程计，该框架包含一个图注意力UWB网络和一个循环神经惯性网络。我们的图网络学习与场景相关的测距模式，并适应任意数量的锚点或标签，从而实现无需校准的精确定位。此外，最小二乘法的集成以及标称坐标系的加入，提高了整体性能和可扩展性。通过在公共和自采集数据集上的大量实验，验证了我们方法的有效性和鲁棒性，这些数据集涵盖室内、室外和隧道环境。结果表明，我们提出的IR-ULSG在处理严峻条件（包括凸包外部的情形和仅单个锚点可用的情形）方面具有优越性。"
    },
    {
        "title": "Design and Implementation of a High-Precision Wind-Estimation UAV with Onboard Sensors",
        "summary": "Accurate real-time wind vector estimation is essential for enhancing the safety, navigation accuracy, and energy efficiency of unmanned aerial vehicles (UAVs). Traditional approaches rely on external sensors or simplify vehicle dynamics, which limits their applicability during agile flight or in resource-constrained platforms. This paper proposes a real-time wind estimation method based solely on onboard sensors. The approach first estimates external aerodynamic forces using a disturbance observer (DOB), and then maps these forces to wind vectors using a thin-plate spline (TPS) model. A custom-designed wind barrel mounted on the UAV enhances aerodynamic sensitivity, further improving estimation accuracy. The system is validated through comprehensive experiments in wind tunnels, indoor and outdoor flights. Experimental results demonstrate that the proposed method achieves consistently high-accuracy wind estimation across controlled and real-world conditions, with speed RMSEs as low as \\SI{0.06}{m/s} in wind tunnel tests, \\SI{0.22}{m/s} during outdoor hover, and below \\SI{0.38}{m/s} in indoor and outdoor dynamic flights, and direction RMSEs under \\ang{7.3} across all scenarios, outperforming existing baselines. Moreover, the method provides vertical wind estimates -- unavailable in baselines -- with RMSEs below \\SI{0.17}{m/s} even during fast indoor translations.",
        "url": "http://arxiv.org/abs/2512.10428v1",
        "published_date": "2025-12-11T08:39:55+00:00",
        "updated_date": "2025-12-11T08:39:55+00:00",
        "categories": [
            "cs.ET",
            "cs.RO"
        ],
        "authors": [
            "Haowen Yu",
            "Na Fan",
            "Xing Liu",
            "Ximin Lyu"
        ],
        "tldr": "This paper presents a high-precision, onboard-sensor-based wind estimation method for UAVs, demonstrating improved accuracy over existing methods in various flight conditions and providing vertical wind estimation.",
        "tldr_zh": "本文提出了一种基于无人机机载传感器的高精度风力估算方法，在各种飞行条件下都表现出比现有方法更高的精度，并提供垂直风力估算。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "精确的实时风矢量估计对于提高无人机（UAV）的安全性、导航精度和能源效率至关重要。传统方法依赖于外部传感器或简化飞行器动力学，这限制了其在敏捷飞行或资源受限平台上的适用性。本文提出了一种仅基于机载传感器的实时风估计方法。该方法首先使用扰动观测器（DOB）估计外部气动力，然后使用薄板样条（TPS）模型将这些力映射到风矢量。安装在无人机上的定制风筒增强了气动敏感性，进一步提高了估计精度。该系统通过风洞、室内和室外飞行中的综合实验进行了验证。实验结果表明，所提出的方法在受控和真实世界条件下均实现了始终如一的高精度风估计，在风洞测试中速度均方根误差（RMSE）低至\\SI{0.06}{m/s}，室外悬停期间为\\SI{0.22}{m/s}，室内和室外动态飞行中低于\\SI{0.38}{m/s}，在所有场景中方向RMSE均低于\\ang{7.3}，优于现有基线。此外，该方法还提供了基线方法无法提供的垂直风估计，即使在快速室内平移期间，RMSE也低于\\SI{0.17}{m/s}。"
    },
    {
        "title": "Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing",
        "summary": "Tendon-driven under-actuated robotic fingers provide advantages for dexterous manipulation through reduced actuator requirements and simplified mechanical design. However, achieving both high load capacity and adaptive compliance in a compact form remains challenging. This paper presents an under-actuated tendon-driven robotic finger (UTRF) featuring a synchronous tendon routing that mechanically couples all joints with fixed angular velocity ratios, enabling the entire finger to be actuated by a single actuator. This approach significantly reduces the number of actuators required in multi-finger hands, resulting in a lighter and more compact structure without sacrificing stiffness or compliance. The kinematic and static models of the finger are derived, incorporating tendon elasticity to predict structural stiffness. A single-finger prototype was fabricated and tested under static loading, showing an average deflection prediction error of 1.0 mm (0.322% of total finger length) and a measured stiffness of 1.2x10^3 N/m under a 3 kg tip load. Integration into a five-finger robotic hand (UTRF-RoboHand) demonstrates effective object manipulation across diverse scenarios, confirming that the proposed routing achieves predictable stiffness and reliable grasping performance with a minimal actuator count.",
        "url": "http://arxiv.org/abs/2512.10349v1",
        "published_date": "2025-12-11T07:05:43+00:00",
        "updated_date": "2025-12-11T07:05:43+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Quan Yuan",
            "Zhenting Du",
            "Daqian Cao",
            "Weibang Bai"
        ],
        "tldr": "This paper presents a novel under-actuated robotic finger design with synchronous tendon routing, enabling single-actuator control of a multi-joint finger with predictable stiffness and demonstrated object manipulation capabilities.",
        "tldr_zh": "本文提出了一种新型的欠驱动机器人手指设计，采用同步肌腱布线，实现了用单个执行器控制多关节手指，并具有可预测的刚度和展示了物体操作能力。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "基于腱驱动的欠驱动机器人手指通过减少驱动器需求和简化机械设计，为灵巧操作提供了优势。然而，在紧凑的结构中同时实现高负载能力和自适应顺应性仍然具有挑战性。本文提出了一种基于腱驱动的欠驱动机器人手指（UTRF），其特点是采用同步腱绳布线方式，该方式以固定的角速度比率将所有关节进行机械耦合，从而使整个手指能够由单个驱动器驱动。这种方法显著减少了多指手所需的驱动器数量，在不牺牲刚度或顺应性的前提下，实现了更轻、更紧凑的结构。推导了手指的运动学和静力学模型，并考虑了腱绳弹性以预测结构刚度。制造了一个单指原型并在静态载荷下进行了测试，结果表明，在3公斤的尖端载荷下，平均挠度预测误差为1.0毫米（占手指总长度的0.322%），实测刚度为1.2x10^3 牛/米。集成到五指机器人手（UTRF-RoboHand）的实验表明，该设计能够在各种场景中实现有效的物体操作，证实了所提出的布线方式能够以最少的驱动器数量实现可预测的刚度和可靠的抓取性能。"
    },
    {
        "title": "Active Optics for Hyperspectral Imaging of Reflective Agricultural Leaf Sensors",
        "summary": "Monitoring plant health increasingly relies on leaf-mounted sensors that provide real-time physiological data, yet efficiently locating and sampling these sensors in complex agricultural environments remains a major challenge. We present an integrated, adaptive, and scalable system that autonomously detects and interrogates plant sensors using a coordinated suite of low-cost optical components including a LiDAR, liquid lens, monochrome camera, filter wheel, and Fast Steering Mirror (FSM). The system first uses LiDAR to identify the distinct reflective signatures of sensors within the field, then dynamically redirects the camera s field of view via the FSM to target each sensor for hyperspectral imaging. The liquid lens continuously adjusts focus to maintain image sharpness across varying depths, enabling precise spectral measurements. We validated the system in controlled indoor experiments, demonstrating accurate detection and tracking of reflective plant sensors and successful acquisition of their spectral data. To our knowledge, no other system currently integrates these sensing and optical modalities for agricultural monitoring. This work establishes a foundation for adaptive, low-cost, and automated plant sensor interrogation, representing a significant step toward scalable, real-time plant health monitoring in precision agriculture.",
        "url": "http://arxiv.org/abs/2512.10213v1",
        "published_date": "2025-12-11T02:03:07+00:00",
        "updated_date": "2025-12-11T02:03:07+00:00",
        "categories": [
            "eess.IV",
            "cs.RO"
        ],
        "authors": [
            "Dexter Burns",
            "Sanjeev Koppal"
        ],
        "tldr": "This paper presents an automated system for detecting and acquiring hyperspectral data from reflective plant sensors using LiDAR, a Fast Steering Mirror (FSM), and a liquid lens, enabling real-time plant health monitoring.",
        "tldr_zh": "本文介绍了一种自动系统，利用激光雷达、快速反射镜（FSM）和液体透镜来检测和获取反射植物传感器的高光谱数据，从而实现实时的植物健康监测。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "植物健康监测越来越依赖于安装在叶片上的传感器来提供实时生理数据，然而，在复杂的农业环境中高效地定位和采样这些传感器仍然是一个主要的挑战。我们提出了一种集成的、自适应的和可扩展的系统，该系统利用一套协同工作的低成本光学组件，包括激光雷达、液体透镜、单色相机、滤光轮和快速扫描反射镜（FSM），来自主检测和询问植物传感器。该系统首先使用激光雷达识别田间传感器独特的反射特征，然后通过快速扫描反射镜动态地重定向相机的视野，以针对每个传感器进行高光谱成像。液体透镜持续调整焦点，以在不同的深度保持图像清晰度，从而实现精确的光谱测量。我们在受控的室内实验中验证了该系统，证明了对反射式植物传感器的准确定位和跟踪，以及成功获取其光谱数据。据我们所知，目前没有其他系统集成了这些传感和光学模式用于农业监测。这项工作为自适应、低成本和自动化的植物传感器询问奠定了基础，代表着迈向精准农业中可扩展的、实时的植物健康监测的重要一步。"
    },
    {
        "title": "Fast Functionally Redundant Inverse Kinematics for Robotic Toolpath Optimisation in Manufacturing Tasks",
        "summary": "Industrial automation with six-axis robotic arms is critical for many manufacturing tasks, including welding and additive manufacturing applications; however, many of these operations are functionally redundant due to the symmetrical tool axis, which effectively makes the operation a five-axis task. Exploiting this redundancy is crucial for achieving the desired workspace and dexterity required for the feasibility and optimisation of toolpath planning. Inverse kinematics algorithms can solve this in a fast, reactive framework, but these techniques are underutilised over the more computationally expensive offline planning methods. We propose a novel algorithm to solve functionally redundant inverse kinematics for robotic manipulation utilising a task space decomposition approach, the damped least-squares method and Halley's method to achieve fast and robust solutions with reduced joint motion. We evaluate our methodology in the case of toolpath optimisation in a cold spray coating application on a non-planar surface. The functionally redundant inverse kinematics algorithm can quickly solve motion plans that minimise joint motion, expanding the feasible operating space of the complex toolpath. We validate our approach on an industrial ABB manipulator and cold-spray gun executing the computed toolpath.",
        "url": "http://arxiv.org/abs/2512.10116v1",
        "published_date": "2025-12-10T22:07:07+00:00",
        "updated_date": "2025-12-10T22:07:07+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Andrew Razjigaev",
            "Hans Lohr",
            "Alejandro Vargas-Uscategui",
            "Peter King",
            "Tirthankar Bandyopadhyay"
        ],
        "tldr": "This paper presents a fast and robust inverse kinematics algorithm for functionally redundant robotic manipulators, validated in a cold spray coating application for toolpath optimization.",
        "tldr_zh": "本文提出了一种快速且鲁棒的逆运动学算法，用于功能冗余的机器人操作臂，并在冷喷涂应用中进行了验证，用于工具路径优化。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "基于六轴机械臂的工业自动化对许多制造任务至关重要，包括焊接和增材制造应用；然而，由于对称的工具轴，其中许多操作在功能上是冗余的，这实际上使操作成为一个五轴任务。利用这种冗余对于实现工具路径规划的可行性和优化所需的期望工作空间和灵巧性至关重要。逆运动学算法可以在快速、反应式的框架中解决这个问题，但与计算量更大的离线规划方法相比，这些技术尚未得到充分利用。我们提出了一种新的算法来解决机器人操作的功能冗余逆运动学问题，该算法利用任务空间分解方法、阻尼最小二乘法和Halley法，以实现快速、稳健的解决方案并减少关节运动。我们在非平面表面上的冷喷涂应用中评估了我们的方法。该功能冗余逆运动学算法可以快速解决能够最小化关节运动的运动规划，从而扩展复杂工具路径的可行操作空间。我们在一台工业ABB机械手和冷喷枪上验证了我们的方法，执行了计算出的工具路径。"
    },
    {
        "title": "T-SKM-Net: Trainable Neural Network Framework for Linear Constraint Satisfaction via Sampling Kaczmarz-Motzkin Method",
        "summary": "Neural network constraint satisfaction is crucial for safety-critical applications such as power system optimization, robotic path planning, and autonomous driving. However, existing constraint satisfaction methods face efficiency-applicability trade-offs, with hard constraint methods suffering from either high computational complexity or restrictive assumptions on constraint structures. The Sampling Kaczmarz-Motzkin (SKM) method is a randomized iterative algorithm for solving large-scale linear inequality systems with favorable convergence properties, but its argmax operations introduce non-differentiability, posing challenges for neural network applications. This work proposes the Trainable Sampling Kaczmarz-Motzkin Network (T-SKM-Net) framework and, for the first time, systematically integrates SKM-type methods into neural network constraint satisfaction. The framework transforms mixed constraint problems into pure inequality problems through null space transformation, employs SKM for iterative solving, and maps solutions back to the original constraint space, efficiently handling both equality and inequality constraints. We provide theoretical proof of post-processing effectiveness in expectation and end-to-end trainability guarantees based on unbiased gradient estimators, demonstrating that despite non-differentiable operations, the framework supports standard backpropagation. On the DCOPF case118 benchmark, our method achieves 4.27ms/item GPU serial forward inference with 0.0025% max optimality gap with post-processing mode and 5.25ms/item with 0.0008% max optimality gap with joint training mode, delivering over 25$\\times$ speedup compared to the pandapower solver while maintaining zero constraint violations under given tolerance.",
        "url": "http://arxiv.org/abs/2512.10461v1",
        "published_date": "2025-12-11T09:35:13+00:00",
        "updated_date": "2025-12-11T09:35:13+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.OC"
        ],
        "authors": [
            "Haoyu Zhu",
            "Yao Zhang",
            "Jiashen Ren",
            "Qingchun Hou"
        ],
        "tldr": "The paper introduces T-SKM-Net, a novel neural network framework integrating the Sampling Kaczmarz-Motzkin method for efficient and trainable linear constraint satisfaction, demonstrating significant speedups on a power system optimization benchmark. It aims to solve constraint satisfaction problems for safety-critical application scenarios.",
        "tldr_zh": "该论文介绍了T-SKM-Net，一种新颖的神经网络框架，它集成了采样Kaczmarz-Motzkin方法，用于高效且可训练的线性约束满足，并在电力系统优化基准测试中实现了显着加速。旨在解决安全关键应用场景下的约束满足问题。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "神经网络约束满足对于电力系统优化、机器人路径规划和自动驾驶等安全关键应用至关重要。然而，现有的约束满足方法面临效率与适用性的权衡，强约束方法要么计算复杂度高，要么对约束结构有严格的假设条件。采样Kaczmarz-Motzkin (SKM)方法是一种随机迭代算法，用于求解具有良好收敛特性的大规模线性不等式系统，但其argmax操作引入了不可微性，给神经网络应用带来了挑战。本文提出了可训练采样Kaczmarz-Motzkin网络 (T-SKM-Net) 框架，并首次将SKM类型的方法系统地集成到神经网络约束满足中。该框架通过零空间变换将混合约束问题转化为纯不等式问题，采用SKM进行迭代求解，并将解映射回原始约束空间，从而高效地处理等式和不等式约束。我们提供了后处理在期望上有效性的理论证明，以及基于无偏梯度估计器的端到端可训练性保证，证明了尽管存在不可微操作，该框架仍支持标准的反向传播。在DCOPF case118基准测试中，我们的方法在GPU上实现了4.27毫秒/项的串行前向推理速度，后处理模式下最优性间隙最大值为0.0025%，联合训练模式下为5.25毫秒/项，最优性间隙最大值为0.0008%，在给定容差下保持零约束违规的情况下，与pandapower求解器相比，速度提升超过25倍。"
    },
    {
        "title": "An M-Health Algorithmic Approach to Identify and Assess Physiotherapy Exercises in Real Time",
        "summary": "This work presents an efficient algorithmic framework for real-time identification, classification, and evaluation of human physiotherapy exercises using mobile devices. The proposed method interprets a kinetic movement as a sequence of static poses, which are estimated from camera input using a pose-estimation neural network. Extracted body keypoints are transformed into trigonometric angle-based features and classified with lightweight supervised models to generate frame-level pose predictions and accuracy scores. To recognize full exercise movements and detect deviations from prescribed patterns, we employ a dynamic-programming scheme based on a modified Levenshtein distance algorithm, enabling robust sequence matching and localization of inaccuracies. The system operates entirely on the client side, ensuring scalability and real-time performance. Experimental evaluation demonstrates the effectiveness of the methodology and highlights its applicability to remote physiotherapy supervision and m-health applications.",
        "url": "http://arxiv.org/abs/2512.10437v1",
        "published_date": "2025-12-11T08:56:03+00:00",
        "updated_date": "2025-12-11T08:56:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Stylianos Kandylakis",
            "Christos Orfanopoulos",
            "Georgios Siolas",
            "Panayiotis Tsanakas"
        ],
        "tldr": "This paper presents a mobile health framework for real-time physiotherapy exercise assessment using pose estimation and dynamic programming to identify and evaluate movements on client-side devices.",
        "tldr_zh": "本文提出了一种移动健康框架，使用姿势估计和动态规划对物理治疗运动进行实时评估，以识别和评估客户端设备上的运动。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "本文提出了一种高效的算法框架，用于利用移动设备实时识别、分类和评估人体理疗运动。该方法将一个动态运动解读为一系列静态姿势，这些姿势通过姿势估计神经网络从摄像头输入中估计得出。提取到的身体关键点被转换成基于三角函数的角度特征，并使用轻量级的监督模型进行分类，以生成帧级别的姿势预测和准确度评分。为了识别完整的运动，并检测与规定模式的偏差，我们采用基于改进的Levenshtein距离算法的动态规划方案，从而实现稳健的序列匹配和误差定位。该系统完全在客户端运行，确保了可扩展性和实时性能。实验评估表明该方法有效，并突出了其在远程物理治疗监督和移动健康应用中的适用性。"
    },
    {
        "title": "Beyond Endpoints: Path-Centric Reasoning for Vectorized Off-Road Network Extraction",
        "summary": "Deep learning has advanced vectorized road extraction in urban settings, yet off-road environments remain underexplored and challenging. A significant domain gap causes advanced models to fail in wild terrains due to two key issues: lack of large-scale vectorized datasets and structural weakness in prevailing methods. Models such as SAM-Road employ a node-centric paradigm that reasons at sparse endpoints, making them fragile to occlusions and ambiguous junctions in off-road scenes, leading to topological errors.This work addresses these limitations in two complementary ways. First, we release WildRoad, a gloabal off-road road network dataset constructed efficiently with a dedicated interactive annotation tool tailored for road-network labeling. Second, we introduce MaGRoad (Mask-aware Geodesic Road network extractor), a path-centric framework that aggregates multi-scale visual evidence along candidate paths to infer connectivity robustly.Extensive experiments show that MaGRoad achieves state-of-the-art performance on our challenging WildRoad benchmark while generalizing well to urban datasets. A streamlined pipeline also yields roughly 2.5x faster inference, improving practical applicability. Together, the dataset and path-centric paradigm provide a stronger foundation for mapping roads in the wild.",
        "url": "http://arxiv.org/abs/2512.10416v1",
        "published_date": "2025-12-11T08:29:27+00:00",
        "updated_date": "2025-12-11T08:29:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenfei Guan",
            "Jilin Mei",
            "Tong Shen",
            "Xumin Wu",
            "Shuo Wang",
            "Cheng Min",
            "Yu Hu"
        ],
        "tldr": "This paper introduces WildRoad, a new off-road road network dataset, and MaGRoad, a path-centric framework for road network extraction that outperforms existing methods, especially in challenging off-road environments, while also improving inference speed.",
        "tldr_zh": "这篇论文介绍了WildRoad，一个新的越野道路网络数据集，以及MaGRoad，一个路径中心的道路网络提取框架，该框架优于现有的方法，尤其是在具有挑战性的越野环境中，并且还提高了推理速度。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "深度学习推动了城市环境中矢量化道路提取的发展，但越野环境仍未得到充分探索，且具有挑战性。显著的领域差距导致先进模型在野外地形中失效，其原因有两点：缺乏大规模矢量化数据集以及现有方法的结构性弱点。诸如SAM-Road之类的模型采用以节点为中心的范式，仅对稀疏的端点进行推理，这使得它们在越野场景中容易受到遮挡和模糊连接点的影响，从而导致拓扑错误。本研究通过两种互补的方式解决这些局限性。首先，我们发布了WildRoad，一个全球越野道路网络数据集，该数据集通过专门为道路网络标注定制的交互式标注工具高效构建。其次，我们引入了MaGRoad（掩码感知的测地线道路网络提取器），一种以路径为中心的框架，它沿候选路径聚合多尺度视觉证据，以稳健地推断连通性。大量实验表明，MaGRoad在具有挑战性的WildRoad基准测试中取得了最先进的性能，同时也能很好地推广到城市数据集。简化的流程也带来了大约2.5倍的推理速度提升，提高了实际应用性。总之，数据集和以路径为中心的范式为绘制野外道路地图提供了更坚实的基础。"
    },
    {
        "title": "Adaptive Dual-Weighted Gravitational Point Cloud Denoising Method",
        "summary": "High-quality point cloud data is a critical foundation for tasks such as autonomous driving and 3D reconstruction. However, LiDAR-based point cloud acquisition is often affected by various disturbances, resulting in a large number of noise points that degrade the accuracy of subsequent point cloud object detection and recognition. Moreover, existing point cloud denoising methods typically sacrifice computational efficiency in pursuit of higher denoising accuracy, or, conversely, improve processing speed at the expense of preserving object boundaries and fine structural details, making it difficult to simultaneously achieve high denoising accuracy, strong edge preservation, and real-time performance. To address these limitations, this paper proposes an adaptive dual-weight gravitational-based point cloud denoising method. First, an octree is employed to perform spatial partitioning of the global point cloud, enabling parallel acceleration. Then, within each leaf node, adaptive voxel-based occupancy statistics and k-nearest neighbor (kNN) density estimation are applied to rapidly remove clearly isolated and low-density noise points, thereby reducing the effective candidate set. Finally, a gravitational scoring function that combines density weights with adaptive distance weights is constructed to finely distinguish noise points from object points. Experiments conducted on the Stanford 3D Scanning Repository, the Canadian Adverse Driving Conditions (CADC) dataset, and in-house FMCW LiDAR point clouds acquired in our laboratory demonstrate that, compared with existing methods, the proposed approach achieves consistent improvements in F1, PSNR, and Chamfer Distance (CD) across various noise conditions while reducing the single-frame processing time, thereby validating its high accuracy, robustness, and real-time performance in multi-noise scenarios.",
        "url": "http://arxiv.org/abs/2512.10386v1",
        "published_date": "2025-12-11T07:49:28+00:00",
        "updated_date": "2025-12-11T07:49:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ge Zhang",
            "Chunyang Wang",
            "Bo Xiao",
            "Xuelian Liu",
            "Bin Liu"
        ],
        "tldr": "The paper presents an adaptive dual-weighted gravitational approach for point cloud denoising, achieving high accuracy, edge preservation, and real-time performance by combining octree partitioning, adaptive voxel occupancy, kNN density estimation, and a novel gravitational scoring function.",
        "tldr_zh": "本文提出了一种自适应双加权引力点云去噪方法，通过结合八叉树分割、自适应体素占用率、kNN密度估计和一种新的引力评分函数，实现了高精度、边缘保持和实时性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "高质量点云数据是自动驾驶和3D重建等任务的关键基础。然而，基于激光雷达的点云获取常常受到各种干扰的影响，导致大量噪声点，进而降低后续点云目标检测和识别的精度。此外，现有的点云去噪方法通常以牺牲计算效率为代价来追求更高的去噪精度，或者相反，以牺牲保持目标边界和精细结构细节为代价来提高处理速度，难以同时实现高去噪精度、强边缘保持和实时性能。为了解决这些局限性，本文提出了一种基于自适应双权重引力的点云去噪方法。首先，采用八叉树对全局点云进行空间分割，从而实现并行加速。然后，在每个叶节点内，利用基于自适应体素的占据统计和k近邻(kNN)密度估计，快速移除明显孤立和低密度的噪声点，从而减少有效候选集。最后，构建一个密度权重与自适应距离权重相结合的引力评分函数，以精细地区分噪声点和目标点。在斯坦福3D扫描库、加拿大恶劣驾驶条件（CADC）数据集以及我们实验室采样的FMCW激光雷达点云上进行的实验表明，与现有方法相比，所提出的方法在各种噪声条件下，在F1、PSNR和Chamfer距离（CD）方面均实现了持续改进，同时减少了单帧处理时间，从而验证了其在多噪声场景下具有高精度、鲁棒性和实时性能。"
    },
    {
        "title": "Point2Pose: A Generative Framework for 3D Human Pose Estimation with Multi-View Point Cloud Dataset",
        "summary": "We propose a novel generative approach for 3D human pose estimation. 3D human pose estimation poses several key challenges due to the complex geometry of the human body, self-occluding joints, and the requirement for large-scale real-world motion datasets. To address these challenges, we introduce Point2Pose, a framework that effectively models the distribution of human poses conditioned on sequential point cloud and pose history. Specifically, we employ a spatio-temporal point cloud encoder and a pose feature encoder to extract joint-wise features, followed by an attention-based generative regressor. Additionally, we present a large-scale indoor dataset MVPose3D, which contains multiple modalities, including IMU data of non-trivial human motions, dense multi-view point clouds, and RGB images. Experimental results show that the proposed method outperforms the baseline models, demonstrating its superior performance across various datasets.",
        "url": "http://arxiv.org/abs/2512.10321v1",
        "published_date": "2025-12-11T06:11:24+00:00",
        "updated_date": "2025-12-11T06:11:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunsoo Lee",
            "Daeum Jeon",
            "Hyeokjae Oh"
        ],
        "tldr": "The paper introduces Point2Pose, a generative framework for 3D human pose estimation from multi-view point clouds, along with a new large-scale multi-modal dataset, MVPose3D, and demonstrates improved performance compared to baselines.",
        "tldr_zh": "本文提出了一种名为 Point2Pose 的生成框架，用于从多视角点云中估计 3D 人体姿势。此外，论文还发布了一个新的大规模多模态数据集 MVPose3D，并展示了相对于基线的性能提升。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "我们提出了一种用于人体三维姿态估计的新型生成式方法。人体三维姿态估计面临若干关键挑战，包括人体复杂的几何结构、关节自遮挡以及对大规模真实世界运动数据集的需求。为了解决这些挑战，我们提出了Point2Pose，一个能够有效建模人类姿态分布的框架，该分布以连续的点云和姿态历史为条件。具体而言，我们采用时空点云编码器和姿态特征编码器来提取关节特征，然后使用基于注意力机制的生成式回归器。此外，我们还提出了一个大型室内数据集MVPose3D，其中包含多种模态，包括非平凡人体运动的IMU数据、密集多视角点云和RGB图像。实验结果表明，所提出的方法优于基线模型，证明了其在各种数据集上的卓越性能。"
    },
    {
        "title": "TraceFlow: Dynamic 3D Reconstruction of Specular Scenes Driven by Ray Tracing",
        "summary": "We present TraceFlow, a novel framework for high-fidelity rendering of dynamic specular scenes by addressing two key challenges: precise reflection direction estimation and physically accurate reflection modeling. To achieve this, we propose a Residual Material-Augmented 2D Gaussian Splatting representation that models dynamic geometry and material properties, allowing accurate reflection ray computation. Furthermore, we introduce a Dynamic Environment Gaussian and a hybrid rendering pipeline that decomposes rendering into diffuse and specular components, enabling physically grounded specular synthesis via rasterization and ray tracing. Finally, we devise a coarse-to-fine training strategy to improve optimization stability and promote physically meaningful decomposition. Extensive experiments on dynamic scene benchmarks demonstrate that TraceFlow outperforms prior methods both quantitatively and qualitatively, producing sharper and more realistic specular reflections in complex dynamic environments.",
        "url": "http://arxiv.org/abs/2512.10095v1",
        "published_date": "2025-12-10T21:36:32+00:00",
        "updated_date": "2025-12-10T21:36:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiachen Tao",
            "Junyi Wu",
            "Haoxuan Wang",
            "Zongxin Yang",
            "Dawen Cai",
            "Yan Yan"
        ],
        "tldr": "TraceFlow introduces a novel framework for high-fidelity rendering of dynamic specular scenes by using a Residual Material-Augmented 2D Gaussian Splatting representation and a hybrid rendering pipeline that combines rasterization and ray tracing for physically grounded specular synthesis.",
        "tldr_zh": "TraceFlow 提出了一种新的框架，通过使用残差材料增强的 2D 高斯溅射表示以及一个混合渲染管线，结合了光栅化和光线追踪，用于对动态镜面场景进行高保真渲染，从而实现物理上合理的镜面合成。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "我们提出了TraceFlow，一种用于高保真渲染动态镜面场景的新颖框架，旨在解决两个关键挑战：精确的反射方向估计和物理上准确的反射建模。为此，我们提出了一种残差材质增强的2D高斯溅射表示，用于建模动态几何体和材质属性，从而实现精确的反射光线计算。此外，我们引入了一种动态环境高斯和一个混合渲染管线，将渲染分解为漫反射和镜面反射分量，从而通过光栅化和光线追踪实现物理上合理的镜面合成。最后，我们设计了一种由粗到精的训练策略，以提高优化稳定性和促进物理上有意义的分解。在动态场景基准上的大量实验表明，TraceFlow在定量和定性方面均优于现有方法，在复杂的动态环境中产生更清晰、更真实的镜面反射。"
    },
    {
        "title": "Mitigating Exposure Bias in Risk-Aware Time Series Forecasting with Soft Tokens",
        "summary": "Autoregressive forecasting is central to predictive control in diabetes and hemodynamic management, where different operating zones carry different clinical risks. Standard models trained with teacher forcing suffer from exposure bias, yielding unstable multi-step forecasts for closed-loop use. We introduce Soft-Token Trajectory Forecasting (SoTra), which propagates continuous probability distributions (``soft tokens'') to mitigate exposure bias and learn calibrated, uncertainty-aware trajectories. A risk-aware decoding module then minimizes expected clinical harm. In glucose forecasting, SoTra reduces average zone-based risk by 18\\%; in blood-pressure forecasting, it lowers effective clinical risk by approximately 15\\%. These improvements support its use in safety-critical predictive control.",
        "url": "http://arxiv.org/abs/2512.10056v1",
        "published_date": "2025-12-10T20:25:45+00:00",
        "updated_date": "2025-12-10T20:25:45+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Alireza Namazi",
            "Amirreza Dolatpour Fathkouhi",
            "Heman Shakeri"
        ],
        "tldr": "The paper introduces Soft-Token Trajectory Forecasting (SoTra) to mitigate exposure bias in time series forecasting, improving risk-aware predictive control in clinical settings like glucose and blood pressure management.",
        "tldr_zh": "该论文介绍了软令牌轨迹预测（SoTra），旨在减轻时间序列预测中的暴露偏差，从而改善葡萄糖和血压管理等临床环境中的风险感知预测控制。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "自回归预测是糖尿病和血流动力学管理中预测控制的核心，其中不同的操作区域承担着不同的临床风险。使用教师强迫训练的标准模型会受到暴露偏差的影响，从而产生不稳定的多步预测，无法用于闭环控制。我们引入了软标记轨迹预测（SoTra），它传播连续概率分布（“软标记”）以减轻暴露偏差，并学习经过校准的、具有不确定性意识的轨迹。随后，一个风险感知解码模块最小化预期的临床危害。在血糖预测中，SoTra 将基于区域的平均风险降低了 18%；在血压预测中，它将有效临床风险降低了约 15%。这些改进支持其在安全关键型预测控制中的应用。"
    },
    {
        "title": "Distribution-Free Stochastic MPC for Joint-in-Time Chance-Constrained Linear Systems",
        "summary": "This work presents a stochastic model predictive control (MPC) framework for linear systems subject to joint-in-time chance constraints under unknown disturbance distributions. Unlike existing stochastic MPC formulations that rely on parametric or Gaussian assumptions or require expensive offline computations, the proposed method leverages conformal prediction (CP) as a streamlined tool to construct finite-sample confidence regions for the system's stochastic error trajectories with minimal computational effort. These regions enable the relaxation of probabilistic constraints while providing formal guarantees. By employing an indirect feedback mechanism and a probabilistic set-based formulation, we prove recursive feasibility of the relaxed optimization problem and establish chance constraint satisfaction in closed-loop. Furthermore, we extend the approach to the more general output feedback setting with unknown measurement noise distributions. Given available noise samples, we establish satisfaction of the joint chance constraints and recursive feasibility via output measurements alone. Numerical examples demonstrate the effectiveness and advantages of the proposed method compared to existing approaches.",
        "url": "http://arxiv.org/abs/2512.10738v1",
        "published_date": "2025-12-11T15:25:02+00:00",
        "updated_date": "2025-12-11T15:25:02+00:00",
        "categories": [
            "eess.SY",
            "cs.RO"
        ],
        "authors": [
            "Lukas Vogel",
            "Andrea Carron",
            "Eleftherios E. Vlahakis",
            "Dimos V. Dimarogonas"
        ],
        "tldr": "This paper introduces a distribution-free stochastic MPC framework using conformal prediction for linear systems with joint-in-time chance constraints, offering a computationally efficient alternative to existing methods with formal guarantees.",
        "tldr_zh": "本文提出了一种使用共形预测的无分布随机 MPC 框架，用于具有时序联合机会约束的线性系统，为现有方法提供了一种具有形式化保证的计算高效替代方案。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "本文提出了一种随机模型预测控制（MPC）框架，用于处理受未知扰动分布影响，且具有时域联合概率约束的线性系统。不同于依赖参数或高斯假设，或需要昂贵离线计算的现有随机MPC公式，本文提出的方法利用符合预测（CP）作为一种简化的工具，以最小的计算代价构建系统随机误差轨迹的有限样本置信区域。这些区域能够在提供正式保证的同时，放宽概率约束。通过采用间接反馈机制和基于概率集合的公式，我们证明了放宽后的优化问题的递归可行性，并建立了闭环下的概率约束满足性。此外，我们将该方法扩展到更一般的具有未知测量噪声分布的输出反馈设置。在给定可用噪声样本的情况下，我们证明了仅通过输出测量即可实现联合概率约束的满足性和递归可行性。数值算例验证了所提出方法的有效性和相对于现有方法的优势。"
    },
    {
        "title": "On the Stabilization of Rigid Formations on Regular Curves",
        "summary": "This work deals with the problem of stabilizing a multi-agent rigid formation on a general class of planar curves. Namely, we seek to stabilize an equilateral polygonal formation on closed planar differentiable curves after a path sweep. The task of finding an inscribed regular polygon centered at the point of interest is solved via a randomized multi-start Newton-Like algorithm for which one is able to ascertain the existence of a minimizer. Then we design a continuous feedback law that guarantees convergence to, and sufficient sweeping of the curve, followed by convergence to the desired formation vertices while ensuring inter-agent avoidance. The proposed approach is validated through numerical simulations for different classes of curves and different rigid formations. Code: https://github.com/mebbaid/paper-elobaid-ifacwc-2026",
        "url": "http://arxiv.org/abs/2512.10700v1",
        "published_date": "2025-12-11T14:41:19+00:00",
        "updated_date": "2025-12-11T14:41:19+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Mohamed Elobaid",
            "Shinkyu Park",
            "Eric Feron"
        ],
        "tldr": "This paper presents a Newton-like algorithm and feedback control law to stabilize multi-agent rigid formations on planar curves after a path sweep, validated with simulations.",
        "tldr_zh": "本文提出了一种类牛顿算法和反馈控制律，以稳定多智能体在平面曲线上的刚性编队，并在路径扫描后通过仿真验证。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "本文研究了在一般类别的平面曲线上稳定多智能体刚性编队的问题。具体而言，我们旨在路径扫掠后，在闭合的、可微的平面曲线上稳定等边多边形编队。为了找到中心位于目标点的内接正多边形，我们采用了一种随机多起点牛顿类算法，并能够确定极小值的存在性。然后，我们设计了一种连续反馈律，以保证收敛到曲线并充分扫掠曲线，随后收敛到期望的编队顶点，同时确保智能体之间的避碰。所提出的方法通过针对不同类型的曲线和不同刚性编队的数值模拟进行了验证。代码：https://github.com/mebbaid/paper-elobaid-ifacwc-2026"
    },
    {
        "title": "Motion Planning for Safe Landing of a Human-Piloted Parafoil",
        "summary": "Most skydiving accidents occur during the parafoil-piloting and landing stages and result from human lapses in judgment while piloting the parafoil. Training of novice pilots is protracted due to the lack of functional and easily accessible training simulators. Moreover, work on parafoil trajectory planning suitable for aiding human training remains limited. To bridge this gap, we study the problem of computing safe trajectories for human-piloted parafoil flight and examine how such trajectories fare against human-generated solutions. For the algorithmic part, we adapt the sampling-based motion planner Stable Sparse RRT (SST) by Li et al., to cope with the problem constraints while minimizing the bank angle (control effort) as a proxy for safety. We then compare the computer-generated solutions with data from human-generated parafoil flight, where the algorithm offers a relative cost improvement of 20\\%-80\\% over the performance of the human pilot. We observe that human pilots tend to, first, close the horizontal distance to the landing area, and then address the vertical gap by spiraling down to the suitable altitude for starting a landing maneuver. The algorithm considered here makes smoother and more gradual descents, arriving at the landing area at the precise altitude necessary for the final approach while maintaining safety constraints. Overall, the study demonstrates the potential of computer-generated guidelines, rather than traditional rules of thumb, which can be integrated into future simulators to train pilots for safer and more cost-effective flights.",
        "url": "http://arxiv.org/abs/2512.10595v1",
        "published_date": "2025-12-11T12:39:48+00:00",
        "updated_date": "2025-12-11T12:39:48+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Maximillian Fainkich",
            "Kiril Solovey",
            "Anna Clarke"
        ],
        "tldr": "The paper presents a motion planning algorithm using Stable Sparse RRT (SST) to generate safe landing trajectories for human-piloted parafoils, outperforming human pilots in simulations by 20-80% and offering smoother descent profiles.",
        "tldr_zh": "该论文提出了一种使用稳定稀疏RRT（SST）的运动规划算法，用于生成人为驾驶的滑翔伞的安全着陆轨迹，其在模拟中比人类飞行员表现高出20-80％，并提供更平滑的下降曲线。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "大多数跳伞事故发生在翼伞操纵和着陆阶段，是由于飞行员在操纵翼伞时出现人为判断失误造成的。由于缺乏实用且易于访问的训练模拟器，新手飞行员的培训过程漫长。此外，适用于辅助人工训练的翼伞轨迹规划研究仍然有限。为了弥合这一差距，我们研究了人为驾驶的翼伞飞行的安全轨迹计算问题，并检验了此类轨迹与人为生成方案的比较结果。在算法方面，我们调整了Li等人提出的基于采样的运动规划器稳定稀疏快速探索随机树（SST），以应对问题约束，同时最小化坡度角（控制力），作为安全性的代理。然后，我们将计算机生成的解决方案与人为生成的翼伞飞行数据进行比较，算法的相对成本比人工飞行员的性能提高了20%-80%。我们观察到，人工飞行员倾向于首先缩短与着陆区域的水平距离，然后通过螺旋下降到着陆机动的合适高度来解决垂直差距。本文考虑的算法进行更平稳和渐进的下降，在满足安全约束的同时，以最终进近所需的精确高度到达着陆区域。总的来说，这项研究证明了计算机生成的指导方针（而不是传统的经验法则）的潜力，这些指导方针可以集成到未来的模拟器中，以训练飞行员进行更安全、更经济高效的飞行。"
    },
    {
        "title": "Seamless Outdoor-Indoor Pedestrian Positioning System with GNSS/UWB/IMU Fusion: A Comparison of EKF, FGO, and PF",
        "summary": "Accurate and continuous pedestrian positioning across outdoor-indoor environments remains challenging because GNSS, UWB, and inertial PDR are complementary yet individually fragile under signal blockage, multipath, and drift. This paper presents a unified GNSS/UWB/IMU fusion framework for seamless pedestrian localization and provides a controlled comparison of three probabilistic back-ends: an error-state extended Kalman filter, sliding-window factor graph optimization, and a particle filter. The system uses chest-mounted IMU-based PDR as the motion backbone and integrates absolute updates from GNSS outdoors and UWB indoors. To enhance transition robustness and mitigate urban GNSS degradation, we introduce a lightweight map-based feasibility constraint derived from OpenStreetMap building footprints, treating most building interiors as non-navigable while allowing motion inside a designated UWB-instrumented building. The framework is implemented in ROS 2 and runs in real time on a wearable platform, with visualization in Foxglove. We evaluate three scenarios: indoor (UWB+PDR), outdoor (GNSS+PDR), and seamless outdoor-indoor (GNSS+UWB+PDR). Results show that the ESKF provides the most consistent overall performance in our implementation.",
        "url": "http://arxiv.org/abs/2512.10480v1",
        "published_date": "2025-12-11T09:59:03+00:00",
        "updated_date": "2025-12-11T09:59:03+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jiaqiang Zhang",
            "Xianjia Yu",
            "Sier Ha",
            "Paola Torrico Moron",
            "Sahar Salimpour",
            "Farhad Kerama",
            "Haizhou Zhang",
            "Tomi Westerlund"
        ],
        "tldr": "This paper presents a unified GNSS/UWB/IMU fusion framework for seamless outdoor-indoor pedestrian localization, comparing ESKF, FGO, and PF approaches with a map-based constraint for enhanced robustness.",
        "tldr_zh": "该论文提出了一个统一的GNSS/UWB/IMU融合框架，用于无缝的室内外行人定位，并比较了扩展卡尔曼滤波器(ESKF)、因子图优化(FGO)和粒子滤波器(PF)方法，利用基于地图的约束来增强鲁棒性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "室外-室内环境中准确且连续的行人定位仍然具有挑战性，因为GNSS、UWB和惯性PDR之间具有互补性，但它们在信号遮蔽、多径效应和漂移影响下又各自脆弱。本文提出了一种统一的GNSS/UWB/IMU融合框架，用于无缝的行人定位，并对三种概率后端进行了一项受控比较：误差状态扩展卡尔曼滤波器、滑动窗口因子图优化和粒子滤波器。该系统采用胸部安装的基于IMU的PDR作为运动主干，并整合来自室外GNSS和室内UWB的绝对更新信息。为了增强过渡的鲁棒性并缓解城市GNSS的性能下降，我们引入了一个基于地图的轻量级可行性约束，该约束源于OpenStreetMap建筑物轮廓，将大多数建筑物内部视为不可导航区域，同时允许在指定的配备UWB的建筑物内运动。该框架在ROS 2中实现，并在可穿戴平台上实时运行，并使用Foxglove进行可视化。我们评估了三种场景：室内（UWB+PDR）、室外（GNSS+PDR）和无缝的室外-室内（GNSS+UWB+PDR）。结果表明，在我们的实现中，ESKF提供了最一致的整体性能。"
    },
    {
        "title": "Design of a six wheel suspension and a three-axis linear actuation mechanism for a laser weeding robot",
        "summary": "Mobile robots are increasingly utilized in agriculture to automate labor-intensive tasks such as weeding, sowing, harvesting and soil analysis. Recently, agricultural robots have been developed to detect and remove weeds using mechanical tools or precise herbicide sprays. Mechanical weeding is inefficient over large fields, and herbicides harm the soil ecosystem. Laser weeding with mobile robots has emerged as a sustainable alternative in precision farming. In this paper, we present an autonomous weeding robot that uses controlled exposure to a low energy laser beam for weed removal. The proposed robot is six-wheeled with a novel double four-bar suspension for higher stability. The laser is guided towards the detected weeds by a three-dimensional linear actuation mechanism. Field tests have demonstrated the robot's capability to navigate agricultural terrains effectively by overcoming obstacles up to 15 cm in height. At an optimal speed of 42.5 cm/s, the robot achieves a weed detection rate of 86.2\\% and operating time of 87 seconds per meter. The laser actuation mechanism maintains a minimal mean positional error of 1.54 mm, combined with a high hit rate of 97\\%, ensuring effective and accurate weed removal. This combination of speed, accuracy, and efficiency highlights the robot's potential for significantly enhancing precision farming practices.",
        "url": "http://arxiv.org/abs/2512.10319v1",
        "published_date": "2025-12-11T06:11:05+00:00",
        "updated_date": "2025-12-11T06:11:05+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "eess.SY"
        ],
        "authors": [
            "Muhammad Usama",
            "Muhammad Ibrahim Khan",
            "Ahmad Hasan",
            "Muhammad Shaaf Nadeem",
            "Khawaja Fahad Iqbal",
            "Jawad Aslam",
            "Mian Ashfaq Ali",
            "Asad Nisar Awan"
        ],
        "tldr": "This paper presents a six-wheeled autonomous robot with a novel suspension and three-axis linear actuation mechanism for laser weeding, demonstrating effective navigation, weed detection, and removal in field tests.",
        "tldr_zh": "本文介绍了一种六轮自主机器人，具有新型悬架和三轴线性驱动机构，用于激光除草，并在田间试验中展示了有效的导航、杂草检测和移除。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "移动机器人在农业领域的应用日益广泛，用于自动化诸如除草、播种、收割和土壤分析等劳动密集型任务。近年来，针对农业机器人，人们开发了利用机械工具或精确除草剂喷洒方式来探测和移除杂草的技术。机械除草在大面积农田中效率低下，而除草剂则会损害土壤生态系统。采用移动机器人进行激光除草已成为精准农业中一种可持续的替代方案。本文提出了一种自主除草机器人，它通过控制低能量激光束的暴露来实现杂草移除。该机器人采用六轮结构，并配备了一种新型的双四连杆悬架，以提高其稳定性。激光通过三维线性驱动机构引导至检测到的杂草处。田间试验表明，该机器人能够有效地在农业地形中导航，并克服高达15厘米的障碍物。在42.5厘米/秒的最佳速度下，该机器人实现了86.2%的杂草检测率和每米87秒的运行时间。激光驱动机构保持了1.54毫米的最小平均位置误差，并结合了97%的高命中率，确保了有效和精确的杂草移除。这种速度、精度和效率的结合突出了该机器人在显著提升精准农业实践方面的潜力。"
    },
    {
        "title": "Inertial Magnetic SLAM Systems Using Low-Cost Sensors",
        "summary": "Spatially inhomogeneous magnetic fields offer a valuable, non-visual information source for positioning. Among systems leveraging this, magnetic field-based simultaneous localization and mapping (SLAM) systems are particularly attractive because they can provide positioning information and build a magnetic field map on the fly. Moreover, they have bounded error within mapped regions. However, state-of-the-art methods typically require low-drift odometry data provided by visual odometry or a wheel encoder, etc. This is because these systems need to minimize/reduce positioning errors while exploring, which happens when they are in unmapped regions. To address these limitations, this work proposes a loosely coupled and a tightly coupled inertial magnetic SLAM (IM-SLAM) system. The proposed systems use commonly available low-cost sensors: an inertial measurement unit (IMU), a magnetometer array, and a barometer. The use of non-visual data provides a significant advantage over visual-based systems, making it robust to low-visibility conditions. Both systems employ state-space representations, and magnetic field models on different scales. The difference lies in how they use a local and global magnetic field model. The loosely coupled system uses these models separately in two state-space models, while the tightly coupled system integrates them into one state-space model. Experiment results show that the tightly coupled IM-SLAM system achieves lower positioning errors than the loosely coupled system in most scenarios, with typical errors on the order of meters per 100 meters traveled. These results demonstrate the feasiblity of developing a full 3D IM-SLAM systems using low-cost sensors and the potential of applying these systems in emergency response scenarios such as mine/fire rescue.",
        "url": "http://arxiv.org/abs/2512.10128v1",
        "published_date": "2025-12-10T22:22:00+00:00",
        "updated_date": "2025-12-10T22:22:00+00:00",
        "categories": [
            "cs.RO",
            "eess.SP"
        ],
        "authors": [
            "Chuan Huang",
            "Gustaf Hendeby",
            "Isaac Skog"
        ],
        "tldr": "This paper introduces loosely and tightly coupled inertial magnetic SLAM (IM-SLAM) systems using low-cost sensors (IMU, magnetometer array, barometer) for robust positioning in low-visibility environments, demonstrating meter-level accuracy per 100 meters traveled.",
        "tldr_zh": "本文提出了一种使用低成本传感器（IMU、磁力计阵列、气压计）的松耦合和紧耦合惯性磁 SLAM (IM-SLAM) 系统，用于在低可见度环境中进行鲁棒定位，实验结果表明，每行进 100 米的典型误差在米级。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "空间非均匀磁场为定位提供了一种有价值的、非视觉信息源。在利用这一特点的系统中，基于磁场的同步定位与建图（SLAM）系统尤其引人关注，因为它们可以提供定位信息并动态构建磁场地图。此外，它们在已建图区域内具有有界误差。然而，最先进的方法通常需要视觉里程计或轮式编码器等提供的低漂移里程计数据。这是因为这些系统需要在探索过程中最小化/减少定位误差，而这通常发生在它们处于未建图区域时。为了解决这些局限性，本文提出了一种松耦合和一种紧耦合的惯性磁 SLAM (IM-SLAM) 系统。所提出的系统使用常见的低成本传感器：一个惯性测量单元 (IMU)、一个磁力计阵列和一个气压计。非视觉数据的使用相对于基于视觉的系统而言，具有显著优势，使其对低能见度条件具有鲁棒性。这两种系统都采用状态空间表示以及不同尺度的磁场模型。区别在于它们如何使用局部和全局磁场模型。松耦合系统在两个状态空间模型中分别使用这些模型，而紧耦合系统将它们集成到一个状态空间模型中。实验结果表明，在大多数场景下，紧耦合 IM-SLAM 系统的定位误差低于松耦合系统，典型误差约为每行驶 100 米误差几米。这些结果证明了开发使用低成本传感器的完整 3D IM-SLAM 系统的可行性，以及将这些系统应用于矿井/火灾救援等应急响应场景中的潜力。"
    },
    {
        "title": "Unified Smart Factory Model: A model-based Approach for Integrating Industry 4.0 and Sustainability for Manufacturing Systems",
        "summary": "This paper presents the Unified Smart Factory Model (USFM), a comprehensive framework designed to translate high-level sustainability goals into measurable factory-level indicators with a systematic information map of manufacturing activities. The manufacturing activities were modelled as set of manufacturing, assembly and auxiliary processes using Object Process Methodology, a Model Based Systems Engineering (MBSE) language. USFM integrates Manufacturing Process and System, Data Process, and Key Performance Indicator (KPI) Selection and Assessment in a single framework. Through a detailed case study of Printed Circuit Board (PCB) assembly factory, the paper demonstrates how environmental sustainability KPIs can be selected, modelled, and mapped to the necessary data, highlighting energy consumption and environmental impact metrics. The model's systematic approach can reduce redundancy, minimize the risk of missing critical information, and enhance data collection. The paper concluded that the USFM bridges the gap between sustainability goals and practical implementation, providing significant benefits for industries specifically SMEs aiming to achieve sustainability targets.",
        "url": "http://arxiv.org/abs/2512.10631v1",
        "published_date": "2025-12-11T13:30:38+00:00",
        "updated_date": "2025-12-11T13:30:38+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Ishaan Kaushal",
            "Amaresh Chakrabarti"
        ],
        "tldr": "The paper introduces the Unified Smart Factory Model (USFM), a framework using Model Based Systems Engineering to integrate Industry 4.0 and sustainability goals by mapping them to factory-level KPIs and data collection, demonstrated through a PCB assembly case study.",
        "tldr_zh": "该论文介绍了统一智能工厂模型 (USFM)，该框架利用基于模型的系统工程将工业 4.0 和可持续发展目标整合起来，通过将其映射到工厂层面的 KPI 和数据收集来实现，并通过 PCB 组装案例研究进行了演示。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "本文提出统一智能工厂模型(USFM)，这是一个综合框架，旨在通过制造业活动系统的的信息映射，将高层可持续发展目标转化为可衡量的工厂级指标。制造活动被建模为一组使用对象过程方法（Object Process Methodology，一种基于模型的系统工程（MBSE）语言）的制造、装配和辅助过程。USFM在一个框架中集成了制造过程与系统、数据过程以及关键绩效指标（KPI）的选择和评估。通过一个印刷电路板（PCB）组装工厂的详细案例研究，本文展示了如何选择、建模环境可持续性KPI，并将其映射到必要的数据，重点关注能源消耗和环境影响指标。该模型的系统性方法可以减少冗余、最大限度地降低遗漏关键信息的风险并增强数据收集能力。本文的结论是USFM填补了可持续发展目标与实际实施之间的差距，为旨在实现可持续发展目标的行业，特别是中小企业，提供了显著的益处。"
    },
    {
        "title": "mmCounter: Static People Counting in Dense Indoor Scenarios Using mmWave Radar",
        "summary": "mmWave radars struggle to detect or count individuals in dense, static (non-moving) groups due to limitations in spatial resolution and reliance on movement for detection. We present mmCounter, which accurately counts static people in dense indoor spaces (up to three people per square meter). mmCounter achieves this by extracting ultra-low frequency (< 1 Hz) signals, primarily from breathing and micro-scale body movements such as slight torso shifts, and applying novel signal processing techniques to differentiate these subtle signals from background noise and nearby static objects. Our problem differs significantly from existing studies on breathing rate estimation, which assume the number of people is known a priori. In contrast, mmCounter utilizes a novel multi-stage signal processing pipeline to extract relevant low-frequency sources along with their spatial information and map these sources to individual people, enabling accurate counting. Extensive evaluations in various environments demonstrate that mmCounter delivers an 87% average F1 score and 0.6 mean absolute error in familiar environments, and a 60% average F1 score and 1.1 mean absolute error in previously untested environments. It can count up to seven individuals in a three square meter space, such that there is no side-by-side spacing and only a one-meter front-to-back distance.",
        "url": "http://arxiv.org/abs/2512.10357v1",
        "published_date": "2025-12-11T07:16:47+00:00",
        "updated_date": "2025-12-11T07:16:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tarik Reza Toha",
            "Shao-Jung",
            "Lu",
            "Shahriar Nirjon"
        ],
        "tldr": "The paper presents mmCounter, a novel system using mmWave radar and signal processing techniques to accurately count static people in dense indoor environments by detecting subtle breathing and micro-movements.",
        "tldr_zh": "该论文提出了mmCounter，一种利用毫米波雷达和信号处理技术的新系统，通过检测细微的呼吸和微小运动来准确统计密集室内环境中静止的人数。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "毫米波雷达由于空间分辨率的限制以及对运动的高度依赖，难以在密集、静态（非移动）的人群中检测或计数个体。我们提出了mmCounter，它可以准确地统计室内密集空间中静态的人数（高达每平方米三人）。mmCounter通过提取超低频（< 1 Hz）信号来实现这一目标，这些信号主要来自呼吸和微型身体运动，例如轻微的躯干位移，并应用新的信号处理技术来区分这些细微信号与背景噪声和附近的静态物体。我们的问题与现有呼吸频率估计的研究有显著不同，后者假设人数是先验已知的。相反，mmCounter利用一种新颖的多阶段信号处理流程来提取相关的低频源及其空间信息，并将这些源映射到个体，从而实现准确的计数。在各种环境中进行的广泛评估表明，mmCounter在熟悉的环境中实现了87%的平均F1分数和0.6的平均绝对误差，在以前未测试的环境中实现了60%的平均F1分数和1.1的平均绝对误差。它可以在三平方米的空间内最多计数七个人，并且这些人没有并排间距，只有一米的前后距离。"
    },
    {
        "title": "GDKVM: Echocardiography Video Segmentation via Spatiotemporal Key-Value Memory with Gated Delta Rule",
        "summary": "Accurate segmentation of cardiac chambers in echocardiography sequences is crucial for the quantitative analysis of cardiac function, aiding in clinical diagnosis and treatment. The imaging noise, artifacts, and the deformation and motion of the heart pose challenges to segmentation algorithms. While existing methods based on convolutional neural networks, Transformers, and space-time memory networks have improved segmentation accuracy, they often struggle with the trade-off between capturing long-range spatiotemporal dependencies and maintaining computational efficiency with fine-grained feature representation. In this paper, we introduce GDKVM, a novel architecture for echocardiography video segmentation. The model employs Linear Key-Value Association (LKVA) to effectively model inter-frame correlations, and introduces Gated Delta Rule (GDR) to efficiently store intermediate memory states. Key-Pixel Feature Fusion (KPFF) module is designed to integrate local and global features at multiple scales, enhancing robustness against boundary blurring and noise interference. We validated GDKVM on two mainstream echocardiography video datasets (CAMUS and EchoNet-Dynamic) and compared it with various state-of-the-art methods. Experimental results show that GDKVM outperforms existing approaches in terms of segmentation accuracy and robustness, while ensuring real-time performance. Code is available at https://github.com/wangrui2025/GDKVM.",
        "url": "http://arxiv.org/abs/2512.10252v1",
        "published_date": "2025-12-11T03:19:50+00:00",
        "updated_date": "2025-12-11T03:19:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Wang",
            "Yimu Sun",
            "Jingxing Guo",
            "Huisi Wu",
            "Jing Qin"
        ],
        "tldr": "The paper introduces GDKVM, a novel spatiotemporal key-value memory network with gated delta rule for echocardiography video segmentation, demonstrating improved accuracy and real-time performance compared to existing methods.",
        "tldr_zh": "该论文介绍了GDKVM，一种新颖的具有门控Delta规则的时空键值记忆网络，用于超声心动图视频分割，与现有方法相比，展示了更高的准确性和实时性能。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "心动超声序列中准确的心脏腔室分割对于心脏功能的定量分析至关重要，能够辅助临床诊断和治疗。成像噪声、伪影以及心脏的形变和运动对分割算法提出了挑战。虽然现有基于卷积神经网络、Transformer和时空记忆网络的方法提高了分割精度，但它们通常难以在捕获长程时空依赖关系和保持具有细粒度特征表示的计算效率之间取得平衡。本文提出了一种用于心动超声视频分割的新型架构GDKVM。该模型采用线性键值关联（LKVA）来有效地建模帧间相关性，并引入门控Delta规则（GDR）来高效地存储中间记忆状态。关键像素特征融合（KPFF）模块旨在整合多尺度的局部和全局特征，从而增强对边界模糊和噪声干扰的鲁棒性。我们在两个主流的心动超声视频数据集（CAMUS和EchoNet-Dynamic）上验证了GDKVM，并将其与各种最先进的方法进行了比较。实验结果表明，GDKVM在分割精度和鲁棒性方面优于现有方法，同时确保了实时性能。代码可在https://github.com/wangrui2025/GDKVM获取。"
    },
    {
        "title": "Quantum Approaches to Urban Logistics: From Core QAOA to Clustered Scalability",
        "summary": "The Traveling Salesman Problem (TSP) is a fundamental challenge in combinatorial optimization, widely applied in logistics and transportation. As the size of TSP instances grows, traditional algorithms often struggle to produce high-quality solutions within reasonable timeframes. This study investigates the potential of the Quantum Approximate Optimization Algorithm (QAOA), a hybrid quantum-classical method, to solve TSP under realistic constraints. We adopt a QUBO-based formulation of TSP that integrates real-world logistical constraints reflecting operational conditions, such as vehicle capacity, road accessibility, and time windows, while ensuring compatibility with the limitations of current quantum hardware. Our experiments are conducted in a simulated environment using high-performance computing (HPC) resources to assess QAOA's performance across different problem sizes and quantum circuit depths. In order to improve scalability, we propose clustering QAOA (Cl-QAOA), a hybrid approach combining classical machine learning with QAOA. This method decomposes large TSP instances into smaller sub-problems, making quantum optimization feasible even on devices with a limited number of qubits. The results offer a comprehensive evaluation of QAOA's strengths and limitations in solving constrained TSP scenarios. This study advances quantum optimization and lays groundwork for future large-scale applications.",
        "url": "http://arxiv.org/abs/2512.10813v1",
        "published_date": "2025-12-11T17:00:24+00:00",
        "updated_date": "2025-12-11T17:00:24+00:00",
        "categories": [
            "quant-ph",
            "cs.LG"
        ],
        "authors": [
            "F. Picariello",
            "G. Turati",
            "R. Antonelli",
            "I. Bailo",
            "S. Bonura",
            "G. Ciarfaglia",
            "S. Cipolla",
            "P. Cremonesi",
            "M. Ferrari Dacrema",
            "M. Gabusi",
            "I. Gentile",
            "V. Morreale",
            "A. Noto"
        ],
        "tldr": "This paper explores the use of Quantum Approximate Optimization Algorithm (QAOA) and a clustered variant (Cl-QAOA) for solving the Traveling Salesman Problem (TSP) with real-world logistics constraints, aiming to enhance scalability for larger instances using hybrid quantum-classical approaches.",
        "tldr_zh": "本文探讨了使用量子近似优化算法 (QAOA) 及其集群变体 (Cl-QAOA) 解决具有实际物流约束的旅行商问题 (TSP)，旨在通过混合量子-经典方法增强更大规模实例的可扩展性。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "旅行商问题（TSP）是组合优化中的一个基本挑战，广泛应用于物流和运输领域。随着TSP实例规模的增长，传统算法通常难以在合理的时间范围内产生高质量的解。本研究探讨了量子近似优化算法（QAOA）的潜力，这是一种混合量子-经典方法，用于在现实约束下求解TSP。我们采用基于QUBO的TSP公式，该公式集成了反映运营条件的实际物流约束，例如车辆容量、道路可达性和时间窗，同时确保与当前量子硬件的限制相兼容。我们的实验在模拟环境中进行，利用高性能计算（HPC）资源来评估QAOA在不同问题规模和量子电路深度下的性能。为了提高可扩展性，我们提出了一种聚类QAOA (Cl-QAOA)，这是一种将经典机器学习与QAOA相结合的混合方法。该方法将大型TSP实例分解为较小的子问题，使得即使在量子比特数量有限的设备上也能实现量子优化。结果全面评估了QAOA在解决受约束的TSP场景中的优势和局限性。本研究推进了量子优化，并为未来大规模应用奠定了基础。"
    },
    {
        "title": "Assessing Neuromorphic Computing for Fingertip Force Decoding from Electromyography",
        "summary": "High-density surface electromyography (HD-sEMG) provides a noninvasive neural interface for assistive and rehabilitation control, but mapping neural activity to user motor intent remains challenging. We assess a spiking neural network (SNN) as a neuromorphic architecture against a temporal convolutional network (TCN) for decoding fingertip force from motor-unit (MU) firing derived from HD-sEMG. Data were collected from a single participant (10 trials) with two forearm electrode arrays; MU activity was obtained via FastICA-based decomposition, and models were trained on overlapping windows with end-to-end causal convolutions. On held-out trials, the TCN achieved 4.44% MVC RMSE (Pearson r = 0.974) while the SNN achieved 8.25% MVC (r = 0.922). While the TCN was more accurate, we view the SNN as a realistic neuromorphic baseline that could close much of this gap with modest architectural and hyperparameter refinements.",
        "url": "http://arxiv.org/abs/2512.10179v1",
        "published_date": "2025-12-11T00:33:31+00:00",
        "updated_date": "2025-12-11T00:33:31+00:00",
        "categories": [
            "cs.LG",
            "eess.SP"
        ],
        "authors": [
            "Abolfazl Shahrooei",
            "Luke Arthur",
            "Om Patel",
            "Derek Kamper"
        ],
        "tldr": "This paper compares a Spiking Neural Network (SNN) neuromorphic architecture against a Temporal Convolutional Network (TCN) for decoding fingertip force from electromyography data, finding the TCN more accurate but suggesting SNNs have potential with refinements.",
        "tldr_zh": "本文比较了脉冲神经网络（SNN）神经形态架构和时间卷积网络（TCN）在从肌电图数据解码指尖力方面的性能。结果显示TCN更准确，但认为SNN经过改进后具有潜力。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "高密度表面肌电图(HD-sEMG)为辅助和康复控制提供了一种非侵入式的神经接口，但将神经活动映射到用户的运动意图仍然具有挑战性。我们评估了一种脉冲神经网络(SNN)作为神经形态架构，并将其与时间卷积网络(TCN)进行对比，用于从HD-sEMG导出的运动单元(MU)放电中解码指尖力。数据来自一名受试者（10次试验），使用两个前臂电极阵列采集； MU活动通过基于FastICA的分解获得，并且模型在具有端到端因果卷积的重叠窗口上进行训练。在预留试验中，TCN实现了4.44% MVC RMSE (Pearson r = 0.974)，而SNN实现了8.25% MVC (r = 0.922)。虽然TCN更准确，但我们将SNN视为一个实际的神经形态基准，可以通过适度的架构和超参数改进来弥补大部分差距。"
    }
]