[
    {
        "title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations",
        "summary": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.",
        "url": "http://arxiv.org/abs/2512.19583v1",
        "published_date": "2025-12-22T17:08:54+00:00",
        "updated_date": "2025-12-22T17:08:54+00:00",
        "categories": [
            "cs.RO",
            "cs.GR"
        ],
        "authors": [
            "Yinhuai Wang",
            "Runyi Yu",
            "Hok Wai Tsui",
            "Xiaoyi Lin",
            "Hui Zhang",
            "Qihan Zhao",
            "Ke Fan",
            "Miao Li",
            "Jie Song",
            "Jingbo Wang",
            "Qifeng Chen",
            "Ping Tan"
        ],
        "tldr": "The paper introduces a system (HOP+HOT) for learning generalizable hand-object tracking controllers purely from synthetic data using reinforcement and imitation learning, achieving dexterous manipulation on diverse objects and hand morphologies.",
        "tldr_zh": "该论文提出了一个系统（HOP+HOT），通过强化学习和模仿学习，仅从合成数据中学习通用的手-物体跟踪控制器，从而在各种物体和手部形态上实现灵巧的操作。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "我们提出了一种系统，旨在仅从合成数据中学习可泛化的手-物跟踪控制器，无需任何人工演示。我们的方法做出了两个关键贡献：（1）HOP，即手-物规划器，它可以合成多样化的手-物轨迹；以及（2）HOT，即手-物跟踪器，它通过强化学习和交互模仿学习桥接了从合成到物理的迁移，提供了一个基于目标手-物状态的可泛化控制器。我们的方法可扩展到不同的物体形状和手部形态。通过大量的评估，我们表明我们的方法能够使灵巧的手跟踪具有挑战性的长时程序列，包括物体重新排列和敏捷的手内重定向。这些结果代表了在可扩展的操纵基础控制器方面迈出的重要一步，该控制器可以完全从合成数据中学习，打破了长期以来限制灵巧操纵进展的数据瓶颈。"
    },
    {
        "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
        "summary": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the π_{0}, π_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
        "url": "http://arxiv.org/abs/2512.19562v1",
        "published_date": "2025-12-22T16:44:23+00:00",
        "updated_date": "2025-12-22T16:44:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Martin Sedlacek",
            "Pavlo Yefanov",
            "Georgy Ponimatkin",
            "Jai Bardhan",
            "Simon Pilc",
            "Mederic Fourmy",
            "Evangelos Kazakos",
            "Cees G. M. Snoek",
            "Josef Sivic",
            "Vladimir Petrik"
        ],
        "tldr": "The paper introduces REALM, a new simulation environment and benchmark for evaluating the generalization capabilities of Vision-Language-Action models in robotic manipulation, demonstrating its potential as a proxy for real-world performance.",
        "tldr_zh": "该论文介绍了REALM，一个新的仿真环境和基准，用于评估视觉-语言-动作模型在机器人操作中的泛化能力，并证明了其作为现实世界性能代理的潜力。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "视觉-语言-动作 (VLA) 模型使机器人能够理解并执行自然语言指令所描述的任务。然而，一个关键挑战在于它们泛化到超出训练所针对的特定环境和条件的能力，这目前在现实世界中难以评估且成本高昂。为了弥合这一差距，我们提出了 REALM，一种新的模拟环境和基准，旨在评估 VLA 模型的泛化能力，并特别强调通过高保真视觉效果和对齐的机器人控制来建立模拟性能和真实世界性能之间的强相关性。我们的环境提供了一套包含 15 个扰动因子、7 个操作技能和 3500 多个物体的套件。最后，我们建立了两个任务集合来构成我们的基准，并评估了 π_{0}、π_{0}-FAST 和 GR00T N1.5 VLA 模型，表明泛化和鲁棒性仍然是一个开放的挑战。更广泛地说，我们还表明，模拟为我们提供了一个有价值的真实世界代理，并允许我们系统地探测和量化 VLA 的弱点和失效模式。项目主页：https://martin-sedlacek.com/realm"
    },
    {
        "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
        "summary": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.",
        "url": "http://arxiv.org/abs/2512.19402v1",
        "published_date": "2025-12-22T13:53:25+00:00",
        "updated_date": "2025-12-22T13:53:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Yujie Zhao",
            "Hongwei Fan",
            "Di Chen",
            "Shengcong Chen",
            "Liliang Chen",
            "Xiaoqi Li",
            "Guanghui Ren",
            "Hao Dong"
        ],
        "tldr": "The paper introduces Real2Edit2Real, a framework that generates robotic manipulation demonstrations by editing 3D scene geometry and synthesizing corresponding multi-view videos, achieving significant data efficiency gains in robot learning.",
        "tldr_zh": "该论文介绍了 Real2Edit2Real 框架，通过编辑 3D 场景几何体并合成相应的多视角视频来生成机器人操作演示，从而在机器人学习中实现显著的数据效率提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "机器人学习领域的近期进展得益于大规模数据集和强大的视觉运动策略架构，然而，策略的鲁棒性仍然受到收集多样化示教数据所需高昂成本的限制，尤其是在操作任务中的空间泛化方面。为了减少重复的数据收集，我们提出了Real2Edit2Real，一个通过3D控制界面，利用3D可编辑性与2D视觉数据桥接，从而生成新示教数据的框架。我们的方法首先利用度量尺度3D重建模型，从多视角RGB观测中重建场景几何结构。基于重建的几何结构，我们对点云执行深度可靠的3D编辑，以生成新的操作轨迹，同时几何校正机器人姿态，以恢复物理一致的深度，这为合成新的示教数据提供了可靠的条件。最后，我们提出了一种以深度为主要控制信号，并结合动作、边缘和光线图的多条件视频生成模型，以合成空间增强的多视角操作视频。在四个真实世界操作任务上的实验表明，仅使用1-5个源示教数据生成的数据训练的策略可以匹配甚至超越使用50个真实世界示教数据训练的策略，将数据效率提高了10-50倍。此外，在高度和纹理编辑上的实验结果证明了该框架的灵活性和可扩展性，表明其有潜力作为统一的数据生成框架。"
    },
    {
        "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
        "summary": "Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting \"ghost\" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.",
        "url": "http://arxiv.org/abs/2512.19021v1",
        "published_date": "2025-12-22T04:27:26+00:00",
        "updated_date": "2025-12-22T04:27:26+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Sihao Lin",
            "Zerui Li",
            "Xunyi Zhao",
            "Gengze Zhou",
            "Liuyi Wang",
            "Rong Wei",
            "Rui Tang",
            "Juncheng Li",
            "Hanqing Wang",
            "Jiangmiao Pang",
            "Anton van den Hengel",
            "Jiajun Liu",
            "Qi Wu"
        ],
        "tldr": "The paper introduces VLNVerse, a new large-scale, versatile, and realistic benchmark for Vision-Language Navigation, designed to address limitations in existing datasets and promote sim-to-real generalization.",
        "tldr_zh": "该论文介绍了VLNVerse，这是一个新的大规模、多功能且逼真的视觉-语言导航基准，旨在解决现有数据集的局限性，并促进从模拟到现实的泛化。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "尽管视觉-语言导航（VLN）取得了显著进展，但现有基准仍然局限于固定、小规模数据集，并采用简化的物理模拟。这些不足限制了基准测试对模拟到真实泛化的洞察力，并造成了重要的研究空白。此外，任务碎片化阻碍了该领域的统一/共享进展，而有限的数据规模无法满足现代基于LLM的预训练需求。为了克服这些限制，我们引入了VLNVerse：一种新型大规模、可扩展的基准，旨在实现通用、具身、真实的模拟和评估。VLNVerse将VLN重新定义为一个可扩展的、全栈具身AI问题。其通用性将先前碎片化的任务统一到一个框架中，并为研究人员提供了一个可扩展的工具包。其具身设计超越了无形的、瞬移的“幽灵”代理，支持在由强大的物理引擎驱动的真实模拟中的完整运动学。我们利用VLNVerse的规模和多样性，对现有方法（从经典模型到基于MLLM的代理）进行了全面评估。我们还提出了一种新型的统一多任务模型，能够处理基准测试中的所有任务。VLNVerse旨在缩小模拟导航与真实世界泛化之间的差距，为社区提供一个重要的工具，以推动对可扩展、通用具身运动代理的研究。"
    },
    {
        "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
        "summary": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
        "url": "http://arxiv.org/abs/2512.18950v1",
        "published_date": "2025-12-22T01:56:28+00:00",
        "updated_date": "2025-12-22T01:56:28+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Saman Forouzandeh",
            "Wei Peng",
            "Parham Moradi",
            "Xinghuo Yu",
            "Mahdi Jalili"
        ],
        "tldr": "The paper presents MACLA, a framework that uses a frozen LLM with external hierarchical procedural memory, Bayesian selection, and contrastive refinement to achieve strong performance across various benchmarks, demonstrating sample-efficient and interpretable agents without LLM updates.",
        "tldr_zh": "该论文提出了一种名为MACLA的框架，该框架使用冻结的LLM与外部分层程序记忆、贝叶斯选择和对比改进相结合，在各种基准测试中实现了强大的性能，展示了样本高效且可解释的代理，无需LLM参数更新。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "我们提出了 MACLA，一种将推理与学习解耦的框架，它通过维护一个冻结的大型语言模型，同时在外部层级程序记忆中执行所有适应性调整。MACLA 从轨迹中提取可重用的程序，通过贝叶斯后验概率追踪可靠性，通过期望效用评分选择行动，并通过对比成功和失败来提炼程序。在四个基准测试（ALFWorld、WebShop、TravelPlanner、InterCodeSQL）中，MACLA 实现了 78.1% 的平均性能，优于所有基线模型。在 ALFWorld 未见任务中，MACLA 达到 90.3% 的性能，并实现 3.1% 的正泛化。该系统在 56 秒内构建记忆，比最先进的 LLM 参数训练基线快 2800 倍，并将 2851 条轨迹压缩为 187 个程序。实验结果表明，具有贝叶斯选择和对比提炼的结构化外部记忆能够实现高效样本利用、可解释性以及持续改进的智能体，而无需更新 LLM 参数。"
    },
    {
        "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
        "summary": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
        "url": "http://arxiv.org/abs/2512.19576v2",
        "published_date": "2025-12-22T17:00:25+00:00",
        "updated_date": "2025-12-23T09:09:53+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG",
            "eess.SY"
        ],
        "authors": [
            "Kirill Djebko",
            "Tom Baumann",
            "Erik Dilger",
            "Frank Puppe",
            "Sergio Montenegro"
        ],
        "tldr": "This paper presents the first successful in-orbit demonstration of an AI-based attitude controller for a nanosatellite, trained entirely in simulation and deployed on the InnoCube 3U, showcasing its robust performance against a classical PD controller.",
        "tldr_zh": "本文介绍了首个基于人工智能的姿态控制器的在轨演示，该控制器完全在仿真环境中训练，并部署在InnoCube 3U纳米卫星上，展示了其相对于传统PD控制器的强大性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "姿态控制对于许多卫星任务至关重要。然而，经典控制器设计耗时，且对模型不确定性和运行边界条件的变化敏感。深度强化学习（DRL）通过与仿真环境的自主交互学习自适应控制策略，提供了一种很有前景的替代方案。克服Sim2Real差距（即将在仿真中训练的智能体部署到真实的物理卫星上）仍然是一项重大挑战。在这项工作中，我们首次成功地在轨演示了用于惯性指向机动的基于人工智能的姿态控制器。该控制器完全在仿真中训练，并部署到由维尔茨堡尤利乌斯-马克西米利安大学与柏林工业大学合作开发的InnoCube 3U纳米卫星上，该卫星于 2025 年 1 月发射。我们介绍了人工智能智能体设计、训练过程的方法、仿真与实际卫星观测行为之间的差异，以及InnoCube中基于人工智能的姿态控制器与经典PD控制器的比较。稳态指标证实了基于人工智能的控制器在重复在轨机动期间的稳健性能。"
    },
    {
        "title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation",
        "summary": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.",
        "url": "http://arxiv.org/abs/2512.19453v1",
        "published_date": "2025-12-22T14:58:52+00:00",
        "updated_date": "2025-12-22T14:58:52+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhenglong Guo",
            "Yiming Zhao",
            "Feng Jiang",
            "Heng Jin",
            "Zongbao Feng",
            "Jianbin Zhou",
            "Siyuan Xu"
        ],
        "tldr": "The paper introduces MaP-AVR, a meta-action planner for robotic agents that uses VLMs and retrieval-augmented generation to abstract action planning, replacing human-centric concepts with robot-intrinsic functionalities, demonstrating improved performance on daily tasks.",
        "tldr_zh": "该论文介绍了一种名为MaP-AVR的元动作规划器，用于机器人智能体。它利用视觉语言模型和检索增强生成技术来抽象动作规划，用机器人固有的功能取代以人为中心的思维，并在日常任务中表现出更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "旨在管理复杂日常任务的具身机器人AI系统依赖于任务规划器来理解和分解高层任务。虽然大多数研究侧重于通过微调或思维链提示来增强LLM/VLM的任务理解能力，但本文认为定义规划的技能集同样至关重要。为了应对日常环境的复杂性，技能集应具备高度的泛化能力。从经验上看，更抽象的表达往往更具泛化性。因此，我们提出将规划结果抽象为一组元动作。每个元动作包含三个组成部分：{移动/旋转，末端执行器状态改变，与环境的关系}。这种抽象用机器人内在功能取代了以人为中心的概念，例如抓取或推动。因此，规划的结果与机器人能够执行的全部动作无缝衔接。此外，为了确保LLM/VLM准确地生成所需的元动作格式，我们采用检索增强生成（RAG）技术，该技术利用人工标注的规划演示数据库来促进上下文学习。随着系统成功完成更多任务，数据库将自我增强，以持续支持多样性。元动作集及其与RAG的集成是我们规划器的两个新颖贡献，我们将其命名为MaP-AVR，即由VLM和RAG组成的代理的元动作规划器。为了验证其有效性，我们使用GPT-4o作为预训练的LLM/VLM模型，并使用OmniGibson作为我们的机器人平台设计实验。与当前最先进的方法相比，我们的方法表现出良好的性能。项目页面：https://map-avr.github.io/。"
    },
    {
        "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
        "summary": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io",
        "url": "http://arxiv.org/abs/2512.19390v1",
        "published_date": "2025-12-22T13:38:11+00:00",
        "updated_date": "2025-12-22T13:38:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Hongwei Fan",
            "Hang Dai",
            "Jiyao Zhang",
            "Jinzhou Li",
            "Qiyang Yan",
            "Yujie Zhao",
            "Mingju Gao",
            "Jinghang Wu",
            "Hao Tang",
            "Hao Dong"
        ],
        "tldr": "TwinAligner is a Real2Sim2Real system enhancing robot learning by visually and dynamically aligning simulation with reality, enabling zero-shot transfer of policies trained in simulation to the real world.",
        "tldr_zh": "TwinAligner是一个Real2Sim2Real系统，通过在视觉和动态上对齐模拟与现实，从而增强机器人学习能力，并能够将模拟训练的策略零样本迁移到现实世界。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "机器人领域正朝着数据驱动、端到端学习的方向发展，受到了多模态大型模型的启发。然而，对昂贵的真实世界数据的依赖限制了发展。仿真器提供了经济高效的替代方案，但仿真与现实之间的差距给有效的策略迁移带来了挑战。本文介绍了一种新型的Real2Sim2Real系统TwinAligner，它同时解决了视觉和动态方面的差距。视觉对齐模块通过SDF重建和可编辑的3DGS渲染实现像素级的对齐，而动态对齐模块通过识别机器人与物体交互中的刚体物理特性来确保动态一致性。TwinAligner通过提供可扩展的数据收集并建立可信的迭代循环来改进机器人学习，加速算法开发。定量评估突显了TwinAligner在视觉和动态真实世界到仿真对齐方面的强大能力。该系统使在仿真中训练的策略能够实现对真实世界的强大零样本泛化。真实世界和仿真策略性能之间的高度一致性突显了TwinAligner在推进可扩展机器人学习方面的潜力。代码和数据将在https://twin-aligner.github.io上发布。"
    },
    {
        "title": "OMP: One-step Meanflow Policy with Directional Alignment",
        "summary": "Robot manipulation, a key capability of embodied AI, has turned to data-driven generative policy frameworks, but mainstream approaches like Diffusion Models suffer from high inference latency and Flow-based Methods from increased architectural complexity. While simply applying meanFlow on robotic tasks achieves single-step inference and outperforms FlowPolicy, it lacks few-shot generalization due to fixed temperature hyperparameters in its Dispersive Loss and misaligned predicted-true mean velocities. To solve these issues, this study proposes an improved MeanFlow-based Policies: we introduce a lightweight Cosine Loss to align velocity directions and use the Differential Derivation Equation (DDE) to optimize the Jacobian-Vector Product (JVP) operator. Experiments on Adroit and Meta-World tasks show the proposed method outperforms MP1 and FlowPolicy in average success rate, especially in challenging Meta-World tasks, effectively enhancing few-shot generalization and trajectory accuracy of robot manipulation policies while maintaining real-time performance, offering a more robust solution for high-precision robotic manipulation.",
        "url": "http://arxiv.org/abs/2512.19347v1",
        "published_date": "2025-12-22T12:45:35+00:00",
        "updated_date": "2025-12-22T12:45:35+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Han Fang",
            "Yize Huang",
            "Yuheng Zhao",
            "Paul Weng",
            "Xiao Li",
            "Yutong Ban"
        ],
        "tldr": "This paper introduces an improved MeanFlow-based policy (OMP) for robot manipulation, addressing limitations in few-shot generalization and trajectory accuracy by aligning velocity directions with a Cosine Loss and optimizing the Jacobian-Vector Product, achieving real-time performance and outperforming baselines on Adroit and Meta-World tasks.",
        "tldr_zh": "该论文提出了一种改进的基于MeanFlow的机器人操作策略（OMP），通过使用余弦损失对齐速度方向和优化雅可比向量积，解决了few-shot泛化和轨迹精度方面的局限性，在Adroit和Meta-World任务上实现了实时性能并优于基线方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "机器人操作是具身人工智能的一项关键能力，已经转向数据驱动的生成式策略框架，但主流方法如扩散模型存在高推理延迟的问题，而基于流的方法则增加了架构复杂性。虽然直接应用MeanFlow于机器人任务可以实现单步推理并优于FlowPolicy，但由于其分散损失中固定的温度超参数以及预测-真实平均速度的错位，导致其缺乏少样本泛化能力。为了解决这些问题，本研究提出了一种改进的基于MeanFlow的策略：我们引入了一个轻量级的余弦损失来对齐速度方向，并使用微分导数方程（DDE）来优化雅可比-向量积（JVP）算子。在Adroit和Meta-World任务上的实验表明，所提出的方法在平均成功率上优于MP1和FlowPolicy，尤其是在具有挑战性的Meta-World任务中，有效增强了机器人操作策略的少样本泛化能力和轨迹精度，同时保持了实时性能，为高精度机器人操作提供了一个更稳健的解决方案。"
    },
    {
        "title": "Translating Flow to Policy via Hindsight Online Imitation",
        "summary": "Recent advances in hierarchical robot systems leverage a high-level planner to propose task plans and a low-level policy to generate robot actions. This design allows training the planner on action-free or even non-robot data sources (e.g., videos), providing transferable high-level guidance. Nevertheless, grounding these high-level plans into executable actions remains challenging, especially with the limited availability of high-quality robot data. To this end, we propose to improve the low-level policy through online interactions. Specifically, our approach collects online rollouts, retrospectively annotates the corresponding high-level goals from achieved outcomes, and aggregates these hindsight-relabeled experiences to update a goal-conditioned imitation policy. Our method, Hindsight Flow-conditioned Online Imitation (HinFlow), instantiates this idea with 2D point flows as the high-level planner. Across diverse manipulation tasks in both simulation and physical world, our method achieves more than $2\\times$ performance improvement over the base policy, significantly outperforming the existing methods. Moreover, our framework enables policy acquisition from planners trained on cross-embodiment video data, demonstrating its potential for scalable and transferable robot learning.",
        "url": "http://arxiv.org/abs/2512.19269v1",
        "published_date": "2025-12-22T11:06:06+00:00",
        "updated_date": "2025-12-22T11:06:06+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Yitian Zheng",
            "Zhangchen Ye",
            "Weijun Dong",
            "Shengjie Wang",
            "Yuyang Liu",
            "Chongjie Zhang",
            "Chuan Wen",
            "Yang Gao"
        ],
        "tldr": "The paper introduces HinFlow, a method that improves low-level robot policies by using hindsight relabeling of online rollouts conditioned on high-level plans (2D point flows) to achieve significant performance gains in manipulation tasks, even leveraging planners trained on cross-embodiment video data.",
        "tldr_zh": "该论文介绍了HinFlow，一种通过使用基于高级计划（2D点流）的回溯重标记在线rollout来改进低级机器人策略的方法，从而在操作任务中实现了显著的性能提升，甚至可以利用在跨实体视频数据上训练的规划器。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "近年来，分层机器人系统的发展利用高层规划器提出任务计划，并利用低层策略生成机器人动作。这种设计允许在高层次上，利用无动作或甚至非机器人数据源（例如，视频）来训练规划器，从而提供可迁移的高层次指导。然而，将这些高层计划落实到可执行的动作仍然具有挑战性，尤其是在高质量机器人数据有限的情况下。为此，我们提出通过在线交互来改进低层策略。具体而言，我们的方法收集在线轨迹，回顾性地从已实现的结果中标记相应的高层目标，并聚合这些后见之明重新标记的经验来更新目标条件模仿策略。我们的方法，后见之明流条件在线模仿（HinFlow），用二维点流实例化了这个想法，作为高层规划器。在模拟和物理世界的各种操作任务中，我们的方法相对于基础策略实现了超过 2 倍的性能提升，显著优于现有方法。此外，我们的框架能够从基于跨形态视频数据训练的规划器中获取策略，展示了其在可扩展和可转移机器人学习方面的潜力。"
    },
    {
        "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
        "summary": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/",
        "url": "http://arxiv.org/abs/2512.19178v1",
        "published_date": "2025-12-22T09:12:48+00:00",
        "updated_date": "2025-12-22T09:12:48+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Jin Wang",
            "Kim Tien Ly",
            "Jacques Cloete",
            "Nikos Tsagarakis",
            "Ioannis Havoutis"
        ],
        "tldr": "This paper introduces a Vision-Language-Policy (VLP) model that enables robots to dynamically plan and execute tasks based on natural language commands and visual scene understanding, adapting to changing instructions during execution.",
        "tldr_zh": "本文介绍了一种视觉-语言-策略（VLP）模型，该模型使机器人能够基于自然语言命令和视觉场景理解动态地规划和执行任务，并适应执行过程中不断变化的指令。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在非结构化环境中，弥合自然语言指令与自主执行之间的差距仍然是机器人领域的一个开放性挑战。这需要机器人通过多种模态感知和推理当前的任务场景，并规划其行为以实现其预期目标。 传统的机器人任务规划方法通常难以将低层执行与高层任务推理连接起来，并且无法在执行过程中指令发生变化时动态更新任务策略，这最终限制了它们的多功能性以及对新任务的适应性。 在这项工作中，我们提出了一种基于语言模型的新型动态机器人任务规划框架。 我们的视觉-语言-策略（VLP）模型，基于在真实世界数据上微调的视觉-语言模型，可以解释语义指令并整合对当前任务场景的推理，以生成控制机器人完成任务的行为策略。 此外，它可以动态调整任务策略以响应任务的变化，从而能够灵活适应不断演进的任务需求。 使用不同机器人和各种真实世界任务进行的实验表明，训练后的模型能够高效地适应全新的场景并动态更新其策略，从而展示了强大的规划自主性和跨具身泛化能力。 视频：https://robovlp.github.io/"
    },
    {
        "title": "A Flexible Field-Based Policy Learning Framework for Diverse Robotic Systems and Sensors",
        "summary": "We present a cross robot visuomotor learning framework that integrates diffusion policy based control with 3D semantic scene representations from D3Fields to enable category level generalization in manipulation. Its modular design supports diverse robot camera configurations including UR5 arms with Microsoft Azure Kinect arrays and bimanual manipulators with Intel RealSense sensors through a low latency control stack and intuitive teleoperation. A unified configuration layer enables seamless switching between setups for flexible data collection training and evaluation. In a grasp and lift block task the framework achieved an 80 percent success rate after only 100 demonstration episodes demonstrating robust skill transfer between platforms and sensing modalities. This design paves the way for scalable real world studies in cross robotic generalization.",
        "url": "http://arxiv.org/abs/2512.19148v1",
        "published_date": "2025-12-22T08:45:33+00:00",
        "updated_date": "2025-12-22T08:45:33+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jose Gustavo Buenaventura Carreon",
            "Floris Erich",
            "Roman Mykhailyshyn",
            "Tomohiro Motoda",
            "Ryo Hanai",
            "Yukiyasu Domae"
        ],
        "tldr": "This paper introduces a cross-robot visuomotor learning framework using diffusion policies and 3D semantic scene representations for category-level generalization in manipulation, demonstrating robust skill transfer across different robotic platforms and sensing modalities.",
        "tldr_zh": "本文介绍了一个跨机器人视觉运动学习框架，该框架利用扩散策略和3D语义场景表示，实现了操作中的类别级别泛化，并展示了不同机器人平台和传感模式之间强大的技能转移能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了一个跨机器人视觉运动学习框架，该框架集成了基于扩散策略的控制和来自D3Fields的3D语义场景表示，以实现操作中的类别级泛化。其模块化设计支持各种机器人相机配置，包括配备微软Azure Kinect阵列的UR5机械臂和配备英特尔RealSense传感器的双手操作器，通过低延迟控制堆栈和直观的遥操作实现。统一的配置层支持在设置之间无缝切换，以实现灵活的数据收集、训练和评估。在一个抓取和抬起积木的任务中，该框架仅经过100个演示片段后，就实现了80%的成功率，展示了平台和传感方式之间强大的技能迁移能力。这种设计为跨机器人泛化中的可扩展真实世界研究铺平了道路。"
    },
    {
        "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
        "summary": "Latent World Models enhance scene representation through temporal self-supervised learning, presenting a perception annotation-free paradigm for end-to-end autonomous driving. However, the reconstruction-oriented representation learning tangles perception with planning tasks, leading to suboptimal optimization for planning. To address this challenge, we propose WorldRFT, a planning-oriented latent world model framework that aligns scene representation learning with planning via a hierarchical planning decomposition and local-aware interactive refinement mechanism, augmented by reinforcement learning fine-tuning (RFT) to enhance safety-critical policy performance. Specifically, WorldRFT integrates a vision-geometry foundation model to improve 3D spatial awareness, employs hierarchical planning task decomposition to guide representation optimization, and utilizes local-aware iterative refinement to derive a planning-oriented driving policy. Furthermore, we introduce Group Relative Policy Optimization (GRPO), which applies trajectory Gaussianization and collision-aware rewards to fine-tune the driving policy, yielding systematic improvements in safety. WorldRFT achieves state-of-the-art (SOTA) performance on both open-loop nuScenes and closed-loop NavSim benchmarks. On nuScenes, it reduces collision rates by 83% (0.30% -> 0.05%). On NavSim, using camera-only sensors input, it attains competitive performance with the LiDAR-based SOTA method DiffusionDrive (87.8 vs. 88.1 PDMS).",
        "url": "http://arxiv.org/abs/2512.19133v1",
        "published_date": "2025-12-22T08:27:44+00:00",
        "updated_date": "2025-12-22T08:27:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Pengxuan Yang",
            "Ben Lu",
            "Zhongpu Xia",
            "Chao Han",
            "Yinfeng Gao",
            "Teng Zhang",
            "Kun Zhan",
            "XianPeng Lang",
            "Yupeng Zheng",
            "Qichao Zhang"
        ],
        "tldr": "The paper introduces WorldRFT, a planning-oriented latent world model framework that utilizes hierarchical planning decomposition, local-aware interactive refinement, and reinforcement learning fine-tuning to improve autonomous driving safety and performance, achieving SOTA results on nuScenes and NavSim benchmarks.",
        "tldr_zh": "该论文介绍了WorldRFT，一个面向规划的潜在世界模型框架，它利用分层规划分解、局部感知的交互式细化和强化学习微调，以提高自动驾驶的安全性和性能，并在nuScenes和NavSim基准测试中实现了SOTA结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "潜在世界模型通过时序自监督学习增强场景表征，为端到端自动驾驶提供了一种无需感知标注的范式。然而，以重建为导向的表征学习将感知与规划任务纠缠在一起，导致规划的次优优化。为了应对这一挑战，我们提出了WorldRFT，一个面向规划的潜在世界模型框架，通过分层规划分解和局部感知交互式细化机制，将场景表征学习与规划对齐，并通过强化学习微调(RFT)来增强安全关键策略的性能。具体来说，WorldRFT集成了视觉-几何基础模型以提升3D空间感知能力，采用分层规划任务分解来指导表征优化，并利用局部感知迭代细化来推导出面向规划的驾驶策略。此外，我们引入了群组相对策略优化（GRPO），它应用轨迹高斯化和碰撞感知奖励来微调驾驶策略，从而系统地提高安全性。WorldRFT在开放环路nuScenes和闭环NavSim基准测试中都实现了最先进（SOTA）的性能。在nuScenes上，它将碰撞率降低了83%（0.30% -> 0.05%）。在NavSim上，仅使用摄像头传感器输入，它获得了与基于激光雷达的SOTA方法DiffusionDrive具有竞争力的性能（87.8 vs. 88.1 PDMS）。"
    },
    {
        "title": "CoDrone: Autonomous Drone Navigation Assisted by Edge and Cloud Foundation Models",
        "summary": "Autonomous navigation for Unmanned Aerial Vehicles faces key challenges from limited onboard computational resources, which restrict deployed deep neural networks to shallow architectures incapable of handling complex environments. Offloading tasks to remote edge servers introduces high latency, creating an inherent trade-off in system design. To address these limitations, we propose CoDrone - the first cloud-edge-end collaborative computing framework integrating foundation models into autonomous UAV cruising scenarios - effectively leveraging foundation models to enhance performance of resource-constrained unmanned aerial vehicle platforms. To reduce onboard computation and data transmission overhead, CoDrone employs grayscale imagery for the navigation model. When enhanced environmental perception is required, CoDrone leverages the edge-assisted foundation model Depth Anything V2 for depth estimation and introduces a novel one-dimensional occupancy grid-based navigation method - enabling fine-grained scene understanding while advancing efficiency and representational simplicity of autonomous navigation. A key component of CoDrone is a Deep Reinforcement Learning-based neural scheduler that seamlessly integrates depth estimation with autonomous navigation decisions, enabling real-time adaptation to dynamic environments. Furthermore, the framework introduces a UAV-specific vision language interaction module incorporating domain-tailored low-level flight primitives to enable effective interaction between the cloud foundation model and the UAV. The introduction of VLM enhances open-set reasoning capabilities in complex unseen scenarios. Experimental results show CoDrone outperforms baseline methods under varying flight speeds and network conditions, achieving a 40% increase in average flight distance and a 5% improvement in average Quality of Navigation.",
        "url": "http://arxiv.org/abs/2512.19083v1",
        "published_date": "2025-12-22T06:48:12+00:00",
        "updated_date": "2025-12-22T06:48:12+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Pengyu Chen",
            "Tao Ouyang",
            "Ke Luo",
            "Weijie Hong",
            "Xu Chen"
        ],
        "tldr": "CoDrone is a cloud-edge-end collaborative framework that integrates foundation models for autonomous UAV navigation, using a DRL-based scheduler and a UAV-specific vision language interaction module to improve flight distance and navigation quality.",
        "tldr_zh": "CoDrone是一个云-边缘-端协同框架，集成了基础模型用于无人机自主导航，使用基于DRL的调度器和特定于无人机的视觉语言交互模块，以提高飞行距离和导航质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "无人机的自主导航面临的主要挑战来自有限的机载计算资源，这限制了部署的深度神经网络只能采用无法处理复杂环境的浅层架构。将任务卸载到远程边缘服务器会导致高延迟，从而在系统设计中产生固有的权衡。为了解决这些限制，我们提出了CoDrone——首个将基础模型集成到自主无人机巡航场景中的云-边-端协同计算框架——有效地利用基础模型来增强资源受限的无人机平台的性能。为了减少机载计算和数据传输开销，CoDrone采用灰度图像用于导航模型。当需要增强环境感知时，CoDrone利用边缘辅助的基础模型Depth Anything V2进行深度估计，并引入了一种新颖的基于一维占据栅格的导航方法——在提高自主导航的效率和表示简洁性的同时，实现精细化的场景理解。CoDrone的一个关键组成部分是基于深度强化学习的神经调度器，该调度器将深度估计与自主导航决策无缝集成，从而能够实时适应动态环境。此外，该框架还引入了一个无人机特定的视觉语言交互模块，该模块集成了针对领域定制的底层飞行原语，以实现云端基础模型与无人机之间的有效交互。VLM的引入增强了复杂且未曾见过的场景中的开放集推理能力。实验结果表明，在不同的飞行速度和网络条件下，CoDrone的性能优于基线方法，平均飞行距离增加了40%，平均导航质量提高了5%。"
    },
    {
        "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
        "summary": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
        "url": "http://arxiv.org/abs/2512.19024v1",
        "published_date": "2025-12-22T04:42:35+00:00",
        "updated_date": "2025-12-22T04:42:35+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Xu Liu",
            "Yu Liu",
            "Hanshuo Qiu",
            "Yang Qirong",
            "Zhouhui Lian"
        ],
        "tldr": "The paper introduces IndoorUAV, a novel benchmark for vision-language navigation (VLN) with indoor UAVs, including datasets for long-horizon (VLN) and short-horizon (VLA) navigation, and a baseline navigation model.",
        "tldr_zh": "该论文介绍了一个名为IndoorUAV的新基准，用于室内无人机视觉语言导航(VLN)，包括长程(VLN)和短程(VLA)导航的数据集，以及一个基线导航模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉-语言导航（VLN）旨在使智能体能够通过遵循基于视觉观察的自然语言指令，在复杂环境中导航。尽管现有的大部分工作都集中在地面机器人或室外无人机（UAV）上，但基于室内无人机的VLN仍未得到充分探索，尽管它与现实世界的应用（如密闭空间内的检查、交付和搜索救援）相关。为了弥补这一差距，我们引入了\\textbf{IndoorUAV}，这是一个专门为室内无人机VLN量身定制的新基准和方法。我们首先从Habitat模拟器中收集了1000多个多样且结构丰富的3D室内场景。在这些环境中，我们模拟了真实的无人机飞行动力学，以手动收集各种3D导航轨迹，并通过数据增强技术进一步丰富这些轨迹。此外，我们设计了一个自动注释流程，为每个轨迹生成不同粒度的自然语言指令。这个过程产生了超过16,000条高质量轨迹，构成了\\textbf{IndoorUAV-VLN}子集，该子集专注于长时域VLN。为了支持短时域规划，我们通过选择语义显著的关键帧并重新生成简洁的指令，将长轨迹分割成子轨迹，从而形成了\\textbf{IndoorUAV-VLA}子集。最后，我们引入了\\textbf{IndoorUAV-Agent}，这是一种专为我们的基准而设计的新型导航模型，利用了任务分解和多模态推理。我们希望IndoorUAV能够作为一个有价值的资源，促进室内空中导航领域视觉-语言具身人工智能的研究。"
    },
    {
        "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
        "summary": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.",
        "url": "http://arxiv.org/abs/2512.18987v1",
        "published_date": "2025-12-22T02:55:25+00:00",
        "updated_date": "2025-12-22T02:55:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Ryosuke Korekata",
            "Quanting Xie",
            "Yonatan Bisk",
            "Komei Sugiura"
        ],
        "tldr": "The paper introduces Affordance RAG, a hierarchical multimodal retrieval framework for open-vocabulary mobile manipulation that leverages affordance-aware embodied memory and achieves improved retrieval and task success rates in real-world experiments.",
        "tldr_zh": "该论文介绍了Affordance RAG，一个用于开放词汇移动操作的分层多模态检索框架，它利用了能感知可供性的具身记忆，并在真实世界的实验中实现了改进的检索和任务成功率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本研究致力于解决开放词汇移动操作问题，即要求机器人根据自由形式的自然语言指令，将各种各样的物体运送到指定容器中。这项任务极具挑战性，因为它涉及理解视觉语义和操作动作的可供性。为了应对这些挑战，我们提出了一种名为 Affordance RAG 的零样本分层多模态检索框架，该框架从预先探索的图像中构建可供性感知的具身记忆。该模型基于区域和视觉语义检索候选目标，并利用可供性分数对其进行重排序，从而使机器人能够识别在现实环境中可能执行的操作选项。在大型室内环境中，我们的方法在移动操作指令的检索性能方面优于现有方法。此外，在真实的室内环境中，机器人根据自由形式的指令执行移动操作的实验中，所提出的方法达到了 85% 的任务成功率，在检索性能和整体任务成功率方面均优于现有方法。"
    },
    {
        "title": "Point What You Mean: Visually Grounded Instruction Policy",
        "summary": "Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.",
        "url": "http://arxiv.org/abs/2512.18933v1",
        "published_date": "2025-12-22T00:44:19+00:00",
        "updated_date": "2025-12-22T00:44:19+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Hang Yu",
            "Juntu Zhao",
            "Yufeng Liu",
            "Kaiyu Li",
            "Cheng Ma",
            "Di Zhang",
            "Yingdong Hu",
            "Guang Chen",
            "Junyuan Xie",
            "Junliang Guo",
            "Junqiao Zhao",
            "Yang Gao"
        ],
        "tldr": "The paper introduces Point-VLA, a vision-language-action model that enhances object referencing in embodied control by augmenting language instructions with visual cues, demonstrating improved performance in cluttered and out-of-distribution environments.",
        "tldr_zh": "该论文介绍了Point-VLA，一种视觉-语言-动作模型，通过用视觉提示增强语言指令来增强具身控制中的对象引用，并在杂乱和分布外的环境中表现出更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉-语言-动作（VLA）模型将视觉和语言与具身控制对齐，但仅依赖文本提示时，其对象指代能力仍然有限，尤其是在杂乱或分布外（OOD）场景中。在本研究中，我们引入了Point-VLA，一种即插即用的策略，它通过显式的视觉线索（例如，边界框）来增强语言指令，以解决指代歧义并实现精确的对象级定位。为了有效地扩展视觉接地的数据集，我们进一步开发了一个自动数据标注流程，该流程只需要极少的人工干预。我们在各种真实世界的指代任务上评估了Point-VLA，并观察到它比仅文本指令的VLA模型表现出更持续的优势，尤其是在杂乱或未见对象场景中，且具有强大的泛化能力。这些结果表明，Point-VLA通过像素级的视觉接地有效地解决了对象指代歧义，从而实现了更具泛化性的具身控制。"
    },
    {
        "title": "InDRiVE: Reward-Free World-Model Pretraining for Autonomous Driving via Latent Disagreement",
        "summary": "Model-based reinforcement learning (MBRL) can reduce interaction cost for autonomous driving by learning a predictive world model, but it typically still depends on task-specific rewards that are difficult to design and often brittle under distribution shift. This paper presents InDRiVE, a DreamerV3-style MBRL agent that performs reward-free pretraining in CARLA using only intrinsic motivation derived from latent ensemble disagreement. Disagreement acts as a proxy for epistemic uncertainty and drives the agent toward under-explored driving situations, while an imagination-based actor-critic learns a planner-free exploration policy directly from the learned world model. After intrinsic pretraining, we evaluate zero-shot transfer by freezing all parameters and deploying the pretrained exploration policy in unseen towns and routes. We then study few-shot adaptation by training a task policy with limited extrinsic feedback for downstream objectives (lane following and collision avoidance). Experiments in CARLA across towns, routes, and traffic densities show that disagreement-based pretraining yields stronger zero-shot robustness and robust few-shot collision avoidance under town shift and matched interaction budgets, supporting the use of intrinsic disagreement as a practical reward-free pretraining signal for reusable driving world models.",
        "url": "http://arxiv.org/abs/2512.18850v1",
        "published_date": "2025-12-21T18:40:15+00:00",
        "updated_date": "2025-12-21T18:40:15+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Feeza Khan Khanzada",
            "Jaerock Kwon"
        ],
        "tldr": "The paper introduces InDRiVE, a DreamerV3-style MBRL agent that uses latent ensemble disagreement as intrinsic motivation for reward-free pretraining in autonomous driving, demonstrating strong zero-shot robustness and few-shot adaptation in CARLA.",
        "tldr_zh": "该论文介绍了InDRiVE，一种DreamerV3风格的基于模型的强化学习智能体，它使用潜在集成不一致性作为内在动机，在自动驾驶中进行无奖励预训练，并在CARLA中展示了强大的零样本鲁棒性和少样本适应性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于模型的强化学习(MBRL)可以通过学习预测世界模型来降低自动驾驶的交互成本，但它通常仍然依赖于特定任务的奖励，这些奖励难以设计且在分布偏移下往往脆弱。本文提出InDRiVE，一种DreamerV3风格的MBRL智能体，它仅使用源自潜在集成不一致性的内在动机在CARLA中执行无奖励预训练。不一致性充当认知不确定性的代理，并驱动智能体探索欠探索的驾驶情境，而基于想象的行动者-评论家直接从学习到的世界模型中学习一种无规划的探索策略。在内在预训练后，我们通过冻结所有参数并在未见城镇和路线中部署预训练的探索策略来评估零样本迁移。然后，我们研究了通过在下游目标（车道保持和避撞）下使用有限的外部反馈训练任务策略来实现的少样本适应。在CARLA中针对不同城镇、路线和交通密度的实验表明，基于不一致性的预训练可在城镇转移和匹配的交互预算下产生更强的零样本鲁棒性和稳健的少样本避撞性能，这支持了使用内在不一致性作为可重用型驾驶世界模型的一种实用的无奖励预训练信号。"
    },
    {
        "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
        "summary": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
        "url": "http://arxiv.org/abs/2512.19554v1",
        "published_date": "2025-12-22T16:34:21+00:00",
        "updated_date": "2025-12-22T16:34:21+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Yongxin Wang",
            "Zhicheng Yang",
            "Meng Cao",
            "Mingfei Han",
            "Haokun Lin",
            "Yingying Zhu",
            "Xiaojun Chang",
            "Xiaodan Liang"
        ],
        "tldr": "The paper introduces CARE, a post-training framework for multimodal reasoning that leverages failure data to improve accuracy and training smoothness in verifiable visual-reasoning tasks. It combines anchored-contrastive learning and reflection-guided resampling to convert failures into supervision signals.",
        "tldr_zh": "该论文介绍了CARE，一个多模态推理的后训练框架，它利用失败数据来提高可验证视觉推理任务的准确性和训练平滑度。它结合了锚定对比学习和反射引导重采样，将失败转化为监督信号。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "基于可验证奖励的群组相对强化学习（RLVR）常常浪费其已有的最具信息量的数据——失败案例。当所有 rollout 都是错误时，梯度会停滞；当其中一个 rollout 恰好正确时，更新通常会忽略其他接近但错误的 rollout 的原因，并且 credit 会被错误地分配给虚假的链。我们提出 CARE (对比锚定反思)，这是一个以失败为中心的后训练框架，用于多模态推理，它可以将错误转化为监督。CARE 结合了：(i) 一个锚定对比目标，该目标围绕最佳 rollout 形成一个紧凑的子群，并包括一组语义上近似的困难负例，执行子群内 Z 分数归一化，并采用仅负例缩放，以及一个全负例救援机制，以防止零信号批次；以及 (ii) 反思引导重采样（RGR），这是一种单次结构化自我修复，它重写具有代表性的失败案例，并使用相同的验证器重新评分，无需任何测试时反思即可将近失案例转化为有用的正例。CARE 提高了准确性和训练平滑性，同时明确增加了来自失败案例的学习信号份额。在 Qwen2.5-VL-7B 上，CARE 在六个可验证的视觉推理基准测试中，宏平均准确率比 GRPO 提高了 4.6 个百分点；在 Qwen3-VL-8B 上，在相同的评估协议下，它在 MathVista 和 MMMU-Pro 上达到了具有竞争力的或最先进的结果。"
    },
    {
        "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
        "summary": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
        "url": "http://arxiv.org/abs/2512.19526v1",
        "published_date": "2025-12-22T16:18:00+00:00",
        "updated_date": "2025-12-22T16:18:00+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Li Puyin",
            "Tiange Xiang",
            "Ella Mao",
            "Shirley Wei",
            "Xinye Chen",
            "Adnan Masood",
            "Li Fei-fei",
            "Ehsan Adeli"
        ],
        "tldr": "The paper introduces QuantiPhy, a new benchmark for quantitatively evaluating the physical reasoning abilities of Vision-Language Models regarding kinematic properties like size, velocity, and acceleration.",
        "tldr_zh": "该论文介绍了QuantiPhy，一个新的基准测试，用于量化评估视觉-语言模型在运动学属性（如大小、速度和加速度）方面的物理推理能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "理解物理世界对于通用人工智能代理至关重要。然而，目前尚不清楚最先进的视觉感知模型（例如，大型视觉-语言模型，VLMs）是否能够定量地推理物理属性。现有的评估主要基于视觉问答(VQA)且侧重于定性分析，对于这些模型是否能够从视频观察中推断运动物体的运动学量缺乏深入的洞察。为了解决这个问题，我们提出了QuantiPhy，这是第一个旨在定量衡量VLM的物理推理能力的基准。QuantiPhy包含超过3.3K的具有数值真值的视频-文本实例，它通过将物体的大小、速度和加速度中的一个属性作为先验输入，来评估VLM在给定的时间戳下估计物体的大小、速度和加速度的性能。该基准统一了提示语和评分方式，以评估数值准确性，从而实现跨模型的公平比较。我们对最先进的VLMs进行的实验揭示了它们的定性合理性与实际数值正确性之间存在持续的差距。我们进一步深入分析了背景噪声、反事实先验和策略性Prompt等关键因素，发现最先进的VLMs在定量推理运动学属性时，严重依赖于预训练的世界知识，而不是忠实地使用提供的视觉和文本输入作为参考。QuantiPhy提供了第一个严谨、可扩展的测试平台，旨在推动VLMs超越单纯的口头合理性，迈向具有数值基础的物理理解。"
    },
    {
        "title": "An Agentic Framework for Autonomous Materials Computation",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
        "url": "http://arxiv.org/abs/2512.19458v1",
        "published_date": "2025-12-22T15:03:57+00:00",
        "updated_date": "2025-12-22T15:03:57+00:00",
        "categories": [
            "cs.AI",
            "cond-mat.mtrl-sci"
        ],
        "authors": [
            "Zeyu Xia",
            "Jinzhe Ma",
            "Congjie Zheng",
            "Shufei Zhang",
            "Yuqiang Li",
            "Hang Su",
            "P. Hu",
            "Changshui Zhang",
            "Xingao Gong",
            "Wanli Ouyang",
            "Lei Bai",
            "Dongzhan Zhou",
            "Mao Su"
        ],
        "tldr": "This paper introduces a domain-specialized agent powered by LLMs for automating first-principles materials computations, demonstrating improved accuracy and robustness compared to standalone LLMs on a new benchmark.",
        "tldr_zh": "该论文介绍了一个基于LLM的领域Agent，用于自动化第一原理材料计算，并在新的基准测试中展示了比独立LLM更高的准确性和鲁棒性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLMs）已成为加速科学发现的强大工具，但其静态知识和幻觉问题阻碍了自主研究应用。 近期进展将LLMs集成到代理框架中，从而能够进行检索、推理和工具使用，以实现复杂的科学工作流程。 在此，我们提出了一种领域专用型代理，旨在可靠地自动化第一性原理材料计算。 通过嵌入领域专业知识，该代理确保物理上连贯的多步骤工作流程，并始终选择收敛、适定的参数，从而实现可靠的端到端计算执行。 一个包含多样计算任务的新基准测试表明，我们的系统在准确性和鲁棒性方面均显著优于独立的LLMs。 这项工作为自主计算实验奠定了可验证的基础，并代表着朝着完全自动化的科学发现迈出的关键一步。"
    },
    {
        "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
        "summary": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
        "url": "http://arxiv.org/abs/2512.19396v1",
        "published_date": "2025-12-22T13:42:18+00:00",
        "updated_date": "2025-12-22T13:42:18+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Runze Li",
            "Yuwen Zhai",
            "Bo Xu",
            "LiWu Xu",
            "Nian Shi",
            "Wei Zhang",
            "Ran Lin",
            "Liang Wang"
        ],
        "tldr": "The paper introduces EchoTrail-GUI, a framework that equips GUI agents with a dynamic memory system, enabling them to learn from past experiences and improve performance on GUI automation tasks by storing and retrieving successful task trajectories.",
        "tldr_zh": "该论文介绍了 EchoTrail-GUI，一个为 GUI 代理配备动态记忆系统的框架，通过存储和检索成功的任务轨迹，使它们能够从过去的经验中学习并提高 GUI 自动化任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "得益于大型视觉-语言模型（VLMs）的进步，当代GUI代理的能力日益增强，但它们通常存在一个关键局限性：它们孤立地处理每个任务，缺乏从过去的成功经验中系统学习的机制。这种数字“失忆症”导致次优性能、重复错误以及对新挑战的泛化能力不足。为了弥补这一差距，我们引入了EchoTrail-GUI，这是一种旨在模仿类人经验学习的新型框架，通过为代理配备动态的、可访问的记忆。我们的框架分三个不同的阶段运行。首先，在经验探索阶段，代理自主地与GUI环境交互，以构建一个由奖励模型验证的、经过精心挑选的成功任务轨迹数据库。至关重要的是，整个知识库的构建过程是完全自动化的，无需人工干预。其次，在记忆注入阶段，当接收到新任务时，我们的系统会高效地检索最相关的历史轨迹，作为可操作的“记忆”。最后，在GUI任务推理阶段，这些记忆被注入为上下文指导，以指导代理的推理和决策过程。我们通过Android World和AndroidLab等基准测试证明了我们方法的有效性。结果表明，EchoTrail-GUI显著提高了基线代理的任务成功率和操作效率，验证了结构化记忆在创建更强大、更智能的GUI自动化方面的能力。"
    },
    {
        "title": "DeliveryBench: Can Agents Earn Profit in Real World?",
        "summary": "LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.",
        "url": "http://arxiv.org/abs/2512.19234v1",
        "published_date": "2025-12-22T10:17:49+00:00",
        "updated_date": "2025-12-22T10:17:49+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Lingjun Mao",
            "Jiawei Ren",
            "Kun Zhou",
            "Jixuan Chen",
            "Ziqiao Ma",
            "Lianhui Qin"
        ],
        "tldr": "The paper introduces DeliveryBench, a new benchmark for evaluating embodied agents in a realistic food delivery scenario, highlighting shortcomings of current VLMs in long-horizon planning and constraint satisfaction compared to humans.",
        "tldr_zh": "该论文提出了DeliveryBench，一个新的基准测试，用于评估具身智能体在现实的食物递送场景中的表现。研究表明，与人类相比，目前的视觉语言模型在长期规划和约束满足方面存在不足。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLMs）和视觉语言模型（VLMs）正日益被部署为具身智能体，然而现有的评测基准大多围绕简单的短期任务展开，难以捕捉塑造真实世界决策的丰富现实约束。为了弥补这一差距，我们提出了 DeliveryBench，这是一个城市尺度的具身评测基准，其基础是现实世界的外卖配送职业。外卖员通常在长期目标（在数小时内实现净利润最大化）下工作，同时管理各种约束，例如，配送截止时间、运输费用、车辆电量，以及与其他外卖员和客户的必要互动。DeliveryBench 在程序化生成的 3D 城市中实例化了这种设置，这些城市具有不同的道路网络、建筑物、功能位置、交通方式和真实的资源动态，从而能够对约束感知、长期规划进行系统评估。我们在九个城市中对一系列基于 VLM 的智能体进行了评测，并将它们与人类玩家进行了比较。我们的结果表明，与人类相比存在显著的性能差距，并发现这些智能体目光短浅，并且经常违反基本的常识约束。此外，我们观察到不同模型之间存在鲜明个性（例如，冒险型的 GPT-5 与保守型的 Claude），突显了当前基于 VLM 的具身智能体在现实的、约束密集的环境中既脆弱又多样。我们的代码、数据和评测基准可在 https://deliverybench.github.io 获取。"
    },
    {
        "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
        "summary": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.",
        "url": "http://arxiv.org/abs/2512.18878v1",
        "published_date": "2025-12-21T20:39:31+00:00",
        "updated_date": "2025-12-21T20:39:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kaidi Liang",
            "Ke Li",
            "Xianbiao Hu",
            "Ruwen Qin"
        ],
        "tldr": "The paper introduces CrashChat, a multimodal large language model built upon VideoLLaMA3, for multitask traffic crash video analysis. It utilizes instruction fine-tuning and a novel multitask learning strategy, achieving state-of-the-art performance on consolidated public datasets.",
        "tldr_zh": "本文介绍了CrashChat，一个基于VideoLLaMA3构建的多模态大型语言模型，用于多任务交通碰撞视频分析。它采用了指令微调和一种新颖的多任务学习策略，并在整合的公共数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自动化的碰撞视频分析对于利用日益增长的驾驶视频数据进行交通安全研究和自动驾驶中的责任归属至关重要。由于视频数据中碰撞事件复杂的时空动态以及涉及的多样化分析需求，碰撞视频分析是一个具有挑战性的多任务问题。它需要涵盖碰撞识别、时间定位和高层次视频理解的能力。然而，现有模型无法在统一框架内执行所有这些任务，并且此类模型的有效训练策略仍未得到充分探索。为了填补这些空白，本文提出了一种用于多任务交通碰撞分析的多模态大型语言模型（MLLM）CrashChat，其构建于VideoLLaMA3之上。CrashChat通过指令微调获取领域特定知识，并采用一种基于任务解耦和分组的新型多任务学习策略，该策略在任务组内部和跨任务组之间最大化联合学习的益处，同时减轻负迁移。在整合的公共数据集上的数值实验表明，CrashChat在所有模型尺度上都持续优于现有的MLLM和传统的基于视觉的方法，实现了最先进的性能。它在碰撞识别方面达到了近乎完美的准确率，在碰撞定位方面提高了176%，在更具挑战性的碰撞前定位方面提高了40%。与通用的MLLM相比，它在碰撞描述和推理任务中显著提高了文本准确性和内容覆盖率，BLEU评分提高了0.18-0.41，ROUGE评分提高了0.18-0.42。除了其强大的性能之外，CrashChat还是一个方便的、端到端的分析工具，可以随时进行实际部署。CrashChat的数据集和实现代码可在https://github.com/Liangkd/CrashChat 获取。"
    },
    {
        "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
        "summary": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.",
        "url": "http://arxiv.org/abs/2512.19443v1",
        "published_date": "2025-12-22T14:42:31+00:00",
        "updated_date": "2025-12-22T14:42:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Evelyn Zhang",
            "Fufu Yu",
            "Aoqi Wu",
            "Zichen Wen",
            "Ke Yan",
            "Shouhong Ding",
            "Biqing Qi",
            "Linfeng Zhang"
        ],
        "tldr": "The paper introduces D2Pruner, a novel token pruning method for MLLMs that combines debiased importance with structural diversity to address limitations of existing methods, achieving significant FLOPs reduction while maintaining or improving performance on both general understanding and fine-grained localization tasks.",
        "tldr_zh": "该论文介绍了一种名为D2Pruner的新型MLLM令牌剪枝方法，该方法结合了去偏重要性和结构多样性，以解决现有方法的局限性。该方法在降低FLOPs的同时，保持或提高了在通用理解和细粒度定位任务上的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "处理长视觉令牌序列给多模态大型语言模型（MLLMs）带来了巨大的计算负担。虽然令牌剪枝提供了一条加速的途径，但我们发现目前的方法虽然足以进行一般理解，但在细粒度定位任务上却会 катастрофически 失败。我们将这种失败归因于两种主流策略的固有缺陷：基于重要性的方法存在强烈的 positional bias，这是一种内在的模型伪像，会分散对语义内容的注意力，而基于多样性的方法则表现出结构盲视，忽略了用户的提示和空间冗余。为了解决这个问题，我们引入了D2Pruner，这是一个独特的框架，通过将去偏的重要性与结构剪枝机制相结合来纠正这些问题。我们的方法首先基于去偏的注意力分数来确保一个最关键令牌的核心集合作为枢轴。然后，它对其余的令牌执行最大独立集（MIS）选择，这些令牌在一个混合图上建模，其中边表示空间邻近性和语义相似性。这个过程迭代地保留最重要的和可用的令牌，同时移除其邻居，确保所选择的补充令牌能够最大化重要性和多样性。大量的实验表明，D2Pruner具有卓越的效率和保真度。应用于LLaVA-1.5-7B进行一般理解任务时，它在保留99.2%原始性能的同时，减少了74.2%的FLOPs。此外，在具有InternVL-2.5-8B的挑战性定位基准测试中，它在90%的令牌缩减率下保持了85.7%的性能，这是一个显著的进步，比现有方法提高了高达63.53%。"
    },
    {
        "title": "Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing",
        "summary": "Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.",
        "url": "http://arxiv.org/abs/2512.19302v1",
        "published_date": "2025-12-22T11:46:42+00:00",
        "updated_date": "2025-12-22T11:46:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Zhang",
            "Junyao Ge",
            "Yang Zheng",
            "Kaitai Guo",
            "Jimin Liang"
        ],
        "tldr": "The paper introduces Think2Seg-RS, a decoupled LVLM-SAM framework for remote sensing image segmentation that uses reinforcement learning to train the LVLM to generate geometric prompts for SAM, achieving strong performance and zero-shot generalization.",
        "tldr_zh": "该论文介绍了 Think2Seg-RS，一个解耦的 LVLM-SAM 框架，用于遥感图像分割。该框架使用强化学习训练 LVLM 生成 SAM 的几何提示，实现了强大的性能和零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型视觉-语言模型（LVLMs）在推进遥感(RS)分析方面具有巨大潜力，然而现有的推理分割框架通过端到端监督微调将语言推理和像素预测耦合在一起，导致几何基础薄弱，且跨任务泛化能力有限。为了解决这个问题，我们开发了Think2Seg-RS，一个解耦框架，训练一个LVLM提示器，通过结构化的几何提示来控制一个冻结的Segment Anything模型（SAM）。通过仅使用mask的强化学习目标，LVLM学习将抽象的语义推理转化为空间上确定的动作，在EarthReason数据集上实现了最先进的性能。值得注意的是，学习到的提示策略可以零样本泛化到多个指代分割基准，揭示了语义层面和实例层面grounding之间明显的差异。我们还发现，在语义层面监督下，紧凑的分割器优于更大的分割器，并且负面提示在异构的航空背景中无效。总而言之，这些发现将语义层面推理分割确立为地理空间理解的新范式，从而为统一的、可解释的LVLM驱动的地球观测开辟了道路。我们的代码和模型可在https://github.com/Ricardo-XZ/Think2Seg-RS获取。"
    },
    {
        "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
        "summary": "Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.",
        "url": "http://arxiv.org/abs/2512.19159v1",
        "published_date": "2025-12-22T08:55:23+00:00",
        "updated_date": "2025-12-22T08:55:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wendong Bu",
            "Kaihang Pan",
            "Yuze Lin",
            "Jiacheng Li",
            "Kai Shen",
            "Wenqiao Zhang",
            "Juncheng Li",
            "Jun Xiao",
            "Siliang Tang"
        ],
        "tldr": "OmniMoGen is a unified framework for human motion generation that uses interleaved text-motion instructions, offering capabilities like compositional editing and self-reflective generation, demonstrated on a new large-scale dataset and benchmark.",
        "tldr_zh": "OmniMoGen是一个统一的人体运动生成框架，它使用交错的文本-运动指令，提供诸如组合编辑和自我反思生成等功能，并在一个新的大规模数据集和基准上进行了演示。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLMs）已在单个框架内统一了各种语言任务，然而这种统一性在人体运动生成领域仍未得到探索。现有方法局限于孤立的任务，限制了自由形式和全目标生成的灵活性。为了解决这个问题，我们提出了OmniMoGen，一个统一的框架，它通过交错的文本-动作指令实现多功能的运动生成。OmniMoGen构建于简洁的RVQ-VAE和Transformer架构之上，支持端到端的指令驱动的运动生成。我们构建了X2Mo，一个包含超过13.7万条交错文本-动作指令的大规模数据集，并引入了AnyContext，一个用于评估交错运动生成的基准。实验表明，OmniMoGen在文本到运动、运动编辑和AnyContext上均达到了最先进的性能，展现出诸如组合编辑、自我反思生成和知识驱动生成等新兴能力。这些结果标志着向下一代智能运动生成迈出了一步。项目主页：https://OmniMoGen.github.io/。"
    },
    {
        "title": "Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding",
        "summary": "Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)",
        "url": "http://arxiv.org/abs/2512.19070v1",
        "published_date": "2025-12-22T06:20:53+00:00",
        "updated_date": "2025-12-22T06:20:53+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Ruiqi Ma",
            "Yu Yan",
            "Chunhong Zhang",
            "Minghao Yin",
            "XinChao Liu",
            "Zhihong Jin",
            "Zheng Hu"
        ],
        "tldr": "This paper introduces a training-free Hallucination Disentangled Decoding (HDD) method to mitigate object hallucinations in Large Vision-Language Models by enhancing the original image with segmentation and using a blank image to reduce language prior dependence.",
        "tldr_zh": "本文介绍了一种无需训练的 Hallucination Disentangled Decoding (HDD) 方法，通过分割图像并使用空白图像来减少对语言先验的依赖，从而减轻大型视觉语言模型中的对象幻觉。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型视觉-语言模型（LVLMs）弥合了视觉和语言模态之间的差距，在各个领域展现出强大的潜力。然而，尽管取得了显著进展，LVLMs仍然在物体识别任务中存在严重的幻觉问题。这些模型经常无法准确识别某些物体，导致文本生成虽然流利，但与视觉内容不符，这在现实世界的应用中可能产生严重的后果。最近，已经提出了几种方法来减轻LVLM幻觉，但大多数方法仅侧重于减少语言模态中的幻觉。为了减轻语言和视觉模态中的幻觉，我们提出了一种无需训练的幻觉解耦解码（HDD）方法。HDD通过分割原始图像并选择增强原始图像的图像来增强原始图像，同时利用空白图像来消除原始图像和分割图像中的语言先验幻觉。这种设计不仅降低了模型对语言先验的依赖，而且提高了其视觉性能。（代码：https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding）"
    },
    {
        "title": "6DAttack: Backdoor Attacks in the 6DoF Pose Estimation",
        "summary": "Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.",
        "url": "http://arxiv.org/abs/2512.19058v1",
        "published_date": "2025-12-22T05:49:57+00:00",
        "updated_date": "2025-12-22T05:49:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jihui Guo",
            "Zongmin Zhang",
            "Zhen Sun",
            "Yuhao Yang",
            "Jinlin Wu",
            "Fu Zhang",
            "Xinlei He"
        ],
        "tldr": "This paper introduces 6DAttack, a novel backdoor attack framework for 6DoF pose estimation using 3D object triggers, demonstrating high attack success rates on various models and datasets while maintaining clean performance and bypassing a representative defense.",
        "tldr_zh": "本文介绍了6DAttack，一种新颖的6DoF姿态估计后门攻击框架，它使用3D对象触发器，在保持正常性能的同时，在各种模型和数据集上展示了高攻击成功率，并绕过了一种代表性的防御。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "深度学习的进步使得精确的目标六自由度 (6DoF) 位姿估计成为可能，该技术被广泛应用于机器人、增强现实/虚拟现实和自主系统中。然而，后门攻击构成了重大的安全风险。尽管大多数研究集中在2D视觉上，但6DoF位姿估计在很大程度上仍未被探索。与仅改变类别的传统后门不同，6DoF攻击必须控制平移和旋转等连续参数，导致2D方法不适用。我们提出了6DAttack，一个使用3D物体触发器来诱导受控的错误位姿，同时保持正常行为的框架。在LINEMOD、YCB-Video和CO3D数据集上对PVNet、DenseFusion和PoseDiffusion的评估表明，该方法具有很高的攻击成功率 (ASR)，且不影响干净性能。后门模型实现了高达100%的干净ADD精度和100%的ASR，触发样本达到了97.70%的ADD-P。此外，一种代表性的防御方法仍然无效。我们的发现揭示了6DoF位姿估计中一个严重且未被充分探索的威胁。"
    },
    {
        "title": "Scaling Online Distributionally Robust Reinforcement Learning: Sample-Efficient Guarantees with General Function Approximation",
        "summary": "The deployment of reinforcement learning (RL) agents in real-world applications is often hindered by performance degradation caused by mismatches between training and deployment environments. Distributionally robust RL (DR-RL) addresses this issue by optimizing worst-case performance over an uncertainty set of transition dynamics. However, existing work typically relies on substantial prior knowledge-such as access to a generative model or a large offline dataset-and largely focuses on tabular methods that do not scale to complex domains. We overcome these limitations by proposing an online DR-RL algorithm with general function approximation that learns an optimal robust policy purely through interaction with the environment, without requiring prior models or offline data, enabling deployment in high-dimensional tasks. We further provide a theoretical analysis establishing a near-optimal sublinear regret bound under a total variation uncertainty set, demonstrating the sample efficiency and effectiveness of our method.",
        "url": "http://arxiv.org/abs/2512.18957v1",
        "published_date": "2025-12-22T02:12:04+00:00",
        "updated_date": "2025-12-22T02:12:04+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Debamita Ghosh",
            "George K. Atia",
            "Yue Wang"
        ],
        "tldr": "This paper introduces an online Distributionally Robust Reinforcement Learning (DR-RL) algorithm with general function approximation, achieving near-optimal regret bounds without prior knowledge or offline data. It addresses the challenge of deploying RL agents in environments with distribution shifts.",
        "tldr_zh": "本文提出了一种在线的基于通用函数逼近的分布鲁棒强化学习算法 (DR-RL)，无需先验知识或离线数据即可实现近乎最优的遗憾界限。 它解决了在具有分布偏移的环境中部署 RL agents 的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "强化学习 (RL) 智能体在真实世界中的部署常常受到训练和部署环境不匹配导致的性能下降所阻碍。分布鲁棒强化学习 (DR-RL) 通过在转移动态的不确定性集合上优化最差情况性能来解决这个问题。然而，现有的工作通常依赖于大量的先验知识——例如访问生成模型或大型离线数据集——并且主要集中在无法扩展到复杂领域的表格方法上。我们通过提出一种具有通用函数逼近的在线 DR-RL 算法来克服这些限制，该算法仅通过与环境的交互来学习最优的鲁棒策略，而无需先验模型或离线数据，从而能够在高维任务中部署。我们进一步提供了理论分析，建立了在全变差不确定性集合下的近最优亚线性遗憾界，证明了我们方法的样本效率和有效性。"
    },
    {
        "title": "Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization",
        "summary": "Collecting large-scale naturalistic driving data is essential for training robust autonomous driving planners. However, real-world datasets often contain a substantial amount of repetitive and low-value samples, which lead to excessive storage costs and bring limited benefits to policy learning. To address this issue, we propose an information-theoretic data pruning method that effectively reduces the training data volume without compromising model performance. Our approach evaluates the trajectory distribution information entropy of driving data and iteratively selects high-value samples that preserve the statistical characteristics of the original dataset in a model-agnostic manner. From a theoretical perspective, we show that maximizing trajectory entropy effectively constrains the Kullback-Leibler divergence between the pruned subset and the original data distribution, thereby maintaining generalization ability. Comprehensive experiments on the NuPlan benchmark with a large-scale imitation learning framework demonstrate that the proposed method can reduce the dataset size by up to 40% while maintaining closed-loop performance. This work provides a lightweight and theoretically grounded approach for scalable data management and efficient policy learning in autonomous driving systems.",
        "url": "http://arxiv.org/abs/2512.19270v1",
        "published_date": "2025-12-22T11:07:18+00:00",
        "updated_date": "2025-12-22T11:07:18+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Zhaoyang Liu",
            "Weitao Zhou",
            "Junze Wen",
            "Cheng Jing",
            "Qian Cheng",
            "Kun Jiang",
            "Diange Yang"
        ],
        "tldr": "This paper presents a data pruning method for autonomous driving datasets, which uses trajectory entropy maximization to reduce data volume by up to 40% while maintaining closed-loop performance on the NuPlan benchmark.",
        "tldr_zh": "本文提出了一种自动驾驶数据集的数据剪枝方法，该方法利用轨迹熵最大化来减少高达 40% 的数据量，同时在 NuPlan 基准测试中保持闭环性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "收集大规模自然驾驶数据对于训练鲁棒的自动驾驶规划器至关重要。然而，真实世界的数据集通常包含大量重复且低价值的样本，这会导致过高的存储成本，并对策略学习带来的益处有限。为了解决这个问题，我们提出了一种基于信息论的数据剪枝方法，该方法可以在不损害模型性能的前提下有效地减少训练数据量。我们的方法评估驾驶数据的轨迹分布信息熵，并以模型无关的方式迭代选择能够保留原始数据集统计特征的高价值样本。从理论角度来看，我们证明了最大化轨迹熵能够有效地约束剪枝后的子集与原始数据分布之间的Kullback-Leibler散度，从而保持泛化能力。在NuPlan基准数据集上的大规模模仿学习框架中进行的全面实验表明，所提出的方法可以在保持闭环性能的前提下，将数据集大小最多减少40%。这项工作为自动驾驶系统中可扩展的数据管理和高效策略学习提供了一种轻量级且具有理论基础的方法。"
    },
    {
        "title": "EGM: Efficiently Learning General Motion Tracking Policy for High Dynamic Humanoid Whole-Body Control",
        "summary": "Learning a general motion tracking policy from human motions shows great potential for versatile humanoid whole-body control. Conventional approaches are not only inefficient in data utilization and training processes but also exhibit limited performance when tracking highly dynamic motions. To address these challenges, we propose EGM, a framework that enables efficient learning of a general motion tracking policy. EGM integrates four core designs. Firstly, we introduce a Bin-based Cross-motion Curriculum Adaptive Sampling strategy to dynamically orchestrate the sampling probabilities based on tracking error of each motion bin, eficiently balancing the training process across motions with varying dificulty and durations. The sampled data is then processed by our proposed Composite Decoupled Mixture-of-Experts (CDMoE) architecture, which efficiently enhances the ability to track motions from different distributions by grouping experts separately for upper and lower body and decoupling orthogonal experts from shared experts to separately handle dedicated features and general features. Central to our approach is a key insight we identified: for training a general motion tracking policy, data quality and diversity are paramount. Building on these designs, we develop a three-stage curriculum training flow to progressively enhance the policy's robustness against disturbances. Despite training on only 4.08 hours of data, EGM generalized robustly across 49.25 hours of test motions, outperforming baselines on both routine and highly dynamic tasks.",
        "url": "http://arxiv.org/abs/2512.19043v1",
        "published_date": "2025-12-22T05:25:24+00:00",
        "updated_date": "2025-12-22T05:25:24+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Chao Yang",
            "Yingkai Sun",
            "Peng Ye",
            "Xin Chen",
            "Chong Yu",
            "Tao Chen"
        ],
        "tldr": "The paper introduces EGM, a framework using curriculum learning and a Composite Decoupled Mixture-of-Experts architecture to efficiently train a general motion tracking policy for humanoid robots, achieving robust performance on highly dynamic motions with limited training data.",
        "tldr_zh": "该论文介绍了EGM，一个使用课程学习和复合解耦混合专家架构的框架，旨在高效训练人形机器人的通用运动跟踪策略，仅用有限的训练数据即可在高度动态的运动中实现稳健的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "从人类运动中学习通用运动跟踪策略，在多功能人形机器人全身控制方面显示出巨大潜力。传统方法不仅数据利用和训练过程效率低下，而且在跟踪高动态运动时表现出有限的性能。为解决这些挑战，我们提出EGM，一个能够高效学习通用运动跟踪策略的框架。EGM整合了四个核心设计。首先，我们引入了一种基于Bin的跨运动课程自适应采样策略，以根据每个运动组的跟踪误差动态地编排采样概率，从而有效地平衡不同难度和持续时间的运动训练过程。采样的数据随后由我们提出的复合解耦混合专家模型（CDMoE）架构处理，该架构通过分别对上半身和下半身进行专家分组，并将正交专家从共享专家中解耦，以分别处理专用特征和通用特征，从而高效地增强了跟踪来自不同分布的运动的能力。我们方法的核心是我们发现的一个关键洞察：对于训练通用运动跟踪策略而言，数据质量和多样性至关重要。基于这些设计，我们开发了一个三阶段课程训练流程，以逐步提高策略对扰动的鲁棒性。尽管仅在4.08小时的数据上进行训练，但EGM在49.25小时的测试运动中表现出强大的泛化能力，并在常规和高动态任务上均优于基线方法。"
    },
    {
        "title": "DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners",
        "summary": "Autonomous buses run on fixed routes but must operate in open, dynamic urban environments. Disengagement events on these routes are often geographically concentrated and typically arise from planner failures in highly interactive regions. Such policy-level failures are difficult to correct using conventional imitation learning, which easily overfits to sparse disengagement data. To address this issue, this paper presents a Disengagement-Triggered Contrastive Continual Learning (DTCCL) framework that enables autonomous buses to improve planning policies through real-world operation. Each disengagement triggers cloud-based data augmentation that generates positive and negative samples by perturbing surrounding agents while preserving route context. Contrastive learning refines policy representations to better distinguish safe and unsafe behaviors, and continual updates are applied in a cloud-edge loop without human supervision. Experiments on urban bus routes demonstrate that DTCCL improves overall planning performance by 48.6 percent compared with direct retraining, validating its effectiveness for scalable, closed-loop policy improvement in autonomous public transport.",
        "url": "http://arxiv.org/abs/2512.18988v1",
        "published_date": "2025-12-22T02:59:37+00:00",
        "updated_date": "2025-12-22T02:59:37+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yanding Yang",
            "Weitao Zhou",
            "Jinhai Wang",
            "Xiaomin Guo",
            "Junze Wen",
            "Xiaolong Liu",
            "Lang Ding",
            "Zheng Fu",
            "Jinyu Miao",
            "Kun Jiang",
            "Diange Yang"
        ],
        "tldr": "The paper introduces DTCCL, a contrastive continual learning framework that uses disengagement events to improve autonomous bus planning by augmenting data and refining policy representations in a cloud-edge loop.",
        "tldr_zh": "该论文介绍了一种名为DTCCL的对比式持续学习框架，该框架利用脱离事件通过在云-边缘循环中增强数据和改进策略表示来改善自动驾驶巴士的规划。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自动驾驶巴士在固定路线上运行，但必须在开放、动态的城市环境中运行。在这些路线上发生的脱离事件往往在地理位置上集中，并且通常由高交互区域中规划器的故障引起。这种策略层面的故障很难使用传统的模仿学习进行纠正，因为模仿学习容易过拟合稀疏的脱离数据。为了解决这个问题，本文提出了一种脱离触发的对比持续学习(DTCCL)框架，该框架使自动驾驶巴士能够通过实际运行来改进规划策略。每次脱离都会触发基于云的数据增强，通过扰动周围的智能体并保留路线上下文来生成正样本和负样本。对比学习改进了策略表示，以更好地区分安全和不安全行为，并且在没有人工监督的情况下，在云-边缘循环中应用持续更新。在城市公交线路上的实验表明，与直接重新训练相比，DTCCL将整体规划性能提高了48.6%，验证了其在自动驾驶公共交通中可扩展的闭环策略改进的有效性。"
    },
    {
        "title": "A Framework for Deploying Learning-based Quadruped Loco-Manipulation",
        "summary": "Quadruped mobile manipulators offer strong potential for agile loco-manipulation but remain difficult to control and transfer reliably from simulation to reality. Reinforcement learning (RL) shows promise for whole-body control, yet most frameworks are proprietary and hard to reproduce on real hardware. We present an open pipeline for training, benchmarking, and deploying RL-based controllers on the Unitree B1 quadruped with a Z1 arm. The framework unifies sim-to-sim and sim-to-real transfer through ROS, re-implementing a policy trained in Isaac Gym, extending it to MuJoCo via a hardware abstraction layer, and deploying the same controller on physical hardware. Sim-to-sim experiments expose discrepancies between Isaac Gym and MuJoCo contact models that influence policy behavior, while real-world teleoperated object-picking trials show that coordinated whole-body control extends reach and improves manipulation over floating-base baselines. The pipeline provides a transparent, reproducible foundation for developing and analyzing RL-based loco-manipulation controllers and will be released open source to support future research.",
        "url": "http://arxiv.org/abs/2512.18938v1",
        "published_date": "2025-12-22T01:19:26+00:00",
        "updated_date": "2025-12-22T01:19:26+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yadong Liu",
            "Jianwei Liu",
            "He Liang",
            "Dimitrios Kanoulas"
        ],
        "tldr": "The paper presents an open-source framework for training and deploying RL-based loco-manipulation controllers on a quadruped robot, addressing sim-to-real transfer and providing a reproducible pipeline for future research.",
        "tldr_zh": "该论文提出了一个开源框架，用于在四足机器人上训练和部署基于强化学习的定位操纵控制器，解决了从模拟到现实的迁移问题，并为未来的研究提供了一个可重现的流程。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "四足移动操作机器人具有敏捷的位移操作的巨大潜力，但仍然难以控制并可靠地从仿真迁移到现实。强化学习（RL）在全身控制方面展现出希望，但大多数框架是专有的，难以在真实硬件上复现。我们提出了一个开放的流程，用于在配备Z1机械臂的宇树B1四足机器人上训练、基准测试和部署基于RL的控制器。该框架通过ROS统一了仿真到仿真和仿真到现实的迁移，重新实现了在Isaac Gym中训练的策略，通过硬件抽象层将其扩展到MuJoCo，并在物理硬件上部署相同的控制器。仿真到仿真的实验揭示了Isaac Gym和MuJoCo接触模型之间的差异，这些差异会影响策略行为，而现实世界的遥操作物体拾取试验表明，协调的全身控制扩大了触及范围，并改善了相对于浮动基座基线情况下的操作性能。该流程为开发和分析基于RL的位移操作控制器提供了一个透明且可复现的基础，并将开源发布以支持未来的研究。"
    },
    {
        "title": "Optimizing Robotic Placement via Grasp-Dependent Feasibility Prediction",
        "summary": "In this paper, we study whether inexpensive, physics-free supervision can reliably prioritize grasp-place candidates for budget-aware pick-and-place. From an object's initial pose, target pose, and a candidate grasp, we generate two path-aware geometric labels: path-wise inverse kinematics (IK) feasibility across a fixed approach-grasp-lift waypoint template, and a transit collision flag from mesh sweeps along the same template. A compact dual-output MLP learns these signals from pose encodings, and at test time its scores rank precomputed candidates for a rank-then-plan policy under the same IK gate and planner as the baseline. Although learned from cheap labels only, the scores transfer to physics-enabled executed trajectories: at a fixed planning budget the policy finds successful paths sooner with fewer planner calls while keeping final success on par or better. This work targets a single rigid cuboid with side-face grasps and a fixed waypoint template, and we outline extensions to varied objects and richer waypoint schemes.",
        "url": "http://arxiv.org/abs/2512.18922v1",
        "published_date": "2025-12-21T23:47:09+00:00",
        "updated_date": "2025-12-21T23:47:09+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Tianyuan Liu",
            "Richard Dazeley",
            "Benjamin Champion",
            "Akan Cosgun"
        ],
        "tldr": "This paper presents a method for predicting grasp-place feasibility using a dual-output MLP trained on geometric labels, improving the efficiency of pick-and-place tasks in robotics.",
        "tldr_zh": "本文提出了一种通过训练一个双输出MLP来预测抓取-放置可行性的方法，该MLP使用几何标签进行训练，从而提高了机器人抓取和放置任务的效率。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文研究了廉价、无物理约束的监督信号是否能可靠地对预算感知的抓取-放置候选方案进行优先级排序。给定对象的初始姿态、目标姿态和一个候选抓取姿态，我们生成两个路径感知的几何标签：经过固定姿态-抓取-抬起路点模版的路径上的逆运动学（IK）可行性，以及沿相同模版进行网格扫描得到的平动碰撞标志。一个紧凑的双输出多层感知机（MLP）从姿态编码中学习这些信号，并在测试时，其得分对预先计算的候选方案进行排序，以用于与基线方法采用相同IK门控和规划器的rank-then-plan策略。尽管仅从廉价标签中学习，但这些得分可以迁移到借助物理引擎实现的执行轨迹中：在固定的规划预算下，该策略能更快地找到可行路径，规划器调用次数更少，同时保持或提高最终的成功率。本文针对具有侧面抓取的单个刚性长方体和固定的路点模版，并概述了扩展到各种对象和更丰富的路点策略的方案。"
    },
    {
        "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
        "summary": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
        "url": "http://arxiv.org/abs/2512.19551v1",
        "published_date": "2025-12-22T16:31:30+00:00",
        "updated_date": "2025-12-22T16:31:30+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Jiawen Wang",
            "Jingjing Wang Tianyang Chen",
            "Min Zhang",
            "Guodong Zhou"
        ],
        "tldr": "This paper introduces a new task, LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG), and proposes a method (ES-MoE) to address the challenges of emotion decoupling and scenario adaptation in generating emotional motion for embodied agents in unseen situations for LLMs.",
        "tldr_zh": "本文介绍了一项新任务：以LLM为中心的终身共情运动生成（L^2-EMG），并提出了一种名为ES-MoE的方法，旨在解决在使用LLM为具身智能体在未知情况下生成情感运动时，面临的情感解耦和场景适应的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "现有的人类中心情感动作生成方法主要集中于提升在单一尺度固定的数据集上的性能，很大程度上忽略了灵活且尺度递增的动作场景（如运动、舞蹈），然而有效地学习这些新兴场景可以显著增强模型在现实世界的泛化能力。受此启发，本文提出了一种新的以LLM为中心的终身共情动作生成（L^2-EMG）任务，旨在使LLM具备在不同未见场景中持续获取情感动作生成知识的能力，从而有助于构建具备共情能力和智能的闭环且自我演进的具身智能体。此外，本文提出了L^2-EMG任务中的两个关键挑战，即情感解耦挑战和场景适应挑战。为此，本文提出了一种情感可迁移和场景自适应的专家混合模型（ES-MoE）方法，该方法分别设计了一个因果引导的情感解耦模块和一个场景自适应的专家构建模块，以应对这两个挑战。特别地，本文构建了多个L^2-EMG数据集来验证ES-MoE方法的有效性。大量的评估结果表明，ES-MoE优于先进的基线方法。"
    },
    {
        "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
        "summary": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
        "url": "http://arxiv.org/abs/2512.19516v1",
        "published_date": "2025-12-22T16:08:03+00:00",
        "updated_date": "2025-12-22T16:08:03+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Xueming Yan",
            "Bo Yin",
            "Yaochu Jin"
        ],
        "tldr": "The paper introduces Latent Causal Diffusion Model (LacaDM) for MORL to learn causal relationships between states and policies, improving generalization and performance in complex multiobjective tasks.",
        "tldr_zh": "该论文介绍了用于多目标强化学习（MORL）的潜在因果扩散模型（LacaDM），旨在学习状态和策略之间的因果关系，从而提高复杂多目标任务中的泛化能力和性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "多目标强化学习（MORL）由于目标之间固有的冲突以及适应动态环境的困难而面临着严峻的挑战。传统方法通常难以有效地泛化，尤其是在大型且复杂的状态-动作空间中。为了解决这些局限性，我们引入了潜在因果扩散模型（LacaDM），这是一种旨在增强多目标强化学习在离散和连续环境中适应性的新方法。与主要解决目标之间冲突的现有方法不同，LacaDM学习环境状态和策略之间的潜在时间因果关系，从而能够在不同的多目标强化学习场景中实现高效的知识迁移。通过将这些因果结构嵌入到基于扩散模型的框架中，LacaDM在冲突的目标之间实现了平衡，同时在以前未见过的环境中保持了强大的泛化能力。来自MOGymnasium框架的各种任务的实证评估表明，LacaDM在超体积、稀疏性和期望效用最大化方面始终优于最先进的基线，展示了其在复杂多目标任务中的有效性。"
    },
    {
        "title": "Learning General Policies with Policy Gradient Methods",
        "summary": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
        "url": "http://arxiv.org/abs/2512.19366v1",
        "published_date": "2025-12-22T13:08:58+00:00",
        "updated_date": "2025-12-22T13:08:58+00:00",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Simon Ståhlberg",
            "Blai Bonet",
            "Hector Geffner"
        ],
        "tldr": "This paper explores how policy gradient methods can be used to learn general policies that generalize across different instances of a domain, similar to combinatorial planning. They use GNNs for policy representation and address limitations related to GNN expressiveness and the optimality-generalization tradeoff.",
        "tldr_zh": "本文探讨了如何使用策略梯度方法学习在同一领域的不同实例中泛化的通用策略，类似于组合规划。他们使用GNN进行策略表示，并解决了与GNN表达能力以及最优性和泛化性之间权衡相关的限制。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "强化学习方法虽然在多个场景中取得了显著成果，但泛化能力（即可靠且系统地生成可泛化策略的能力）仍然是一个挑战。泛化问题已在经典规划中得到了正式研究，在经典规划中，可以使用组合方法学习在给定领域的所有实例上都能泛化的、可证明正确的策略。这项工作的目的是将这两个研究方向结合起来，阐明在什么条件下，（深度）强化学习方法，特别是策略优化方法，可以像组合方法一样用于学习可泛化的策略。我们借鉴了之前组合方法和深度学习方法中的经验，并以一种便捷的方式对其进行了扩展。从前者来看，我们将策略建模为状态转移分类器，因为（底层）动作是不通用的，并且随着实例的不同而变化。从后者来看，我们使用适用于处理关系结构的图神经网络（GNN）来表示规划状态上的价值函数，以及在本例中，表示策略。有了这些要素，我们发现可以使用Actor-Critic方法来学习几乎与使用组合方法获得的策略一样好，同时避免了可扩展性瓶颈和特征池的使用。此外，DRL方法在所考虑基准上的局限性与深度学习或强化学习算法关系不大，而是源于GNN的众所周知的表达能力限制，以及最优性和泛化之间的权衡（通用策略在某些领域不可能最优）。通过添加派生谓词和优化的替代成本结构，可以在不改变基本DRL方法的情况下解决这两个限制。"
    },
    {
        "title": "First-Order Representation Languages for Goal-Conditioned RL",
        "summary": "First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.",
        "url": "http://arxiv.org/abs/2512.19355v1",
        "published_date": "2025-12-22T12:54:32+00:00",
        "updated_date": "2025-12-22T12:54:32+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Simon Ståhlberg",
            "Hector Geffner"
        ],
        "tldr": "This paper explores using first-order languages to represent states and goals in goal-conditioned RL, showing improved performance on large planning instances with sparse rewards by automatically creating a curriculum of easier goals.",
        "tldr_zh": "该论文探讨了在目标条件强化学习中使用一阶语言来表示状态和目标，并通过自动创建难度递增的简单目标课程，在具有稀疏奖励的大型规划实例上表现出改进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "一阶关系语言已被用于MDP规划和强化学习(RL)中，主要有两个目的：以紧凑的形式指定MDP，以及表示和学习不依赖于特定实例或状态空间的通用策略。在这项工作中，我们转而考虑在一阶语言中应用目标条件强化学习和广义规划。问题在于当训练实例很大且仅靠随机探索无法达到目标时，如何学习目标条件和通用策略。回溯经验重放(HER)技术为此提供了一种解决方案：它通过将原始目标替换为实际达到的目标，将不成功的轨迹重新标记为成功的轨迹。如果目标策略必须在状态和目标之间泛化，那么未达到原始目标状态的轨迹可以实现更高的数据和时间效率的学习。在这项工作中，我们表明当状态和目标由原子集合表示时，可以实现进一步的性能提升。我们考虑了三个版本：目标作为完整状态，目标作为原始目标的子集，以及目标作为这些子目标的提升版本。结果表明，后两个版本通过自动创建难度递增的更简单目标的课程，成功地在具有稀疏奖励的大型规划实例上学习了通用策略。实验结果展示了这些版本的计算优势、局限性以及解决这些局限的机会。"
    },
    {
        "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
        "summary": "Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking",
        "url": "http://arxiv.org/abs/2512.19154v1",
        "published_date": "2025-12-22T08:50:30+00:00",
        "updated_date": "2025-12-22T08:50:30+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Geraud Nangue Tasse",
            "Matthew Riemer",
            "Benjamin Rosman",
            "Tim Klinger"
        ],
        "tldr": "This paper introduces an Adaptive Stacking meta-algorithm that learns to manage memory in non-Markovian environments, achieving computational and memory savings compared to traditional frame stacking.",
        "tldr_zh": "本文介绍了一种自适应堆叠元算法，该算法学习在非马尔可夫环境中管理内存，与传统的帧堆叠相比，实现了计算和内存节省。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于序列模型的通用智能体近年来的成功，使得人们更加关注如何在远比受控环境复杂得多的现实世界中部署计算能力有限的智能体。在这些更逼真的环境中遇到的一个关键挑战是，相对于智能体的观察，存在高度非马尔可夫依赖关系，这在小型受控领域中并不常见。文献中处理此问题的常见方法是将最近的一系列观察堆叠在一起（帧堆叠），但此窗口大小必须随着非马尔可夫依赖关系的程度而增长，从而导致动作推断和学习的计算和内存需求过于庞大。本文受到这样一个洞察的启发：在许多相对于时间而言高度非马尔可夫的环境中，环境仅因果地依赖于该时间尺度上相对较少数量的观察。一个自然的方向是考虑保持相对较小的自适应记忆堆栈的元算法，以便在考虑每一步观察较少的情况下表达关于时间的高度非马尔可夫依赖关系，从而在计算和内存需求方面实现显著节省。因此，我们提出了一种元算法（自适应堆叠）来实现这一点，并具有收敛性保证，并量化了基于 MLP、LSTM 和 Transformer 的智能体的计算和内存约束的减少。我们的实验利用了流行的记忆任务，这使我们能够控制非马尔可夫依赖关系的程度。这使我们能够证明，适当的元算法可以在不过度移除重要经验的情况下，学习移除那些不能预测未来奖励的记忆。代码：https://github.com/geraudnt/adaptive-stacking"
    },
    {
        "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
        "summary": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
        "url": "http://arxiv.org/abs/2512.19107v1",
        "published_date": "2025-12-22T07:21:07+00:00",
        "updated_date": "2025-12-22T07:21:07+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Zhe Yang",
            "Xiaoshuang Sheng",
            "Zhengnan Zhang",
            "Jidong Wu",
            "Zexing Wang",
            "Xin He",
            "Shenghua Xu",
            "Guanjing Xiong"
        ],
        "tldr": "The paper introduces FC-MIR, a framework for mobile UI intent prediction using frame-compressed multimodal trajectory reasoning, achieving efficient intent summarization with MLLMs while exploring operation suggestion generation.",
        "tldr_zh": "该论文介绍了FC-MIR，一个基于帧压缩多模态轨迹推理的移动UI意图预测框架。该框架利用MLLM实现高效的意图总结，并探索了操作建议的生成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "从移动UI操作轨迹中识别用户意图对于提升UI理解能力和实现任务自动化代理至关重要。尽管多模态大型语言模型(MLLM)在视频理解任务中表现出色，但其实时移动部署受到沉重计算成本和低效冗余帧处理的限制。为了解决这些问题，我们提出了FC-MIR框架：它利用关键帧采样和自适应拼接，减少视觉冗余以提高推理效率，同时集成最先进的闭源MLLM或微调模型（例如，Qwen3-VL）来进行轨迹摘要和意图预测。我们进一步扩展了任务范围，探索生成预测后的操作和搜索建议，并引入了一种细粒度指标来评估摘要、预测和建议的实际效用。为了进行严格评估，我们构建了一个UI轨迹数据集，涵盖了来自UI代理（Agent-I）和真实用户互动（Person-I）的场景。实验结果表明，我们的压缩方法在50%-60%的压缩率下保持了性能；闭源和微调的MLLM都表现出强大的意图摘要能力，支持潜在的轻量级设备端部署。然而，MLLM在有用的和“令人惊喜的”建议方面仍然存在不足，留下改进空间。最后，我们在真实环境中部署了该框架，整合了UI感知和UI代理，为该领域未来的发展奠定了基础。"
    },
    {
        "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
        "summary": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.",
        "url": "http://arxiv.org/abs/2512.19546v1",
        "published_date": "2025-12-22T16:28:27+00:00",
        "updated_date": "2025-12-22T16:28:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziqiao Peng",
            "Yi Chen",
            "Yifeng Ma",
            "Guozhen Zhang",
            "Zhiyao Sun",
            "Zixiang Zhou",
            "Youliang Zhang",
            "Zhengguang Zhou",
            "Zhaoxin Fan",
            "Hongyan Liu",
            "Yuan Zhou",
            "Qinglin Lu",
            "Jun He"
        ],
        "tldr": "ActAvatar introduces a novel framework for generating talking avatars with precise action control and temporal alignment between actions and audio, leveraging phase-aware attention and progressive audio-visual alignment.",
        "tldr_zh": "ActAvatar 提出了一个新颖的框架，用于生成具有精确动作控制和动作与音频之间时间对齐的会说话的头像，利用了相位感知注意力和渐进式音频-视觉对齐。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "摘要：\n尽管在会说话的虚拟化身生成方面取得了显著进展，但现有方法仍面临严峻挑战：对于多样化动作的文本遵循能力不足、动作与音频内容之间缺乏时间对齐，以及对姿势骨骼等额外控制信号的依赖。我们提出了ActAvatar，该框架通过文本指导实现动作控制的相位级精度，捕捉动作语义和时间上下文。我们的方法引入了三个核心创新：（1）相位感知交叉注意力（PACA），它将提示分解为全局基础块和时间锚定相位块，使模型能够专注于与相位相关的token，以实现精确的时间-语义对齐；（2）渐进式视听对齐，它将模态影响与分层特征学习过程对齐——早期层优先考虑文本以建立动作结构，而更深层则强调音频以细化嘴唇运动，从而防止模态干扰；（3）一个两阶段训练策略，首先在多样化数据上建立鲁棒的视听对应关系，然后通过在结构化标注上进行微调来注入动作控制，从而保持视听对齐和模型的文本遵循能力。大量实验表明，ActAvatar在动作控制和视觉质量方面均显著优于最先进的方法。"
    },
    {
        "title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
        "summary": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.",
        "url": "http://arxiv.org/abs/2512.19283v1",
        "published_date": "2025-12-22T11:26:41+00:00",
        "updated_date": "2025-12-22T11:26:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kyungwon Cho",
            "Hanbyul Joo"
        ],
        "tldr": "The paper introduces HaMoS, a sequence-level diffusion framework for reconstructing full-body motion from egocentric videos, using intermittent hand cues and addressing challenges of real-world egocentric devices with a novel augmentation method and local attention mechanism.",
        "tldr_zh": "该论文介绍了HaMoS，一种序列级别的扩散框架，用于从以自我为中心的视频中重建全身运动，使用间歇性的手部线索，并通过一种新颖的增强方法和局部注意力机制来解决现实世界中以自我为中心的设备的挑战。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自中心视觉系统正变得越来越普及，为人机交互创造了新的机会。一个核心挑战是从第一人称视角视频中估计穿戴者的全身运动，这对于理解人类行为至关重要。然而，这项任务十分困难，因为从自中心视角来看，大部分身体部位都是不可见的。以往的方法主要依赖于头部轨迹，导致模糊性；或者假设手部被连续追踪，这对于轻量级的自中心设备来说是不现实的。在这项工作中，我们提出了HaMoS，这是第一个手势感知的、序列级别的扩散框架，它直接以头部轨迹和由视野限制与遮挡导致的间歇性可见的手部线索为条件，就像在真实的自中心设备中一样。为了克服缺乏将多样化的相机视角与人体运动配对的数据集的问题，我们引入了一种新颖的增强方法，该方法对这种现实世界的条件进行建模。我们还证明了序列级别的上下文，如体型和视场，对于准确的运动重建至关重要，因此我们采用局部注意力来高效地推断长序列。在公共基准上的实验表明，我们的方法实现了最先进的精度和时间平滑性，为可靠的野外自中心3D运动理解迈出了实际的一步。"
    },
    {
        "title": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
        "summary": "Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.",
        "url": "http://arxiv.org/abs/2512.19088v1",
        "published_date": "2025-12-22T06:57:42+00:00",
        "updated_date": "2025-12-22T06:57:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Khanh Nguyen",
            "Dasith de Silva Edirimuni",
            "Ghulam Mubashar Hassan",
            "Ajmal Mian"
        ],
        "tldr": "This paper proposes a new method for retrieving objects from 3D scenes using a box-guided open-vocabulary instance segmentation approach that leverages a 2D detector to generate 3D instance masks, improving efficiency and handling of rare object categories compared to existing methods.",
        "tldr_zh": "本文提出了一种新的方法，用于从3D场景中检索对象，该方法采用框引导式开放词汇实例分割方法，利用2D检测器生成3D实例掩码，与现有方法相比，提高了效率并能处理稀有对象类别。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "从场景级点云中定位和检索物体是一项具有挑战性的问题，在机器人技术和增强现实领域有着广泛的应用。这项任务通常被表述为开放词汇的 3D 实例分割。尽管最近的方法表现出强大的性能，但它们严重依赖 SAM 和 CLIP 从伴随点云的图像中生成和分类 3D 实例掩码，导致大量的计算开销和缓慢的处理速度，限制了它们在现实世界中的部署。Open-YOLO 3D 通过使用实时 2D 检测器来分类由预训练的 3D 分割器直接从点云生成的类别无关掩码来缓解这个问题，从而消除了对 SAM 和 CLIP 的需求，并显著减少了推理时间。然而，Open-YOLO 3D 通常无法推广到 3D 训练数据中不常出现的物体类别。在本文中，我们提出了一种方法，该方法利用 2D 开放词汇检测器的引导，从 RGB 图像中为新物体生成 3D 实例掩码。 我们的方法继承了 2D 检测器识别新物体的能力，同时保持高效的分类，从而能够从开放式文本查询中快速准确地检索罕见实例。我们的代码将在 https://github.com/ndkhanh360/BoxOVIS 上提供。"
    },
    {
        "title": "Decoupled Generative Modeling for Human-Object Interaction Synthesis",
        "summary": "Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.",
        "url": "http://arxiv.org/abs/2512.19049v1",
        "published_date": "2025-12-22T05:33:59+00:00",
        "updated_date": "2025-12-22T05:33:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hwanhee Jung",
            "Seunggwan Lee",
            "Jeongyoon Yoon",
            "SeungHyeon Kim",
            "Giljoo Nam",
            "Qixing Huang",
            "Sangpil Kim"
        ],
        "tldr": "The paper introduces DecHOI, a decoupled generative model for synthesizing realistic human-object interactions by separating path planning and action synthesis, achieving superior performance in benchmarks.",
        "tldr_zh": "该论文介绍了DecHOI，一种解耦生成模型，通过分离路径规划和动作合成来合成逼真的人与物体交互，并在基准测试中实现了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "合成逼真的人体-物体交互(HOI)对于3D计算机视觉和机器人技术至关重要，是动画和具身控制的基础。现有方法通常需要手动指定中间路径点，并将所有优化目标都放置在一个网络中，这增加了复杂性，降低了灵活性，并导致诸如人体和物体运动不同步或穿透等错误。为了解决这些问题，我们提出了用于人体-物体交互合成的解耦生成建模（DecHOI），它将路径规划和动作合成分离。轨迹生成器首先生成没有人为指定路径点的人体和物体轨迹，然后动作生成器以这些路径为条件来合成详细的动作。为了进一步提高接触的真实性，我们采用对抗训练，并使用一个专注于远端关节动力学的判别器。该框架还对移动的对应对象进行建模，并支持动态场景中响应迅速的长序列规划，同时保持规划的一致性。在FullBodyManipulation和3D-FUTURE这两个基准测试中，DecHOI在大多数定量指标和定性评估上都优于先前的方法，感知研究同样更倾向于我们的结果。"
    },
    {
        "title": "VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion",
        "summary": "Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.\n  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.\n  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2512.18954v1",
        "published_date": "2025-12-22T02:05:45+00:00",
        "updated_date": "2025-12-22T02:05:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zaidao Han",
            "Risa Higashita",
            "Jiang Liu"
        ],
        "tldr": "This paper introduces VOIC, a novel dual-decoder network that decouples monocular 3D semantic scene completion into visible-region perception and occluded-region reasoning, achieving state-of-the-art performance on SemanticKITTI and SSCBench-KITTI360.",
        "tldr_zh": "本文介绍了VOIC，一种新型双解码器网络，将单目 3D 语义场景补全分解为可见区域感知和遮挡区域推理，在SemanticKITTI和SSCBench-KITTI360上实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于相机的3D语义场景补全(SSC)是自动驾驶和机器人场景理解的关键任务。它旨在从单张图像推断语义和几何体的完整3D体素表示。现有方法通常侧重于端到端的2D到3D特征提升和体素补全。然而，它们往往忽略了单图像输入导致的高置信度可见区域感知和低置信度遮挡区域推理之间的干扰，这可能导致特征稀释和误差传播。\n\n为了解决这些挑战，我们引入了一种离线可见区域标签提取(VRLE)策略，该策略显式地从密集3D真值中分离和提取可见区域的体素级监督。该策略净化了两个互补子任务的监督空间：可见区域感知和遮挡区域推理。基于这个想法，我们提出了可见-遮挡交互补全网络(VOIC)，这是一种新颖的双解码器框架，它将SSC明确地解耦为可见区域语义感知和遮挡区域场景补全。 VOIC首先通过将图像特征与深度导出的占据率融合来构建基础3D体素表示。可见解码器专注于生成高保真度的几何和语义先验，而遮挡解码器利用这些先验以及跨模态交互来执行连贯的全局场景推理。\n\n在SemanticKITTI和SSCBench-KITTI360 benchmark上的大量实验表明，VOIC在几何补全和语义分割精度方面均优于现有的单目SSC方法，实现了最先进的性能。"
    },
    {
        "title": "Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions",
        "summary": "Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.",
        "url": "http://arxiv.org/abs/2512.19104v1",
        "published_date": "2025-12-22T07:18:57+00:00",
        "updated_date": "2025-12-22T07:18:57+00:00",
        "categories": [
            "math.OC",
            "cs.LG"
        ],
        "authors": [
            "Haishan Ye"
        ],
        "tldr": "This paper introduces a rank-based zeroth-order optimization algorithm for stochastic functions with ordinal feedback and proves its query complexity matches state-of-the-art value-based methods, even with only ordinal information.",
        "tldr_zh": "本文提出了一种基于排序的零阶优化算法，用于处理具有序数反馈的随机函数，并证明即使仅使用序数信息，其查询复杂度也能与最先进的基于价值的方法相匹配。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于排序反馈的零阶（ZO）优化已成为现代机器学习系统中的一个基本问题，尤其是在人机交互环境中，例如基于人类反馈的强化学习、偏好学习和进化策略。尽管基于排序的ZO算法在经验上取得了显著成功和鲁棒性，但其理论理解仍然有限，尤其是在随机目标和标准平滑性假设下。在本文中，我们研究了针对随机函数的基于排序的零阶优化，其中仅能获得随机函数的排序反馈。我们提出了一种简单且计算高效的基于排序的ZO算法。在包括平滑性、强凸性和随机梯度的有界二阶矩在内的标准假设下，我们建立了凸和非凸目标的显式非渐近查询复杂度界限。值得注意的是，我们的结果与基于值的ZO算法的最优已知查询复杂度相匹配，表明仅凭排序信息足以在随机设置中实现最优查询效率。我们的分析不同于现有的基于漂移和信息几何的技术，为研究噪声环境下的基于排序的优化提供了新的工具。这些发现缩小了理论与实践之间的差距，并为人类偏好驱动的优化提供了原则性的基础。"
    },
    {
        "title": "Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness",
        "summary": "The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.",
        "url": "http://arxiv.org/abs/2512.19155v1",
        "published_date": "2025-12-22T08:52:07+00:00",
        "updated_date": "2025-12-22T08:52:07+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yin Jun Phua"
        ],
        "tldr": "This paper explores theories of consciousness (GWT, IIT, HOT) by implementing artificial agents, performing ablations, and observing the functional consequences, revealing dissociations suggesting complementary functional roles of these theories. They find that GWT provides broadcast capacity, and HOT provides quality control.",
        "tldr_zh": "本文通过构建人工代理来研究意识理论(GWT, IIT, HOT)，对其进行消融实验并观察其功能结果。研究发现这些理论描述的是互补的功能层级。GWT提供广播能力，而HOT提供质量控制。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "寻找可靠的意识指标已分裂为相互竞争的理论阵营（全局工作空间理论(GWT)、整合信息理论(IIT)和高阶理论(HOT)），每个理论都提出了不同的神经特征。我们采用一种合成神经现象学方法：构建体现这些机制的人工智能体，通过在生物系统中不可能实现的精确架构消融来测试它们的功能后果。在三个实验中，我们报告了解离，表明这些理论描述的是互补的功能层级，而不是竞争性的解释。在实验 1 中，一个无重连的自我模型损伤消除了元认知校准，同时保留了一阶任务的表现，产生了一个与 HOT 预测一致的合成盲视类似物。在实验 2 中，工作空间容量被证明是信息访问的因果必需因素：一个完整的工作空间病灶导致访问相关标记的定性崩溃，而部分减少则显示出渐进式退化，这与 GWT 的点火框架一致。在实验 3 中，我们发现了一种广播-放大效应：GWT 风格的广播放大了内部噪声，导致了极度脆弱性。B2 智能体家族对相同的潜在扰动具有鲁棒性；这种鲁棒性在自我模型关闭/工作空间读取控制中仍然存在，告诫我们不要将该效应仅归因于 $z_{\\text{self}}$ 压缩。我们还報告了一个明确的负面结果：原始扰动复杂性 (PCI-A) 在工作空间瓶颈下降低，告诫我们反对将 IIT 相关代理天真地转移到工程智能体中。这些结果表明了一种分层设计原则：GWT 提供广播能力，而 HOT 提供质量控制。我们强调，我们的智能体并非有意识；它们是用于测试意识理论功能预测的参考实现。"
    },
    {
        "title": "$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
        "summary": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $γ(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.",
        "url": "http://arxiv.org/abs/2512.19084v1",
        "published_date": "2025-12-22T06:48:53+00:00",
        "updated_date": "2025-12-22T06:48:53+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Mark Burgess"
        ],
        "tldr": "The paper proposes a knowledge representation method using Semantic Spacetime graphs and promise theoretic semantics to bridge the gap between vectorized ML and Knowledge Graphs, aiming for efficient reasoning under uncertainty, particularly in robotics and autonomous systems.",
        "tldr_zh": "该论文提出了一种使用语义时空图和承诺理论语义的知识表示方法，旨在弥合向量化机器学习和知识图之间的差距，目标是在不确定性下进行高效推理，尤其是在机器人和自主系统中。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 6,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "“注意力”的语义和动态特性与为自主代理设计的承诺理论概念密切相关，因此可以很容易地用承诺框架进行描述。通过这种方式，可以在向量化机器学习和知识图谱表示之间建立桥梁，而无需隐式地依赖语言模型。我们对知识的期望预设了一定程度的统计稳定性，即在重复观察下的平均不变性，或者说对数据的“信任”。学习网络和知识图谱表示可以有意义地共存，以保留数据的不同方面。向量化数据对于概率估计是有用的，而图谱则在数据分片的情况下也能保留来源的意图。使用语义时空$γ(3,4)$图，可以避免复杂的本体论，转而通过其在语义过程中的作用对特征进行分类。后者有利于在不确定条件下进行推理的方法。适当关注因果边界条件可能会显著压缩上下文确定所需的数据量级，这在自主机器人、国防部署和临时紧急服务等背景下是必需的。"
    },
    {
        "title": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training",
        "summary": "In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.",
        "url": "http://arxiv.org/abs/2512.19528v1",
        "published_date": "2025-12-22T16:18:45+00:00",
        "updated_date": "2025-12-22T16:18:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marc Peral",
            "Guillem Capellera",
            "Luis Ferraz",
            "Antonio Rubio",
            "Antonio Agudo"
        ],
        "tldr": "The paper presents a multi-modal transformer-based approach for soccer scene analysis, focusing on ball trajectory inference, ball state classification, and ball possessor identification, using player trajectories, player types, and image crops as input. They introduce a modality-specific masking pre-training strategy called CropDrop and demonstrate improvements over state-of-the-art methods.",
        "tldr_zh": "本文提出了一种基于多模态Transformer的足球场景分析方法，重点关注球的轨迹推断、球的状态分类和持球者识别，使用球员轨迹、球员类型和图像切片作为输入。他们引入了一种名为CropDrop的特定模态掩码预训练策略，并展示了相对于现有技术的改进。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "在这项工作中，我们提出了一种多模态架构，用于分析战术摄像机镜头中的足球场景，重点关注三个核心任务：球轨迹推断、球状态分类和持球者识别。为此，我们的解决方案将三种不同的输入模态（球员轨迹、球员类型和单个球员的图像剪裁）集成到一个统一的框架中，该框架使用级联的社交时空Transformer模块来处理空间和时间动态。与先前严重依赖精确球跟踪或手工设计启发式方法的方法不同，我们的方法无需直接访问球的过去或未来位置即可推断出球的轨迹，并在真实的顶级联赛比赛中，在嘈杂或遮挡的条件下稳健地识别球的状态和持球者。我们还引入了CropDrop，一种模态特定的掩码预训练策略，可防止过度依赖图像特征，并鼓励模型在预训练期间依赖跨模态模式。我们在大型数据集上展示了我们方法的有效性，在所有任务中均显著优于最先进的基线。我们的结果突出了在基于Transformer的架构中结合结构化和视觉线索的优势，以及在多模态学习中现实掩码策略的重要性。"
    },
    {
        "title": "AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction",
        "summary": "Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking.\" These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future\" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead\" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.",
        "url": "http://arxiv.org/abs/2512.19150v1",
        "published_date": "2025-12-22T08:46:59+00:00",
        "updated_date": "2025-12-22T08:46:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruikai Li",
            "Xinrun Li",
            "Mengwei Xie",
            "Hao Shan",
            "Shoumeng Qiu",
            "Xinyuan Chang",
            "Yizhe Fan",
            "Feng Xiong",
            "Han Jiang",
            "Yilong Ren",
            "Haiyang Yu",
            "Mu Xu",
            "Yang Long",
            "Varun Ojha",
            "Zhiyong Cui"
        ],
        "tldr": "The paper introduces AMap, a novel online HD map construction framework that uses future information to improve current-frame perception in forward regions, addressing a critical safety flaw in existing methods which are backward-looking.",
        "tldr_zh": "该论文介绍了AMap，一种新颖的在线高清地图构建框架，它使用未来信息来改进当前帧对前方区域的感知，解决了现有方法中向后看的关键安全缺陷。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "在线高精地图（HD Map）构建对于自动驾驶至关重要。虽然近期的研究方法利用历史时序融合来提升性能，但我们发现这种范式存在一个关键的安全缺陷：它本质上是“空间后向看的”。这些方法主要增强已行驶区域的地图重建，而对于前方未见道路的改善甚微。至关重要的是，我们对下游规划任务的分析揭示了一个严重的不对称性：虽然后向感知误差通常是可容忍的，但前向区域的不准确性会直接导致危险的驾驶行为。为了弥合这一安全差距，我们提出 AMap，这是一个用于前瞻感知的在线高精地图构建的新框架。我们开创了一种“从未来提炼”的范式，其中一个具有访问未来时序上下文特权的教师模型引导一个仅限于当前帧的轻量级学生模型。这个过程隐式地将前瞻知识压缩到学生模型中，赋予它“前瞻”能力，且无需额外的推理时间开销。在技术上，我们引入了一种具有空间掩码的多层BEV蒸馏策略和一个非对称查询适应模块，以有效地将未来感知的表征转移到学生的静态查询中。在nuScenes和Argoverse 2基准上的大量实验表明，AMap显著增强了当前帧感知。最值得注意的是，它在关键的前向区域超越了最先进的时序模型，同时保持了单当前帧推理的效率。"
    },
    {
        "title": "Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition",
        "summary": "Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.",
        "url": "http://arxiv.org/abs/2512.19036v1",
        "published_date": "2025-12-22T05:13:58+00:00",
        "updated_date": "2025-12-22T05:13:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyang Li",
            "Mingming Lu",
            "Ruiqi Wang",
            "Hao Li",
            "Zewei Le"
        ],
        "tldr": "This paper introduces CLIP-SPM, a framework for few-shot action recognition that uses text prompts and prototype modulation to address temporal modeling, visual similarity, and modality gap challenges; it achieves competitive performance on standard benchmarks.",
        "tldr_zh": "本文介绍了一种用于少样本动作识别的框架CLIP-SPM，该框架使用文本提示和原型调制来解决时间建模、视觉相似性和模态差距的挑战；并在标准基准测试上获得了竞争性的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "小样本动作识别旨在使模型能够从有限的标注样本中快速学习新的动作类别，从而解决现实应用中数据稀缺的挑战。当前的研究主要解决三个核心挑战：（1）时间建模，模型容易受到无关静态背景信息的干扰，难以捕捉动态动作特征的本质；（2）视觉相似性，具有细微视觉差异的类别难以区分；（3）视觉-文本支持原型与纯视觉查询之间的模态差异，这使得在共享嵌入空间内的对齐变得复杂。为了解决这些挑战，本文提出了一种CLIP-SPM框架，它包含三个组成部分：（1）分层协同运动细化（HSMR）模块，它对齐深层和浅层运动特征，通过减少静态背景干扰来改进时间建模；（2）语义原型调制（SPM）策略，它生成与查询相关的文本提示，以弥合模态差异并将它们与视觉特征集成，从而增强相似动作之间的区分能力；（3）原型-锚点双重调制（PADM）方法，它细化支持原型并将查询特征与全局语义锚点对齐，从而提高支持样本和查询样本之间的一致性。在包括Kinetics、SSv2-Full、SSv2-Small、UCF101和HMDB51在内的标准基准上的全面实验表明，我们的CLIP-SPM在1-shot、3-shot和5-shot设置下都取得了具有竞争力的性能。广泛的消融研究和可视化分析进一步验证了每个组件的有效性及其对解决核心挑战的贡献。源代码和模型已在GitHub上公开。"
    },
    {
        "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
        "summary": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",
        "url": "http://arxiv.org/abs/2512.19564v1",
        "published_date": "2025-12-22T16:46:40+00:00",
        "updated_date": "2025-12-22T16:46:40+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Yanliang Huang",
            "Xia Yan",
            "Peiran Yin",
            "Zhenduo Zhang",
            "Zeyan Shao",
            "Youran Wang",
            "Haoliang Huang",
            "Matthias Althoff"
        ],
        "tldr": "This paper presents the results of the 2024 CommonRoad Motion Planning Competition for autonomous vehicles, comparing different planners on standardized benchmarks focusing on efficiency, safety, comfort, and rule compliance.",
        "tldr_zh": "本文介绍了2024年CommonRoad自动驾驶运动规划竞赛的结果，比较了不同规划器在标准化基准上的表现，重点关注效率、安全性、舒适性和规则遵守。",
        "relevance_score": 4,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "在过去的十年中，已经开发了各种用于自动驾驶车辆的运动规划方法，以应对日益复杂的交通场景。然而，这些方法很少在标准化基准上进行比较，这限制了对相对优势和劣势的评估。为了弥补这一差距，我们介绍了 2024 年举行的第四届 CommonRoad 运动规划竞赛的设置和结果，该竞赛是使用 CommonRoad 基准套件进行的。这项年度竞赛为运动规划算法的基准测试提供了一个开源且可复现的框架。基准场景涵盖高速公路和城市环境，包含各种交通参与者，包括乘用车、公共汽车和自行车。规划器的性能从四个维度进行评估：效率、安全性、舒适性和对选定交通规则的遵守情况。本报告介绍了比赛形式，并提供了 2023 年和 2024 年代表性的高性能规划器的比较。"
    },
    {
        "title": "Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements",
        "summary": "This paper tackles the problem of estimating the relative position, orientation, and velocity between a UAV and a planar platform undergoing arbitrary 3D motion during approach and landing. The estimation relies on measurements from Inertial Measurement Units (IMUs) mounted on both systems, assuming there is a suitable communication channel to exchange data, together with visual information provided by an onboard monocular camera, from which the bearing (line-of-sight direction) to the platform's center and the normal vector of its planar surface are extracted. We propose a cascade observer with a complementary filter on SO(3) to reconstruct the relative attitude, followed by a linear Riccati observer for relative position and velocity estimation. Convergence of both observers is established under persistently exciting conditions, and the cascade is shown to be almost globally asymptotically and locally exponentially stable. We further extend the design to the case where the platform's rotation is restricted to its normal axis and show that its measured linear acceleration can be exploited to recover the remaining unobservable rotation angle. A sufficient condition to ensure local exponential convergence in this setting is provided. The performance of the proposed observers is validated through extensive simulations.",
        "url": "http://arxiv.org/abs/2512.19245v1",
        "published_date": "2025-12-22T10:28:20+00:00",
        "updated_date": "2025-12-22T10:28:20+00:00",
        "categories": [
            "eess.SY",
            "cs.RO"
        ],
        "authors": [
            "Tarek Bouazza",
            "Alessandro Melis",
            "Soulaimane Berkane",
            "Robert Mahony",
            "Tarek Hamel"
        ],
        "tldr": "This paper presents a cascade observer for estimating the relative state (position, orientation, velocity) between a UAV and a moving platform using IMU and monocular camera data, with proven convergence and demonstrated performance in simulations.",
        "tldr_zh": "本文提出了一种级联观测器，利用IMU和单目相机数据估计无人机和移动平台之间的相对状态（位置、姿态、速度），并证明了其收敛性，并通过仿真验证了性能。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "本文研究了无人机在接近和着陆过程中，估计其与进行任意三维运动的平面平台之间的相对位置、姿态和速度的问题。该估计依赖于安装在两个系统上的惯性测量单元(IMU)的测量数据，假设存在合适的通信信道来交换数据，以及机载单目相机提供的视觉信息，基于视觉信息提取平台中心的方向（视线方向）及其平面表面的法向量。我们提出了一种级联观测器，使用SO(3)上的互补滤波器重构相对姿态，然后使用线性Riccati观测器估计相对位置和速度。在持续激励条件下，证明了两个观测器的收敛性，并表明该级联几乎全局渐近稳定且局部指数稳定。我们进一步将设计扩展到平台旋转限制在其法轴上的情况，并表明可以利用其测量的线性加速度来恢复剩余的不可观测旋转角。提供了确保此设置下局部指数收敛性的充分条件。通过大量仿真验证了所提出的观测器的性能。"
    },
    {
        "title": "PalpAid: Multimodal Pneumatic Tactile Sensor for Tissue Palpation",
        "summary": "The tactile properties of tissue, such as elasticity and stiffness, often play an important role in surgical oncology when identifying tumors and pathological tissue boundaries. Though extremely valuable, robot-assisted surgery comes at the cost of reduced sensory information to the surgeon; typically, only vision is available. Sensors proposed to overcome this sensory desert are often bulky, complex, and incompatible with the surgical workflow. We present PalpAid, a multimodal pneumatic tactile sensor equipped with a microphone and pressure sensor, converting contact force into an internal pressure differential. The pressure sensor acts as an event detector, while the auditory signature captured by the microphone assists in tissue delineation. We show the design, fabrication, and assembly of sensory units with characterization tests to show robustness to use, inflation-deflation cycles, and integration with a robotic system. Finally, we show the sensor's ability to classify 3D-printed hard objects with varying infills and soft ex vivo tissues. Overall, PalpAid aims to fill the sensory gap intelligently and allow improved clinical decision-making.",
        "url": "http://arxiv.org/abs/2512.19010v1",
        "published_date": "2025-12-22T03:53:09+00:00",
        "updated_date": "2025-12-22T03:53:09+00:00",
        "categories": [
            "eess.SP",
            "cs.RO"
        ],
        "authors": [
            "Devi Yuliarti",
            "Ravi Prakash",
            "Hiu Ching Cheung",
            "Amy Strong",
            "Patrick J. Codd",
            "Shan Lin"
        ],
        "tldr": "The paper presents PalpAid, a multimodal pneumatic tactile sensor for robot-assisted surgery that uses a microphone and pressure sensor to identify tissue properties and improve clinical decision-making by addressing the sensory gap in robot-assisted surgery.",
        "tldr_zh": "该论文提出了 PalpAid，一种用于机器人辅助手术的多模态气动触觉传感器，它使用麦克风和压力传感器来识别组织特性，并通过解决机器人辅助手术中的感觉缺失来改进临床决策。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "在外科肿瘤学中，组织的触觉特性，如弹性与硬度，通常在识别肿瘤和病理组织边界方面起着重要作用。尽管机器人辅助手术极具价值，但其代价是降低了外科医生的感觉信息；通常，只有视觉可用。为了克服这种感觉缺失而提出的传感器往往体积庞大、结构复杂，且与手术流程不兼容。我们提出了PalpAid，一种配备麦克风和压力传感器的多模态气动触觉传感器，可将接触力转换为内部压差。压力传感器用作事件检测器，而麦克风捕获的听觉特征有助于组织划界。我们展示了传感单元的设计、制造和组装，并通过表征测试来证明其使用的稳健性、充气-放气循环性能以及与机器人系统的集成。最后，我们展示了该传感器对具有不同填充密度的3D打印硬质物体和离体软组织的分类能力。总而言之，PalpAid旨在智能地填补感觉缺失，并改善临床决策。"
    },
    {
        "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
        "summary": "Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.",
        "url": "http://arxiv.org/abs/2512.19387v1",
        "published_date": "2025-12-22T13:36:26+00:00",
        "updated_date": "2025-12-22T13:36:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yueyao Chen",
            "Kai-Ni Wang",
            "Dario Tayupo",
            "Arnaud Huaulm'e",
            "Krystel Nyangoh Timoh",
            "Pierre Jannin",
            "Qi Dou"
        ],
        "tldr": "The paper introduces DSTED, a dual-pathway framework for surgical workflow recognition that improves stability and discrimination by decoupling temporal stabilization and ambiguous phase enhancement. It achieves state-of-the-art performance on a surgical dataset.",
        "tldr_zh": "该论文提出了DSTED，一个用于手术工作流程识别的双路径框架，通过解耦时间稳定化和模糊阶段增强来提高稳定性和区分度。它在一个手术数据集上实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "目的：手术流程识别能够在计算机辅助干预中实现情境感知辅助和技能评估。尽管近年来取得了进展，但当前方法仍面临两个关键挑战：连续帧之间的预测抖动以及对模糊阶段的区分能力不足。本文旨在开发一个稳定的框架，通过选择性地传播可靠的历史信息并显式地建模不确定性以增强困难样本。\n方法：我们提出了一个双通路框架 DSTED，包含可靠记忆传播（RMP）和不确定性感知原型检索（UPR）。RMP 通过多标准可靠性评估来过滤和融合高置信度的历史特征，从而保持时间连贯性。UPR 从高不确定性样本中构建可学习的类特定原型，并执行自适应原型匹配以优化模糊帧表示。最后，一个置信度驱动的门控机制根据预测确定性动态平衡两个通路。\n结果：我们的方法在 AutoLaparo-hysterectomy 数据集上取得了最先进的性能，准确率达到 84.36%，F1 分数达到 65.51%，分别超过第二优的方法 3.51% 和 4.88%。消融实验表明，RMP (2.19%) 和 UPR (1.93%) 带来了互补增益，并且结合使用时具有协同效应。广泛的分析证实了时间抖动的显著降低以及对具有挑战性的阶段转换的显著改进。\n结论：我们的双通路设计引入了一种用于稳定流程识别的新范例，表明解耦时间一致性和阶段模糊性的建模可以产生卓越的性能和临床适用性。"
    },
    {
        "title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
        "summary": "Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.",
        "url": "http://arxiv.org/abs/2512.19275v1",
        "published_date": "2025-12-22T11:19:46+00:00",
        "updated_date": "2025-12-22T11:19:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ivan DeAndres-Tame",
            "Chengwei Ye",
            "Ruben Tolosana",
            "Ruben Vera-Rodriguez",
            "Shiqi Yu"
        ],
        "tldr": "This paper evaluates the biometric fidelity of gait patterns in animations generated by state-of-the-art GenAI models, finding that while visual quality is high, biometric identification performance is low, indicating that current methods rely on visual attributes rather than temporal dynamics.",
        "tldr_zh": "本文评估了最先进的GenAI模型生成的动画中步态模式的生物识别保真度，发现虽然视觉质量很高，但生物识别身份验证性能很低，表明目前的方法依赖于视觉属性而不是时间动态。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "生成式人工智能（GenAI）模型已经彻底改变了动画领域，其能够以卓越的视觉保真度合成人物和运动模式。然而，生成真正逼真的人物动画仍然是一项艰巨的挑战，即使是细微的不一致也可能使主体显得不自然。当人工智能生成的视频用于行为生物识别评估时，这种局限性尤为关键，因为定义身份的细微运动线索很容易丢失或扭曲。本研究调查了最先进的GenAI人物动画模型是否能够保留通过步态生物识别进行人员识别所需的细微时空细节。具体而言，我们评估了四种不同的GenAI模型，涵盖两个主要评估任务，以评估它们的能力，即 i) 在不同复杂程度的条件下从参考视频中恢复步态模式，以及 ii) 将这些步态模式转移到不同的视觉身份。我们的结果表明，虽然视觉质量通常很高，但在专注于识别的任务中，生物特征保真度仍然很低，表明当前的GenAI模型难以将身份与运动分离。此外，通过身份转移任务，我们揭示了基于外观的步态识别的一个根本缺陷：当纹理与运动分离时，识别就会崩溃，这证明了当前的GenAI模型依赖于视觉属性而不是时间动态。"
    },
    {
        "title": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
        "summary": "As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided \"Pretrain-then-Reinforce\" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.",
        "url": "http://arxiv.org/abs/2512.19001v1",
        "published_date": "2025-12-22T03:39:43+00:00",
        "updated_date": "2025-12-22T03:39:43+00:00",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Lingjie Zhao",
            "Xue Yu",
            "Yongzhi Qi",
            "Hao Hu",
            "Jianshen Zhang",
            "Yingzheng Ma",
            "Shuyu Han",
            "Wei Qi",
            "Zuo-Jun Max Shen"
        ],
        "tldr": "This paper introduces ORPR, an OR-guided pretrain-then-reinforce learning framework for inventory management, achieving significant real-world improvements at JD.com by aligning AI with OR principles.",
        "tldr_zh": "该论文介绍了一种OR引导的预训练-强化学习框架ORPR，用于库存管理。通过将人工智能与运筹学原理相结合，该框架在京东实现了显著的实际改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "随着人工智能(AI)和运筹学(OR)在处理复杂库存系统中的协同作用越来越受到重视，一个关键挑战依然存在：如何有效地协调AI的自适应感知和OR的结构严谨性。为了弥合这一差距，我们提出了一种新颖的OR引导的“预训练-再强化”框架。为了提供结构化的指导，我们提出了一种模拟增强的OR模型，该模型生成高质量的参考决策，隐式地捕捉复杂的业务约束和管理偏好。利用这些OR派生的决策作为基础训练标签，我们设计了一个领域感知的深度学习基础模型，以建立基础决策能力，然后进行强化学习(RL)微调阶段。独特的是，我们将RL定位为一种深度对齐机制，使AI代理能够内化OR的最优性原则，同时利用探索进行通用策略改进，并允许专家指导针对特定场景进行适应（例如，促销活动）。通过广泛的数值实验以及京东的现场部署并辅以差异中差异(DiD)分析进行验证，我们的模型显著优于现有的工业实践，带来了现实世界的收益，包括周转天数减少5.27天，现货率提高2.29%，同时持有成本降低29.95%。与目前普遍存在的暴力模型扩展趋势相反，我们的研究表明，当以结构化的OR逻辑为指导时，轻量级的、领域感知的模型可以提供最先进的性能和强大的可迁移性。这种方法为智能供应链管理提供了一个可扩展且具有成本效益的范例，突出了AI与OR深度对齐的价值。"
    },
    {
        "title": "ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation",
        "summary": "Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.",
        "url": "http://arxiv.org/abs/2512.18991v1",
        "published_date": "2025-12-22T03:13:08+00:00",
        "updated_date": "2025-12-22T03:13:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Gyeongrok Oh",
            "Youngdong Jang",
            "Jonghyun Choi",
            "Suk-Ju Kang",
            "Guang Lin",
            "Sangpil Kim"
        ],
        "tldr": "The paper introduces ICP-4D, a training-free framework for 4D LiDAR panoptic segmentation that leverages the Iterative Closest Point (ICP) algorithm and Sinkhorn matching for robust instance association, achieving state-of-the-art performance without extra training data.",
        "tldr_zh": "该论文介绍了 ICP-4D，一个无需训练的 4D LiDAR 全景分割框架，它利用迭代最近点 (ICP) 算法和 Sinkhorn 匹配来实现鲁棒的实例关联，并在没有额外训练数据的情况下实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "用于4D激光雷达全景分割的主流范式通常需要使用大型叠加点云训练深度神经网络，或设计专用模块进行实例关联。然而，这些方法执行了冗余的点处理，因此计算成本高昂，同时忽略了原始点云固有的丰富几何先验信息。为此，我们引入了ICP-4D，一个简单而有效的免训练框架，它通过实例级点集之间的几何关系统一了空间和时间推理。具体来说，我们应用迭代最近点（ICP）算法，通过估计的变换直接对齐源点集和目标点集，从而关联时间上一致的实例。为了稳定噪声实例预测下的关联，我们引入了基于Sinkhorn的软匹配。这利用了潜在的实例分布来获得准确的逐点对应关系，从而实现稳健的几何对齐。此外，我们精心设计的流程，考虑了三种实例类型——静态、动态和缺失——提供了计算效率和具有遮挡感知能力的匹配。我们在SemanticKITTI和panoptic nuScenes上的大量实验表明，即使没有额外的训练或额外的点云输入，我们的方法也始终优于最先进的方法。"
    },
    {
        "title": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage",
        "summary": "Mass Casualty Incidents can overwhelm emergency medical systems and resulting delays or errors in the assessment of casualties can lead to preventable deaths. We present a decision support framework that fuses outputs from multiple computer vision models, estimating signs of severe hemorrhage, respiratory distress, physical alertness, or visible trauma, into a Bayesian network constructed entirely from expert-defined rules. Unlike traditional data-driven models, our approach does not require training data, supports inference with incomplete information, and is robust to noisy or uncertain observations. We report performance for two missions involving 11 and 9 casualties, respectively, where our Bayesian network model substantially outperformed vision-only baselines during evaluation of our system in the DARPA Triage Challenge (DTC) field scenarios. The accuracy of physiological assessment improved from 15% to 42% in the first scenario and from 19% to 46% in the second, representing nearly threefold increase in performance. More importantly, overall triage accuracy increased from 14% to 53% in all patients, while the diagnostic coverage of the system expanded from 31% to 95% of the cases requiring assessment. These results demonstrate that expert-knowledge-guided probabilistic reasoning can significantly enhance automated triage systems, offering a promising approach to supporting emergency responders in MCIs. This approach enabled Team Chiron to achieve 4th place out of 11 teams during the 1st physical round of the DTC.",
        "url": "http://arxiv.org/abs/2512.18908v1",
        "published_date": "2025-12-21T22:59:58+00:00",
        "updated_date": "2025-12-21T22:59:58+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Szymon Rusiecki",
            "Cecilia G. Morales",
            "Kimberly Elenberg",
            "Leonard Weiss",
            "Artur Dubrawski"
        ],
        "tldr": "This paper introduces a Bayesian network-based decision support framework for autonomous triage in mass casualty incidents, utilizing expert-defined rules to fuse computer vision outputs and improve triage accuracy compared to vision-only baselines in DARPA Triage Challenge scenarios.",
        "tldr_zh": "本文介绍了一个基于贝叶斯网络的决策支持框架，用于大规模伤亡事件中的自主分诊。该框架利用专家定义的规则融合计算机视觉的输出，并在DARPA分诊挑战赛场景中，与仅使用视觉信息的基线相比，提高了分诊的准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "大规模伤亡事件会使紧急医疗系统不堪重负，由此导致的伤情评估延误或错误可能导致可预防的死亡。我们提出了一种决策支持框架，该框架融合了多个计算机视觉模型的输出，将严重出血、呼吸窘迫、精神警觉性或可见创伤的迹象估计结果融入到一个完全由专家定义的规则构建的贝叶斯网络中。与传统的数据驱动模型不同，我们的方法不需要训练数据，支持在信息不完整的情况下进行推断，并且对嘈杂或不确定的观测具有鲁棒性。我们报告了两个分别涉及11名和9名伤员的任务的性能，在这两个任务中，在DARPA分类挑战赛（DTC）的现场场景中评估我们的系统时，我们的贝叶斯网络模型明显优于仅使用视觉信息的基线模型。在第一个场景中，生理评估的准确率从15%提高到42%，在第二个场景中从19%提高到46%，性能提高了近三倍。更重要的是，在所有患者中，总体分类准确率从14%提高到53%，同时系统的诊断覆盖率从需要评估的病例的31%扩大到95%。这些结果表明，专家知识指导下的概率推理可以显著增强自动化分类系统，为支持MCI中的应急响应人员提供了一种有前景的方法。这种方法使Chiron团队在DTC的第一次物理轮中获得了11个团队中的第4名。"
    },
    {
        "title": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization",
        "summary": "Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer",
        "url": "http://arxiv.org/abs/2512.19365v1",
        "published_date": "2025-12-22T13:07:04+00:00",
        "updated_date": "2025-12-22T13:07:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongwei Chen",
            "Hai-Jun Rong",
            "Zhao-Xu Yang",
            "Guoqi Li"
        ],
        "tldr": "The paper introduces SpikeViMFormer, a novel spiking neural network (SNN) framework for drone-view geo-localization that addresses limitations of SNNs such as information loss and difficulties in learning long-range dependencies via a spike-driven transformer backbone and a hierarchical re-ranking alignment learning strategy.",
        "tldr_zh": "该论文介绍了 SpikeViMFormer，一种用于无人机视角地理定位的新型脉冲神经网络 (SNN) 框架，通过脉冲驱动的 Transformer 主干网络和分层重新排序对齐学习策略，解决了 SNN 的信息丢失和难以学习远程依赖等局限性。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "基于人工神经网络(ANN)的传统无人机视角地理定位(DVGL)方法已取得了显著的性能。然而，ANN依赖于密集计算，导致高功耗。相比之下，受益于脉冲驱动计算的脉冲神经网络(SNN)在本质上提供了低功耗。遗憾的是，SNN在DVGL中的潜力尚未得到充分研究。同时，脉冲驱动计算在表征学习场景下的固有稀疏性也导致了关键信息的丢失，以及在对齐异构视觉数据源时学习长程依赖关系的困难。为了解决这些问题，我们提出了SpikeViMFormer，这是第一个专为DVGL设计的SNN框架。在该框架中，采用轻量级的脉冲驱动Transformer主干网络来提取粗粒度特征。为了减少关键信息的丢失，设计了脉冲驱动的选择性注意力(SSA)模块，该模块使用脉冲驱动的门控机制来实现选择性特征增强并突出判别性区域。此外，还引入了脉冲驱动的混合状态空间(SHS)模块，以利用混合状态空间来学习长程依赖关系。此外，在推理阶段仅使用主干网络以降低计算成本。为了确保主干网络的有效性，提出了一种新颖的层次重排序对齐学习(HRAL)策略。它通过邻域重排序来细化特征并保持跨批次一致性，从而直接优化主干网络。实验结果表明，SpikeViMFormer优于最先进的SNN。与先进的ANN相比，它也实现了具有竞争力的性能。我们的代码可在https://github.com/ISChenawei/SpikeViMFormer 获取。"
    },
    {
        "title": "Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction",
        "summary": "This work presents two novel solvers for estimating the relative poses among views with known vertical directions. The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs). Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors. In this paper, a linear closed-form solution has been described, requiring only four point correspondences in three views. We also propose a minimal solution with three point correspondences using the latest Gröbner basis solver. Since the proposed methods require fewer point correspondences, they can be efficiently applied within the RANSAC framework for outliers removal and pose estimation in visual odometry. The proposed method has been tested on both synthetic data and real-world scenes from KITTI. The experimental results show that the accuracy of the estimated poses is superior to other alternative methods.",
        "url": "http://arxiv.org/abs/2512.19110v1",
        "published_date": "2025-12-22T07:26:40+00:00",
        "updated_date": "2025-12-22T07:26:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Li",
            "Zhenbao Yu",
            "Banglei Guan",
            "Jianli Han",
            "Weimin Lv",
            "Friedrich Fraundorfer"
        ],
        "tldr": "The paper introduces novel linear and minimal solvers for relative pose estimation using trifocal tensors, leveraging known vertical directions obtained from IMUs, showing improved accuracy on KITTI dataset. It reduces point correspondence requirements, enhancing RANSAC-based visual odometry.",
        "tldr_zh": "该论文提出利用已知的垂直方向信息（来自IMU），通过三焦张量进行相对姿态估计的新型线性和最小化解算器。实验表明，该方法在KITTI数据集上具有更高的精度。它减少了点对应需求，从而改进了基于RANSAC的视觉里程计。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "此工作提出了两种用于估计已知垂直方向视图之间相对位姿的新型求解器。相机视图的垂直方向可以使用惯性测量单元（IMU）轻松获得，IMU已被广泛应用于自动驾驶车辆、移动电话和无人机（UAV）。在已知垂直方向的情况下，我们的算法只需要求解两个旋转角和两个平移向量。在本文中，我们描述了一种线性闭合解，仅需要三个视图中的四个点对应。我们还提出了一种使用最新Gröbner基求解器的、具有三个点对应的最小解。由于所提出的方法需要的点对应较少，因此可以在RANSAC框架内有效地应用于异常值去除和视觉里程计中的位姿估计。所提出的方法已经通过来自KITTI的合成数据和真实场景进行了测试。实验结果表明，估计位姿的精度优于其他替代方法。"
    },
    {
        "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
        "summary": "Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.",
        "url": "http://arxiv.org/abs/2512.19048v1",
        "published_date": "2025-12-22T05:33:59+00:00",
        "updated_date": "2025-12-22T05:33:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Utae Jeong",
            "Sumin In",
            "Hyunju Ryu",
            "Jaewan Choi",
            "Feng Yang",
            "Jongheon Jeong",
            "Seungryong Kim",
            "Sangpil Kim"
        ],
        "tldr": "The paper introduces WaTeRFlow, a framework designed to improve the robustness of image watermarks when watermarked images are converted to video using image-to-video models, enhancing watermark recovery accuracy in such scenarios.",
        "tldr_zh": "该论文介绍了WaTeRFlow，一个旨在提高图像水印在通过图像到视频模型转换为视频时的鲁棒性的框架，从而提高在这种场景下的水印恢复准确性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "图像水印技术支持图像的真实性和溯源性，但许多方案仍然容易被各种失真和强大的生成式编辑所绕过。基于深度学习的水印技术提高了对基于扩散的图像编辑的鲁棒性，但当水印图像通过图像到视频 (I2V) 转换成视频时，仍然存在差距，其中逐帧水印检测能力减弱。I2V 技术已从短暂、抖动的片段迅速发展到数秒、时间上连贯的场景，现在不仅服务于内容创作，还服务于世界建模和模拟工作流程，这使得跨模态水印恢复至关重要。我们提出了 WaTeRFlow，一种专为I2V下的鲁棒性而设计的框架。它包括：（i）FUSE（流引导的统一合成引擎），通过指令驱动的编辑和快速视频扩散代理，在训练期间将编码器-解码器暴露于现实的失真中；（ii）具有时间一致性损失 (TCL) 的光流扭曲，用于稳定逐帧预测；以及（iii）保持条件信号的语义保持损失。在具有代表性的 I2V 模型上的实验表明，能够准确地从帧中恢复水印，在视频生成之前或之后应用各种失真时，具有更高的首帧和逐帧比特准确度和韧性。"
    },
    {
        "title": "Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models",
        "summary": "Advances in neuroscience and artificial intelligence have enabled preliminary decoding of brain activity. However, despite the progress, the interpretability of neural representations remains limited. A significant challenge arises from the intrinsic properties of electroencephalography (EEG) signals, including high noise levels, spatial diffusion, and pronounced temporal variability. To interpret the neural mechanism underlying thoughts, we propose a transformers-based framework to extract spatial-temporal representations associated with observed visual stimuli from EEG recordings. These features are subsequently incorporated into the attention mechanisms of Latent Diffusion Models (LDMs) to facilitate the reconstruction of visual stimuli from brain activity. The quantitative evaluations on publicly available benchmark datasets demonstrate that the proposed method excels at modeling the semantic structures from EEG signals; achieving up to 6.5% increase in latent space clustering accuracy and 11.8% increase in zero shot generalization across unseen classes while having comparable Inception Score and Fréchet Inception Distance with existing baselines. Our work marks a significant step towards generalizable semantic interpretation of the EEG signals.",
        "url": "http://arxiv.org/abs/2512.18843v1",
        "published_date": "2025-12-21T18:20:21+00:00",
        "updated_date": "2025-12-21T18:20:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hasib Aslam",
            "Muhammad Talal Faiz",
            "Muhammad Imran Malik"
        ],
        "tldr": "This paper introduces Brain-Gen, a transformer-based framework for reconstructing visual stimuli from EEG data using Latent Diffusion Models. It achieves improvements in EEG signal modeling and zero-shot generalization while maintaining comparable image quality metrics.",
        "tldr_zh": "本文介绍Brain-Gen，一种基于Transformer的框架，使用潜在扩散模型从脑电图（EEG）数据重建视觉刺激。该方法在脑电信号建模和零样本泛化方面取得了提升，同时保持了可比的图像质量指标。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "神经科学和人工智能的进步已经实现了对大脑活动初步的解码。然而，尽管取得了进展，神经表征的可解释性仍然有限。一个重要的挑战来源于脑电图（EEG）信号的内在属性，包括高噪声水平、空间扩散和显著的时间变异性。为了解释思想背后的神经机制，我们提出了一种基于Transformer的框架，用于从脑电图记录中提取与观察到的视觉刺激相关的时空表征。这些特征随后被整合到潜在扩散模型（LDMs）的注意力机制中，以促进从大脑活动中重建视觉刺激。在公开的基准数据集上的定量评估表明，所提出的方法擅长对脑电图信号中的语义结构进行建模；在潜在空间聚类精度方面提高了高达6.5%，在未见类别的零样本泛化能力方面提高了11.8%，同时具有与现有基线相当的Inception Score和Fréchet Inception Distance。我们的工作标志着在脑电图信号的通用语义解释方面迈出了重要一步。"
    },
    {
        "title": "GLUE: Generative Latent Unification of Expertise-Informed Engineering Models",
        "summary": "Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.",
        "url": "http://arxiv.org/abs/2512.19469v1",
        "published_date": "2025-12-22T15:23:19+00:00",
        "updated_date": "2025-12-22T15:23:19+00:00",
        "categories": [
            "cs.CE",
            "cs.LG"
        ],
        "authors": [
            "Tim Aebersold",
            "Soheyl Massoudi",
            "Mark D. Fuge"
        ],
        "tldr": "The paper introduces GLUE, a method for coordinating pre-trained, frozen subsystem generators to create full-system engineering designs, achieving feasibility, optimality, and diversity through data-driven and data-free training approaches.",
        "tldr_zh": "该论文介绍了GLUE，一种协调预训练和冻结的子系统生成器以创建完整的系统工程设计的方法，通过数据驱动和无数据训练方法实现可行性、最优性和多样性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "工程复杂系统（飞机、建筑、车辆）的设计需要考虑子系统之间几何和性能的耦合。随着针对特定领域（机翼、结构、发动机）的生成模型的涌现，一个关键的研究缺口是如何协调冻结的、预训练的子模型，以生成可行、多样且高性能的完整系统设计。我们提出了基于专业知识工程模型的生成潜在统一（GLUE），它在强制执行系统级可行性、最优性和多样性的同时，协调预训练的、冻结的子系统生成器。我们提出并评估了（i）在预生成的系统级设计上训练的数据驱动型GLUE模型，以及（ii）在可微分几何层上在线训练的无数据GLUE模型。在一个具有五个耦合约束的无人机设计问题上，我们发现数据驱动方法产生多样且高性能的设计，但需要大型数据集才能可靠地满足约束。无数据方法在性能和可行性方面与贝叶斯优化和基于梯度的优化方法具有竞争力，同时仅在RTX 4090 GPU上用10分钟训练一个完整的生成模型，比数据驱动方法需要少两个数量级以上的几何评估和FLOPs。针对无数据训练的消融实验表明，子系统输出连续性会影响协调，并且如果不加以缓解，等式约束可能会触发模式崩溃。通过将未经修改的、基于领域知识的子模型集成到模块化的生成工作流程中，这项工作为将生成式设计扩展到复杂的现实世界工程系统提供了一条可行的途径。"
    },
    {
        "title": "Interpretable Hybrid Deep Q-Learning Framework for IoT-Based Food Spoilage Prediction with Synthetic Data Generation and Hardware Validation",
        "summary": "The need for an intelligent, real-time spoilage prediction system has become critical in modern IoT-driven food supply chains, where perishable goods are highly susceptible to environmental conditions. Existing methods often lack adaptability to dynamic conditions and fail to optimize decision making in real time. To address these challenges, we propose a hybrid reinforcement learning framework integrating Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) for enhanced spoilage prediction. This hybrid architecture captures temporal dependencies within sensor data, enabling robust and adaptive decision making. In alignment with interpretable artificial intelligence principles, a rule-based classifier environment is employed to provide transparent ground truth labeling of spoilage levels based on domain-specific thresholds. This structured design allows the agent to operate within clearly defined semantic boundaries, supporting traceable and interpretable decisions. Model behavior is monitored using interpretability-driven metrics, including spoilage accuracy, reward-to-step ratio, loss reduction rate, and exploration decay. These metrics provide both quantitative performance evaluation and insights into learning dynamics. A class-wise spoilage distribution visualization is used to analyze the agents decision profile and policy behavior. Extensive evaluations on simulated and real-time hardware data demonstrate that the LSTM and RNN based agent outperforms alternative reinforcement learning approaches in prediction accuracy and decision efficiency while maintaining interpretability. The results highlight the potential of hybrid deep reinforcement learning with integrated interpretability for scalable IoT-based food monitoring systems.",
        "url": "http://arxiv.org/abs/2512.19361v1",
        "published_date": "2025-12-22T12:59:48+00:00",
        "updated_date": "2025-12-22T12:59:48+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Isshaan Singh",
            "Divyansh Chawla",
            "Anshu Garg",
            "Shivin Mangal",
            "Pallavi Gupta",
            "Khushi Agarwal",
            "Nimrat Singh Khalsa",
            "Nandan Patel"
        ],
        "tldr": "This paper presents an interpretable hybrid deep Q-learning framework utilizing LSTM and RNN for real-time food spoilage prediction in IoT-based food supply chains, integrating synthetic data generation and hardware validation for enhanced accuracy and decision-making.",
        "tldr_zh": "本文提出了一种可解释的混合深度Q学习框架，该框架利用LSTM和RNN，用于在基于物联网的食品供应链中进行实时食品腐败预测，并结合了合成数据生成和硬件验证，以提高准确性和决策能力。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "在现代物联网驱动的食品供应链中，易腐货物极易受到环境条件的影响，因此对智能实时腐败预测系统的需求变得至关重要。现有的方法通常缺乏对动态条件的适应性，并且无法实时优化决策。为了解决这些挑战，我们提出了一种混合强化学习框架，该框架集成了长短期记忆网络（LSTM）和循环神经网络（RNN）以增强腐败预测能力。这种混合架构能够捕获传感器数据中的时间依赖性，从而实现稳健且自适应的决策。为了与可解释人工智能的原则保持一致，该框架采用基于规则的分类器环境，根据特定领域的阈值提供腐败程度的透明真实标签。这种结构化的设计允许代理在明确定义的语义边界内运行，从而支持可追溯和可解释的决策。通过使用可解释性驱动的指标来监控模型行为，这些指标包括腐败准确率、奖励-步数比、损失减少率和探索衰减。这些指标既提供了定量的性能评估，也提供了对学习动态的深入了解。我们使用基于类别的腐败分布可视化来分析代理的决策概况和策略行为。在模拟和实时硬件数据上的大量评估表明，基于LSTM和RNN的代理在预测准确性和决策效率方面优于其他强化学习方法，同时保持了可解释性。结果突出了具有集成可解释性的混合深度强化学习在可扩展的基于物联网的食品监控系统中的潜力。"
    },
    {
        "title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry",
        "summary": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.",
        "url": "http://arxiv.org/abs/2512.19567v1",
        "published_date": "2025-12-22T16:50:10+00:00",
        "updated_date": "2025-12-22T16:50:10+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Carlos Pérez-Ruiz",
            "Joan Solà"
        ],
        "tldr": "LIMOncello is a LiDAR-Inertial Odometry system using $\\mathrm{SGal}(3)$ manifold for motion modeling and a lightweight i-Octree mapping backend, showing improved accuracy and robustness in sparse environments with real-time performance.",
        "tldr_zh": "LIMOncello是一个激光雷达-惯性里程计系统，使用$\\mathrm{SGal}(3)$流形进行运动建模和一个轻量级的i-Octree映射后端，在稀疏环境中显示出改进的准确性和鲁棒性，并具有实时性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "本文介绍LIMOncello，一个紧耦合的激光雷达-惯性里程计系统，其在迭代误差状态卡尔曼滤波器后端建模 $\\mathrm{SGal}(3)$ 流形上的6自由度运动。与定义在 $\\mathrm{SO}(3)\\times\\mathbb{R}^6$ 上的状态表示相比，使用 $\\mathrm{SGal}(3)$ 提供了一个连贯且数值稳定的离散时间传播模型，有助于限制在低可观测性条件下的漂移。LIMOncello还包含一个轻量级的增量式i-Octree地图构建后端，与增量式 kd-tree 风格的地图结构相比，它能够实现更快的更新和更低的内存占用，且不依赖于局部性限制的搜索启发式方法。在多个真实世界数据集上的实验表明，LIMOncello在几何稀疏环境中实现了具有竞争力的精度，同时提高了鲁棒性。该系统保持了实时的性能和稳定的内存增长，并以可扩展的开源实现形式发布于 https://github.com/CPerezRuiz335/LIMOncello。"
    },
    {
        "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
        "summary": "Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.",
        "url": "http://arxiv.org/abs/2512.19451v1",
        "published_date": "2025-12-22T14:55:54+00:00",
        "updated_date": "2025-12-22T14:55:54+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Nitin Kumar Singh",
            "Arie Rachmad Syulistyo",
            "Yuichiro Tanaka",
            "Hakaru Tamukoh"
        ],
        "tldr": "This paper proposes a lightweight sign language recognition (SLR) system using parallel bidirectional reservoir computing (PBRC) with MediaPipe for real-time hand tracking, achieving competitive accuracy with significantly reduced training time compared to deep learning methods, making it suitable for edge devices.",
        "tldr_zh": "本文提出了一种基于并行双向储层计算（PBRC）和MediaPipe的轻量级手语识别（SLR）系统，用于实时手部跟踪。与深度学习方法相比，该系统在训练时间方面具有显著优势，同时保持了有竞争力的准确性，适用于边缘设备。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "手语识别（SLR）促进了聋人和健听人社群之间的交流。基于深度学习的SLR模型被广泛使用，但需要大量的计算资源，使其不适合部署在边缘设备上。为了解决这些限制，我们提出了一种轻量级的SLR系统，该系统将并行双向储备池计算（PBRC）与MediaPipe相结合。MediaPipe能够实现实时手部追踪和精确的手部关节坐标提取，这些坐标作为PBRC架构的输入特征。所提出的PBRC架构由两个基于回声状态网络（ESN）的双向储备池计算（BRC）模块并行排列组成，以捕获时间依赖性，从而为分类创建丰富的特征表示。我们在单词级美国手语（WLASL）视频数据集上训练了基于PBRC的SLR系统，分别获得了60.85%、85.86%和91.74%的top-1、top-5和top-10准确率。由于储备池计算的内在特性，训练时间显著缩短至18.67秒，而基于深度学习的方法（如Bi-GRU）的训练时间超过55分钟。该方法为边缘设备上的实时SLR提供了一种轻量级、经济高效的解决方案。"
    },
    {
        "title": "Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals",
        "summary": "Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.",
        "url": "http://arxiv.org/abs/2512.19280v1",
        "published_date": "2025-12-22T11:24:42+00:00",
        "updated_date": "2025-12-22T11:24:42+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Chang Dong",
            "Jianfeng Tao",
            "Chengliang Liu"
        ],
        "tldr": "The paper introduces a digital twin-driven zero-shot fault diagnosis framework for axial piston pumps using fluid-borne noise signals, achieving high accuracy in data-scarce scenarios by calibrating a DT model with healthy data to generate synthetic fault signals for training deep learning classifiers.",
        "tldr_zh": "该论文介绍了一种基于数字孪生的轴向柱塞泵零样本故障诊断框架，该框架利用流体噪声信号，通过使用健康数据校准数字孪生模型来生成合成故障信号，用于训练深度学习分类器，从而在数据稀缺的情况下实现高精度。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "轴向柱塞泵是液压系统中的关键部件，可靠的故障诊断对于确保运行安全和效率至关重要。传统的数据驱动方法需要大量的带标签的故障数据，这通常难以获取，而基于模型的方法则受到参数不确定性的影响。本文提出了一种数字孪生（DT）驱动的零样本故障诊断框架，该框架利用液压噪声（FBN）信号。该框架仅使用健康状态数据校准高保真DT模型，生成合成故障信号用于训练深度学习分类器，并采用物理信息神经网络（PINN）作为虚拟传感器来估计流量脉动。集成了梯度加权类激活映射（Grad-CAM）以可视化神经网络的决策过程，揭示了时间域输入中与子序列长度匹配的大型卷积核以及时频域输入中的小型卷积核，通过聚焦于具有物理意义的特征，能够实现更高的诊断精度。实验验证表明，基于校准的DT模型生成的信号进行训练，在真实基准测试中可获得超过95%的诊断精度，而未校准的模型会导致性能显着下降，突显了该框架在数据稀缺场景下的有效性。"
    },
    {
        "title": "SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates",
        "summary": "Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.",
        "url": "http://arxiv.org/abs/2512.19534v1",
        "published_date": "2025-12-22T16:21:29+00:00",
        "updated_date": "2025-12-22T16:21:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chi Zhang",
            "Braedon Gunn",
            "Andrew M. Read-Fuller"
        ],
        "tldr": "The paper introduces SlicerOrbitSurgerySim, an open-source 3D Slicer extension for virtual planning and quantitative comparison of preformed orbital plates, aiming to improve preoperative decision-making and reduce complications in orbital implant surgery.",
        "tldr_zh": "该论文介绍了一个名为SlicerOrbitSurgerySim的开源3D Slicer扩展，用于预制眼眶板的虚拟规划和定量比较，旨在改善术前决策并减少眼眶植入手术的并发症。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "轨道植入体适应不良仍然是术后并发症和修复手术的主要原因。虽然与定制植入体相比，预制轨道板被广泛应用于降低成本和手术时间，但外科医生目前缺乏公开可用的工具和标准化指标，以定量比较不同供应商、尺寸和患者解剖结构中板的贴合度。我们开发了SlicerOrbitSurgerySim，这是一个用于3D Slicer平台的开源扩展，可在患者特异性虚拟计划环境中实现多个预制轨道板的交互式虚拟配准、评估和比较。该软件生成可重复的定量板-轨道距离指标和可视化工具，从而支持患者特异性计划和板适应性的人群水平统计分析。通过促进对植入体设计和放置策略的客观比较，该工具旨在改进术前决策，减少术中板修改，并促进协作研究和外科教育。我们提供了初步研究、样本数据集和详细教程，以支持测试、透明性和可重复性。"
    },
    {
        "title": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
        "summary": "Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.",
        "url": "http://arxiv.org/abs/2512.19190v1",
        "published_date": "2025-12-22T09:28:23+00:00",
        "updated_date": "2025-12-22T09:28:23+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Marios Thoma",
            "Zenonas Theodosiou",
            "Harris Partaourides",
            "Vassilis Vassiliades",
            "Loizos Michael",
            "Andreas Lanitis"
        ],
        "tldr": "The paper introduces the PEDESTRIAN dataset, a collection of egocentric videos for training and benchmarking obstacle detection algorithms on sidewalks, aiming to improve pedestrian safety in urban environments.",
        "tldr_zh": "该论文介绍了PEDESTRIAN数据集，该数据集包含第一人称视角的视频，用于训练和评估人行道上的障碍物检测算法，旨在提高城市环境中行人的安全。",
        "relevance_score": 3,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "步行一直是主要的交通方式，也被认为是保持良好健康的重要活动。尽管城市环境需要安全的步行条件，但人行道经常被各种障碍物阻碍，妨碍行人自由通行。任何阻碍行人路径的物体都可能构成安全隐患。普适计算和以自我为中心的视觉技术的进步为设计能够自动实时检测此类障碍物的系统提供了潜力，从而提高行人安全性。开发有效且高效的识别算法依赖于全面且均衡的以自我为中心数据的可用性。在这项工作中，我们引入了PEDESTRIAN数据集，其中包含29种常见的城市人行道障碍物的以自我为中心数据。我们使用手机摄像头收集了总共340个视频，捕捉了行人的视角。此外，我们还展示了一系列实验的结果，这些实验涉及使用我们提出的数据集训练几种最先进的深度学习算法，该数据集可用作障碍物检测和识别任务的基准。该数据集可用于训练路面障碍物检测器，以提高城市地区行人的安全性。"
    },
    {
        "title": "Finite-sample guarantees for data-driven forward-backward operator methods",
        "summary": "We establish finite sample certificates on the quality of solutions produced by data-based forward-backward (FB) operator splitting schemes. As frequently happens in stochastic regimes, we consider the problem of finding a zero of the sum of two operators, where one is either unavailable in closed form or computationally expensive to evaluate, and shall therefore be approximated using a finite number of noisy oracle samples. Under the lens of algorithmic stability, we then derive probabilistic bounds on the distance between a true zero and the FB output without making specific assumptions about the underlying data distribution. We show that under weaker conditions ensuring the convergence of FB schemes, stability bounds grow proportionally to the number of iterations. Conversely, stronger assumptions yield stability guarantees that are independent of the iteration count. We then specialize our results to a popular FB stochastic Nash equilibrium seeking algorithm and validate our theoretical bounds on a control problem for smart grids, where the energy price uncertainty is approximated by means of historical data.",
        "url": "http://arxiv.org/abs/2512.19172v1",
        "published_date": "2025-12-22T09:07:09+00:00",
        "updated_date": "2025-12-22T09:07:09+00:00",
        "categories": [
            "math.OC",
            "cs.LG",
            "eess.SY"
        ],
        "authors": [
            "Filippo Fabiani",
            "Barbara Franci"
        ],
        "tldr": "This paper provides finite-sample guarantees for data-driven forward-backward operator splitting methods, particularly relevant in stochastic settings where one operator is approximated using noisy samples. It derives probabilistic bounds on the FB output's accuracy and validates the method on a smart grid control problem.",
        "tldr_zh": "本文为数据驱动的前向-后向算子分裂方法提供了有限样本保证，尤其适用于随机环境，其中一个算子使用噪声样本近似。它推导了 FB 输出精度的概率界限，并在智能电网控制问题上验证了该方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "我们建立了基于数据的前向-后向 (FB) 算子分裂方案所产生解的质量的有限样本证书。考虑到随机情况下经常出现的情形，我们研究了求解两个算子和的零点问题，其中一个算子要么无法以闭合形式获得，要么计算复杂度过高，因此需要使用有限数量的带噪声的神谕样本进行近似。在算法稳定性的视角下，我们推导了真实零点与 FB 输出之间距离的概率界限，而无需对底层数据分布作具体假设。我们表明，在确保 FB 方案收敛的较弱条件下，稳定性界限与迭代次数成正比增长。相反，更强的假设会产生与迭代次数无关的稳定性保证。然后，我们将结果专用于一种流行的 FB 随机纳什均衡求解算法，并在智能电网控制问题上验证了我们的理论界限，在该问题中，能源价格的不确定性通过历史数据进行近似。"
    },
    {
        "title": "Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework",
        "summary": "An energy-based modeling framework for the nonlinear dynamics of spatial Cosserat rods undergoing large displacements and rotations is proposed. The mixed formulation features independent displacement, velocity and stress variables and is further objective and locking-free. Finite rotations are represented using a director formulation that avoids singularities and yields a constant mass matrix. This results in an infinite-dimensional nonlinear port-Hamiltonian (PH) system governed by partial differential-algebraic equations with a quadratic energy functional. Using a time-differentiated compliance form of the stress-strain relations allows for the imposition of kinematic constraints, such as inextensibility or shear-rigidity. A structure-preserving finite element discretization leads to a finite-dimensional system with PH structure, thus facilitating the design of an energy-momentum consistent integration scheme. Dissipative material behavior (via the generalized-Maxwell model) and non-standard actuation approaches (via pneumatic chambers or tendons) integrate naturally into the framework. As illustrated by selected numerical examples, the present framework establishes a new approach to energy-momentum consistent formulations in computational mechanics involving finite rotations.",
        "url": "http://arxiv.org/abs/2512.19408v1",
        "published_date": "2025-12-22T14:04:03+00:00",
        "updated_date": "2025-12-22T14:04:03+00:00",
        "categories": [
            "math.NA",
            "cs.CE",
            "cs.RO",
            "eess.SY",
            "math.DS"
        ],
        "authors": [
            "Philipp L. Kinon",
            "Simon R. Eugster",
            "Peter Betsch"
        ],
        "tldr": "The paper presents an energy-based, structure-preserving discretization method for Cosserat rod dynamics, particularly focusing on large rotations and displacements within a port-Hamiltonian framework. This leads to energy-momentum consistent simulations.",
        "tldr_zh": "本文提出了一种基于能量的、结构保持的 Cosserat 杆动力学离散化方法，特别关注于 port-Hamiltonian 框架内的大旋转和大位移。这使得能量-动量一致的仿真成为可能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 3,
        "summary_zh": "提出了一种基于能量的建模框架，用于模拟经历大位移和旋转的空间 Cosserat 杆的非线性动力学。该混合公式具有独立的位移、速度和应力变量，并且同时具备客观性和无锁定的特性。有限旋转采用方向向量公式表示，避免了奇异性并产生了恒定的质量矩阵。这导致了一个由偏微分代数方程控制的、具有二次能量泛函的无限维非线性端口哈密顿(PH)系统。使用应力-应变关系的导数形式的柔度形式，可以施加运动学约束，例如不可延展性或抗剪刚度。一种保结构的有限元离散化方法得到了保持 PH 结构的有限维系统，从而有助于设计能量-动量一致的积分方案。耗散材料行为（通过广义麦克斯韦模型）和非标准驱动方法（通过气动腔室或肌腱）可以自然地集成到该框架中。如选定的数值例子所示，本框架为涉及有限旋转的计算力学中能量-动量一致的公式建立了一种新的方法。"
    },
    {
        "title": "Comparison and Evaluation of Different Simulation Environments for Rigid Body Systems",
        "summary": "Rigid body dynamics simulators are important tools for the design, analysis and optimization of mechanical systems in a variety of technical and scientific applications. This study examines four different simulation environments (Adams, Simscape, OpenModelica, and VEROSIM), focusing in particular on the comparison of the modeling methods, the numerical solvers, and the treatment of numerical problems that arise especially in closed-loop kinematics (esp. redundant boundary conditions and static equilibrium problem). A novel and complex crane boom of a real forestry machine serves as a practical benchmark application example. The direct comparison of the different solution approaches in the examined simulation tools supports the user in selecting the most suitable tool for his application.",
        "url": "http://arxiv.org/abs/2512.19289v1",
        "published_date": "2025-12-22T11:31:50+00:00",
        "updated_date": "2025-12-22T11:31:50+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Longxiang Shao",
            "Ulrich Dahmen",
            "Juergen Rossmann"
        ],
        "tldr": "This paper compares and evaluates four rigid body dynamics simulation environments (Adams, Simscape, OpenModelica, and VEROSIM) using a forestry crane boom benchmark, focusing on modeling methods, numerical solvers, and handling of closed-loop kinematics issues.",
        "tldr_zh": "本文比较并评估了四种刚体动力学仿真环境(Adams, Simscape, OpenModelica, 和 VEROSIM)，使用一个林业起重机臂作为 benchmark，重点关注建模方法、数值求解器和处理闭环运动学问题的能力。",
        "relevance_score": 2,
        "novelty_claim_score": 3,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 3,
        "summary_zh": "刚体动力学仿真器是各种技术和科学应用中机械系统设计、分析和优化的重要工具。 本研究考察了四种不同的仿真环境（Adams、Simscape、OpenModelica 和 VEROSIM），特别关注建模方法、数值求解器以及在闭环运动学中（尤其是冗余边界条件和静态平衡问题）出现的数值问题的处理的比较。 一种新型且复杂的真实林业机械的起重臂被用作一个实际的基准应用示例。 对所考察的仿真工具中不同求解方法的直接比较有助于用户选择最适合其应用的工具。"
    },
    {
        "title": "Construction and deformation of P-hedra using control polylines",
        "summary": "In the 19th International Symposium on Advances in Robot Kinematics the author introduced a novel class of continuous flexible discrete surfaces and mentioned that these so-called P-hedra (or P-nets) allow direct access to their spatial shapes by three control polylines. In this follow-up paper we study this intuitive method, which makes these flexible planar quad surfaces suitable for transformable design tasks by means of interactive tools. The construction of P-hedra from the control polylines can also be used for an efficient algorithmic computation of their isometric deformations. In addition we discuss flexion limits, bifurcation configurations, developable/flat-foldable pattern and tubular P-hedra.",
        "url": "http://arxiv.org/abs/2512.18869v1",
        "published_date": "2025-12-21T20:08:42+00:00",
        "updated_date": "2025-12-21T20:08:42+00:00",
        "categories": [
            "cs.RO",
            "cs.CG"
        ],
        "authors": [
            "Georg Nawratil"
        ],
        "tldr": "This paper explores a method for constructing and deforming P-hedra (flexible planar quad surfaces) using control polylines, making them suitable for transformable design and efficient computation of isometric deformations.",
        "tldr_zh": "本文探讨了一种使用控制折线构造和变形 P-hedra（柔性平面四边形曲面）的方法，使其适用于可转换设计和等距变形的高效计算。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 4,
        "overall_priority_score": 3,
        "summary_zh": "在第 19 届机器人运动学进展国际研讨会上，作者介绍了一种新型的连续柔性离散曲面，并提到这些所谓的 P-体（或 P-网）允许通过三条控制折线直接访问其空间形状。在本文的后续研究中，我们研究了这种直观的方法，该方法使得这些柔性平面四边形曲面适合通过交互式工具进行可变形设计任务。从控制折线构造 P-体也可用于高效算法计算其等距变形。此外，我们还讨论了挠曲极限、分岔构型、可展/平面折叠模式以及管状 P-体。"
    },
    {
        "title": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome",
        "summary": "Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either \"bounce\", \"net\", or \"empty_event\" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.",
        "url": "http://arxiv.org/abs/2512.19327v1",
        "published_date": "2025-12-22T12:25:50+00:00",
        "updated_date": "2025-12-22T12:25:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Moamal Fadhil Abdul",
            "Jonas Bruun Hubrechts",
            "Thomas Martini Jørgensen",
            "Emil Hovad"
        ],
        "tldr": "This paper introduces an extension to the OpenTTGames dataset with detailed annotations of table tennis strokes, player posture, and rally outcomes, designed to enable fine-grained analysis and understanding of table tennis tactics.",
        "tldr_zh": "该论文介绍了OpenTTGames数据集的扩展，其中包含乒乓球击球、球员姿势和回合结果的详细注释，旨在实现对乒乓球战术的细粒度分析和理解。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 5,
        "overall_priority_score": 3,
        "summary_zh": "自动检测和分类乒乓球视频中的击球动作可以简化训练流程，丰富广播叠加层，并实现细粒度的性能分析。要实现这一点，需要带标注的乒乓球视频数据。我们扩展了公共的OpenTTGames数据集，加入了高度详细、帧精确的击球类型标注（正手、反手及其子类型）、运动员姿势标签（身体倾斜和腿部站姿）以及回合结束时球局结果标签。OpenTTGames是一组来自球桌侧面的录像，包含官方标注的击球反弹、球在球网上方以及触网等事件。数据集已经包含事件附近的球坐标，这些事件在原始OpenTTGames数据集中标注为“bounce”（反弹）、“net”（触网）或“empty_event”（空事件），以及语义掩码（人、球桌、记分牌）。我们的扩展为事件添加了击球类型以及每个运动员的分类，以便模型能够超越事件定位，进而实现战术理解（例如，击球是否可能赢得一分或建立优势）。我们提供了一个紧凑的编码方案和代码辅助标注流程，以支持可复现的标注和球拍运动中细粒度击球理解的基线。这填补了社区中的一个实际空白，因为许多以前的视频资源要么没有公开发布，要么带有限制性或不明确的许可，这阻碍了重用和基准测试。我们的标注以与OpenTTGames相同的CC BY-NC-SA 4.0许可发布，允许在适当署名的情况下进行免费的非商业用途、修改和再分发。"
    }
]