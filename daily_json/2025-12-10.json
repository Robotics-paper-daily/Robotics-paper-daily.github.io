[
    {
        "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning",
        "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.",
        "url": "http://arxiv.org/abs/2512.08580v2",
        "published_date": "2025-12-09T13:19:37+00:00",
        "updated_date": "2025-12-10T12:05:30+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Peijun Tang",
            "Shangjin Xie",
            "Binyan Sun",
            "Baifu Huang",
            "Kuncheng Luo",
            "Haotian Yang",
            "Weiqi Jin",
            "Jianan Wang"
        ],
        "tldr": "The paper introduces Lumo-1, a vision-language-action model for generalist robot control, trained via a three-stage process integrating VLMs, cross-embodiment robot data, and reinforcement learning to achieve strong performance in embodied vision-language reasoning and real-world tasks.",
        "tldr_zh": "本文介绍了Lumo-1，一种用于通用机器人控制的视觉-语言-动作模型。该模型通过三阶段训练流程，整合了VLMs、跨具身机器人数据和强化学习，在具身视觉-语言推理和现实世界任务中取得了优异的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "人类的行为具有上下文和意图，其中推理起着核心作用。虽然互联网规模的数据已使人工智能系统具备了广泛的推理能力，但将这些能力应用于物理行动仍然是一个主要挑战。我们介绍了 Lumo-1，一个通用视觉-语言-行动（VLA）模型，它将机器人推理（“脑”）与机器人行动（“手”）统一起来。我们的方法建立在预训练视觉-语言模型（VLMs）的通用多模态推理能力之上，逐步将其扩展到具身推理和行动预测，最终扩展到结构化推理和推理-行动对齐。这形成了一个三阶段的预训练流程：（1）在经过精选的视觉-语言数据上继续进行 VLM 预训练，以增强具身推理能力，例如规划、空间理解和轨迹预测；（2）在跨具身机器人数据以及视觉-语言数据上进行协同训练；（3）在 Astribot S1（一种具有类人灵巧性和敏捷性的双臂移动操作机器人）上收集的轨迹上，通过推理过程进行行动训练。最后，我们整合了强化学习，以进一步完善推理-行动的一致性，并闭合语义推理和运动控制之间的环路。大量实验表明，Lumo-1 在具身视觉-语言推理方面取得了显著的性能提升，这是通用机器人控制的关键组成部分。真实世界的评估进一步表明，Lumo-1 在各种具有挑战性的机器人任务中超越了强大的基线，并且对新物体和环境具有很强的泛化能力，尤其擅长长时程任务，并能响应需要对策略、概念和空间进行推理的人类自然指令。"
    },
    {
        "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging",
        "summary": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.",
        "url": "http://arxiv.org/abs/2512.08333v1",
        "published_date": "2025-12-09T08:02:11+00:00",
        "updated_date": "2025-12-09T08:02:11+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Yajat Yadav",
            "Zhiyuan Zhou",
            "Andrew Wagenmaker",
            "Karl Pertsch",
            "Sergey Levine"
        ],
        "tldr": "This paper introduces a parameter merging strategy to finetune vision-language-action robot policies. This strategy mitigates overfitting in new tasks while preserving generalist capabilities, outperforming both pretrained and finetuned models in simulated and real-world settings.",
        "tldr_zh": "本文提出了一种参数合并策略，用于微调视觉-语言-动作机器人策略。该策略减轻了新任务中的过拟合，同时保留了通用能力，在模拟和真实环境下均优于预训练和微调模型。",
        "relevance_score": 10,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "在大型且多样化的数据集上训练的通用机器人策略，已经展现出在广泛行为范围内泛化的能力，使得单个策略能够在各种真实世界环境中行动。然而，当面对训练数据未覆盖的新任务时，它们仍然表现不足。在少量新任务演示数据上进行微调时，这些策略常常过度拟合于特定的演示数据，不仅失去了先前解决各种通用任务的能力，也无法在新任务本身内泛化。在这项工作中，我们的目标是开发一种方法，能够在微调过程中保留通用策略的泛化能力，从而使单个策略能够稳健地将新的技能整合到其能力组合中。我们的目标是，一个能够泛化到新任务的各种变体，并且保留从预训练中获得的广泛能力的策略。我们表明，这可以通过一个简单而有效的策略来实现：将微调模型的权重与预训练模型的权重进行插值。我们通过广泛的模拟和真实世界实验表明，这种模型融合产生了一个单一的模型，该模型继承了基础模型的通用能力，并能够稳健地解决新任务，在超出分布的新任务变体上优于预训练和微调模型。此外，我们表明模型融合能够在终身学习环境中持续获得新的技能，而不会牺牲先前学习的通用能力。"
    },
    {
        "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation",
        "summary": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.",
        "url": "http://arxiv.org/abs/2512.08271v1",
        "published_date": "2025-12-09T05:59:38+00:00",
        "updated_date": "2025-12-09T05:59:38+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Srijan Dokania",
            "Dharini Raghavan"
        ],
        "tldr": "Zero-Splat TeleAssist is a zero-shot sensor-fusion pipeline using CCTV streams and 3D Gaussian Splatting to create a 6-DoF world model for multilateral robot teleoperation without extra sensors or fiducials.",
        "tldr_zh": "Zero-Splat TeleAssist 是一个零样本传感器融合管道，它利用闭路电视流和 3D 高斯溅射来创建用于多边机器人遥操作的 6-DoF 世界模型，无需额外的传感器或基准标记。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "我们推出了零样本 TeleAssist，这是一个零样本传感器融合流水线，可将商业闭路电视摄像头的数据流转换为共享的、六自由度世界模型，用于多边遥操作。通过集成视觉-语言分割、单目深度估计、加权 PCA 姿态提取和 3D 高斯溅射 (3DGS) 技术，TeleAssist 为每个操作员提供多个机器人的实时全局位置和姿态信息，无需在以交互为中心的遥操作设置中使用 fiducials 或深度传感器。"
    },
    {
        "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation",
        "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.",
        "url": "http://arxiv.org/abs/2512.08186v1",
        "published_date": "2025-12-09T02:29:36+00:00",
        "updated_date": "2025-12-09T02:29:36+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Meng Wei",
            "Chenyang Wan",
            "Jiaqi Peng",
            "Xiqian Yu",
            "Yuqiang Yang",
            "Delin Feng",
            "Wenzhe Cai",
            "Chenming Zhu",
            "Tai Wang",
            "Jiangmiao Pang",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces DualVLN, a dual-system foundation model for Vision-Language Navigation (VLN) that combines a VLM-based global planner with a diffusion-based local policy for improved generalization, real-time control, and adaptability in dynamic environments.",
        "tldr_zh": "该论文介绍了 DualVLN，一种用于视觉-语言导航 (VLN) 的双系统基础模型，它结合了基于 VLM 的全局规划器和基于扩散的局部策略，以提高在动态环境中泛化、实时控制和适应性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "尽管最近的大型视觉-语言模型(VLMs)已经提高了视觉-语言导航(VLN)的泛化能力，但现有方法通常依赖于端到端流水线，将视觉-语言输入直接映射到短视距的离散动作。这种设计通常会产生碎片化的运动，造成高延迟，并且难以应对真实世界的挑战，例如动态避障。我们提出了DualVLN，这是第一个双系统VLN基础模型，它协同地整合了高层推理和低层动作执行。系统2，一个基于VLM的全局规划器，通过图像接地的推理预测中期航点目标，从而“缓慢扎根”。系统1，一个轻量级的、多模态条件扩散Transformer策略，通过利用来自系统2的显式像素目标和潜在特征，生成平滑而准确的轨迹，从而“快速移动”。双系统设计能够在复杂、动态环境中实现鲁棒的实时控制和自适应的局部决策。通过解耦训练，VLM保留了其泛化能力，而系统1实现了可解释且有效的局部导航。DualVLN优于所有VLN基准测试上的现有方法，并且真实世界的实验证明了其在动态环境中鲁棒的长视距规划和实时适应性。"
    },
    {
        "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making",
        "summary": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.",
        "url": "http://arxiv.org/abs/2512.08366v1",
        "published_date": "2025-12-09T08:44:59+00:00",
        "updated_date": "2025-12-09T08:44:59+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Wentao Zhang",
            "Qunbo Wang",
            "Tao Zhang",
            "Junsheng Wu",
            "Hongping Gan",
            "Yang Liu",
            "Ling Dai",
            "Shizhuang Deng",
            "Shuntong Sun"
        ],
        "tldr": "The paper introduces DuSAR, a novel dual-strategy framework for LLM agents that improves performance and reduces computational cost by co-adaptively integrating a global plan with a local policy, achieving SOTA results on ALFWorld and Mind2Web.",
        "tldr_zh": "该论文介绍了DuSAR，一种新颖的LLM agent双策略框架，通过协同整合全局计划和局部策略，提高了性能并降低了计算成本，在ALFWorld和Mind2Web上实现了SOTA结果。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9,
        "summary_zh": "大型语言模型（LLM）Agent通常依赖于外部演示或检索增强的规划，导致脆弱性、泛化能力差和计算开销高。受到人类问题解决的启发，我们提出了DuSAR（具有反思的双策略Agent）——一个无需演示的框架，它使单个冻结LLM能够通过两种互补策略进行协同自适应推理：一个高层次的整体计划和一个基于上下文的局部策略。这些策略通过一个轻量级的反思机制进行交互，其中Agent通过策略适应度评分持续评估进展，并在遇到困难时动态修订其全局计划，或在取得有意义进展时完善它，从而模仿人类的元认知行为。在ALFWorld和Mind2Web上，DuSAR利用开源LLM（7B-70B）实现了最先进的性能，在ALFWorld上达到37.1%的成功率（Llama3.1-70B）——超过先前最佳结果（13.0%）的两倍以上——在Mind2Web上达到4.02%，也超过了最强的基线一倍以上。值得注意的是，它在保持强大性能的同时，将每步的token消耗降低了3-9倍。消融研究证实了双策略协同的必要性。此外，可选地集成专家演示可以进一步提高结果，突显了DuSAR的灵活性和与外部知识的兼容性。"
    },
    {
        "title": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes",
        "summary": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.",
        "url": "http://arxiv.org/abs/2512.08656v1",
        "published_date": "2025-12-09T14:42:26+00:00",
        "updated_date": "2025-12-09T14:42:26+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Lauritz Rismark Fosso",
            "Herman Biørn Amundsen",
            "Marios Xanthidis",
            "Sveinung Johan Ohrem"
        ],
        "tldr": "This paper introduces Sim2Swim, a zero-shot sim2real DRL controller for agile AUV maneuvering, trained in 3 minutes, addressing challenges in underwater vehicle control and enabling robust 6DOF motion tracking without post-processing.",
        "tldr_zh": "本文介绍了Sim2Swim，一种零样本sim2real DRL控制器，用于水下自主航行器（AUV）的敏捷机动，可在3分钟内完成训练。该控制器解决了水下车辆控制的挑战，并实现了无需后处理的稳健6自由度运动跟踪。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "全驱动自主水下航行器 (AUVs) 在平动和转动自由度 (DOFs) 上具有灵活机动的硬件能力。然而，由于水下航行器固有的挑战，例如复杂的水静力学和水动力学、参数不确定性以及因有效载荷变化导致的动态频繁变化，控制具有挑战性。性能通常依赖于针对特定平台配置精心调整的控制器，并且需要在不同有效载荷和水动力条件下重新调整。因此，在实践中很少使用同时跟踪平动和转动自由度中时变参考的敏捷机动。据我们所知，本文提出了第一个通用的零样本 sim2real 基于深度强化学习 (DRL) 的速度控制器，该控制器仅需 3 分钟的训练时间即可实现路径跟踪和敏捷的 6DOF 机动。Sim2Swim，提出的方法，受到最先进的基于 DRL 的位置控制的启发，利用域随机化和大规模并行化训练收敛到适用于各种特征 AUV 的现场可部署控制策略，而无需后处理或调整。Sim2Swim 在水池试验中针对各种配置进行了广泛验证，展示了高度敏捷运动的鲁棒控制。"
    },
    {
        "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations",
        "summary": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.",
        "url": "http://arxiv.org/abs/2512.08548v1",
        "published_date": "2025-12-09T12:45:12+00:00",
        "updated_date": "2025-12-09T12:45:12+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Yuchi Zhang",
            "Churui Sun",
            "Shiqi Liang",
            "Diyuan Liu",
            "Chao Ji",
            "Wei-Nan Zhang",
            "Ting Liu"
        ],
        "tldr": "This paper introduces a language-based action representation that normalizes robotic actions by emphasizing directionality over numerical scale, improving generalization and transferability in robotic manipulation tasks. The proposed method mitigates distribution shifts and modality gaps between action tokens and standard vocabulary tokens, enhancing pretraining effectiveness.",
        "tldr_zh": "本文提出了一种基于语言的动作表征，通过强调方向性而非数值尺度来标准化机器人动作，从而提高机器人操作任务的泛化性和可迁移性。该方法缓解了动作令牌和标准词汇令牌之间的分布偏移和模态差距，提高了预训练的有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "近期端到端机器人操控研究越来越多地采用受大型语言模型启发的架构，以实现鲁棒的操控。然而，一个关键的挑战源于机器人动作数据之间严重的分布偏移，这主要是由于不同机器人平台和任务中动作指令的数值变化巨大，从而阻碍了预训练知识的有效迁移。为了解决这个限制，我们提出了一种语义对齐的语言表示来规范化动作，以实现高效的预训练。与对数值尺度敏感的传统离散动作表示不同，我们的运动表示专门忽略数值尺度效应，而强调方向性。 这种抽象缓解了分布偏移，产生了一种更具泛化能力的预训练表示。此外，使用运动表示缩小了动作标记和标准词汇标记之间的特征距离，从而减轻了模态差距。在两个基准测试上的多任务实验表明，该方法显著提高了机器人操控任务中的泛化性能和可迁移性。"
    },
    {
        "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems",
        "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.",
        "url": "http://arxiv.org/abs/2512.08476v1",
        "published_date": "2025-12-09T10:50:19+00:00",
        "updated_date": "2025-12-09T10:50:19+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Po-An Shih",
            "Shao-Hua Wang",
            "Yung-Che Li",
            "Chia-Heng Tu",
            "Chih-Han Chang"
        ],
        "tldr": "This paper introduces a multi-agent LLM framework for autonomous driving system design space exploration, automating execution output interpretation and design optimization, achieving superior performance compared to a genetic algorithm baseline.",
        "tldr_zh": "该论文提出了一种用于自动驾驶系统设计空间探索的多智能体LLM框架，能够自动解释执行输出并优化设计，与遗传算法基线相比，性能更优。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "设计自动驾驶系统需要在多样化的环境条件下（例如，不同的交通状况、天气以及道路布局）高效探索庞大的硬件/软件配置空间。传统的设计空间探索 (DSE) 方法难以应对多模态的执行输出和复杂的性能权衡，并且通常需要人工介入来评估基于执行输出的正确性。本文提出了一种基于多智能体、大型语言模型 (LLM) 的 DSE 框架，该框架集成了多模态推理与 3D 模拟和分析工具，以自动解释执行输出并指导系统设计的探索。利用专门的 LLM 智能体来处理用户输入解释、设计点生成、执行编排以及对视觉和文本执行输出的分析，从而在无需人工干预的情况下识别潜在瓶颈。开发并评估了一个基于出租车机器人案例研究（SAE L4 级自动驾驶应用）的原型实现。与遗传算法基线相比，所提出的框架在相同的探索预算下，能够识别出更多帕累托最优、更具成本效益的解决方案，并缩短导航时间。实验结果还证明了基于 LLM 的方法在 DSE 中的效率。我们相信，该框架为自动驾驶系统的设计自动化铺平了道路。"
    },
    {
        "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems",
        "summary": "Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.",
        "url": "http://arxiv.org/abs/2512.08411v1",
        "published_date": "2025-12-09T09:40:34+00:00",
        "updated_date": "2025-12-09T09:40:34+00:00",
        "categories": [
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Mingwei Li",
            "Xiaoyuan Zhang",
            "Chengwei Yang",
            "Zilong Zheng",
            "Yaodong Yang"
        ],
        "tldr": "The paper introduces Prismatic World Model (PRISM-WM), a novel world model architecture using a mixture-of-experts approach to handle hybrid dynamics in robotic systems, showing improved performance in continuous control benchmarks with sharp mode transitions. It addresses the over-smoothing problem of conventional monolithic neural network world models.",
        "tldr_zh": "本文介绍了棱镜世界模型 (PRISM-WM)，这是一种新颖的世界模型架构，使用混合专家方法来处理机器人系统中的混合动力学，在具有尖锐模式转换的连续控制基准测试中表现出更好的性能。它解决了传统单片神经网络世界模型的过度平滑问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于模型的机器人领域规划面临着物理动力学混合本质的根本挑战，其中连续运动会被离散事件（例如接触和碰撞）打断。传统的潜在世界模型通常采用强制全局连续性的单体神经网络，不可避免地过度平滑了不同的动力学模式（例如，粘滞与滑动、飞行与站立）。对于规划器来说，这种平滑会导致长时程前瞻中的灾难性复合误差，使得搜索过程在物理边界处变得不可靠。为了解决这个问题，我们引入了棱镜世界模型（PRISM-WM），这是一种结构化的架构，旨在将复杂的混合动力学分解为可组合的基元。PRISM-WM利用上下文感知混合专家（MoE）框架，其中门控机制隐式地识别当前的物理模式，并且专门的专家预测相关的过渡动力学。我们进一步引入了潜在正交化目标，以确保专家的多样性，从而有效地防止模式崩溃。通过精确地建模系统动力学中的急剧模式转换，PRISM-WM显著减少了滚动展开漂移。在具有挑战性的连续控制基准测试中进行了广泛的实验，包括高维人形机器人和多样化的多任务设置，结果表明，PRISM-WM为轨迹优化算法（例如，TD-MPC）提供了卓越的高保真基底，证明了其作为下一代基于模型代理的强大基础模型的潜力。"
    },
    {
        "title": "Learning Robot Manipulation from Audio World Models",
        "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.",
        "url": "http://arxiv.org/abs/2512.08405v1",
        "published_date": "2025-12-09T09:36:51+00:00",
        "updated_date": "2025-12-09T09:36:51+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Fan Zhang",
            "Michael Gienger"
        ],
        "tldr": "This paper presents a generative model for predicting future audio states to improve robot manipulation tasks, especially those requiring understanding rhythmic patterns in sound. They demonstrate its effectiveness in two audio-related manipulation tasks.",
        "tldr_zh": "本文提出了一种生成模型，用于预测未来音频状态，以改进机器人操作任务，特别是那些需要理解声音中节奏模式的任务。他们在两个与音频相关的操作任务中展示了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "世界模型已在机器人学习任务上展现出令人印象深刻的性能。许多此类任务本质上需要多模态推理；例如，用水填充瓶子仅凭视觉信息会变得模糊或不完整，因此需要对音频的时间演变进行推理，考虑到其底层的物理特性和音高模式。在本文中，我们提出了一种生成式潜在流匹配模型来预测未来的音频观测，从而使系统能够在集成到机器人策略中时，能够推理长期后果。通过两个需要感知真实音频或音乐信号的操作任务，我们证明了我们的系统相比于没有未来前瞻的方法而言，具有更卓越的能力。我们进一步强调，这些任务中成功的机器人动作学习不仅仅依赖于多模态输入，更关键的是对体现内在节奏模式的未来音频状态的准确预测。"
    },
    {
        "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making",
        "summary": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.",
        "url": "http://arxiv.org/abs/2512.08280v1",
        "published_date": "2025-12-09T06:26:02+00:00",
        "updated_date": "2025-12-09T06:26:02+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "eess.SY"
        ],
        "authors": [
            "Haldun Balim",
            "Na Li",
            "Yilun Du"
        ],
        "tldr": "The paper introduces MPDiffuser, a model-based diffusion framework for offline decision-making that combines a planner, a dynamics model, and a ranker to generate dynamically feasible and task-aligned trajectories. It demonstrates improvements on D4RL, DSRL, and a real quadrupedal robot.",
        "tldr_zh": "该论文介绍了MPDiffuser，一个基于模型的扩散框架，用于离线决策，它结合了规划器、动力学模型和排序器来生成动态可行且任务对齐的轨迹。它在D4RL、DSRL和一个真实的四足机器人上展示了改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "离线决策需要在没有额外交互的情况下，从固定数据集中合成可靠的行为，但现有的生成式方法常常产生动态不可行的轨迹。我们提出模型预测扩散器 (MPDiffuser)，一个组合式的基于模型的扩散框架，由以下部分组成：（i）一个规划器，用于生成多样化的、与任务对齐的轨迹；（ii）一个动力学模型，用于强制与底层系统动力学的一致性；以及（iii）一个排序器模块，用于选择与任务目标对齐的行为。MPDiffuser 采用一种交替的扩散采样方案，其中规划器和动力学更新交错进行，以便在采样过程中逐步细化轨迹，使其同时满足任务对齐和可行性。我们还为这个过程提供了一个理论基础，展示了它如何在对数据先验的忠实度和动力学一致性之间取得平衡。从实验上看，组合式设计提高了样本效率，因为它利用了即便是低质量的数据进行动力学学习，并能无缝地适应新的动力学。我们在非约束 (D4RL) 和约束 (DSRL) 离线决策基准上评估了 MPDiffuser，证明了其相对于现有方法具有一致的优势。此外，我们还进行了一项初步研究，将MPDiffuser扩展到基于视觉的控制任务，展示了其扩展到高维感官输入的潜力。最后，我们将我们的方法部署在真实的四足机器人上，展示了其在现实世界控制中的实用性。"
    },
    {
        "title": "Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior",
        "summary": "Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.",
        "url": "http://arxiv.org/abs/2512.08233v1",
        "published_date": "2025-12-09T04:19:58+00:00",
        "updated_date": "2025-12-09T04:19:58+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Timothy Chen",
            "Marcus Dominguez-Kuhne",
            "Aiden Swann",
            "Xu Liu",
            "Mac Schwager"
        ],
        "tldr": "This paper introduces a novel Bayesian Risk Field framework that leverages human demonstration videos and VLM common sense to learn human-aligned risk models for robot planning and trajectory optimization in dynamic environments.",
        "tldr_zh": "本文介绍了一种新颖的贝叶斯风险场框架，该框架利用人类演示视频和VLM常识来学习与人类对齐的风险模型，用于动态环境中机器人的规划和轨迹优化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人类对安全的理解并非二元信号，而是风险的连续、上下文相关且空间依赖性的概念。 尽管风险具有主观性，但人类会形成理性的心理模型，在动态环境中指导行动选择。 本文提出了一种框架，通过引入一种新颖的、语义条件化且空间变化的风险参数化方法，直接从安全的人类演示视频和视觉语言模型（VLM）常识中进行监督，来提取隐式的人类风险模型。 值得注意的是，我们通过贝叶斯公式定义风险。 先验由预训练的视觉语言模型提供。 为了鼓励风险估计与人类更加一致，似然函数调节先验，以产生风险的相对度量。 具体而言，似然是一个学习到的ViT，它将预训练的特征映射到像素对齐的风险值。 我们的流程接收RGB图像和查询对象字符串，生成像素密集的风险图像。 这些图像可以作为机器人规划任务中的价值预测器使用，或者可以投影到3D空间中，用于传统的轨迹优化，从而产生类人运动。 这种学习到的映射能够泛化到新的物体和环境，并有可能扩展到更大的训练数据集。 特别是，所引入的贝叶斯框架能够使我们的模型快速适应额外的观察结果或常识规则。 我们证明，我们提出的框架产生了与人类偏好相符的上下文风险。 此外，我们还展示了该模型的几个下游应用：作为视觉运动规划器的价值学习器，或与经典轨迹优化算法结合使用。 我们的结果表明，我们的框架是使自主系统能够内化类人风险的重要一步。 代码和结果可在 https://riskbayesian.github.io/bayesian_risk/ 找到。"
    },
    {
        "title": "Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems",
        "summary": "Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.",
        "url": "http://arxiv.org/abs/2512.08229v1",
        "published_date": "2025-12-09T04:14:05+00:00",
        "updated_date": "2025-12-09T04:14:05+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Tony Salloom",
            "Dandi Zhou",
            "Xinhai Sun"
        ],
        "tldr": "This paper introduces a geometry-aware sparse depth sampling strategy for RGB-D depth completion, improving accuracy and realism in robotic systems by considering sensor reliability based on surface normal estimation.",
        "tldr_zh": "该论文提出了一种几何感知的稀疏深度采样策略，用于RGB-D深度补全，通过考虑基于表面法线估计的传感器可靠性来提高机器人系统的准确性和真实感。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "准确的三维感知对于执行操纵、检测和导航任务的现代工业机器人系统至关重要。RGB-D和立体视觉传感器被广泛用于此目的，但由于传感器限制和环境条件，它们生成的深度图往往存在噪声、不完整或偏差。深度补全方法旨在从RGB图像和稀疏深度输入生成稠密且可靠的深度图。然而，当前深度补全流程的一个关键限制是稀疏深度生成的不真实性：稀疏像素通常从稠密的真实深度图中均匀随机选取，忽略了真实传感器表现出几何依赖性和空间非均匀可靠性的事实。在这项工作中，我们提出了一种法线引导的稀疏深度采样策略，该策略利用基于PCA的RGB-D点云表面法线估计来计算每个像素的深度可靠性度量。然后根据该可靠性分布抽取稀疏深度样本。我们将这种采样方法与基于扩散的 Marigold-DC 深度补全模型集成，并在 NYU Depth v2 数据集上使用标准指标对其进行评估。实验表明，我们的几何感知稀疏深度提高了准确性，减少了边缘和不连续处的伪影，并产生了更能反映真实传感器行为的更真实的训练条件。"
    },
    {
        "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model",
        "summary": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .",
        "url": "http://arxiv.org/abs/2512.08188v1",
        "published_date": "2025-12-09T02:36:26+00:00",
        "updated_date": "2025-12-09T02:36:26+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wenjiang Xu",
            "Cindy Wang",
            "Rui Fang",
            "Mingkang Zhang",
            "Lusong Li",
            "Jing Xu",
            "Jiayuan Gu",
            "Zecui Zeng",
            "Rui Chen"
        ],
        "tldr": "The paper introduces Embodied Tree of Thoughts (EToT), a novel planning framework using a physics-based simulator to enhance robot manipulation by combining priori and reflective branching to overcome limitations of video-based world models.",
        "tldr_zh": "该论文介绍了具身思维树 (EToT)，一种新颖的规划框架，它使用基于物理的模拟器来增强机器人操作能力，通过结合先验和反思分支来克服基于视频的世界模型的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "世界模型已成为机器人操作规划中的关键组成部分，使代理能够预测未来的环境状态并在执行前推理动作的后果。虽然视频生成模型日益普及，但它们通常缺乏严格的物理基础，导致幻觉，并且无法在长时程物理约束中保持一致性。为了解决这些局限性，我们提出了一种新颖的Real2Sim2Real规划框架，具身思维树 (EToT)，它利用基于物理的交互式数字孪生作为具身世界模型。 EToT将操作规划形式化为通过两种协同机制扩展的树搜索：（1）先验分支，它基于语义和空间分析生成多样化的候选执行路径；（2）反思分支，它利用视觉语言模型 (VLMs) 诊断模拟器内的执行失败，并使用纠正措施迭代地细化规划树。通过将高级推理建立在物理模拟器之上，我们的框架确保生成的计划符合刚体动力学和碰撞约束。我们在一系列短时程和长时程操作任务上验证了 EToT，结果表明 EToT始终优于基线模型，原因在于其可以有效地预测物理动力学并适应潜在的失败。网站：https://embodied-tree-of-thoughts.github.io 。"
    },
    {
        "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models",
        "summary": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",
        "url": "http://arxiv.org/abs/2512.08145v1",
        "published_date": "2025-12-09T00:55:40+00:00",
        "updated_date": "2025-12-09T00:55:40+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Haoran Wang",
            "Zhuohang Chen",
            "Guang Li",
            "Bo Ma",
            "Chuanghuang Li"
        ],
        "tldr": "This paper introduces a dual-agent Human-UAV Interaction (HUI) framework powered by Large Language Models (LLMs) to improve task planning and execution for UAVs in complex scenarios, showing promising results in user studies across typical UAV applications.",
        "tldr_zh": "本文提出了一种基于大型语言模型（LLM）的双代理人机交互（HUI）框架，旨在改进无人机在复杂场景中的任务规划和执行。用户研究表明，该框架在典型的无人机应用中表现出良好的效果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "无人机交互系统的未来正从工程师驱动向用户驱动演变，旨在取代传统预定义的人机交互设计。这种转变侧重于实现更个性化的任务规划和设计，从而获得更高质量的交互体验和更大的灵活性，可应用于农业、航空摄影、物流和环境监测等诸多领域。然而，由于用户与无人机之间缺乏通用语言，此类交互往往难以实现。大型语言模型的发展具备理解自然语言和机器人（无人机）行为的能力，标志着个性化人机交互的可能性。近年来，一些基于大型语言模型的人机交互框架已被提出，但它们普遍存在混合任务规划和执行困难的问题，导致在复杂场景中的适应性较低。在本文中，我们提出了一种新颖的双智能体人机交互框架。该框架构建了两个独立的大型语言模型智能体（一个任务规划智能体和一个执行智能体），并应用不同的提示工程来分别处理任务的理解、规划和执行。为了验证该框架的有效性和性能，我们构建了一个涵盖无人机四个典型应用场景的任务数据库，并使用三个独立指标量化了人机交互框架的性能。同时，我们选择了不同的LLM模型来控制无人机，并比较了它们的性能。我们的用户研究实验结果表明，该框架提高了我们设定的任务场景中人机交互的流畅性和任务执行的灵活性，有效地满足了用户的个性化需求。"
    },
    {
        "title": "VLD: Visual Language Goal Distance for Reinforcement Learning Navigation",
        "summary": "Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-\"where to go\"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.",
        "url": "http://arxiv.org/abs/2512.07976v1",
        "published_date": "2025-12-08T19:05:51+00:00",
        "updated_date": "2025-12-08T19:05:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Lazar Milikic",
            "Manthan Patel",
            "Jonas Frey"
        ],
        "tldr": "This paper introduces Vision-Language Distance (VLD) learning, a framework for goal-conditioned navigation that decouples perception and policy learning by training a self-supervised distance-to-goal predictor on internet-scale video data and using it as a signal for reinforcement learning.",
        "tldr_zh": "本文介绍了视觉语言距离（VLD）学习，它是一种目标条件导航框架，通过在互联网规模的视频数据上训练自监督的距离-目标预测器，并将其用作强化学习的信号，从而将感知和策略学习分离。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "从图像数据训练端到端策略，以直接预测机器人系统的导航动作已被证明非常困难。现有方法通常面临策略迁移中的仿真-真实差距或带有动作标签的训练数据有限的问题。为了解决这个问题，我们引入了视觉-语言距离(VLD)学习，这是一种可扩展的目标条件导航框架，它将感知学习与策略学习解耦。我们没有在策略训练期间依赖原始感官输入，而是首先在互联网规模的视频数据上训练一个自监督的到目标距离预测器。该预测器可以推广到基于图像和基于文本的目标，提供一个可以被强化学习(RL)策略最小化的距离信号。RL策略可以在完全在模拟中训练，使用特权几何距离信号，并注入噪声来模拟训练的距离预测器的不确定性。在部署时，策略消耗VLD预测，继承了大规模视觉训练中的语义目标信息——“去哪里”，同时保留了在模拟中学习到的稳健的底层导航行为。我们建议使用序数一致性来直接评估距离函数，并证明VLD优于先前的时序距离方法，如ViNT和VIP。实验表明，我们的解耦设计在模拟中实现了具有竞争力的导航性能，同时支持灵活的目标模式，为可靠的、多模态的导航策略提供了一种替代方案，最重要的是，提供了一条可扩展的路径。"
    },
    {
        "title": "Towards Foundation Models with Native Multi-Agent Intelligence",
        "summary": "Foundation models (FMs) are increasingly assuming the role of the \"brain\" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.",
        "url": "http://arxiv.org/abs/2512.08743v1",
        "published_date": "2025-12-09T15:51:36+00:00",
        "updated_date": "2025-12-09T15:51:36+00:00",
        "categories": [
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Shuyue Hu",
            "Haoyang Yan",
            "Yiqun Zhang",
            "Yang Chen",
            "Dongzhan Zhou",
            "Lei Bai"
        ],
        "tldr": "This paper argues that current foundation models lack native multi-agent intelligence despite strong single-agent performance, and it outlines research directions to address this gap, covering dataset construction, evaluation, training, and safety.",
        "tldr_zh": "本文指出，尽管当前的基模型在单智能体任务中表现出色，但缺乏原生的多智能体智能。文章概述了弥补这一差距的研究方向，包括数据集构建、评估、训练和安全考虑。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基础模型(FMs)正日益成为人工智能体的“大脑”。 虽然最近的工作已经开始赋予FMs原生的单智能体能力——如GUI交互或集成工具使用——我们认为下一个前沿是赋予FMs原生的多智能体智能。我们提出了FMs在多智能体环境中的四个核心能力：理解、规划、高效沟通和适应。 与关于这些能力自发涌现的假设相反，我们通过对41个大型语言模型进行的大量实证研究表明，强大的单智能体性能本身并不能自动产生稳健的多智能体智能。 为了解决这个问题，我们概述了关键的研究方向——涵盖数据集构建、评估、训练范式和安全考量——以构建具有原生多智能体智能的FMs。"
    },
    {
        "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning",
        "summary": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.",
        "url": "http://arxiv.org/abs/2512.08639v1",
        "published_date": "2025-12-09T14:25:24+00:00",
        "updated_date": "2025-12-09T14:25:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Huilin Xu",
            "Zhuoyang Liu",
            "Yixiang Luomei",
            "Feng Xu"
        ],
        "tldr": "The paper presents a unified aerial vision-language navigation framework for UAVs using only monocular RGB images, formulating navigation as a next-token prediction problem. They also propose keyframe selection and action merging techniques to improve performance and address data imbalance.",
        "tldr_zh": "该论文提出了一个统一的无人机视觉语言导航框架，仅使用单目RGB图像，将导航问题建模为下一个token预测问题。他们还提出了关键帧选择和动作合并技术来提高性能并解决数据不平衡问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "空中视觉-语言导航（VLN）旨在使无人航空载具（UAV）能够解读自然语言指令，并利用机载视觉观测在复杂的城市环境中进行导航。该任务对于低空巡检、搜救以及自主空中配送等实际应用具有重要意义。现有方法通常依赖于全景图像、深度输入或里程计，以支持空间推理和动作规划。这些需求增加了系统成本和集成复杂度，从而阻碍了轻型无人机在实践中的部署。我们提出了一种统一的空中VLN框架，该框架仅基于第一人称视角的单目RGB观测和自然语言指令运行。该模型将导航建模为下一个token预测问题，通过提示引导的多任务学习联合优化空间感知、轨迹推理和动作预测。此外，我们提出了一种关键帧选择策略，通过保留语义信息丰富的帧来减少视觉冗余，以及一种动作合并和标签重加权机制，以缓解长尾监督不平衡，并促进稳定的多任务协同训练。在空中VLN基准上的大量实验验证了我们方法的有效性。在具有挑战性的仅单目RGB设置下，我们的模型在已见和未见环境中都取得了优异的结果。它明显优于现有的仅RGB基线，并缩小了与最先进的全景RGB-D方法的性能差距。全面的消融研究进一步证明了我们的任务设计和架构选择的贡献。"
    },
    {
        "title": "See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.",
        "url": "http://arxiv.org/abs/2512.08629v1",
        "published_date": "2025-12-09T14:14:37+00:00",
        "updated_date": "2025-12-09T14:14:37+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.HC"
        ],
        "authors": [
            "Haoyu Zhao",
            "Weizhong Ding",
            "Yuhao Yang",
            "Zheng Tian",
            "Linyi Yang",
            "Kun Shao",
            "Jun Wang"
        ],
        "tldr": "The paper introduces See-Control, a novel framework for smartphone operation using a robotic arm controlled by an MLLM, bypassing the need for ADB and providing a platform-agnostic solution, along with a new benchmark dataset.",
        "tldr_zh": "该论文介绍了See-Control，一个新颖的框架，使用由MLLM控制的机器人手臂进行智能手机操作，绕过了对ADB的需求，并提供了一种平台无关的解决方案，以及一个新的基准数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "多模态大型语言模型（MLLM）的最新进展使其能够作为智能代理应用于智能手机操作。然而，现有方法依赖于Android调试桥（ADB）进行数据传输和动作执行，限制了其在Android设备上的适用性。本文中，我们引入了全新的具身智能手机操作（ESO）任务，并提出了See-Control，一个通过低自由度机械臂直接物理交互实现智能手机操作的框架，提供了一种平台无关的解决方案。See-Control包含三个关键组成部分：（1）一个包含155个任务和相应评估指标的ESO基准；（2）一个基于MLLM的具身智能代理，无需ADB或系统后端访问即可生成机器人控制指令；以及（3）一个带有丰富标注的操作事件数据集，为未来的研究提供有价值的资源。通过弥合数字代理和物理世界之间的差距，See-Control为实现家用机器人能够在真实环境中执行依赖于智能手机的任务迈出了坚实的一步。"
    },
    {
        "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks",
        "summary": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.",
        "url": "http://arxiv.org/abs/2512.08545v1",
        "published_date": "2025-12-09T12:40:39+00:00",
        "updated_date": "2025-12-09T12:40:39+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Indrajit Kar",
            "Kalathur Chenchu Kishore Kumar"
        ],
        "tldr": "This paper presents a hierarchical multi-agent system with a spatial curriculum and confidence-based training for solving long-horizon tasks, demonstrating improved stability and reasoning on a Tower of Hanoi benchmark.",
        "tldr_zh": "本文提出了一种分层多智能体系统，该系统具有空间课程学习和基于置信度的训练方法，用于解决长时程任务。在汉诺塔基准测试中，该系统表现出更高的稳定性和推理能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型和多智能体系统在分解复杂任务方面展现出潜力，但它们在长时程推理任务和不断攀升的计算成本方面面临挑战。 本文介绍了一种分层多智能体架构，该架构将推理分布在一个由轻量级智能体组成的 64*64 网格中，并由一个选择性指导者提供支持。 空间课程逐步扩展网格的运行区域，确保智能体在处理更困难的周边任务之前，掌握更容易的中心任务。 为了提高可靠性，该系统集成了负对数似然作为置信度度量，允许课程优先考虑智能体既准确又校准良好的区域。 一个Thompson采样课程管理器基于能力和由NLL驱动的奖励信号自适应地选择训练区域。 我们在一个空间接地的汉诺塔基准上评估了该方法，该基准反映了许多机器人操作和规划任务的长时程结构。 结果表明，分布式智能体协作提高了稳定性，减少了指导者的使用，并增强了远程推理能力。"
    },
    {
        "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform",
        "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.",
        "url": "http://arxiv.org/abs/2512.08478v1",
        "published_date": "2025-12-09T10:54:58+00:00",
        "updated_date": "2025-12-09T10:54:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.GR"
        ],
        "authors": [
            "Yuning Gong",
            "Yifei Liu",
            "Yifan Zhan",
            "Muyao Niu",
            "Xueying Li",
            "Yuanjun Liao",
            "Jiaming Chen",
            "Yuanyuan Gao",
            "Jiaqi Chen",
            "Minming Chen",
            "Li Zhou",
            "Yuning Zhang",
            "Wei Wang",
            "Xiaoqing Hou",
            "Huaxi Huang",
            "Shixiang Tang",
            "Le Ma",
            "Dingwen Zhang",
            "Xue Yang",
            "Junchi Yan",
            "Yanchi Zhang",
            "Yinqiang Zheng",
            "Xiao Sun",
            "Zhihang Zhong"
        ],
        "tldr": "Visionary is a new WebGPU-based platform for real-time rendering of Gaussian Splatting and meshes, enabling dynamic neural processing and generative post-processing in the browser, lowering the barrier to 3DGS deployment.",
        "tldr_zh": "Visionary是一个新的基于WebGPU的平台，用于实时渲染高斯溅射（Gaussian Splatting）和网格，能够在浏览器中实现动态神经处理和生成后处理，降低了3DGS部署的门槛。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "神经渲染，特别是3D高斯溅射（3DGS），发展迅速，并已成为构建世界模型的关键组成部分。然而，现有的查看器解决方案仍然分散、笨重，或受限于传统流程，导致高部署摩擦和对动态内容及生成模型的有限支持。在本文中，我们提出了Visionary，一个开放的、原生于Web平台的、用于实时渲染各种高斯溅射和网格的平台。Visionary构建于高效的WebGPU渲染器之上，并结合了逐帧ONNX推理，能够在保持轻量级、“点击即运行”的浏览器体验的同时，实现动态的神经处理。它引入了一个标准化的Gaussian Generator接口，不仅支持标准的3DGS渲染，还允许即插即用的算法来逐帧生成或更新高斯分布。这种推理还使我们能够应用前馈生成式的后处理。该平台还提供了一个插件式的three.js库，具有简洁的TypeScript API，可以无缝集成到现有的Web应用程序中。实验表明，在相同的3DGS资源下，Visionary由于基于GPU的图元排序，与当前的Web查看器相比，实现了卓越的渲染效率。它已经支持多种变体，包括基于MLP的3DGS、4DGS、神经化身，以及风格转换或增强网络。通过直接在浏览器中统一推理和渲染，Visionary显著降低了3DGS系列方法的复现、比较和部署的门槛，作为重建和生成范式的统一世界模型载体。"
    },
    {
        "title": "rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection",
        "summary": "Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.",
        "url": "http://arxiv.org/abs/2512.08300v1",
        "published_date": "2025-12-09T06:55:39+00:00",
        "updated_date": "2025-12-09T06:55:39+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Sijia Chen",
            "Baochun Li",
            "Di Niu"
        ],
        "tldr": "This paper introduces rSIM, a reinforcement learning-based method for injecting reasoning strategies into LLMs, allowing smaller models to outperform larger ones by adaptively guiding the chain of thought process with a trained planner.",
        "tldr_zh": "本文介绍了一种名为rSIM的强化学习方法，通过将推理策略注入LLM来增强其推理能力。该方法使用训练好的规划器自适应地引导思维链过程，使较小的模型能够胜过较大的模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLMs）通过强化学习（RL）进行后训练，演化为推理语言模型（RLMs），这种高级推理的标志是“顿悟”时刻，即它们开始在思维链（CoTs）中执行诸如自我反思和深度思考等策略。受此启发，本文提出了一种新的强化策略注入机制（rSIM），通过采用一个小型规划器来引导LLM的CoT，自适应地注入推理策略，从而使任何LLM都能成为RLM。为了实现这一目标，规划器（领导者代理）与LLM（追随者代理）基于领导者-追随者框架和简单的基于规则的奖励，使用多智能体强化学习（MARL）进行联合训练。实验结果表明，rSIM使Qwen2.5-0.5B能够成为RLM，并且显著优于Qwen2.5-14B。此外，该规划器具有通用性：它只需要训练一次，就可以作为插件应用，从而大幅提高现有LLM的推理能力。此外，该规划器支持跨各种任务的持续学习，使其规划能力逐步提高并推广到更广泛的问题。"
    },
    {
        "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models",
        "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.",
        "url": "http://arxiv.org/abs/2512.08153v1",
        "published_date": "2025-12-09T01:17:34+00:00",
        "updated_date": "2025-12-09T01:17:34+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zheng Ding",
            "Weirui Ye"
        ],
        "tldr": "The paper introduces TreeGRPO, a novel reinforcement learning framework for efficient post-training of diffusion models, achieving significant speedups and improved performance compared to existing methods by using a tree-structured approach.",
        "tldr_zh": "该论文介绍了 TreeGRPO，一种用于高效扩散模型后训练的新型强化学习框架，通过使用树状结构方法，与现有方法相比，实现了显著的加速和性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "强化学习（RL）后训练对于使生成模型与人类偏好对齐至关重要，但其高昂的计算成本仍然是广泛应用的主要障碍。我们引入了\\textbf{TreeGRPO}，一种新颖的RL框架，通过将去噪过程重构为搜索树，从而显著提高训练效率。TreeGRPO从共享的初始噪声样本出发，策略性地分支以生成多个候选轨迹，同时高效地重用它们的公共前缀。这种树形结构的方法具有三个关键优势：（1）\\emph{高样本效率}，在相同训练样本下实现更好的性能；（2）通过计算步骤特定的优势（advantage）的奖励反向传播实现\\emph{细粒度的信用分配}，克服了基于轨迹的方法中统一信用分配的局限性；（3）\\emph{分摊计算}，其中多子分支使得每次前向传递能够进行多次策略更新。在基于扩散和基于流的模型上的大量实验表明，TreeGRPO实现了\\textbf{2.4倍更快的训练速度}，同时在效率-奖励权衡空间中建立了卓越的Pareto前沿。我们的方法在多个基准和奖励模型上始终优于GRPO基线，为基于RL的视觉生成模型对齐提供了一种可扩展且有效的途径。项目网站可在 treegrpo.github.io 上访问。"
    },
    {
        "title": "Scalable Offline Model-Based RL with Action Chunks",
        "summary": "In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \\emph{action-chunk} model that predicts a future state from a sequence of actions (an \"action chunk\") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \\textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.",
        "url": "http://arxiv.org/abs/2512.08108v1",
        "published_date": "2025-12-08T23:26:29+00:00",
        "updated_date": "2025-12-08T23:26:29+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Kwanyoung Park",
            "Seohong Park",
            "Youngwoon Lee",
            "Sergey Levine"
        ],
        "tldr": "This paper introduces Model-Based RL with Action Chunks (MAC) for scalable offline RL, addressing the value expansion bias-variance trade-off by predicting state transitions from action sequences and using rejection sampling to avoid out-of-distribution exploitation.",
        "tldr_zh": "本文介绍了基于动作块的模型强化学习（MAC），用于可扩展的离线强化学习。它通过预测动作序列的状态转换并使用拒绝抽样来避免超出分布的利用，从而解决了价值扩展的偏差-方差权衡问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文研究了基于模型的强化学习 (RL)，特别是基于模型的价值扩展，是否可以为离线 RL 中解决复杂、长周期任务提供一种可扩展的方法。基于模型的价值扩展使用当前策略和已学习的动力学模型生成的长度为 n 的虚拟轨迹来拟合 on-policy 价值函数。虽然较大的 n 减少了价值引导的偏差，但它放大了长周期内累积的模型误差，从而降低了未来的预测准确性。为了解决这种权衡，我们采用了一种 \\emph{动作块} 模型，该模型从一系列动作（一个“动作块”）而非单个动作来预测未来的状态，从而减少复合误差。此外，我们没有直接训练策略来最大化奖励，而是采用一种来自表达能力强的行为动作块策略的拒绝抽样方法，以防止模型利用分布外的动作。我们将此方法称为 **基于动作块的强化学习模型 (MAC)**。通过对高达 1 亿个转移的大规模数据集进行的高度挑战性任务的实验，我们表明 MAC 在离线基于模型的 RL 算法中取得了最佳性能，尤其是在具有挑战性的长周期任务中。"
    },
    {
        "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling",
        "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.",
        "url": "http://arxiv.org/abs/2512.07821v1",
        "published_date": "2025-12-08T18:54:12+00:00",
        "updated_date": "2025-12-08T18:54:12+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shaoheng Fang",
            "Hanwen Jiang",
            "Yunpeng Bai",
            "Niloy J. Mitra",
            "Qixing Huang"
        ],
        "tldr": "WorldReel is a novel 4D video generator that produces spatio-temporally consistent videos by jointly modeling RGB frames and 4D scene representations, enabling better geometry and motion modeling, and trained using a combination of synthetic and real data.",
        "tldr_zh": "WorldReel是一种新型的4D视频生成器，通过联合建模RGB帧和4D场景表示，生成时空一致的视频，从而实现更好的几何和运动建模。该模型使用合成数据和真实数据相结合的方式进行训练。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "近期视频生成器在实现惊人的照片真实感方面取得了显著进展，但本质上仍然在3D上不一致。我们提出了WorldReel，一种原生时空一致的4D视频生成器。WorldReel联合生成RGB帧和4D场景表示，包括点云图、相机轨迹和稠密光流映射，从而实现随时间推移的连贯几何和外观建模。我们显式的4D表示强制执行一个跨越视点和动态内容的单一底层场景，从而生成即使在大型非刚性运动和显著相机运动下仍然保持一致的视频。我们通过精心组合合成数据和真实数据来训练WorldReel：合成数据提供精确的4D监督（几何、运动和相机），而真实视频贡献视觉多样性和真实感。这种混合使WorldReel能够推广到真实场景的素材，同时保持强大的几何保真度。大量的实验表明，WorldReel为动态场景和移动相机的连贯视频生成建立了新的技术水平，在几何一致性、运动连贯性的指标上有所改进，并减少了竞争方法中存在的视角-时间伪影。我们相信WorldReel使视频生成更接近于4D一致的世界建模，在这种建模中，代理可以通过单一且稳定的时空表示来渲染、交互和推理场景。"
    },
    {
        "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
        "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.",
        "url": "http://arxiv.org/abs/2512.08765v1",
        "published_date": "2025-12-09T16:13:55+00:00",
        "updated_date": "2025-12-09T16:13:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruihang Chu",
            "Yefei He",
            "Zhekai Chen",
            "Shiwei Zhang",
            "Xiaogang Xu",
            "Bin Xia",
            "Dingdong Wang",
            "Hongwei Yi",
            "Xihui Liu",
            "Hengshuang Zhao",
            "Yu Liu",
            "Yingya Zhang",
            "Yujiu Yang"
        ],
        "tldr": "Wan-Move introduces a scalable framework for motion-controllable video generation using latent trajectory guidance, achieving precise control and high-quality results comparable to commercial alternatives and introduces a new benchmark, MoveBench.",
        "tldr_zh": "Wan-Move 提出了一种可扩展的运动可控视频生成框架，通过潜在轨迹引导实现精确控制和高质量效果，可与商业产品媲美，并引入了一个新的基准测试 MoveBench。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了Wan-Move，一个简单且可扩展的框架，为视频生成模型引入运动控制。现有的运动可控方法通常存在控制粒度粗糙和可扩展性有限的问题，导致其输出结果在实际应用中不足。我们通过实现精确和高质量的运动控制来缩小这一差距。我们的核心思想是直接使原始条件特征具有运动感知能力，从而指导视频合成。为此，我们首先使用密集点轨迹来表示物体运动，从而实现对场景的细粒度控制。然后，我们将这些轨迹投影到潜在空间，并沿着每个轨迹传播第一帧的特征，生成对齐的时空特征图，该地图指示每个场景元素应如何移动。这个特征图作为更新的潜在条件，被自然地集成到现成的图像到视频模型中，例如Wan-I2V-14B，作为运动指导，而无需任何架构更改。它消除了对辅助运动编码器的需求，并使基础模型的微调易于扩展。通过规模化训练，Wan-Move生成5秒、480p的视频，其运动可控性与Kling 1.5 Pro的商业Motion Brush相媲美，用户研究表明了这一点。为了支持全面的评估，我们进一步设计了MoveBench，一个经过严格管理的基准，具有多样化的内容类别和混合验证的标注。它以更大的数据量、更长的视频时长和高质量的运动标注而著称。在MoveBench和公共数据集上的大量实验一致表明了Wan-Move优越的运动质量。代码、模型和基准数据已公开提供。"
    },
    {
        "title": "OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics",
        "summary": "Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.",
        "url": "http://arxiv.org/abs/2512.08625v1",
        "published_date": "2025-12-09T14:10:23+00:00",
        "updated_date": "2025-12-09T14:10:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jisang Yoo",
            "Gyeongjin Kang",
            "Hyun-kyu Ko",
            "Hyeonwoo Yu",
            "Eunbyung Park"
        ],
        "tldr": "The paper presents OpenMonoGS-SLAM, a novel monocular SLAM framework combining 3D Gaussian Splatting with open-set semantics using visual foundation models, achieving comparable or superior performance to existing methods without depth sensors or 3D semantic ground truth.",
        "tldr_zh": "该论文提出了OpenMonoGS-SLAM，一种新颖的单目SLAM框架，它将3D高斯溅射与开放集语义结合，利用视觉基础模型，在没有深度传感器或3D语义真值的情况下，实现了与现有方法相当或更优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "同步定位与地图构建 (SLAM) 是机器人、增强现实/虚拟现实和自主系统中的一个基础组件。 随着近年来对空间人工智能的日益关注，将SLAM与语义理解相结合对于实现智能感知和交互变得越来越重要。 近期的研究工作探索了这种集成，但它们通常依赖于深度传感器或封闭集语义模型，从而限制了其在开放世界环境中的可扩展性和适应性。 在这项工作中，我们提出了 OpenMonoGS-SLAM，这是第一个将 3D 高斯溅射 (3DGS) 与开放集语义理解相结合的单目 SLAM 框架。 为了实现我们的目标，我们利用了视觉基础模型 (VFMs) 的最新进展，包括用于视觉几何的 MASt3R 和用于开放词汇语义的 SAM 和 CLIP。 这些模型在各种任务中提供了强大的泛化能力，从而实现了精确的单目相机跟踪和地图构建，以及对开放世界环境中语义的丰富理解。 我们的方法在没有任何深度输入或 3D 语义真值的情况下运行，仅依赖于自监督学习目标。 此外，我们提出了一种专门设计的用于管理高维语义特征的记忆机制，该机制有效地构建了高斯语义特征图，从而实现了强大的整体性能。 实验结果表明，我们的方法在封闭集和开放集分割任务中都达到了与现有基线相当或超过的性能，所有这些都无需依赖深度图或语义标注等补充传感器。"
    },
    {
        "title": "Thinking with Images via Self-Calling Agent",
        "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\\%$ with $\\sim 75\\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.",
        "url": "http://arxiv.org/abs/2512.08511v1",
        "published_date": "2025-12-09T11:53:21+00:00",
        "updated_date": "2025-12-09T11:53:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenxi Yang",
            "Yuzhong Zhao",
            "Fang Wan",
            "Qixiang Ye"
        ],
        "tldr": "The paper introduces Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates interleaved multimodal CoT as a language-only CoT with self-calling, achieving improved performance and efficiency compared to baselines.",
        "tldr_zh": "该论文介绍了自调用思维链 (sCoT)，这是一种新颖的视觉推理范式，它将交叉多模态 CoT 重构为具有自调用的纯语言 CoT，与基线方法相比，实现了更高的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "通过将视觉信息作为动态元素整合到思维链（CoT）中，图像思维范式已经展示了卓越的视觉推理能力。然而，通过强化学习优化交错式多模态CoT（iMCoT）仍然具有挑战性，因为它依赖于稀缺的高质量推理数据。在本研究中，我们提出了一种新颖的视觉推理范式——自调用思维链（sCoT），它将iMCoT重构为一种纯语言的CoT，并具有自调用能力。具体来说，一个主代理将复杂的视觉推理任务分解为原子子任务，并调用其虚拟副本，即参数共享的子代理，在隔离的上下文中解决这些子任务。sCoT具有显著的训练有效性和效率，因为它不需要模态之间的显式交错。sCoT采用群体相对策略优化来强化有效的推理行为，从而增强优化效果。在HR-Bench 4K上的实验表明，与强大的基线方法相比，sCoT在推理性能上提升了高达1.9%，同时减少了约75%的GPU时数。代码已发布在https://github.com/YWenxi/think-with-images-through-self-calling。"
    },
    {
        "title": "Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions",
        "summary": "Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/",
        "url": "http://arxiv.org/abs/2512.08500v1",
        "published_date": "2025-12-09T11:30:56+00:00",
        "updated_date": "2025-12-09T11:30:56+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Jianan Li",
            "Xiao Chen",
            "Tao Huang",
            "Tien-Tsin Wong"
        ],
        "tldr": "The paper presents Mimic2DM, a framework that learns 3D character motion control policies from 2D keypoint trajectories extracted from videos, without relying on 3D motion capture data. It uses a combination of 2D motion tracking and generation to achieve physically plausible and diverse motions.",
        "tldr_zh": "该论文提出了Mimic2DM框架，该框架仅从视频中提取的2D关键点轨迹学习3D角色运动控制策略，无需依赖3D运动捕捉数据。它结合了2D运动跟踪和生成，实现了物理上合理且多样化的运动。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "视频数据相比于动作捕捉数据，在学习3D角色运动控制器方面更具成本效益，然而直接从视频中合成逼真且多样的行为仍然充满挑战。以往的方法通常依赖于现成的运动重建技术来获取基于物理的模仿所需的3D轨迹。这些重建方法在泛化性上存在困难，因为它们要么需要3D训练数据（可能稀缺），要么无法产生物理上合理的姿势，阻碍了它们在诸如人-物交互（HOI）或非人类角色等具有挑战性的场景中的应用。我们通过引入Mimic2DM来解决这一挑战，这是一个新颖的运动模仿框架，它直接且仅从视频中提取的广泛可用的2D关键点轨迹学习控制策略。通过最小化重投影误差，我们训练了一个通用的单视图2D运动跟踪策略，该策略能够仅使用2D运动数据，在物理模拟中跟踪任意2D参考运动。该策略在从不同或略有不同的视角捕获的各种2D运动上训练时，可以通过聚合多个视图进一步获得3D运动跟踪能力。此外，我们开发了一个基于Transformer的自回归2D运动生成器，并将其集成到一个分层控制框架中，其中生成器产生高质量的2D参考轨迹来引导跟踪策略。我们证明，该方法用途广泛，并且能够有效地学习合成跨越各种领域（包括舞蹈、足球运球和动物运动）的物理上合理且多样的运动，而无需依赖任何显式的3D运动数据。项目网站：https://jiann-li.github.io/mimic2dm/"
    },
    {
        "title": "Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval",
        "summary": "Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.",
        "url": "http://arxiv.org/abs/2512.08410v1",
        "published_date": "2025-12-09T09:40:20+00:00",
        "updated_date": "2025-12-09T09:40:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Chen",
            "Shaobo Ju",
            "Qiong Wu",
            "Chenxin Fang",
            "Kun Zhang",
            "Jun Peng",
            "Hui Li",
            "Yiyi Zhou",
            "Rongrong Ji"
        ],
        "tldr": "The paper introduces OneClip-RAG, a novel approach to enhance Multimodal Large Language Models (MLLMs) for long video understanding by using one-shot clip retrieval and a query-guided video chunking algorithm. It achieves significant performance and efficiency gains on long video benchmarks.",
        "tldr_zh": "该论文介绍了OneClip-RAG，一种新颖的方法，通过使用单次clip检索和查询引导的视频分块算法来增强多模态大型语言模型（MLLM）对长视频的理解。它在长视频基准测试中实现了显著的性能和效率提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "摘要：\n由于过高的内存开销，大多数多模态大型语言模型(MLLMs)只能处理帧数有限的视频。在本文中，我们提出了一种有效且高效的范式来弥补这一缺陷，称为基于单样本视频片段检索增强（OneClip-RAG）。与现有的视频RAG方法相比，OneClip-RAG充分利用了视频片段的优势，在知识完整性和语义连贯性方面增强了视频理解。此外，它还配备了一种新颖的查询引导视频分块算法，可以将片段分块和跨模态检索统一在一个处理步骤中，避免冗余计算。为了提高指令遵循能力，我们进一步提出了一个名为SynLongVideo 的新数据集，并为OneClip-RAG设计了一个渐进式训练方案。OneClip-RAG被嵌入到五个最新的MLLMs中，并在多个长视频基准上进行了验证。实验结果不仅表明，OneClip-RAG相比MLLMs具有明显的性能提升，例如将InternLV2 8B和Qwen2-VL 7B在MLVU上的表现提升到GPT-4o的水平，而且还展示了其在处理长视频方面的卓越效率，例如，使得LLaVA-Video能够在单张4090 GPU上在不到2.2分钟的时间内理解长达一小时的视频。"
    },
    {
        "title": "CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning",
        "summary": "We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.",
        "url": "http://arxiv.org/abs/2512.08135v1",
        "published_date": "2025-12-09T00:21:13+00:00",
        "updated_date": "2025-12-09T00:21:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyuan Chen",
            "Xiang Zhang",
            "Haiyang Xu",
            "Jianwen Xie",
            "Zhuowen Tu"
        ],
        "tldr": "The paper introduces CVP, a multimodal model inspired by central and peripheral vision, for improved spatial reasoning in 3D scene understanding by integrating target-affinity tokens and allocentric grids into a Large Multimodal Model.",
        "tldr_zh": "该论文介绍了一种名为CVP的多模态模型，该模型受到中央和周边视觉的启发，通过将目标亲和力令牌和以自我为中心的网格集成到大型多模态模型中，从而改进了3D场景理解中的空间推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了一种受中心-周边视觉启发的框架（CVP），这是一个简单而有效的多模态模型，用于空间推理，其灵感来源于人类的两种视觉视野——中心视觉和周边视觉。现有方法主要依赖于非结构化的表示，例如点云、体素或块状特征，并通过坐标嵌入隐式地注入场景上下文信息。然而，由于缺乏显式、高层次的结构理解，这往往会导致空间推理能力受限。为了解决这一局限，我们将两个互补的组件引入到基于大型多模态模型的架构中：目标亲和令牌，类似于中心视觉，引导模型将注意力集中在与查询相关的对象上；以及以自我为中心的网格，类似于周边视觉，捕获全局场景上下文和空间布局。这些组件协同工作，能够实现对复杂3D环境的结构化、上下文感知的理解。实验表明，CVP在一系列3D场景理解基准测试中实现了最先进的性能。"
    },
    {
        "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes",
        "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.",
        "url": "http://arxiv.org/abs/2512.07807v1",
        "published_date": "2025-12-08T18:39:58+00:00",
        "updated_date": "2025-12-08T18:39:58+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Shai Krakovsky",
            "Gal Fiebelman",
            "Sagie Benaim",
            "Hadar Averbuch-Elor"
        ],
        "tldr": "Lang3D-XL introduces a novel method for embedding language fields into 3D Gaussian representations for large-scale scenes, addressing challenges in semantic feature misalignment and efficiency. It achieves improved performance and efficiency on the HolyScenes dataset.",
        "tldr_zh": "Lang3D-XL 提出了一种新颖的方法，将语言场嵌入到大规模场景的 3D 高斯表示中，解决了语义特征未对齐和效率方面的挑战。在 HolyScenes 数据集上实现了更高的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "将语言字段嵌入到3D表示中，可以将几何结构与描述性含义联系起来，从而实现更丰富的空间环境语义理解。这允许更直观的人机交互，能够使用自然语言查询或编辑场景，并且可能改进场景检索、导航和多模态推理等任务。尽管这些能力具有变革性，尤其是对于大规模场景而言，但我们发现，由于语义特征未对齐以及内存和运行时效率方面的挑战，近期的特征蒸馏方法无法在海量互联网数据上有效学习。为此，我们提出了一种新方法来解决这些挑战。首先，我们引入了极低维度的语义瓶颈特征，作为底层3D高斯表示的一部分，通过渲染处理并通过一个基于多分辨率特征的哈希编码器对其进行处理。这显著提高了运行时和GPU内存的效率。其次，我们引入了一个衰减下采样器模块，并提出了几种正则化方法，以解决真实2D特征的语义未对齐问题。我们在实际场景 HolyScenes 数据集上评估了我们的方法，并证明其在性能和效率方面都优于现有方法。"
    },
    {
        "title": "Robust Agents in Open-Ended Worlds",
        "summary": "The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.",
        "url": "http://arxiv.org/abs/2512.08139v1",
        "published_date": "2025-12-09T00:30:33+00:00",
        "updated_date": "2025-12-09T00:30:33+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Mikayel Samvelyan"
        ],
        "tldr": "This thesis explores methods for training and evaluating robust AI agents in open-ended worlds, using environments like MiniHack and techniques like adversarial curricula and quality diversity to enhance generalizability in RL and LLMs.",
        "tldr_zh": "该论文探讨了在开放世界中训练和评估鲁棒AI agent的方法，采用MiniHack等环境以及对抗课程和质量多样性等技术，以增强RL和LLM的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人工智能在各种应用中日益普及，凸显了对能够成功导航并适应瞬息万变、开放性世界的智能体的需求。一个关键挑战是确保这些人工智能体具有鲁棒性，不仅在训练期间观察到的熟悉环境中表现出色，而且能够有效地泛化到以前未见过的各种场景中。在本论文中，我们利用开放性和多智能体学习的方法来训练和评估具有鲁棒性的人工智能体，使其能够泛化到新环境中、分布式外输入以及与其他协同智能体的交互。我们首先介绍 MiniHack，这是一个通过程序化内容生成创建多样化环境的沙盒框架。基于 NetHack 游戏，MiniHack 能够为强化学习 (RL) 智能体构建新的任务，重点是泛化。然后，我们提出 Maestro，一种用于生成对抗性课程的新方法，逐步提高 RL 智能体在双人零和游戏中的鲁棒性和泛化能力。我们进一步探索多智能体领域中的鲁棒性，利用质量-多样性方法系统地识别复杂视频足球游戏中最先进的预训练 RL 策略的漏洞，该游戏以相互交织的合作和竞争动态为特征。最后，我们将对鲁棒性的探索扩展到 LLM 领域。在这里，我们的重点是诊断和增强 LLM 针对对抗性提示的鲁棒性，采用进化搜索来生成各种有效的输入，旨在从 LLM 中引发不良输出。这项工作共同为人工智能鲁棒性的未来发展铺平了道路，使人工智能体不仅能够适应不断发展的世界，而且能够在面对不可预见的挑战和互动时蓬勃发展。"
    },
    {
        "title": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks",
        "summary": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.",
        "url": "http://arxiv.org/abs/2512.08813v1",
        "published_date": "2025-12-09T17:06:21+00:00",
        "updated_date": "2025-12-09T17:06:21+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Connor York",
            "Zachary R Madin",
            "Paul O'Dowd",
            "Edmund R Hunt"
        ],
        "tldr": "This paper investigates the performance trade-offs in multi-robot systems performing continuous environmental monitoring tasks when interrupted by urgent tasks, and shows behavioral and sensing heterogeneity can improve this trade-off.",
        "tldr_zh": "本文研究了多机器人系统在执行连续环境监测任务时，被紧急任务中断时的性能权衡，并表明行为和感知异质性可以改善这种权衡。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "执行连续性任务的多机器人系统在被紧急、时间敏感的子任务中断时，面临性能权衡。我们研究了一种场景下的这种权衡，在这种场景中，一个团队必须在区域巡逻和定位异常无线电信号之间取得平衡。为了解决这种权衡，我们评估了通过代理角色专业化（“巡逻者”和“搜索者”）实现的行为异构性，以及传感异构性（即，只有搜索者可以感知无线电信号）。通过仿真，我们识别了在不同团队组成下帕累托最优的权衡，其中行为异构团队在大多数情况下表现出最平衡的权衡。当传感能力受到限制时，具有一半具有传感能力的代理的异构团队的表现与同构团队相当，为限制传感器有效载荷部署提供了节省成本的理由。我们的研究结果表明，预先部署的角色和传感专业化是多机器人系统面临时间冲突任务时强大的设计考虑因素，其中改变行为异构性的程度可以调整系统性能以适应任一任务。"
    },
    {
        "title": "Data-Driven Dynamic Parameter Learning of manipulator robots",
        "summary": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems",
        "url": "http://arxiv.org/abs/2512.08767v1",
        "published_date": "2025-12-09T16:15:58+00:00",
        "updated_date": "2025-12-09T16:15:58+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Mohammed Elseiagy",
            "Tsige Tadesse Alemayoh",
            "Ranulfo Bezerra",
            "Shotaro Kojima",
            "Kazunori Ohno"
        ],
        "tldr": "This paper presents a Transformer-based approach for dynamic parameter estimation of robot manipulators, utilizing an automated pipeline for diverse robot models and trajectory data, achieving high accuracy in estimating mass and inertia.",
        "tldr_zh": "本文提出了一种基于Transformer的机器人机械臂动态参数估计方法，采用自动化流程生成多样化的机器人模型和轨迹数据，并在估计质量和惯性方面实现了高精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于Transformer的机器人动力学参数估计：自动化管线与运动学特征增强\n\n弥合虚实差距仍然是机器人领域的根本挑战，精确的动力学参数估计对于可靠的基于模型的控制、逼真的仿真以及机械臂的安全部署至关重要。传统的解析方法在面对复杂的机器人结构和交互时往往表现不佳。数据驱动的方法提供了一种有前景的替代方案，但诸如循环模型之类的传统神经网络难以捕捉对于精确估计至关重要的长程依赖关系。在本研究中，我们提出了一种基于Transformer架构的动力学参数估计方法，并由自动化管线支持，该管线利用雅可比导出的特征生成多样化的机器人模型和丰富化的轨迹数据。该数据集包含8,192个具有不同惯性和摩擦特性的机器人。利用注意力机制，我们的模型有效地捕捉了时序和空间依赖关系。实验结果突出了序列长度、采样率和架构的影响，最佳配置（序列长度64，64 Hz，四层，32头）实现了验证R2值为0.8633。质量和惯量的估计几乎完美，库仑摩擦的估计精度为中等到高，而粘性摩擦和末端连杆质心的估计仍然更具挑战性。这些结果表明，将Transformer与自动化数据集生成和运动学增强相结合，可以实现可扩展、精确的动力学参数估计，从而有助于改进机器人系统中的虚实迁移。"
    },
    {
        "title": "A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models",
        "summary": "This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.",
        "url": "http://arxiv.org/abs/2512.08754v1",
        "published_date": "2025-12-09T16:05:22+00:00",
        "updated_date": "2025-12-09T16:05:22+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jason Hughes",
            "Marcel Hussing",
            "Edward Zhang",
            "Shenbagaraj Kannapiran",
            "Joshua Caswell",
            "Kenneth Chaney",
            "Ruichen Deng",
            "Michaela Feehery",
            "Agelos Kratimenos",
            "Yi Fan Li",
            "Britny Major",
            "Ethan Sanchez",
            "Sumukh Shrote",
            "Youkang Wang",
            "Jeremy Wang",
            "Daudi Zein",
            "Luying Zhang",
            "Ruijun Zhang",
            "Alex Zhou",
            "Tenzi Zhouga",
            "Jeremy Cannon",
            "Zaffir Qasim",
            "Jay Yelon",
            "Fernando Cladera",
            "Kostas Daniilidis",
            "Camillo J. Taylor",
            "Eric Eaton"
        ],
        "tldr": "This paper presents a multi-robot system (UAVs and UGVs) for remote triage in mass-casualty incidents, encompassing victim localization, vital sign measurement, injury classification, mental status assessment, and data consolidation.",
        "tldr_zh": "本文介绍了一种用于大规模伤亡事件中远程分类的多机器人系统（无人机和无人地面车辆），涵盖了受害者定位、生命体征测量、损伤分类、精神状态评估和数据整合。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "本报告提出了一种异构机器人系统，该系统专为大规模伤亡事件（MCI）中的远程初步检伤分类而设计。该系统采用由无人机（UAV）和无人地面车辆（UGV）组成的协同空地团队，用于定位受害者、评估其伤情，并在不危及急救人员生命的情况下，对其医疗援助进行优先级排序。无人机识别并提供伤亡人员的鸟瞰图，而配备专用传感器的无人地面车辆则测量生命体征，并检测和定位身体损伤。与以往侧重于探索或有限医疗评估的工作不同，该系统解决了完整的检伤分类流程：受害者定位、生命体征测量、伤情严重程度分类、精神状态评估，以及为急救人员提供的数据整合。作为DARPA检伤分类挑战赛的一部分开发，该方法展示了多机器人系统如何在灾难响应场景中增强人类能力，从而最大限度地挽救生命。"
    },
    {
        "title": "Ergodic Trajectory Planning with Dynamic Sensor Footprints",
        "summary": "This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.",
        "url": "http://arxiv.org/abs/2512.08661v1",
        "published_date": "2025-12-09T14:47:14+00:00",
        "updated_date": "2025-12-09T14:47:14+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Ziyue Zheng",
            "Yongce Liu",
            "Hesheng Wang",
            "Zhongqiang Ren"
        ],
        "tldr": "This paper introduces a novel ergodic planning approach that accounts for dynamic sensor footprints for robotic information gathering, demonstrating significant improvements in ergodicity compared to conventional methods, particularly in multi-drone systems.",
        "tldr_zh": "本文提出了一种新的遍历规划方法，该方法考虑了机器人信息收集的动态传感器足迹，与传统方法相比，在遍历性方面表现出显着改进，尤其是在多无人机系统中。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文研究了具有动态且分辨率可变传感器覆盖区域的信息采集轨迹规划问题。遍历规划提供了一种原则性的框架，通过规划使机器人在某一区域的停留时间与该区域的信息量成正比的轨迹，来平衡探索（访问所有区域）和利用（关注高信息区域）。现有的遍历规划通常通过假设点传感器或具有恒定形状和分辨率的覆盖区域来过度简化感知模型。 在实践中，传感器覆盖区域可能随机器人移动而急剧变化，例如配备向下摄像头的小型无人机，其视野取决于朝向和高度。 为了克服这一限制，我们提出了一种考虑动态传感器覆盖区域的新度量，分析了理论上的局部最优条件，并提出了数值轨迹优化算法。实验结果表明，所提出的方法可以同时优化轨迹和传感器覆盖区域，其遍历性比传统方法提高一个数量级。我们还在一个多无人机系统中部署了我们的方法，以遍历地覆盖三维空间中的一个物体。"
    },
    {
        "title": "A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation",
        "summary": "Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.",
        "url": "http://arxiv.org/abs/2512.08653v1",
        "published_date": "2025-12-09T14:41:20+00:00",
        "updated_date": "2025-12-09T14:41:20+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Doumegna Mawuto Koudjo Felix",
            "Xianjia Yu",
            "Zhuo Zou",
            "Tomi Westerlund"
        ],
        "tldr": "This paper introduces a sensor-aware framework for simulating lidar degradations on real point clouds to evaluate the robustness of lidar-based SLAM systems, highlighting distinct robustness patterns across different lidar architectures and SLAM systems.",
        "tldr_zh": "本文介绍了一种传感器感知的框架，用于在真实点云上模拟激光雷达退化，以评估基于激光雷达的 SLAM 系统的鲁棒性，强调了不同激光雷达架构和 SLAM 系统中不同的鲁棒性模式。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于激光雷达的SLAM系统对诸如遮挡、噪声和视场 (FoV) 退化等不利条件高度敏感，然而现有的鲁棒性评估方法或缺乏物理基础，或无法捕捉到传感器特定的行为。本文提出了一种传感器感知的现象学框架，用于直接在真实点云上模拟可解释的激光雷达退化，从而实现可控且可重复的SLAM压力测试。与图像衍生的损坏基准（例如SemanticKITTI-C）或纯仿真方法（例如lidarsim）不同，该系统在应用结构化丢弃、FoV缩减、高斯噪声、遮挡掩蔽、稀疏化和运动畸变的同时，保留了每个点的几何结构、强度和时间结构。该框架具有自主主题和传感器检测、具有四个严重程度等级（轻微--极端）的模块化配置，以及与ROS工作流程兼容的实时性能（每帧小于20毫秒）。跨三种激光雷达架构和五种最先进的SLAM系统的实验验证揭示了由传感器设计和环境背景塑造的独特鲁棒性模式。该开源实现为在具有物理意义的退化场景下对基于激光雷达的SLAM进行基准测试提供了坚实的基础。"
    },
    {
        "title": "vEDGAR -- Can CARLA Do HiL?",
        "summary": "Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR",
        "url": "http://arxiv.org/abs/2512.08541v1",
        "published_date": "2025-12-09T12:39:30+00:00",
        "updated_date": "2025-12-09T12:39:30+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Nils Gehrke",
            "David Brecht",
            "Dominik Kulmer",
            "Dheer Patel",
            "Frank Diermeyer"
        ],
        "tldr": "The paper presents vEDGAR, a simulation framework built on CARLA, designed to enable Hardware-in-the-Loop (HiL) testing for automated vehicles, aiming to improve the open-source development workflow by allowing consistent evaluation throughout the development process. The framework and modified CARLA are available open source.",
        "tldr_zh": "本文介绍了一个名为 vEDGAR 的仿真框架，它构建于 CARLA 之上，旨在为自动驾驶车辆实现硬件在环 (HiL) 测试，通过在整个开发过程中实现一致的评估，从而改进开源开发工作流程。该框架和修改后的 CARLA 均以开源方式提供。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "仿真在自动驾驶功能的整个开发过程中，包括研究和产品开发阶段，都具有显著优势。诸如CARLA等常见的开源仿真器被广泛应用于新型自动驾驶算法的训练、评估以及软件在环测试。然而，目前的CARLA仿真器缺乏对研究和自动驾驶车辆进行实时全传感器和执行器堆栈仿真的评估能力。因此，本工作的目标是创建一个仿真框架，用于在专用硬件上测试自动化软件并确定其极限。实现此目标将极大地促进自动驾驶功能的开源开发流程，并将 CARLA 指定为整个开发过程中一致的评估工具。为实现此目标，首先推导出需求，然后指定并实现一种仿真架构。基于这些已制定的需求，对所提出的 vEDGAR 软件进行评估，最终得出关于CARLA应用于自动驾驶车辆硬件在环测试的适用性结论。该工具已开源：CARLA修改版本：https://github.com/TUMFTM/carla，vEDGAR框架：https://github.com/TUMFTM/vEDGAR"
    },
    {
        "title": "Using reinforcement learning to probe the role of feedback in skill acquisition",
        "summary": "Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.",
        "url": "http://arxiv.org/abs/2512.08463v1",
        "published_date": "2025-12-09T10:37:42+00:00",
        "updated_date": "2025-12-09T10:37:42+00:00",
        "categories": [
            "cs.AI",
            "cs.LG",
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Antonio Terpin",
            "Raffaello D'Andrea"
        ],
        "tldr": "This paper uses reinforcement learning to control a cylinder in a water channel for drag maximization/minimization, finding that learned policies can be executed without feedback, and that the need for feedback during training depends on the goal.",
        "tldr_zh": "本文利用强化学习控制水槽中圆柱体的阻力最大化/最小化，发现学习到的策略可以在没有反馈的情况下执行，并且训练期间对反馈的需求取决于目标。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "许多高性能的人类活动在几乎没有或没有外部反馈的情况下执行：例如花样滑冰运动员完成三周跳，投手投出弧线球获得好球，或咖啡师制作拿铁艺术。为了在完全受控的条件下研究技能习得的过程，我们绕过了人类受试者。相反，我们将一个通用强化学习智能体直接与桌面循环水槽中的旋转圆柱体连接，以最大化或最小化阻力。这种设置具有几个理想的特性。首先，它是一个物理系统，具有只有物理世界才具备的丰富交互和复杂动力学：流动是高度混沌的，并且极难甚至不可能对其进行精确建模或模拟。其次，目标 -- 阻力最小化或最大化 -- 很容易表述，并且可以直接在奖励中体现，然而好的策略事先并不明显。第三，数十年前的实验研究提供了简单、高性能的开环策略的方案。最后，该设置成本低廉，并且比人类研究更容易重现。在我们的实验中，我们发现高维流动反馈让智能体仅用几分钟的真实世界交互就能发现高性能的阻力控制策略。当我们随后在没有任何反馈的情况下重放相同的动作序列时，我们获得了几乎相同的性能。这表明执行已学习的策略不需要反馈，特别是流动反馈。令人惊讶的是，在训练期间没有流动反馈的情况下，智能体在阻力最大化方面未能发现任何表现良好的策略，但在阻力最小化方面仍然成功，尽管速度较慢且可靠性较低。我们的研究表明，学习一项高性能技能可能需要比执行它更丰富的信息，并且学习条件可能因目标的善恶而异，而不是取决于动力学或策略的复杂性。"
    },
    {
        "title": "SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking",
        "summary": "Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.",
        "url": "http://arxiv.org/abs/2512.08430v1",
        "published_date": "2025-12-09T09:58:35+00:00",
        "updated_date": "2025-12-09T09:58:35+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Nico Leuze",
            "Maximilian Hoh",
            "Samed Doğan",
            "Nicolas R. -Peña",
            "Alfred Schoettl"
        ],
        "tldr": "This paper introduces SDT-6D, a fully sparse depth-transformer framework for 6D pose estimation in cluttered bin-picking environments, leveraging multi-view depth maps and a novel per-voxel voting strategy for efficient and accurate object pose prediction.",
        "tldr_zh": "本文介绍了 SDT-6D，一个用于在杂乱的箱子拣选环境中进行 6D 姿势估计的全稀疏深度转换器框架，利用多视图深度图和一种新的基于体素的投票策略，来实现高效和准确的物体姿势预测。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在密集堆积的工业箱式拣选环境中，精确地恢复 6D 姿态仍然是一个严峻的挑战，这归因于遮挡、反射和无纹理部件。我们提出了一种整体的、仅基于深度信息的 6D 姿态估计方法，该方法将多视角深度图融合为精细的 3D 点云（其原始版本），或者融合为稀疏的截断符号距离场 (TSDF)。我们框架的核心在于一个分阶段的热图机制，该机制在不同的分辨率下产生场景自适应的注意力先验，引导计算朝着前景区域进行，从而使高分辨率下的内存需求保持在可行的范围内。同时，我们提出了一种密度感知的稀疏 Transformer 块，该模块动态地关注（自）遮挡和 3D 数据的非均匀分布。尽管稀疏 3D 方法已被证明对于远距离感知是有效的，但其在近距离机器人应用中的潜力仍未得到充分探索。我们的框架完全以稀疏方式运行，从而能够实现高分辨率的体素表示，以捕获对于在杂乱环境中进行精确姿态估计至关重要的精细几何细节。我们的方法整体地处理整个场景，通过一种新颖的基于体素的投票策略预测 6D 姿态，从而允许同时预测任意数量目标对象的姿态。我们在最近发布的 IPD 和 MV-YCB 多视角数据集上验证了我们的方法，展示了在高度杂乱的工业和家庭箱式拣选场景中具有竞争力的性能。"
    },
    {
        "title": "An Introduction to Deep Reinforcement and Imitation Learning",
        "summary": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.",
        "url": "http://arxiv.org/abs/2512.08052v1",
        "published_date": "2025-12-08T21:21:01+00:00",
        "updated_date": "2025-12-08T21:21:01+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Pedro Santana"
        ],
        "tldr": "This paper provides a concise introduction to Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL) for embodied agents, covering foundational algorithms like REINFORCE, PPO, Behavioral Cloning, DAgger, and GAIL.",
        "tldr_zh": "本文简要介绍了具身智能体的深度强化学习（DRL）和深度模仿学习（DIL），涵盖了REINFORCE、PPO、行为克隆、DAgger和GAIL等基础算法。",
        "relevance_score": 8,
        "novelty_claim_score": 3,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7,
        "summary_zh": "具身智能体，例如机器人和虚拟角色，必须持续选择动作以有效地执行任务，从而解决复杂的序列决策问题。鉴于手动设计此类控制器的难度，基于学习的方法已经成为有希望的替代方案，其中最值得注意的是深度强化学习 (DRL) 和深度模仿学习 (DIL)。DRL 利用奖励信号来优化行为，而 DIL 使用专家演示来指导学习。本文介绍了具身智能体背景下的 DRL 和 DIL，并以简洁的深度优先方式呈现相关文献。它是自包含的，根据需要介绍所有必要的数学和机器学习概念。本文并非旨在对该领域进行综述，而是侧重于一小部分基础算法和技术，优先考虑深入理解而非广泛覆盖。涉及的材料范围从马尔可夫决策过程到 DRL 的 REINFORCE 和近端策略优化 (PPO)，再到 DIL 的行为克隆、数据集聚合 (DAgger) 和生成对抗模仿学习 (GAIL)。"
    },
    {
        "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
        "summary": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
        "url": "http://arxiv.org/abs/2512.08769v1",
        "published_date": "2025-12-09T16:23:05+00:00",
        "updated_date": "2025-12-09T16:23:05+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Eranga Bandara",
            "Ross Gore",
            "Peter Foytik",
            "Sachin Shetty",
            "Ravi Mukkamala",
            "Abdul Rahman",
            "Xueping Liang",
            "Safdar H. Bouk",
            "Amin Hass",
            "Sachini Rajapakse",
            "Ng Wee Keong",
            "Kasun De Zoysa",
            "Aruna Withanage",
            "Nilaan Loganathan"
        ],
        "tldr": "This paper presents a practical guide for designing, developing, and deploying production-grade agentic AI workflows, offering best practices, architectural guidance, and a case study to build robust and scalable systems.",
        "tldr_zh": "本文提供了一个关于设计、开发和部署生产级Agentic AI工作流的实用指南，提供了最佳实践、架构指导和一个案例研究，以构建健壮且可扩展的系统。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "自主智能（Agentic AI）标志着自主系统在推理、规划和执行多步骤任务方式上的一次重大转变。与传统的单模型提示不同，自主智能工作流整合了多个具有不同大型语言模型(LLMs)、工具增强能力、编排逻辑和外部系统交互的专业化代理，形成能够进行自主决策和行动的动态管道。随着行业和研究领域加速采用，各组织面临着一个核心挑战：如何设计、工程化和运营生产级别的自主智能工作流，使其具有可靠性、可观测性、可维护性，并符合安全和治理要求。本文提供了一个实用且端到端的指南，用于设计、开发和部署高质量的生产级自主智能系统。我们介绍了一个结构化的工程生命周期，涵盖工作流分解、多代理设计模式、模型上下文协议(MCP)和工具集成、确定性编排、负责任的AI考量以及环境感知部署策略。 随后，我们提出了九个核心最佳实践，用于工程化生产级自主智能工作流，包括基于模型上下文协议(MCP)的工具优先设计、纯函数调用、单工具和单职责代理、外部化提示管理、符合负责任AI的模型联盟设计、工作流逻辑与模型上下文协议(MCP)服务器之间的清晰分离、用于可扩展操作的容器化部署，以及遵守“保持简单，笨蛋”（KISS）原则以保持简单性和稳健性。为了在实践中展示这些原则，我们提供了一个全面的案例研究：一个多模态新闻分析和媒体生成工作流。通过结合架构指导、运营模式和实践实施见解，本文提供了一个基础参考，用于构建稳健、可扩展且可用于生产的自主智能工作流。"
    },
    {
        "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
        "summary": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.",
        "url": "http://arxiv.org/abs/2512.08609v1",
        "published_date": "2025-12-09T13:54:18+00:00",
        "updated_date": "2025-12-09T13:54:18+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Hui Wang",
            "Yang Liu",
            "Xiaoyu Zhang",
            "Chaoxu Mu"
        ],
        "tldr": "The paper introduces CogMCTS, a novel LLM-guided MCTS framework for automated heuristic design, which uses multi-round cognitive feedback and dual-track node expansion to improve performance over existing methods.",
        "tldr_zh": "该论文介绍了一种名为CogMCTS的新型LLM引导的MCTS框架，用于自动启发式设计。该框架采用多轮认知反馈和双轨节点扩展，以提高性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自动启发式设计（AHD）是解决复杂优化问题的有效框架。大型语言模型（LLM）的发展使得自动生成启发式策略成为可能。现有基于LLM的进化方法依赖于种群策略，并且容易陷入局部最优。将LLM与蒙特卡洛树搜索（MCTS）相结合可以改善探索与利用之间的权衡，但多轮认知整合仍然有限，搜索多样性受到约束。为了克服这些限制，本文提出了一种新颖的认知引导MCTS框架（CogMCTS）。CogMCTS将LLM的认知指导机制与MCTS紧密结合，以实现高效的自动启发式优化。该框架采用多轮认知反馈来整合历史经验、节点信息和负面结果，从而动态改进启发式生成。双轨节点扩展结合精英启发式管理，平衡了对不同启发式策略的探索和对高质量经验的利用。此外，策略性变异修改启发式策略的形式和参数，以进一步增强解决方案的多样性和整体优化性能。实验结果表明，CogMCTS在稳定性、效率和解决方案质量方面均优于现有的基于LLM的AHD方法。"
    },
    {
        "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans",
        "summary": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.",
        "url": "http://arxiv.org/abs/2512.08536v1",
        "published_date": "2025-12-09T12:34:54+00:00",
        "updated_date": "2025-12-09T12:34:54+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Tammy Zhong",
            "Yang Song",
            "Maurice Pagnucco"
        ],
        "tldr": "Principles2Plan is a system using human-LLM collaboration to generate context-sensitive ethical rules from high-level principles for automated planning, aiming to make ethical automated planning more practical.",
        "tldr_zh": "Principles2Plan是一个系统，利用人机协作，从高级原则生成上下文相关的道德规则，用于自动规划，旨在使道德自动规划更具实用性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在人类环境中运行的机器人，其伦理意识至关重要，然而现有的自动化规划工具对此提供的支持甚少。手动指定伦理规则既费力又高度依赖于具体情境。我们提出了Principles2Plan，这是一个交互式研究原型，展示了人类与大型语言模型（LLM）如何协作以生成上下文敏感的伦理规则并指导自动化规划。领域专家提供规划领域、问题细节以及相关的高级原则，如行善和隐私。系统生成与这些原则相符的可操作伦理规则，用户可以审查、优先排序这些规则，并将其提供给规划器以生成符合伦理的计划。据我们所知，此前没有系统支持用户为经典规划环境生成基于原则的规则。Principles2Plan展示了人机协作在使伦理自动化规划更实用和可行的潜力。"
    },
    {
        "title": "Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions",
        "summary": "Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called \"empowerment\" which maximizes mutual information between actions and their outcomes. \"Empowerment\" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.",
        "url": "http://arxiv.org/abs/2512.08230v1",
        "published_date": "2025-12-09T04:14:48+00:00",
        "updated_date": "2025-12-09T04:14:48+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Eunice Yiu",
            "Kelsey Allen",
            "Shiry Ginosar",
            "Alison Gopnik"
        ],
        "tldr": "This paper explores the connection between empowerment (an intrinsic reward signal maximizing action-outcome mutual information) and causal model construction in humans, suggesting it as a bridge between Bayesian causal learning and reinforcement learning, with an empirical study on children and adults.",
        "tldr_zh": "本文探讨了内在奖励信号'赋权'(最大化行动与结果之间的互信息)与人类因果模型构建之间的联系，并提出它作为贝叶斯因果学习和强化学习之间的桥梁，并通过儿童和成人的实证研究进行了验证。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "对世界因果结构的认知是人类认知的一个根本性问题。 对于使用标准深度学习技术的大型预训练模型而言，因果模型，尤其是因果学习，已经被证明是困难的。 相反，认知科学家已将我们在计算机科学中对因果关系进行形式化理解的进展（尤其是在因果贝叶斯网络形式体系内）应用于理解人类的因果学习。 在截然不同的强化学习传统中，研究人员描述了一种内在奖励信号，称为“赋能”，它最大限度地提高了行动与结果之间的互信息。“赋能”可能是经典贝叶斯因果学习和强化学习之间的重要桥梁，可能有助于刻画人类的因果学习并在机器中实现它。 如果智能体学习了一个准确的因果世界模型，他们必然会增加其赋能，而增加赋能将导致更准确的因果世界模型。 赋能可能还可以解释儿童因果学习的独特特征，并为如何实现这种学习提供更易于处理的计算解释。 在一项经验研究中，我们系统地测试了儿童和成年人如何使用赋能线索来推断因果关系，并设计有效的因果干预。"
    },
    {
        "title": "Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference",
        "summary": "Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.",
        "url": "http://arxiv.org/abs/2512.08860v1",
        "published_date": "2025-12-09T17:52:57+00:00",
        "updated_date": "2025-12-09T17:52:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Amit Bendkhale"
        ],
        "tldr": "The paper introduces Tri-Bench, a benchmark to stress-test VLMs' spatial reasoning abilities under camera tilt and object interference, revealing limitations in utilizing explicit frame-of-reference hints and recognizing specific triangle shapes.",
        "tldr_zh": "该论文介绍了Tri-Bench，一个用于压力测试VLM在相机倾斜和物体干扰下空间推理能力的基准，揭示了模型在利用显式参考系提示和识别特定三角形形状方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "可验证的几何推理是可信赖和可控的智能体人工智能的关键组成部分。尽管视觉-语言模型（VLMs）展现了令人印象深刻的能力，但在现实场景变化下，它们经常失效。我们提出了Tri-Bench，一个紧凑的平面三角形问题基准，它隔离了相对几何推理，同时强调了两个部署关键因素：相机姿态（平面与倾斜）以及通过物体干扰产生的场景上下文（10个日常物体）。为了测试可验证性和可控性，我们使用一个固定的提示评估了四个最新的VLMs，该提示中的安全护栏明确描述了一个周围的正方形边界，从而可以通过单应性变换得到正确的答案。我们评估了六个关于二元和连续目标的简单任务，并观察到相对于3D真实值的总体准确率不高，平均约为69％（最佳约为75％，最差约为64％）。在图像平面中，相同的响应与2D投影对齐得更紧密，平均准确率约为72％。所有四个VLMs均表现出一致的失败，在识别少数形状类别（等边三角形、等腰三角形、直角三角形）时，准确率降至约0％。此外，在相机倾斜的情况下，VLM的总体准确率下降约4.1％。这表明模型未能正确利用提示中提供的显式参考系提示，而默认使用2D图像平面线索。最后，我们发现物体干扰对VLM的准确率没有显著影响。"
    },
    {
        "title": "MVP: Multiple View Prediction Improves GUI Grounding",
        "summary": "GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.",
        "url": "http://arxiv.org/abs/2512.08529v1",
        "published_date": "2025-12-09T12:19:00+00:00",
        "updated_date": "2025-12-09T12:19:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunzhu Zhang",
            "Zeyu Pan",
            "Zhengwen Zeng",
            "Shuheng Shen",
            "Changhua Meng",
            "Linchao Zhu"
        ],
        "tldr": "The paper introduces MVP, a training-free framework for improving GUI grounding by using multi-view inference to enhance coordinate prediction stability. It addresses instability issues in existing models by aggregating predictions from multiple cropped views and shows significant performance boosts across various models and benchmarks.",
        "tldr_zh": "该论文介绍了MVP，一个无需训练的框架，通过多视角推断来提高GUI接地的效果，从而增强坐标预测的稳定性。它通过聚合来自多个裁剪视角的预测来解决现有模型中的不稳定性问题，并在各种模型和基准测试中显示出显著的性能提升。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "GUI 地标定位（GUI grounding）将自然语言指令翻译成精确的像素坐标，对于开发实用的 GUI 智能体至关重要。然而，我们观察到现有的地标定位模型表现出显著的坐标预测不稳定性，微小的视觉扰动（例如，裁剪几个像素）可能会剧烈改变预测结果，在正确和错误之间翻转。这种不稳定性严重削弱了模型性能，特别是对于具有高分辨率和小 UI 元素的样本。为了解决这个问题，我们提出了多视角预测（MVP），这是一个无训练框架，通过多视角推理来增强地标定位性能。我们的核心思想是，虽然单视角预测可能不稳定，但聚合来自多个精心裁剪的视角的预测可以有效地将正确的坐标与异常值区分开来。MVP 包括两个组成部分：（1）注意力引导的视角建议，它根据指令-图像的注意力分数导出不同的视角；（2）多坐标聚类，它通过选择最密集空间聚类的质心来集成预测结果。大量的实验证明了 MVP 在各种模型和基准测试中的有效性。值得注意的是，在 ScreenSpot-Pro 上，MVP 将 UI-TARS-1.5-7B 提升到 56.1%，GTA1-7B 提升到 61.7%，Qwen3VL-8B-Instruct 提升到 65.3%，以及 Qwen3VL-32B-Instruct 提升到 74.0%。代码可在 https://github.com/ZJUSCL/MVP 获取。"
    },
    {
        "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
        "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.",
        "url": "http://arxiv.org/abs/2512.08358v1",
        "published_date": "2025-12-09T08:35:42+00:00",
        "updated_date": "2025-12-09T08:35:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Lu",
            "Weitao Xiong",
            "Jiacheng Deng",
            "Peng Li",
            "Tianyu Huang",
            "Zhiyang Dou",
            "Cheng Lin",
            "Sai-Kit Yeung",
            "Yuan Liu"
        ],
        "tldr": "The paper introduces TrackingWorld, a novel monocular 3D tracking pipeline for densely tracking almost all pixels in a video within a world-centric 3D coordinate system, addressing limitations of existing methods in separating camera motion and tracking emerging dynamic objects.",
        "tldr_zh": "该论文介绍了 TrackingWorld，一种新颖的单目3D跟踪管道，用于在以世界为中心的3D坐标系中密集跟踪视频中几乎所有像素，解决了现有方法在分离相机运动和跟踪新兴动态对象方面的局限性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "单目3D跟踪旨在从单个单目视频中捕捉像素在3D空间中的长期运动，近年来取得了快速进展。然而，我们认为现有的单目3D跟踪方法在将相机运动从前景动态运动中分离出来方面仍然存在不足，并且无法密集地跟踪视频中新出现的动态主体。为了解决这两个限制，我们提出TrackingWorld，一种新颖的流水线，用于在以世界为中心的3D坐标系中对几乎所有像素进行密集3D跟踪。首先，我们引入了一种跟踪上采样器，可以有效地将任意稀疏的2D轨迹提升为密集的2D轨迹。然后，为了将当前的跟踪方法推广到新出现的对象，我们将上采样器应用于所有帧，并通过消除重叠区域中的轨迹来减少2D轨迹的冗余。最后，我们提出了一个基于优化的高效框架，通过估计相机姿态和这些2D轨迹的3D坐标，将密集的2D轨迹反投影到以世界为中心的3D轨迹中。在合成和真实世界数据集上的大量评估表明，我们的系统在以世界为中心的坐标系中实现了准确而密集的3D跟踪。"
    },
    {
        "title": "PAVAS: Physics-Aware Video-to-Audio Synthesis",
        "summary": "Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.",
        "url": "http://arxiv.org/abs/2512.08282v1",
        "published_date": "2025-12-09T06:28:50+00:00",
        "updated_date": "2025-12-09T06:28:50+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Oh Hyun-Bin",
            "Yuhta Takida",
            "Toshimitsu Uesaka",
            "Tae-Hyun Oh",
            "Yuki Mitsufuji"
        ],
        "tldr": "This paper introduces PAVAS, a physics-aware video-to-audio synthesis method that incorporates physical parameters into a latent diffusion model, enabling more realistic sound generation by considering object mass and motion.",
        "tldr_zh": "本文介绍了PAVAS，一种物理感知的视频到音频合成方法，它将物理参数整合到潜在扩散模型中，通过考虑物体质量和运动，实现更真实的音频生成。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "视频转音频（V2A）生成领域的最新进展已经实现了令人印象深刻的感知质量和时间同步，然而，大多数模型仍是表观驱动型的，捕捉视觉-声学相关性，而未考虑塑造真实世界声音的物理因素。我们提出了基于物理认知的视频转音频合成（PAVAS），一种通过物理驱动音频适配器（Phy-Adapter）将物理推理融入基于潜在扩散的V2A生成的方法。该适配器接收由物理参数估计器（PPE）估计的物体级物理参数，该估计器使用视觉-语言模型（VLM）推断移动物体的质量，并使用基于分割的动态3D重建模块来恢复其运动轨迹以进行速度计算。这些物理线索使得模型能够合成反映底层物理因素的声音。为了评估物理真实性，我们整理了一个专注于物体与物体交互的基准数据集VGG-Impact，并引入了音频-物理相关系数（APCC），该评估指标衡量物理属性和听觉属性之间的一致性。综合实验表明，PAVAS生成了物理上合理且感知上连贯的音频，在定量和定性评估中均优于现有的V2A模型。请访问https://physics-aware-video-to-audio-synthesis.github.io观看演示视频。"
    },
    {
        "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video",
        "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.",
        "url": "http://arxiv.org/abs/2512.08269v1",
        "published_date": "2025-12-09T05:53:39+00:00",
        "updated_date": "2025-12-09T05:53:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Taewoong Kang",
            "Kinam Kim",
            "Dohyeon Kim",
            "Minho Park",
            "Junha Hyung",
            "Jaegul Choo"
        ],
        "tldr": "The paper introduces EgoX, a framework for generating egocentric videos from single exocentric videos using a diffusion model with tailored conditioning and geometry-guided attention to ensure geometric coherence and realism.",
        "tldr_zh": "该论文介绍了EgoX，一个从单个外视视频生成自我中心视频的框架。它使用具有定制条件和几何引导注意力的扩散模型，以确保几何连贯性和真实性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自中心视角感知使人类能够直接从自身的视角体验和理解世界。将以外中心（第三人称）视频转换成自中心（第一人称）视频为沉浸式理解开启了新的可能性，但由于极端的相机姿态变化和最小的视野重叠，这仍然极具挑战性。此任务需要忠实地保留可见内容，同时以几何一致的方式合成未见区域。为了实现这一目标，我们提出了 EgoX，一种从单个外中心输入生成自中心视频的新框架。 EgoX 通过轻量级的 LoRA 适配，利用了大规模视频扩散模型的预训练时空知识，并引入了一种统一的条件策略，该策略通过宽度和通道方向的连接来结合外中心和自中心的先验信息。此外，一种几何引导的自注意力机制有选择性地关注空间相关的区域，确保几何一致性和高视觉保真度。我们的方法实现了连贯逼真的自中心视频生成，同时展示了在未见视频和真实场景视频中的强大可扩展性和鲁棒性。"
    },
    {
        "title": "Reinforcement Learning From State and Temporal Differences",
        "summary": "TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.",
        "url": "http://arxiv.org/abs/2512.08855v1",
        "published_date": "2025-12-09T17:48:28+00:00",
        "updated_date": "2025-12-09T17:48:28+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Lex Weaver",
            "Jonathan Baxter"
        ],
        "tldr": "This paper identifies a shortcoming in TD($\\lambda$) where it can converge to suboptimal policies due to focusing on absolute state value errors rather than relative ordering, proposes STD($\\lambda$) to address this, and provides theoretical analysis and empirical validation.",
        "tldr_zh": "该论文指出TD($\\lambda$)的一个缺陷，即它可能因关注绝对状态值误差而非相对排序而收敛到次优策略，提出了STD($\\lambda$)来解决这个问题，并提供了理论分析和实证验证。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "带有函数逼近的TD($λ$)已经在一些复杂的强化学习问题中被证实具有经验上的成功应用。对于线性逼近，TD($λ$)已被证明能够最小化每个状态的近似值和真实值之间的均方误差。然而，就策略而言，关键在于状态相对排序中的误差，而不是状态值的误差。我们用简单的两状态和三状态系统（TD($λ$)从最优策略出发却收敛到次优策略）以及双陆棋来阐述这一点。然后，我们提出了一种TD($λ$)的改进形式，称为STD($λ$)，其中函数逼近器根据二元决策问题中的相对状态值进行训练。本文给出了理论分析，包括在两状态系统中STD($λ$)单调策略改进的证明，并与Bertsekas的差分训练方法[1]进行了比较。随后，我们在两状态系统和一个广为人知的杂技机器人问题的一个变体上成功演示了STD($λ$)。"
    },
    {
        "title": "Direct transfer of optimized controllers to similar systems using dimensionless MPC",
        "summary": "Scaled model experiments are commonly used in various engineering fields to reduce experimentation costs and overcome constraints associated with full-scale systems. The relevance of such experiments relies on dimensional analysis and the principle of dynamic similarity. However, transferring controllers to full-scale systems often requires additional tuning. In this paper, we propose a method to enable a direct controller transfer using dimensionless model predictive control, tuned automatically for closed-loop performance. With this reformulation, the closed-loop behavior of an optimized controller transfers directly to a new, dynamically similar system. Additionally, the dimensionless formulation allows for the use of data from systems of different scales during parameter optimization. We demonstrate the method on a cartpole swing-up and a car racing problem, applying either reinforcement learning or Bayesian optimization for tuning the controller parameters. Software used to obtain the results in this paper is publicly available at https://github.com/josipkh/dimensionless-mpcrl.",
        "url": "http://arxiv.org/abs/2512.08667v1",
        "published_date": "2025-12-09T14:52:15+00:00",
        "updated_date": "2025-12-09T14:52:15+00:00",
        "categories": [
            "eess.SY",
            "cs.LG"
        ],
        "authors": [
            "Josip Kir Hromatko",
            "Shambhuraj Sawant",
            "Šandor Ileš",
            "Sébastien Gros"
        ],
        "tldr": "This paper introduces a novel method for direct transfer of model predictive controllers between dynamically similar systems of different scales by using a dimensionless formulation, demonstrated on cartpole swing-up and car racing tasks and tuned with reinforcement learning and Bayesian optimization.",
        "tldr_zh": "本文提出了一种新颖的方法，通过使用无量纲公式，可以直接在不同规模的动态相似系统之间转移模型预测控制器。该方法在倒立摆起摆和赛车任务中得到了验证，并使用强化学习和贝叶斯优化进行了参数调整。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "缩比模型实验广泛应用于各个工程领域，以降低实验成本并克服与全尺寸系统相关的约束。此类实验的相关性依赖于量纲分析和动态相似性原理。然而，将控制器转移到全尺寸系统通常需要额外的调整。在本文中，我们提出了一种使用无量纲模型预测控制来实现直接控制器转移的方法，该控制根据闭环性能自动调整。通过这种重新构建，优化控制器的闭环行为可以直接转移到新的、动态相似的系统。此外，无量纲公式允许在参数优化期间使用来自不同尺度系统的数据。我们在倒立摆起摆和赛车问题上验证了该方法，并应用强化学习或贝叶斯优化来调整控制器参数。本文中使用来获得结果的软件可在 https://github.com/josipkh/dimensionless-mpcrl 上公开获取。"
    },
    {
        "title": "Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis",
        "summary": "Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.",
        "url": "http://arxiv.org/abs/2512.08601v1",
        "published_date": "2025-12-09T13:40:08+00:00",
        "updated_date": "2025-12-09T13:40:08+00:00",
        "categories": [
            "stat.ML",
            "cs.LG",
            "math.OC"
        ],
        "authors": [
            "Orit Davidovich",
            "Shimrit Shtern",
            "Segev Wasserkrug",
            "Nimrod Megiddo"
        ],
        "tldr": "This paper introduces a unified framework to model combinatorial optimization problems as MDPs, proves convergence of value-based RL under certain conditions, and provides an optimality gap analysis.",
        "tldr_zh": "本文提出了一个统一的框架，将组合优化问题建模为MDP，证明了在特定条件下基于价值的强化学习的收敛性，并提供了最优性差距分析。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自20世纪90年代以来，已经开展了大量实证研究，旨在训练统计模型（如神经网络 (NNs)）作为组合优化 (CO) 问题的学习启发式算法。如果成功，这种方法就不需要专家为每种问题类型设计启发式算法。由于其结构，许多难解的CO问题都可以通过强化学习 (RL) 来处理。事实上，我们发现大量文献使用基于价值、策略梯度或Actor-Critic方法训练NNs，在经验最优性差距和推理运行时间方面都取得了可喜的成果。然而，支持将RL用于CO问题的理论工作却相对匮乏。为此，我们引入了一个统一框架，通过马尔可夫决策过程 (MDPs) 对CO问题进行建模，并使用RL技术解决它们。我们提供了易于测试的假设，在这些假设下，CO问题可以被表述为等价的无折扣MDPs，从而为原始CO问题提供最优解。此外，我们建立了价值型RL技术收敛到CO问题近似解的条件，并保证与相关的最优性差距。我们的收敛性分析提供了：(1) 每次RL迭代中批量大小和投影梯度下降步数的充分增长率；(2) 根据问题参数和目标RL精度得出的最优性差距；以及(3) 状态空间嵌入选择的重要性。总的来说，我们的分析阐明了著名的深度Q学习算法在这种问题背景下的成功（和局限性）。"
    },
    {
        "title": "Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning",
        "summary": "Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.",
        "url": "http://arxiv.org/abs/2512.08485v2",
        "published_date": "2025-12-09T11:04:37+00:00",
        "updated_date": "2025-12-10T07:43:34+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Junnan Qiu",
            "Yuanjie Zhao",
            "Jie Li"
        ],
        "tldr": "This paper proposes a novel data poisoning attack strategy for offline RL that allocates perturbation budgets based on TD-error sensitivity, outperforming existing methods in terms of performance degradation and stealthiness.",
        "tldr_zh": "该论文提出了一种新的离线强化学习数据投毒攻击策略，该策略基于TD误差敏感性分配扰动预算，在性能下降和隐蔽性方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "离线强化学习（RL）允许从静态数据集进行策略优化，但本质上容易受到数据投毒攻击。现有的攻击策略通常依赖于局部均匀扰动，对所有样本一视同仁。这种方法效率低下，因为它将扰动预算浪费在低影响样本上，并且由于显著的统计偏差而缺乏隐蔽性。在本文中，我们提出了一种新的全局预算分配攻击策略。利用样本对价值函数收敛的影响与其时间差分(TD)误差成正比的理论见解，我们将攻击建模为一个全局资源分配问题。我们推导出一种闭式解，其中扰动幅度与全局L2约束下的TD误差敏感性成正比。在D4RL基准测试上的实验结果表明，我们的方法显著优于基线策略，能够在最小扰动下实现高达80%的性能下降，并能规避最先进的统计和谱防御检测。"
    },
    {
        "title": "Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks",
        "summary": "The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.",
        "url": "http://arxiv.org/abs/2512.08341v1",
        "published_date": "2025-12-09T08:11:21+00:00",
        "updated_date": "2025-12-09T08:11:21+00:00",
        "categories": [
            "cs.NI",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Thai Duong Nguyen",
            "Ngoc-Tan Nguyen",
            "Thanh-Dao Nguyen",
            "Nguyen Van Huynh",
            "Dinh-Hieu Tran",
            "Symeon Chatzinotas"
        ],
        "tldr": "This paper proposes a MARL approach for UAV relay networks to maximize throughput and avoid collisions under jamming attacks. The method demonstrates a 50% throughput increase and near-zero collision rate with an emergent anti-jamming strategy.",
        "tldr_zh": "本文提出了一种用于无人机中继网络的 MARL 方法，以最大化吞吐量并避免在干扰攻击下的碰撞。该方法展示了 50% 的吞吐量增加和接近零的碰撞率，以及一种涌现的反干扰策略。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "将无人机 (UAV) 集群部署为动态通信中继对于下一代战术网络至关重要。然而，在对抗环境中运行需要在复杂的权衡中求解，包括最大化系统吞吐量，同时确保防撞和对抗干扰的韧性。由于该问题的动态性和多目标性，现有的基于启发式的方案通常难以找到有效解。本文将这一挑战建模为合作多智能体强化学习 (MARL) 问题，并使用集中式训练分散式执行 (CTDE) 框架进行求解。我们的方法采用一个集中式评论家，利用全局状态信息来指导仅使用局部观测进行操作的分散式执行者。仿真结果表明，我们提出的框架显著优于启发式基线，将总系统吞吐量提高了约 50%，同时实现了接近零的碰撞率。一个关键发现是，智能体们发展出了一种无需显式编程的新兴抗干扰策略。它们学会智能地定位自己，以平衡缓解来自干扰机的干扰和维持与地面用户有效通信链路之间的权衡。"
    },
    {
        "title": "Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care",
        "summary": "In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.\n  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.",
        "url": "http://arxiv.org/abs/2512.08012v1",
        "published_date": "2025-12-08T20:09:15+00:00",
        "updated_date": "2025-12-08T20:09:15+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Aryaman Bansal",
            "Divya Sharma"
        ],
        "tldr": "This paper benchmarks offline multi-objective reinforcement learning (MORL) algorithms in critical care using the MIMIC-IV dataset, demonstrating that a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer outperforms single-objective baselines in flexibility.",
        "tldr_zh": "本文在重症监护环境中，使用MIMIC-IV数据集，对离线多目标强化学习 (MORL) 算法进行了基准测试。结果表明，一种改进的Pareto有效决策代理 (PEDA) 决策转换器在灵活性方面优于单目标基线。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在重症监护病房等危重护理环境中，临床医生面临着平衡冲突性目标的复杂挑战，主要是在最大化患者生存率的同时最小化资源利用（例如，住院时长）。单目标强化学习方法通常通过优化固定的标量化奖励函数来解决这个问题，导致策略僵化，无法适应不同的临床优先级。多目标强化学习（MORL）通过学习Pareto前沿上的一组最优策略，从而在测试时实现动态偏好选择，为这个问题提供了一种解决方案。然而，在医疗保健领域应用MORL需要严格地从历史数据中进行离线学习。\n\n本文中，我们以MIMIC-IV数据集为基准，针对三种标量化单目标基线（BC、CQL和DDQN）评估了三种离线MORL算法：条件保守 Pareto Q-Learning (CPQL)、自适应 CPQL 以及修改后的Pareto有效决策代理 (PEDA) 决策Transformer (PEDA DT)。使用离线策略评估 (OPE) 指标，我们证明了 PEDA DT 算法相比于静态标量化基线提供了卓越的灵活性。值得注意的是，我们的结果扩展了之前关于医疗保健中单目标决策Transformer的研究发现，证实了序列建模架构在扩展到多目标条件生成时仍然稳健有效。这些发现表明，离线MORL是一个前景广阔的框架，能够在危重护理中实现个性化、可调整的决策制定，而无需重新训练。"
    },
    {
        "title": "Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility",
        "summary": "This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.",
        "url": "http://arxiv.org/abs/2512.08688v1",
        "published_date": "2025-12-09T15:08:02+00:00",
        "updated_date": "2025-12-09T15:08:02+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Mark Pustilnik",
            "Francesco Borrelli"
        ],
        "tldr": "This paper presents a dynamic game formulation with non-normalized shared constraints for cooperative human-robot navigation, enabling asymmetric responsibility in safety enforcement.",
        "tldr_zh": "本文提出了一种具有非标准化共享约束的动态博弈公式，用于人机协作导航，从而能够在安全执行方面实现不对称责任。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "本文提出了一种应用于共享工作空间存在障碍物情形下的人机协同导航动态博弈模型。在该模型中，人和机器人共同完成一项任务，并联合满足共享安全约束。一个关键贡献是引入了针对共享约束的非归一化均衡结构。 该结构允许两个智能体以不同的努力程度来执行诸如避撞和智能体间距等安全要求 。我们将这种非归一化均衡嵌入到一种滚动时域优化控制方案中。"
    },
    {
        "title": "RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight",
        "summary": "This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.",
        "url": "http://arxiv.org/abs/2512.08574v1",
        "published_date": "2025-12-09T13:13:27+00:00",
        "updated_date": "2025-12-09T13:13:27+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Vit Kratky",
            "Robert Penicka",
            "Parakh M. Gupta",
            "Ondrej Prochazka",
            "Martin Saska"
        ],
        "tldr": "This paper introduces a novel NMPC-based collision avoidance approach for agile UAV flight using Reciprocal Velocity Constraints, demonstrating significant improvements in flight time compared to state-of-the-art methods with high-frequency control and real-world validation.",
        "tldr_zh": "本文提出了一种基于NMPC的无人机敏捷飞行避障方法，利用互易速度约束，与现有技术相比，在飞行时间方面有显著改进，并具有高频控制和真实世界验证。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "本文提出了一种基于非线性模型预测控制(NMPC)和时变互易速度约束(RVC)的相互避碰方法。与现有的大多数方法不同，该方法仅依赖于对其他机器人可观测的信息，从而消除了对过度通信使用的需求。用于计算RVC的高效算法，以及将这些约束直接集成到控制器层面的NMPC问题公式中，使得整个流程能够以100 Hz的频率运行。这种高处理速率，结合受控无人机(UAV)的建模非线性动力学，是该方法能够应用于敏捷无人机飞行的关键特性。通过大量的仿真（模拟高达10架无人机和高达25米/秒速度的真实世界条件）以及实际实验（加速度高达30米/秒$^2$）对该方法进行了评估。与最先进方法相比，在具有挑战性的场景中，飞行时间减少了31%，同时在所有试验中都保持了无碰撞导航。"
    },
    {
        "title": "RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera",
        "summary": "Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.",
        "url": "http://arxiv.org/abs/2512.08262v1",
        "published_date": "2025-12-09T05:38:30+00:00",
        "updated_date": "2025-12-09T05:38:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Hafeez Husain Cholakkal",
            "Stefano Arrigoni",
            "Francesco Braghin"
        ],
        "tldr": "RLCNet is a deep learning framework for online, simultaneous calibration of LiDAR, RADAR, and camera sensors, demonstrating robust performance and real-time capability in dynamic environments with online drift correction.",
        "tldr_zh": "RLCNet是一个深度学习框架，用于同时在线校准激光雷达、雷达和相机传感器，在动态环境中展示了强大的性能和实时能力，并具有在线漂移校正。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "激光雷达、雷达和相机传感器的精确外参标定对于自动驾驶车辆中可靠的感知至关重要。 然而，由于机械振动和动态环境中累积的传感器漂移等因素，这项任务仍然具有挑战性。本文提出 RLCNet，一种新颖的端到端可训练深度学习框架，用于同时在线标定这些多模态传感器。RLCNet 在真实世界数据集上验证，专为实际部署而设计，并在各种条件下表现出稳健的性能。为了支持实时操作，我们引入了一种在线标定框架，该框架结合了加权移动平均和离群值剔除，从而能够动态调整标定参数，同时减少预测噪声并提高对漂移的鲁棒性。一项消融研究突出了架构选择的重要性，而与现有方法的比较证明了所提出方法的优越准确性和鲁棒性。"
    },
    {
        "title": "High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement",
        "summary": "We propose Synchronous Dual-Arm Rearrangement Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal configurations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR-M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state-of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.",
        "url": "http://arxiv.org/abs/2512.08206v1",
        "published_date": "2025-12-09T03:33:34+00:00",
        "updated_date": "2025-12-09T03:33:34+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Duo Zhang",
            "Junshan Huang",
            "Jingjin Yu"
        ],
        "tldr": "The paper introduces SDAR, a novel task and motion planning framework for dual-arm tabletop rearrangement, achieving 100% success rate in complex tasks and demonstrating successful transfer to robot hardware.",
        "tldr_zh": "该论文提出了SDAR，一种用于双臂桌面重排的新型任务和运动规划框架，在复杂任务中实现了100%的成功率，并展示了成功迁移到机器人硬件的能力。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "我们提出了同步双臂重排规划器（SDAR），这是一种用于桌面重排的任务和运动规划（TAMP）框架。在该框架中，配备双指夹具的两个机械臂必须紧密合作，以重新排列那些起始和目标构型高度纠缠的物体。为了应对这些挑战，SDAR紧密结合了其依赖驱动的任务规划器（SDAR-T）和同步双臂运动规划器（SDAR-M），从而能够智能地筛选大量的可能任务和运动规划。具体来说，SDAR-T应用了一种简单而有效的策略来分解由重排任务所引起的全局对象依赖图，从而产生比单个机械臂的最优任务规划所得出的解决方案更优的双臂任务规划。利用先进的基于GPU SIMD的运动规划工具，SDAR-M采用分层运动规划策略来筛选许多任务规划，以获得最佳的同步双臂运动规划，同时确保高成功率。全面的评估表明，SDAR在解决复杂的、非单调的、长时程的桌面重排任务中，实现了100%的成功率，且解决方案质量远远超过了先前的最先进水平。在两个UR-5e机械臂上的实验进一步证实了SDAR可以直接且可靠地迁移到机器人硬件上。"
    },
    {
        "title": "DIJIT: A Robotic Head for an Active Observer",
        "summary": "We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.",
        "url": "http://arxiv.org/abs/2512.07998v1",
        "published_date": "2025-12-08T19:37:40+00:00",
        "updated_date": "2025-12-08T19:37:40+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Mostafa Kamali Tabrizi",
            "Mingshi Chi",
            "Bir Bikram Dey",
            "Yu Qing Yuan",
            "Markus D. Solbach",
            "Yiqian Liu",
            "Michael Jenkin",
            "John K. Tsotsos"
        ],
        "tldr": "The paper introduces DIJIT, a novel binocular robotic head with human-like kinematic properties designed for active vision research, and presents a new method for saccadic camera movements.",
        "tldr_zh": "该论文介绍了一种新型双目机器人头部DIJIT，其具有类似人类的运动学特性，专为主动视觉研究而设计，并提出了一种新的扫视相机运动方法。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "我们提出DIJIT，一种专为充当主动观察者的移动代理设计的全新双目机器人头部。 DIJIT卓越的功能广度使其能够进行主动视觉研究，并研究类人眼部和头部-颈部运动、它们之间的相互关系以及每个如何促进视觉能力。 DIJIT也被用于探索人类视觉如何利用眼/头运动来解决视觉任务与当前计算机视觉方法之间的差异。 DIJIT的设计具有九个机械自由度，而相机和镜头提供了额外的四个光学自由度。机械设计的范围和速度与人类表现相当。我们的设计包括汇聚立体所需的运动范围，即辐辏、版本和旋扭。探索这些对人类和机器视觉的效用正在进行中。在此，我们介绍DIJIT的设计并评估其性能的各个方面。我们提出了一种用于扫视相机运动的新方法。在该方法中，建立了相机方向和电机值之间的直接关系。由此产生的扫视相机运动在精度方面接近人类运动。"
    },
    {
        "title": "Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization",
        "summary": "Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \\emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \\textbf{2$\\times$--35$\\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.",
        "url": "http://arxiv.org/abs/2512.07969v1",
        "published_date": "2025-12-08T19:02:56+00:00",
        "updated_date": "2025-12-08T19:02:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Alan Papalia",
            "Nikolas Sanderson",
            "Haoyu Han",
            "Heng Yang",
            "Hanumant Singh",
            "Michael Everett"
        ],
        "tldr": "This paper introduces a sparse variable projection scheme for nonlinear least-squares problems in robotic perception, achieving significant speedups by exploiting separability and addressing gauge symmetries. It offers a matrix-free Schur complement operator for efficient cost evaluation, gradient calculation, and Hessian-vector products within standard iterative NLS solvers.",
        "tldr_zh": "本文提出了一种用于机器人感知中非线性最小二乘问题的稀疏变量投影方案，通过利用可分离性并解决规范对称性，实现了显著的加速。它提供了一种无矩阵Schur补算子，用于在标准迭代NLS求解器中高效地进行成本评估、梯度计算和 Hessian-向量积计算。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "机器人感知通常需要求解大型非线性最小二乘(NLS)问题。尽管稀疏性已被充分利用以扩展求解器，但一个互补且未被充分利用的结构是\\emph{可分离性}——其中一些变量（例如，视觉地标）在线性残差中线性出现，并且对于剩余变量（例如，姿态）的任何估计，都具有闭式解。变量投影(VarPro)方法是一系列利用这种结构的技术，它通过解析地消除线性变量，并在剩余变量中呈现出一个具有良好性质的简化问题。然而，VarPro在机器人感知中的应用有限；一个主要挑战来自规范对称性（例如，成本对全局平移和旋转的不变性），这在感知中很常见，并且在标准VarPro方法中引起特定的计算挑战。我们提出了一种为具有规范对称性的问题设计的VarPro方案，该方案联合利用了可分离性和稀疏性。我们的方法可以作为一次性的预处理步骤来构建一个\\emph{无矩阵 Schur 补算子}。该算子可以高效地评估成本、梯度和简化问题的 Hessian-向量积，并且可以轻松地与标准迭代NLS求解器集成。我们提供了该方法适用的精确条件，并描述了在这些条件仅部分满足时的扩展。在SLAM、SNL和SfM中的合成和真实基准测试中，我们的方法在保持精度的前提下，比最先进的方法实现了高达\\textbf{2$\\times$--35$\\times$的运行速度提升}。我们发布了一个开源C++实现以及我们实验中的所有数据集。"
    },
    {
        "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation",
        "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.",
        "url": "http://arxiv.org/abs/2512.07819v1",
        "published_date": "2025-12-08T18:52:10+00:00",
        "updated_date": "2025-12-08T18:52:10+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Shubham S. Kumbhar",
            "Abhijeet M. Kulkarni",
            "Panagiotis Artemiadis"
        ],
        "tldr": "This paper presents a control framework for humanoid robots to collaboratively transport objects with humans, validated through real-world experiments using the Digit robot, and includes an efficiency metric to quantify collaboration quality.",
        "tldr_zh": "本文提出了一个控制框架，使人形机器人能够与人类伙伴协作运输物体。该框架通过在Digit机器人上进行的真实世界实验进行了验证，并包含一个用于量化协作质量的效率指标。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "我们提出了一种控制框架，使人形机器人能够与人类伙伴执行协同运输任务。该框架支持平移和旋转运动，这对于协同运输场景至关重要。它包含三个组成部分：高层规划器、低层控制器和刚度调节机制。在规划层，我们引入了交互线性倒立摆（I-LIP），它与顺应性模型和MPC公式相结合，生成动态可行的足迹规划。这些规划由基于QP的全身控制器执行，该控制器考虑了耦合的人形机器人-物体动力学。刚度调节控制机器人-物体交互，确保收敛到由物体和机器人质心之间的距离定义的期望相对配置。我们通过在 Digit 人形机器人平台上进行的真实世界实验验证了该框架的有效性。为了量化协作质量，我们提出了一个效率指标，该指标捕捉了任务性能和智能体间协调。我们证明了该指标突出了顺应性在协作任务中的作用，并提供了关于高低层控制层所需轨迹特性的见解。最后，我们展示了协作行为的实验结果，包括平移、转弯和诸如半圆轨迹之类的组合运动，这些都代表了自然发生的协同运输任务。"
    },
    {
        "title": "Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration",
        "summary": "Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.",
        "url": "http://arxiv.org/abs/2512.08036v1",
        "published_date": "2025-12-08T20:53:57+00:00",
        "updated_date": "2025-12-08T20:53:57+00:00",
        "categories": [
            "cs.HC",
            "cs.AI",
            "eess.SY"
        ],
        "authors": [
            "Mohammadreza Jalaeian",
            "Dane A. Morey",
            "Michael F. Rayo"
        ],
        "tldr": "This paper synthesizes 14 heuristics from various fields to aid in the design and evaluation of technologies that support effective joint human-machine activity, focusing on macrocognitive functions within teams.",
        "tldr_zh": "本文综合了来自各个领域的14条启发式方法，旨在帮助设计和评估支持有效人机协作的技术，重点关注团队内部的宏认知功能。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "联合活动描述的是多个主体（人或机器）共同参与完成一项任务或活动的情况。针对联合活动的设计侧重于明确支持主体间为实现有效协作所必需的相互依赖性。它是在可用性设计基础上构建和扩展的，进一步探讨了如何设计技术使其能够作为有效的团队成员发挥作用。有效的联合活动至少需要在团队内部支持五项主要的宏认知功能：事件检测、意义构建、适应性、视角转换和协调。支持这些功能与使技术可用同等重要。我们从相关文献中综合了十四条启发式规则，这些文献包括显示设计、人因工程、认知系统工程、认知心理学和计算机科学，以帮助设计、开发和评估支持人机联合活动的技术。"
    },
    {
        "title": "Trajectory Densification and Depth from Perspective-based Blur",
        "summary": "In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.",
        "url": "http://arxiv.org/abs/2512.08627v1",
        "published_date": "2025-12-09T14:11:43+00:00",
        "updated_date": "2025-12-09T14:11:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianchen Qiu",
            "Qirun Zhang",
            "Jiajian He",
            "Zhengyue Zhuge",
            "Jiahui Xu",
            "Yueting Chen"
        ],
        "tldr": "This paper proposes a novel method to estimate depth and densify trajectories from video streams by analyzing perspective-based blur patterns, leveraging vision encoders, point trackers, and vision-language models.",
        "tldr_zh": "本文提出了一种新方法，通过分析基于透视的模糊模式，从视频流中估计深度并密集化轨迹，利用视觉编码器、点跟踪器和视觉语言模型。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "在缺乏机械稳定器的情况下，相机在拍摄过程中不可避免地产生旋转动态，从而导致基于透视的模糊，尤其是在长曝光场景下。从光学的角度来看，基于透视的模糊是深度-位置相关的：即使在相同的成像设置下，位于不同空间位置的物体也会产生不同的模糊程度。受此启发，我们提出了一种新颖的方法，通过联合光学设计算法，检查视频流的模糊模式和密集轨迹来估计度量深度。具体而言，我们采用现成的视觉编码器和点追踪器来提取视频信息。然后，我们通过窗口嵌入和多窗口聚合来估计深度图，并使用视觉-语言模型对来自光学算法的稀疏轨迹进行稠密化。在多个深度数据集上的评估表明，我们的方法在大深度范围内取得了强大的性能，同时保持了良好的泛化能力。相对于手持拍摄设置中的真实轨迹，我们的光学算法实现了卓越的精度，并且密集重建保持了很高的准确性。"
    },
    {
        "title": "SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds",
        "summary": "This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.",
        "url": "http://arxiv.org/abs/2512.08557v1",
        "published_date": "2025-12-09T12:58:10+00:00",
        "updated_date": "2025-12-09T12:58:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexander Dow",
            "Manduhu Manduhu",
            "Matheus Santos",
            "Ben Bartlett",
            "Gerard Dooly",
            "James Riordan"
        ],
        "tldr": "The paper introduces SSCATeR, a novel sparse convolution algorithm that leverages temporal data recycling in LiDAR point clouds for real-time 3D object detection, achieving significant speedups without sacrificing accuracy.",
        "tldr_zh": "该论文介绍了一种新的稀疏卷积算法SSCATeR，该算法利用激光雷达点云中的时间数据回收进行实时3D目标检测，在不牺牲准确性的前提下实现了显著的加速。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "本研究利用激光雷达扫描的连续扫描运动，将目标检测的重心放在特定区域，这些区域从一帧到另一帧的点云数据发生变化。我们通过使用具有短步长的滑动时间窗口，并考虑时间维度，存储多次扫描之间的卷积结果来实现这一点。这样就允许我们忽略未变化的区域，从而在不牺牲精度的前提下，显著减少每次前向传播的卷积操作数量。这种数据重用方案为检测数据带来了极高的稀疏性。为了利用这种稀疏性，我们扩展了先前关于基于散布的卷积的研究，以允许数据重用，并因此提出了基于时间数据循环的稀疏散布卷积算法（SSCATeR）。该操作将输入的激光雷达数据视为一个连续流，并且仅对点云中发生变化的部分进行处理。通过这样做，我们在实现相同结果的同时，处理时间最多可减少6.61倍。我们的测试结果表明，该方法输出的特征图与传统稀疏卷积技术产生的特征图相同，同时大大提高了网络的计算效率。"
    },
    {
        "title": "On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs",
        "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.",
        "url": "http://arxiv.org/abs/2512.08498v1",
        "published_date": "2025-12-09T11:26:20+00:00",
        "updated_date": "2025-12-09T11:26:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yijia Guo",
            "Tong Hu",
            "Zhiwei Li",
            "Liwen Hu",
            "Keming Qian",
            "Xitong Lin",
            "Shengbo Chen",
            "Tiejun Huang",
            "Lei Ma"
        ],
        "tldr": "This paper presents an on-the-fly 3D reconstruction framework for multi-camera rigs using 3D Gaussian Splatting, achieving fast, robust, and accurate scene reconstruction from raw video streams.",
        "tldr_zh": "本文提出了一种使用3D高斯溅射技术的多相机设备实时3D重建框架，能够从原始视频流中实现快速、稳健和精确的场景重建。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "三维高斯溅射(3DGS)的最新进展实现了高效的自由视点渲染和照片级真实感的场景重建。虽然3DGS的即时扩展在利用单目RGB流进行实时重建方面展现了潜力，但由于有限的视场(FOV)，它们通常无法实现完整的3D覆盖。采用多摄像头阵列从根本上解决了这个限制。在本文中，我们提出了第一个用于多摄像头阵列的即时3D重建框架。我们的方法将来自多个重叠相机的密集RGB流增量式地融合到统一的高斯表示中，从而实现无漂移的轨迹估计和高效的在线重建。我们提出了一种分层相机初始化方案，该方案无需校准即可实现粗略的相机间对齐， 随后进行轻量级的多相机光束法平差，在保持实时性能的同时稳定轨迹。此外，我们引入了一种无冗余的高斯采样策略和频率感知优化调度器，以减少高斯基元的数量和所需的优化迭代次数，从而保证效率和重建保真度。我们的方法仅使用原始多摄像头视频流，即可在短短2分钟内重建数百米的3D场景，展示了前所未有的速度、鲁棒性和保真度，适用于即时3D场景重建。"
    },
    {
        "title": "GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification",
        "summary": "Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.",
        "url": "http://arxiv.org/abs/2512.08325v1",
        "published_date": "2025-12-09T07:40:51+00:00",
        "updated_date": "2025-12-09T07:40:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuedeng Liu",
            "Jiabao Guo",
            "Zheng Zhang",
            "Fei Wang",
            "Zhi Liu",
            "Dan Guo"
        ],
        "tldr": "This paper introduces GeoDiffMM, a diffusion-based Lagrangian video motion magnification framework conditioned on optical flow, demonstrating improved performance over state-of-the-art methods, particularly when motion displacements are very small.",
        "tldr_zh": "本文介绍了 GeoDiffMM，这是一个基于扩散模型的拉格朗日视频运动放大框架，以光流为条件，展示了优于现有方法的性能，尤其是在运动位移非常小的情况下。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "视频运动放大（VMM）将细微的宏观运动放大到可感知的程度。 近年来，现有的主流欧拉方法通过解耦表示学习（例如纹理、形状和频率方案）来解决放大引起的噪声，但当运动位移非常小时，它们仍然难以将光子噪声与真实的微运动分离。我们提出GeoDiffMM，一种新颖的基于扩散的拉格朗日VMM框架，以光流作为几何线索为条件，从而实现结构一致的运动放大。 具体而言，我们设计了一种无噪声光流增强策略，该策略合成不含光子噪声的各种非刚性运动场作为监督，从而帮助模型学习更准确的几何感知光流并更好地泛化。 接下来，我们开发了一种扩散运动放大器，该放大器以（i）光流作为几何先验和（ii）可学习的放大因子控制幅度为条件来控制去噪过程，从而有选择地放大与场景语义和结构一致的运动分量，同时抑制与内容无关的扰动。 最后，我们执行基于光流的视频合成，以高保真度将放大的运动映射回图像域。 在真实和合成数据集上的大量实验表明，GeoDiffMM优于最先进的方法，并显着提高了运动放大效果。"
    },
    {
        "title": "FastBEV++: Fast by Algorithm, Deployable by Design",
        "summary": "The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the \"Deployable by Design\" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is \"Fast by Algorithm\", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev",
        "url": "http://arxiv.org/abs/2512.08237v1",
        "published_date": "2025-12-09T04:37:46+00:00",
        "updated_date": "2025-12-09T04:37:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuanpeng Chen",
            "Hui Song",
            "Wei Tao",
            "ShanHui Mo",
            "Shuang Zhang",
            "Xiao Hua",
            "TianKun Zhao"
        ],
        "tldr": "FastBEV++ addresses the performance-deployment trade-off in camera-only BEV perception by introducing an efficient, TensorRT-native view transformation and depth-aware fusion, achieving state-of-the-art accuracy at real-time speeds on automotive hardware.",
        "tldr_zh": "FastBEV++ 通过引入高效的、TensorRT 原生的视图转换和深度感知融合，解决了纯摄像头 BEV 感知中性能和部署之间的权衡问题，在汽车硬件上实现了最先进的精度和实时速度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "目前，纯视觉鸟瞰图（BEV）感知的发展受到最先进性能和车载部署可行性之间根本矛盾的阻碍。 这种瓶颈源于对计算量巨大的视角变换以及定制的、特定于平台的内核的深度依赖。 本文介绍了FastBEV++，一个旨在调和这种矛盾的框架，它证明了高性能和部署效率可以通过两个指导原则协同实现：“算法加速”和“设计可部署”。 我们通过一种新颖的视角变换范式来实现“设计可部署”原则，该范式将整体投影分解为标准的索引-聚集-重塑流水线。 通过确定的预排序策略，这种变换完全使用基本的、算子原生的原语（例如聚集、矩阵乘法）来执行，从而消除了对专用CUDA内核的需求，并确保了完全TensorRT原生可移植性。 同时，我们的框架通过“算法加速”，利用这种分解结构来无缝集成端到端的、深度感知的融合机制。 这种联合学习的深度调制，在时间聚合和鲁棒的数据增强的进一步支持下，显著提高了BEV表示的几何保真度。 在nuScenes基准上的实证验证证实了我们方法的有效性。 FastBEV++建立了新的最先进水平，达到0.359 NDS，同时保持了卓越的实时性能，在汽车级硬件（例如Tesla T4）上超过134 FPS。 通过提供一种无需自定义插件但高度准确的解决方案，FastBEV++为生产型自动驾驶系统提供了一种成熟且可扩展的设计理念。 代码已发布在：https://github.com/ymlab/advanced-fastbev"
    },
    {
        "title": "Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment",
        "summary": "Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.",
        "url": "http://arxiv.org/abs/2512.08040v1",
        "published_date": "2025-12-08T21:05:46+00:00",
        "updated_date": "2025-12-08T21:05:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youngjoon Jang",
            "Liliane Momeni",
            "Zifan Jiang",
            "Joon Son Chung",
            "Gül Varol",
            "Andrew Zisserman"
        ],
        "tldr": "This paper introduces a unified model for sign language understanding, performing sign language translation (SLT) and sign-subtitle alignment (SSA) with SOTA results on BOBSL and good generalization ability to How2Sign.",
        "tldr_zh": "本文介绍了一个统一的用于手语理解的模型，该模型执行手语翻译（SLT）和符号字幕对齐（SSA），并在BOBSL上获得了SOTA结果，并在How2Sign上具有良好的泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "我们的目标是开发一个统一的符号语言理解模型，该模型能够执行符号语言翻译(SLT)和符号-字幕对齐(SSA)。 这两项任务共同实现了将连续的符号视频转换为口语文本，以及符号与字幕的时间对齐——这两者对于实际通信、大规模语料库构建和教育应用至关重要。 为了实现这一点，我们的方法建立在三个组件之上：(i) 一个轻量级的视觉骨干网络，能够从人体关键点和唇部区域图像中捕捉手势和非手势线索，同时保护符号者的隐私；(ii) 一个滑动感知器映射网络，将连续的视觉特征聚合为词级别的嵌入，从而弥补视觉-文本之间的差距； 以及(iii) 一种多任务可扩展训练策略，联合优化SLT和SSA，以加强语言和时间对齐。 为了促进跨语言的泛化，我们使用来自BOBSL和YouTube-SL-25数据集的大规模符号-文本语料库，对模型进行预训练，涵盖英国手语(BSL)和美国手语(ASL)。 凭借这种多语言预训练和强大的模型设计，我们在具有挑战性的BOBSL (BSL)数据集上，在SLT和SSA方面都取得了最先进的结果。 我们的模型还在How2Sign (ASL) 上展示了强大的零样本泛化和微调后的SLT性能，突出了跨不同符号语言进行可扩展翻译的潜力。"
    },
    {
        "title": "Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems",
        "summary": "This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.",
        "url": "http://arxiv.org/abs/2512.08630v1",
        "published_date": "2025-12-09T14:14:40+00:00",
        "updated_date": "2025-12-09T14:14:40+00:00",
        "categories": [
            "cs.RO",
            "cs.MA"
        ],
        "authors": [
            "Marta Manzoni",
            "Alessandro Nazzari",
            "Roberto Rubinacci",
            "Marco Lovera"
        ],
        "tldr": "The paper explores Multi-Task Bayesian Optimization for tuning decentralized trajectory generation in multi-drone systems, showing single-task optimization achieves shorter mission times with larger swarms but requires more optimization time than optimizing for the average task.",
        "tldr_zh": "该论文探索了多任务贝叶斯优化在多无人机系统中调整分散轨迹生成算法的应用。结果表明，单任务优化在无人机群规模较大时能实现更短的任务时间，但与优化平均任务相比，需要更长的优化时间。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "本文研究了使用多任务贝叶斯优化方法来调整多无人机系统中分散式轨迹生成算法。我们将每个任务视为一种轨迹生成场景，其由指定数量的无人机间交互定义。 为了对不同场景间的关系进行建模，我们采用了多任务高斯过程，该方法可以捕获跨任务的共享结构，并可在优化过程中实现高效的信息传递。我们比较了两种策略：优化所有任务的平均任务时间和分别优化每个任务。通过全面的仿真实验，我们表明，单任务优化能够随着集群规模的增大逐渐缩短任务时间， 但需要比平均任务方法显著更多的优化时间。"
    },
    {
        "title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery",
        "summary": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.",
        "url": "http://arxiv.org/abs/2512.08577v1",
        "published_date": "2025-12-09T13:15:32+00:00",
        "updated_date": "2025-12-09T13:15:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Yuna Kato",
            "Shohei Mori",
            "Hideo Saito",
            "Yoshifumi Takatsume",
            "Hiroki Kajita",
            "Mariko Isogawa"
        ],
        "tldr": "This paper automates the alignment and selection of surgical video feeds from multiple cameras on a shadowless lamp to generate stable, unobstructed videos for surgical education and research, showing improved usability compared to conventional methods.",
        "tldr_zh": "本文提出了一种自动化方法，用于对安装在无影灯上的多个摄像头拍摄的手术视频进行对齐和选择，从而生成稳定、无遮挡的手术视频，用于手术教育和研究，并表明其用户体验优于传统方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "开放手术的影像记录对于教育和研究目的而言需求量很大。然而，捕捉无遮挡的视频极具挑战性，因为外科医生经常会阻挡摄像机的视野。为了避免遮挡，摄像机的位置和角度必须频繁调整，这非常耗费人力。先前的研究已经通过在无影灯上安装多个摄像机来解决这个问题，并安排它们完全环绕手术区域。这种设置增加了某些摄像机捕捉到无遮挡视野的机会。然而，由于每次外科医生为了获得最佳照明而移动灯具时，摄像机的配置都会发生变化，因此需要在后期处理中进行手动图像对齐。本文旨在完全自动化此对齐任务。所提出的方法识别照明系统移动的帧，重新对齐它们，并选择遮挡最少的摄像机，以生成一个始终从固定视角呈现手术视野的视频。一项涉及外科医生的用户研究表明，由我们方法生成的视频在确认手术区域的简易性和视频观看的舒适性方面优于传统方法。此外，我们的方法在视频质量方面优于现有技术。更进一步，我们为所提出的视图合成方法实现了几个合成选项，并进行了一项用户研究以评估外科医生对每个选项的偏好。"
    },
    {
        "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking",
        "summary": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.",
        "url": "http://arxiv.org/abs/2512.08518v2",
        "published_date": "2025-12-09T12:08:21+00:00",
        "updated_date": "2025-12-10T11:46:50+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Nadezhda Kushina",
            "Ko Watanabe",
            "Aarthi Kannan",
            "Ashita Ashok",
            "Andreas Dengel",
            "Karsten Berns"
        ],
        "tldr": "This paper investigates how eye-tracking and machine learning can be used to estimate human comfort levels when interacting with a humanoid robot at varying distances, finding that a Decision Tree classifier using pupil diameter performs best.",
        "tldr_zh": "本文研究了如何使用眼动追踪和机器学习来估计人类在不同距离与人形机器人互动时的舒适度，发现使用瞳孔直径的决策树分类器表现最佳。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "为了确保用户的舒适性和参与度，社交机器人必须适应人类的亲近距离规范。虽然先前的研究表明，眼动追踪特征能够可靠地估计人际互动中的舒适度，但它们在与人形机器人互动中的适用性仍未得到探索。在本研究中，我们使用移动眼动追踪和主观报告（N=19）研究了用户在四种实验控制距离（0.5米到2.0米）下与机器人“Ameca”互动的舒适度。我们评估了多种机器学习和深度学习模型，以基于注视特征估计舒适度。与以往Transformer模型表现出色的人际互动研究不同，决策树分类器取得了最高的性能（F1值为0.73），其中最小瞳孔直径被确定为最关键的预测因子。这些发现表明，人机互动中的生理舒适度阈值与人际互动动态不同，并且可以使用可解释的逻辑进行有效建模。"
    },
    {
        "title": "Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception",
        "summary": "Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\\% to 90\\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a \"trade-off\" group that modulated their physical responses according to disturbance likelihood, and an \"always-compensate\" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.",
        "url": "http://arxiv.org/abs/2512.08481v1",
        "published_date": "2025-12-09T10:55:58+00:00",
        "updated_date": "2025-12-09T10:55:58+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yixiang Lin",
            "Tiancheng Yang",
            "Jonathan Eden",
            "Ying Tan"
        ],
        "tldr": "This paper investigates human behavior in physical human-robot interaction (pHRI) under varying probabilities of robot disturbance, finding that humans exhibit individualized risk preferences that deviate from optimality, advocating for models like cumulative prospect theory.",
        "tldr_zh": "本文研究了在不同概率的机器人干扰下，人类在物理人机交互（pHRI）中的行为，发现人类表现出个体化的风险偏好，这偏离了最优状态，并提倡使用累积前景理论等模型。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "理解人类如何应对不确定性对于设计安全有效的物理人机交互（pHRI）至关重要，因为与机器人进行物理协作会引入多种不确定性来源，包括信任、舒适度和感知安全性。传统的pHRI控制框架通常建立在最优控制理论之上，该理论假设人类行为会使成本函数最小化；然而，人类在不确定性下的行为往往偏离这种最优模式。为了弥补这一差距，需要进一步理解人类在不确定性下的行为。本初步研究实施了一项物理耦合的目标到达任务，其中机器人以系统性变化的概率（10%至90%）提供辅助或干扰。对参与者力输入和决策策略的分析揭示了两个不同的行为簇：一个“权衡”组，他们根据干扰的可能性调整其物理反应；以及一个“始终补偿”组，其特征是对风险的高度厌恶，与概率无关。这些发现提供了经验证据，表明pHRI中的人类决策具有高度个体化特征，并且概率的感知可能与其真实值不同。因此，该研究强调需要更易于解释的行为模型，例如累积前景理论（CPT），以更准确地捕捉这些行为并为未来自适应机器人控制器的设计提供信息。"
    },
    {
        "title": "Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks",
        "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments.",
        "url": "http://arxiv.org/abs/2512.08248v1",
        "published_date": "2025-12-09T05:08:52+00:00",
        "updated_date": "2025-12-09T05:08:52+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Ahan Basu",
            "Ratnangshu Das",
            "Pushpak Jagtap"
        ],
        "tldr": "This paper introduces a Physics-Informed Neural Network (PINN) based control framework, utilizing Spatiotemporal Tubes (STT), to achieve reach-avoid-stay tasks for nonlinear systems with unknown dynamics and disturbances, validated through robot and aerial vehicle examples.",
        "tldr_zh": "本文提出了一种基于物理信息神经网络（PINN）的控制框架，该框架利用时空管道（STT）来实现具有未知动力学和扰动的非线性系统的到达-避开-停留任务，并通过机器人和飞行器实例进行了验证。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文提出一种基于时空管（STT）的控制框架，用于控制具有未知动态的通用控制仿射MIMO非线性纯反馈系统，使其在外部扰动下满足预定的时间可达-规避-驻留任务。STT被定义为一个时变球，其中心和半径由物理信息神经网络（PINN）联合逼近。首先，将控制STT的约束公式化为PINN的损失函数，并提出一种训练算法来最小化总体违反。在特定配置点上训练PINN后，我们提出一种基于Lipschitz的有效性条件，以正式验证学习到的PINN在连续时间范围内满足条件。基于学习到的STT表示，定义了一个无近似的闭式控制器，以保证满足T-RAS规范。最后，通过两个涉及移动机器人和空中车辆在杂乱环境中导航的案例研究验证了该框架的有效性和可扩展性。"
    },
    {
        "title": "RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features",
        "summary": "In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.",
        "url": "http://arxiv.org/abs/2512.08170v1",
        "published_date": "2025-12-09T01:58:43+00:00",
        "updated_date": "2025-12-09T01:58:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haoxin Zhang",
            "Shuaixin Li",
            "Xiaozhou Zhu",
            "Hongbo Chen",
            "Wen Yao"
        ],
        "tldr": "This paper introduces RAVES-Calib, a robust and accurate LiDAR-camera calibration toolkit that uses optimal geometric features, requiring only single-pair data in targetless environments and demonstrating superior performance compared to SOTA methods. The code is open-sourced.",
        "tldr_zh": "本文介绍RAVES-Calib，一个稳健且精确的激光雷达-相机标定工具包，它使用最佳几何特征，只需在无目标环境中配对数据，并展示出优于SOTA方法的性能。该代码已开源。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文提出了一种用户友好的激光雷达-相机标定工具包，该工具包兼容各种激光雷达和相机传感器，并且在无目标环境下仅需一对激光点和相机图像即可完成标定。我们的方法无需初始变换，即使在激光雷达-相机外参存在较大的位置和旋转偏差时，仍能保持鲁棒性。我们利用Gluestick流程来建立2D-3D点和线特征的对应关系，以获得鲁棒且自动的初始猜测。为了提高精度，我们定量分析了特征分布对标定结果的影响，并基于这些指标自适应地加权每个特征的代价。因此，外参的优化通过滤除劣质特征的不利影响来实现。我们在室内和室外环境，通过多种激光雷达-相机传感器的大量实验验证了我们的方法。结果表明，与最先进的技术相比，我们的方法提供了卓越的鲁棒性和精度。我们的代码已在GitHub上开源，以造福社区。"
    },
    {
        "title": "Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations",
        "summary": "This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.",
        "url": "http://arxiv.org/abs/2512.08028v1",
        "published_date": "2025-12-08T20:41:08+00:00",
        "updated_date": "2025-12-08T20:41:08+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Lampis Papakostas",
            "Aristeidis Geladaris",
            "Athanasios Mastrogeorgiou",
            "Jim Sharples",
            "Gautier Hattenberger",
            "Panagiotis Chatzakos",
            "Panagiotis Polygerinos"
        ],
        "tldr": "This paper presents a UAV swarm system for disaster response, utilizing an ESDF map for collision avoidance and a TSP variant for optimized area coverage prioritizing POIs, validated through simulations.",
        "tldr_zh": "本文提出了一种用于灾害响应的无人机群系统，该系统利用局部欧几里德有符号距离场（ESDF）地图进行避障，并采用旅行商问题（TSP）的变体来优化区域覆盖，优先考虑感兴趣区域（POI），并通过仿真验证。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "本文提出了一种无人机集群系统，旨在协助急救人员应对类似野火的灾难场景。通过在多个智能体上分布传感器，系统延长了飞行时长，提高了数据可用性，并降低了因碰撞导致任务失败的风险。为了进一步降低此风险，我们引入了一个自主导航框架，该框架利用局部欧几里德符号距离场 (ESDF) 地图进行避障，同时以最小的路径偏差保持集群队形。此外，我们还融入了一种旅行商问题 (TSP) 的变体来优化区域覆盖，根据预设值对感兴趣点 (POI) 进行优先级排序，这些预设值来源于环境行为和关键基础设施。通过不同集群规模的仿真验证了所提出的系统，结果表明该系统能够在确保无人机与障碍物之间避撞的同时，最大化覆盖范围。"
    },
    {
        "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion",
        "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.",
        "url": "http://arxiv.org/abs/2512.07813v1",
        "published_date": "2025-12-08T18:47:04+00:00",
        "updated_date": "2025-12-08T18:47:04+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Hari Prakash Thanabalan",
            "Lars Bengtsson",
            "Ugo Lafont",
            "Giovanni Volpe"
        ],
        "tldr": "This paper presents an inchworm-inspired soft robot that uses a single actuator and groove-patterned substrates for directional control, simplifying design and energy consumption compared to traditional multi-actuator systems.",
        "tldr_zh": "本文介绍了一种受尺蠖启发的软体机器人，它使用单个致动器和带凹槽图案的基板来实现方向控制，与传统的多致动器系统相比，简化了设计并降低了能耗。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "软体机器人需要定向控制才能在复杂地形中导航。然而，实现这种控制通常需要多个执行器，这增加了机械复杂性，使得控制系统复杂化，并提高了能量消耗。在此，我们介绍了一种受尺蠖启发的软体机器人，其运动方向由图案化基底被动控制。该机器人采用单个卷绕式介电弹性体执行器，而3D打印基底上的凹槽图案引导其对齐和轨迹。通过系统性实验，我们证明了改变凹槽角度能够在无需复杂驱动策略的情况下实现对运动方向的精确控制。这种凹槽引导方法降低了能量消耗，简化了机器人设计，并拓展了仿生软体机器人在搜索救援、管道检测以及行星探索等领域的适用性。"
    },
    {
        "title": "Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata",
        "summary": "Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.",
        "url": "http://arxiv.org/abs/2512.08360v1",
        "published_date": "2025-12-09T08:36:54+00:00",
        "updated_date": "2025-12-09T08:36:54+00:00",
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Ali Sakour"
        ],
        "tldr": "This paper introduces Conditional Neural Cellular Automata (c-NCA) for generating class-conditional structural digits (MNIST) from a single seed, offering a biologically plausible and lightweight alternative to traditional generative models by leveraging local rules and translation equivariance.",
        "tldr_zh": "本文介绍了一种条件神经元胞自动机 (c-NCA)，用于从单一种子生成类别条件结构数字 (MNIST)，通过利用局部规则和平移等变性，提供了一种生物学上合理的、轻量级的传统生成模型的替代方案。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "生物系统展现出卓越的形态发生可塑性，其中单个基因组可以编码由局部化学信号触发的各种专门化细胞结构。在深度学习领域，可微分神经元胞自动机 (NCA) 已经成为模仿这种自组织的一种范式。然而，现有的NCA研究主要集中在连续纹理合成或单目标物体恢复上，而类别条件结构生成这一挑战在很大程度上仍未被探索。在本研究中，我们提出了一种新型的有条件神经元胞自动机 (c-NCA) 架构，它能够从单个通用种子生长出不同的拓扑结构——具体来说是MNIST数字——仅由空间广播的类别向量引导。与依赖于全局感受野的传统生成模型（如GAN、VAE）不同，我们的模型强制执行严格的局部性和平移等变性。我们证明，通过将独热条件注入到细胞感知场中，一组局部的规则可以学习打破对称性并自组装成十个不同的几何吸引子。实验结果表明，我们的c-NCA实现了稳定的收敛，能从单个像素正确地形成数字拓扑，并展现出生物系统所特有的鲁棒性。这项工作弥合了基于纹理的NCA和结构模式形成之间的差距，为条件生成提供了一种轻量级、生物学上合理的替代方案。"
    },
    {
        "title": "Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection",
        "summary": "Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.",
        "url": "http://arxiv.org/abs/2512.08247v1",
        "published_date": "2025-12-09T05:01:29+00:00",
        "updated_date": "2025-12-09T05:01:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haowen Zheng",
            "Hu Zhu",
            "Lu Deng",
            "Weihao Gu",
            "Yang Yang",
            "Yanyan Liang"
        ],
        "tldr": "This paper introduces Future Temporal Knowledge Distillation (FTKD), a novel knowledge distillation approach for camera-based 3D object detection that leverages future frames to improve online model performance without increasing inference cost.",
        "tldr_zh": "本文介绍了一种名为未来时间知识蒸馏（FTKD）的新型知识蒸馏方法，用于基于相机的3D目标检测，该方法利用未来的帧来提高在线模型的性能，而不会增加推理成本。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "基于相机的时序3D目标检测在自动驾驶领域展现出令人瞩目的成果，离线模型通过利用未来帧来提高精度。知识蒸馏（KD）提供了一个有吸引力的框架，可以将丰富的知识从离线模型迁移到在线模型。然而，现有的KD方法忽略了未来帧，因为它们主要关注严格帧对齐下的空间特征蒸馏或时序关系蒸馏，这使得在线模型难以有效地学习未来知识。为此，我们提出了一种基于稀疏查询的方法，即未来时序知识蒸馏（FTKD），它可以有效地将未来帧知识从离线教师模型迁移到在线学生模型。具体来说，我们提出了一种未来感知特征重构策略，以鼓励学生模型在没有严格帧对齐的情况下捕获未来特征。此外，我们进一步引入了未来引导的Logit蒸馏，以利用教师模型稳定的前景和背景上下文。FTKD应用于两个高性能3D目标检测基线模型，在nuScenes数据集上获得了高达1.3 mAP和1.3 NDS的性能提升，并且实现了最精确的速度估计，而没有增加推理成本。"
    },
    {
        "title": "A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation",
        "summary": "Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.",
        "url": "http://arxiv.org/abs/2512.08747v1",
        "published_date": "2025-12-09T15:57:29+00:00",
        "updated_date": "2025-12-09T15:57:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Artúr I. Károly",
            "Péter Galambos"
        ],
        "tldr": "This paper presents a pipeline combining procedural 3D graphics in Blender with guided diffusion for generating photorealistic synthetic training data for white button mushroom segmentation, achieving state-of-the-art results in a zero-shot setting on real-world datasets.",
        "tldr_zh": "本文提出了一种结合Blender中的程序化3D图形和引导扩散的流程，用于生成用于白色蘑菇分割的逼真合成训练数据，在真实世界数据集的零样本设置中取得了最先进的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "工业化蘑菇栽培越来越依赖于计算机视觉进行监控和自动化采摘。然而，开发精确的检测和分割模型需要大量、精确标注的数据集，而这些数据集的生产成本很高。合成数据提供了一种可扩展的替代方案，但往往缺乏足够的真实感，难以泛化到真实场景中。本文提出了一种新颖的工作流程，该流程集成了 Blender 中的 3D 渲染与约束扩散模型，以自动生成高质量、带标注的、照片级真实的双孢蘑菇合成图像。这种方法保留了对 3D 场景配置和标注的完全控制，同时实现了照片级真实感，而无需专业的计算机图形学知识。我们发布了两个合成数据集（每个数据集包含 6,000 张图像，描绘了超过 25 万个蘑菇实例），并在零样本设置下评估了基于这些数据集训练的 Mask R-CNN 模型。在两个独立的真实世界数据集（包括一个新收集的基准数据集）上进行测试时，我们的方法实现了最先进的分割性能（在 M18K 上 F1 = 0.859），尽管仅使用合成训练数据。虽然该方法是在双孢蘑菇上进行演示的，但所提出的流程可以很容易地适用于其他蘑菇种类或其他农业领域，例如水果和叶片检测。"
    },
    {
        "title": "Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture",
        "summary": "Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\\% accuracy and 60.00\\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting",
        "url": "http://arxiv.org/abs/2512.08738v1",
        "published_date": "2025-12-09T15:49:23+00:00",
        "updated_date": "2025-12-09T15:49:23+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Samuel Ebimobowei Johnny",
            "Blessed Guda",
            "Emmanuel Enejo Aaron",
            "Assane Gueye"
        ],
        "tldr": "This paper introduces an end-to-end pose-based method for sign language spotting, framing it as a sign retrieval task, and achieving promising results on the WSLP 2025 dataset.",
        "tldr_zh": "该论文介绍了一种端到端的基于姿势的体感语言定位方法，将其框架作为体感检索任务，并在WSLP 2025数据集上取得了有希望的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "自动手语识别（ASLR）已经成为弥合聋人与健听人群体之间鸿沟的关键领域。 然而，在连续手语序列中进行手语检索或检测特定手语的问题在很大程度上仍未被探索。 我们将这个新颖的任务定义为手语定位。 在本文中，我们提出了一种对手语检索的初步尝试，旨在解决在句子级别的词汇注释或手语视频中检测查询手语视频是否存在的问题。 与依赖中间词汇注释识别或基于文本匹配的传统方法不同，我们提出了一种端到端模型，该模型直接对从手语视频中提取的姿势关键点进行操作。 我们的架构采用仅编码器主干，并带有一个二元分类头，以确定查询手语是否出现在目标序列中。 通过专注于姿势表示而不是原始 RGB 帧，我们的方法显著降低了计算成本并减轻了视觉噪声。 我们在 WSLP 2025 共享任务中的词汇存在预测数据集上评估了我们的方法，获得了 61.88% 的准确率和 60.00% 的 F1 分数。 这些结果证明了我们基于姿势框架在手语定位方面的有效性，为自动手语检索和验证方面的未来研究奠定了坚实的基础。 代码可在 https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting 获取。"
    },
    {
        "title": "C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition",
        "summary": "Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.",
        "url": "http://arxiv.org/abs/2512.08647v2",
        "published_date": "2025-12-09T14:35:35+00:00",
        "updated_date": "2025-12-10T02:33:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Keito Inoshita"
        ],
        "tldr": "The paper proposes a computationally efficient and domain-invariant method, C-DIRA, for driver behavior recognition using in-vehicle cameras, achieving high accuracy with low FLOPs and latency while generalizing well to unseen domains.",
        "tldr_zh": "该论文提出了一种计算高效且领域不变的方法 C-DIRA，用于使用车载摄像头进行驾驶员行为识别，在低 FLOPs 和低延迟下实现高精度，同时能很好地泛化到未见领域。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "使用车载摄像头进行驾驶员分心行为识别需要在边缘设备上进行实时推理。然而，轻量级模型通常无法捕捉到细粒度的行为线索，导致在未曾见过的驾驶员或不同条件下性能下降。基于感兴趣区域（ROI）的方法也会增加计算成本，难以平衡效率和准确性。本研究旨在解决对轻量级架构的需求，使其克服这些约束。我们提出了一种计算高效的动态感兴趣区域路由和领域不变对抗学习方法用于轻量级驾驶员行为识别（C-DIRA）。该框架结合了显著性驱动的Top-K ROI池化和融合分类，用于局部特征提取和整合。动态ROI路由通过仅对高难度数据样本应用ROI推理来实现选择性计算。此外，使用伪领域标注和对抗学习来学习对驾驶员和背景变化具有鲁棒性的领域不变特征。在State Farm Distracted Driver Detection数据集上的实验表明，与先前的轻量级模型相比，C-DIRA在保持高精度的同时，显著减少了FLOPs并降低了延迟。它还展示了在视觉退化（例如模糊和低光照）下的鲁棒性，以及在未知领域中的稳定性能。这些结果证实了C-DIRA在实现紧凑性、效率和泛化方面的有效性。"
    },
    {
        "title": "OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds",
        "summary": "A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.",
        "url": "http://arxiv.org/abs/2512.08506v1",
        "published_date": "2025-12-09T11:47:57+00:00",
        "updated_date": "2025-12-09T11:47:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialu Sui",
            "Rui Liu",
            "Hongsheng Zhang"
        ],
        "tldr": "The paper introduces OCCDiff, an occupancy diffusion model, for high-fidelity 3D building reconstruction from noisy LiDAR point clouds, utilizing a latent diffusion process and a point encoder within a function autoencoder framework. It claims improved robustness and fidelity.",
        "tldr_zh": "该论文介绍了OCCDiff，一种用于从嘈杂的LiDAR点云中进行高保真3D建筑物重建的占据扩散模型，它利用了潜在扩散过程和一个函数自编码器框架中的点编码器。 它声称提高了鲁棒性和保真度。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "从 LiDAR 点云重建建筑物的一个主要挑战在于，如何在不同的点密度和噪声干扰下准确捕捉建筑物表面。为了在各种分辨率下灵活地收集高质量的建筑物 3D 轮廓，我们提出了 OCCDiff，它将潜在扩散应用于占据函数空间。我们的 OCCDiff 将潜在扩散过程与函数自编码器架构相结合，以生成可在任意位置评估的连续占据函数。此外，我们提出了一个点编码器，为扩散学习提供条件特征、约束占据解码器的最终占据预测，并将多模态特征插入到用于潜在编码的潜在生成器中。为了进一步提高模型性能，我们采用了一种多任务训练策略，确保点编码器学习到多样且鲁棒的特征表示。实验结果表明，我们的方法生成了物理上一致的样本，对目标分布具有高保真度，并对噪声数据表现出鲁棒性。"
    },
    {
        "title": "LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training",
        "summary": "Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.",
        "url": "http://arxiv.org/abs/2512.08439v1",
        "published_date": "2025-12-09T10:09:31+00:00",
        "updated_date": "2025-12-09T10:09:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qing Xu",
            "Kun Yuan",
            "Yuxiang Luo",
            "Yuhao Zhai",
            "Wenting Duan",
            "Nassir Navab",
            "Zhen Chen"
        ],
        "tldr": "LapFM introduces a foundation model for laparoscopic image segmentation, pre-trained with a hierarchical concept evolving approach on a large-scale dataset, demonstrating superior performance in granularity-adaptive generalization.",
        "tldr_zh": "LapFM 推出了一种用于腹腔镜图像分割的基础模型，通过分层概念演化方法在大型数据集上进行预训练，展示了在粒度自适应泛化方面的卓越性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "手术分割对于场景理解至关重要，但仍然受到标注稀缺和不同手术过程间语义不一致的限制。现有方法通常使用有限的监督微调自然预训练模型（例如，SAM），其作用仅仅是领域适配器，而不是手术预训练模型。 导致它们难以在广泛的手术目标变异性中进行泛化。 为了弥合这一差距，我们提出了 LapFM，一个旨在从大量未标记手术图像中发展出强大的分割能力的预训练模型。 与依赖于低效自监督代理任务的医学预训练模型不同，LapFM 利用了分层概念演化预训练范式。 首先，我们通过具有父子查询嵌入的分层掩码解码器建立了一个腹腔镜概念层级 (LCH)，将各种实体（即，解剖结构、组织和器械）统一到一个具有跨粒度语义一致性的可扩展知识结构中。 其次，我们提出了一种置信度驱动的演化标注方法，该方法基于分层一致性迭代地生成和过滤伪标签，逐步将来自未标记图像的可靠样本纳入训练中。 这一过程产生了 LapBench-114K，一个包含 11.4 万个图像-掩码对的大规模基准。 大量实验表明，LapFM 显著优于最先进的方法，为通用腹腔镜分割中的粒度自适应泛化建立了新的标准。 源代码可在 https://github.com/xq141839/LapFM 获得。"
    },
    {
        "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos",
        "summary": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.",
        "url": "http://arxiv.org/abs/2512.08406v1",
        "published_date": "2025-12-09T09:37:31+00:00",
        "updated_date": "2025-12-09T09:37:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingqi Gao",
            "Yunqi Miao",
            "Jungong Han"
        ],
        "tldr": "The paper introduces SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust 4D human body mesh recovery from videos using a promptable video segmentation model and an occlusion-aware refinement module to guide SAM 3D Body.",
        "tldr_zh": "该论文介绍了SAM-Body4D，一个无需训练的框架，通过使用可提示的视频分割模型和一个遮挡感知精炼模块来指导SAM 3D Body，从而实现从视频中进行时间上一致且具有遮挡鲁棒性的4D人体网格重建。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "人体网格重建（HMR）旨在从二维观测重建三维人体姿态和形状，是现实场景中以人为中心的理解的基础。虽然最近基于图像的HMR方法，如SAM 3D Body，在真实场景图像上取得了强大的鲁棒性，但当应用于视频时，它们依赖于逐帧推理，导致时间上的不一致以及遮挡下的性能下降。我们通过利用视频中固有人体连续性来解决这些问题，而无需额外的训练。我们提出了SAM-Body4D，一个无需训练的框架，用于从视频中进行时间上一致且对遮挡具有鲁棒性的HMR。我们首先使用可提示的视频分割模型生成身份一致的masklets（小掩码），然后使用一个遮挡感知模块对它们进行细化，以恢复缺失的区域。细化后的masklets引导SAM 3D Body生成一致的全身网格轨迹，同时一个基于填充的并行策略实现了高效的多人推理。实验结果表明，SAM-Body4D在具有挑战性的真实场景视频中实现了更好的时间稳定性和鲁棒性，而无需任何重新训练。我们的代码和演示可在以下网址获得：https://github.com/gaomingqi/sam-body4d。"
    },
    {
        "title": "SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation",
        "summary": "Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.",
        "url": "http://arxiv.org/abs/2512.08362v1",
        "published_date": "2025-12-09T08:38:11+00:00",
        "updated_date": "2025-12-09T08:38:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ju-Young Kim",
            "Ji-Hong Park",
            "Gun-Woo Kim"
        ],
        "tldr": "The paper introduces SCU-CGAN, a conditional GAN for generating synthetic fire images to augment fire detection datasets, leading to significant improvements in fire detection accuracy using YOLOv5 nano.",
        "tldr_zh": "本文提出了一种名为SCU-CGAN的条件生成对抗网络，用于生成合成火灾图像以扩充火灾检测数据集，从而显著提高了YOLOv5 nano的火灾检测精度。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "火灾长期以来与人类生活息息相关，造成严重的灾难和损失。早期检测至关重要，随着家庭物联网技术的兴起，家用火灾检测系统应运而生。然而，缺乏充足的火灾数据集限制了检测模型的性能。我们提出一种SCU-CGAN模型，该模型集成了U-Net、CBAM和一个额外的判别器，以根据非火灾图像生成逼真的火灾图像。我们评估了图像质量，并证实SCU-CGAN的表现优于现有模型。具体而言，SCU-CGAN在KID评分方面比CycleGAN提高了41.5%，证明了生成的火灾图像具有更高的质量。此外，实验表明，增强数据集显著提高了火灾检测模型的准确性，而无需改变其结构。对于YOLOv5 nano模型，最显著的改进体现在mAP@0.5:0.95指标上，该指标提高了56.5%，突出了所提出方法的有效性。"
    },
    {
        "title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting",
        "summary": "Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.",
        "url": "http://arxiv.org/abs/2512.08334v1",
        "published_date": "2025-12-09T08:02:30+00:00",
        "updated_date": "2025-12-09T08:02:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chang Liu",
            "Hongliang Yuan",
            "Lianghao Zhang",
            "Sichao Wang",
            "Jianwei Guo",
            "Shi-Sheng Huang"
        ],
        "tldr": "The paper introduces HybridSplat, a novel Gaussian splatting method that accelerates reflection rendering by baking reflections into Gaussian primitives and using a hybrid splatting framework, achieving significant speedup and memory reduction compared to existing methods.",
        "tldr_zh": "本文介绍了一种新的高斯溅射方法HybridSplat，通过将反射烘焙到高斯基元中，并使用混合溅射框架，加速了反射渲染，与现有方法相比，实现了显著的加速和内存减少。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "使用三维高斯溅射渲染真实场景的复杂反射，对于照片级真实感的新视角合成而言是一种很有前景的解决方案，但仍然面临瓶颈，尤其是在渲染速度和内存存储方面。本文提出了一种新的混合溅射（HybridSplat）机制用于高斯图元。我们的核心思想是一种新的反射烘焙高斯追踪方法，它将每个高斯图元内的视角相关反射进行烘焙，同时使用基于瓦片的高斯溅射来渲染反射。然后，我们使用统一的混合溅射框架将反射高斯图元与基础高斯图元集成，以实现高保真度的场景重建。此外，我们进一步引入了混合溅射的流水线级加速和反射敏感型高斯剪枝，以减少模型大小，从而在保持反射渲染质量的同时，实现更快的渲染速度和更低的内存存储。通过大量评估，我们的HybridSplat在来自Ref-NeRF、NeRF-Casting的复杂反射场景中实现了约7倍的渲染速度提升，并且比类似的基于光线追踪的高斯溅射基线减少了4倍的高斯图元，成为一种新的最先进的方法，尤其适用于复杂的反射场景。"
    },
    {
        "title": "Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation",
        "summary": "Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.",
        "url": "http://arxiv.org/abs/2512.08253v1",
        "published_date": "2025-12-09T05:18:30+00:00",
        "updated_date": "2025-12-09T05:18:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "YiLin Zhou",
            "Lili Wei",
            "Zheming Xu",
            "Ziyi Chen",
            "Congyan Lang"
        ],
        "tldr": "This paper introduces a Query-aware Hub Prototype learning method (QHP) for few-shot 3D point cloud semantic segmentation, addressing prototype bias by modeling semantic correlations between support and query sets and optimizing prototype distributions.",
        "tldr_zh": "本文提出了一种查询感知中心原型学习方法（QHP），用于少样本3D点云语义分割，通过建模支持集和查询集之间的语义相关性并优化原型分布，解决原型偏差问题。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "小样本三维点云语义分割（FS-3DSeg）旨在仅使用少量带标签的样本分割新类别。然而，现有的基于度量学习的原型学习方法仅从支持集中生成原型，而未考虑其与查询数据的相关性。这通常会导致原型偏差，即原型过度拟合支持集特有的特征，并且无法推广到查询分布，尤其是在存在分布偏移的情况下，从而导致分割性能下降。为了解决这个问题，我们提出了一种新颖的查询感知枢纽原型（QHP）学习方法，该方法显式地建模了支持集和查询集之间的语义相关性。具体而言，我们提出了一种枢纽原型生成（HPG）模块，该模块构建连接查询点和支持点的二分图，识别频繁连接的支持枢纽，并生成与查询相关的原型，从而更好地捕获跨集合语义。为了进一步减轻不良枢纽和类边界附近模糊原型的影响，我们引入了一个原型分布优化（PDO）模块，该模块采用纯度加权对比损失来细化原型表示，通过将不良枢纽和异常原型拉近到其相应的类中心。在S3DIS和ScanNet上的大量实验表明，QHP在最先进的方法上实现了显著的性能提升，有效地缩小了FS-3DSeg中原型和查询集之间的语义差距。"
    },
    {
        "title": "Animal Re-Identification on Microcontrollers",
        "summary": "Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.",
        "url": "http://arxiv.org/abs/2512.08198v1",
        "published_date": "2025-12-09T03:09:22+00:00",
        "updated_date": "2025-12-09T03:09:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yubo Chen",
            "Di Zhao",
            "Yun Sing Koh",
            "Talia Xu"
        ],
        "tldr": "This paper presents a framework for animal re-identification on microcontrollers, focusing on efficient model design and adaptation for resource-constrained devices in wildlife monitoring and livestock management. The approach involves systematically scaling down a CNN architecture and using data-efficient fine-tuning.",
        "tldr_zh": "本文提出了一个在微控制器上进行动物重识别的框架，重点关注在野生动物监测和牲畜管理中资源受限的设备上的高效模型设计和适应。该方法包括系统地缩小CNN架构并使用数据高效的微调。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "基于摄像头的动物重识别（Animal Re-ID）技术可以在无线连接受限的大型户外环境中支持野生动物监测和精准畜牧管理。在这种场景下，推理必须直接在项圈标签或基于微控制器（MCU）的低功耗边缘节点上运行，然而大多数Animal Re-ID模型是为工作站或服务器设计的，对于内存小、输入分辨率低的设备来说过于庞大。我们提出了一种端侧框架。首先，我们描述了最先进的Animal Re-ID模型与MCU级硬件之间的差距，表明一旦内存和输入分辨率受到限制，从大型教师模型进行直接的知识蒸馏所带来的收益有限。其次，在这一分析的指导下，我们通过系统地缩放基于CNN的MobileNetV2骨干网络，为低分辨率输入设计了一种高精度Animal Re-ID架构。第三，我们使用真实世界的数据集评估了该框架，并引入了一种数据高效的微调策略，仅需每个动物身份在新站点中的三张图像即可实现快速适应。在六个公开的Animal Re-ID数据集上，我们的紧凑型模型在降低模型大小两个数量级以上的同时，实现了具有竞争力的检索精度。在一个自采集的牛群数据集上，部署的模型执行完全的端侧推理，与集群版本相比，仅有很小的精度下降，且Top-1准确率不变。我们证明了在MCU级设备上实现实用且可适应的Animal Re-ID是可行的，为在实际野外环境中进行可扩展的部署铺平了道路。"
    },
    {
        "title": "Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators",
        "summary": "Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.",
        "url": "http://arxiv.org/abs/2512.08163v1",
        "published_date": "2025-12-09T01:42:00+00:00",
        "updated_date": "2025-12-09T01:42:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuki Kubota",
            "Taiki Fukiage"
        ],
        "tldr": "This paper investigates the relationship between accuracy and human-likeness in monocular depth estimation, finding that higher accuracy doesn't guarantee human-aligned perception, suggesting a need for human-centric evaluations.",
        "tldr_zh": "本文研究了单目深度估计中准确性和人类相似性之间的关系，发现更高的准确性并不能保证与人类感知对齐，表明需要以人为中心的评估。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "单目深度估计是自动驾驶和机器人等现实应用的一项基本能力。尽管深度神经网络（DNNs）在基于物理的基准测试中取得了超人的准确率，但仍然存在一个关键挑战：使模型表征与人类感知对齐，这是一种增强模型鲁棒性和可解释性的有前景的策略。物体识别方面的研究揭示了模型准确率和类人行为之间复杂的权衡，提出了一个问题：在深度估计中是否存在类似的差异，特别是对于自然户外场景，其基准测试依赖于基于传感器的真实值，而不是人类的感知估计。在本研究中，我们使用 KITTI 数据集系统地研究了 69 种单目深度估计器中模型准确率与人类相似性之间的关系。为了逐要素地剖析误差模式的结构，我们应用仿射拟合将预测误差分解为可解释的组成部分。有趣的是，我们的结果表明，虽然人类和 DNNs 存在某些估计偏差（正误差相关性），但我们观察到模型准确率和人类相似性之间存在明显的权衡关系。这一发现表明，提高准确率并不一定导致更类人的行为，突显了开发超越传统准确率的多方面的、以人为中心的评估的必要性。"
    },
    {
        "title": "Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data",
        "summary": "We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.",
        "url": "http://arxiv.org/abs/2512.08859v1",
        "published_date": "2025-12-09T17:51:01+00:00",
        "updated_date": "2025-12-09T17:51:01+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Lars Ole Häusler",
            "Lena Uhlenberg",
            "Göran Köber",
            "Diyora Salimova",
            "Oliver Amft"
        ],
        "tldr": "The paper proposes refining a diffusion model for text-to-IMU motion synthesis using an acceleration loss, leading to more realistic IMU data and improved performance in Human Activity Recognition.",
        "tldr_zh": "该论文提出了一种通过加速度损失函数改进扩散模型，用于文本到IMU运动合成的方法。该方法能生成更真实的IMU数据，并提升了人体活动识别的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "我们提出了一种文本到IMU（惯性测量单元）的运动合成框架，通过使用基于加速度的二阶损失 (L_acc) 微调预训练扩散模型来获得真实的IMU数据。L_acc强制生成运动的离散二阶时间差分的一致性，从而使扩散先验与IMU特定的加速度模式对齐。我们将L_acc集成到现有扩散模型的训练目标中，微调该模型以获得IMU特定的运动先验，并使用包含表面建模和虚拟传感器仿真的现有文本到IMU框架评估该模型。我们分析了加速度信号的保真度以及合成运动表示与实际IMU记录之间的差异。作为一个下游应用，我们评估了人类活动识别（HAR），并将使用我们方法的数据与早期扩散模型和另外两个扩散模型基线的分类性能进行了比较。当我们使用L_acc增强早期扩散模型的目标并继续训练时，L_acc相对于原始模型降低了12.7%。在高动态活动（即跑步、跳跃）中，这些改进比低动态活动（即坐着、站立）要大得多。在低维嵌入中，我们改进后的模型生成的合成IMU数据更接近真实IMU记录的分布。仅在我们改进后的合成IMU数据上训练的HAR分类，其性能比早期扩散模型提高了8.7%，比性能最佳的对比扩散模型提高了7.6%。我们得出结论，加速度感知扩散精化提供了一种有效的方法来对齐运动生成和IMU合成，并突出了灵活的深度学习管道如何专门用于将通用的文本到运动先验应用于传感器特定的任务。"
    },
    {
        "title": "Beyond Wave Variables: A Data-Driven Ensemble Approach for Enhanced Teleoperation Transparency and Stability",
        "summary": "Time delays in communication channels present significant challenges for bilateral teleoperation systems, affecting both transparency and stability. Although traditional wave variable-based methods for a four-channel architecture ensure stability via passivity, they remain vulnerable to wave reflections and disturbances like variable delays and environmental noise. This article presents a data-driven hybrid framework that replaces the conventional wave-variable transform with an ensemble of three advanced sequence models, each optimized separately via the state-of-the-art Optuna optimizer, and combined through a stacking meta-learner. The base predictors include an LSTM augmented with Prophet for trend correction, an LSTM-based feature extractor paired with clustering and a random forest for improved regression, and a CNN-LSTM model for localized and long-term dynamics. Experimental validation was performed in Python using data generated from the baseline system implemented in MATLAB/Simulink. The results show that our optimized ensemble achieves a transparency comparable to the baseline wave-variable system under varying delays and noise, while ensuring stability through passivity constraints.",
        "url": "http://arxiv.org/abs/2512.08436v1",
        "published_date": "2025-12-09T10:06:05+00:00",
        "updated_date": "2025-12-09T10:06:05+00:00",
        "categories": [
            "eess.SY",
            "cs.LG"
        ],
        "authors": [
            "Nour Mitiche",
            "Farid Ferguene",
            "Mourad Oussalah"
        ],
        "tldr": "This paper presents a data-driven ensemble of sequence models to replace the traditional wave-variable transform for enhanced transparency and stability in teleoperation systems with time delays, demonstrating comparable transparency and guaranteed stability via passivity constraints.",
        "tldr_zh": "该论文提出了一种数据驱动的序列模型集成方法，以替代传统的波变量变换，从而增强具有时间延迟的遥操作系统的透明度和稳定性，并展示了可比的透明度和通过无源约束保证的稳定性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "通信信道中的时延对双边遥操作系统提出了显著挑战，影响透明性和稳定性。尽管基于四通道架构的传统波变量方法通过无源性确保稳定性，但它们仍然容易受到波反射和扰动的影响，如可变时延和环境噪声。本文提出了一种数据驱动的混合框架，该框架用一个由三种先进序列模型组成的集成代替了传统的波变量变换，每个模型都通过最先进的 Optuna 优化器单独优化，并通过堆叠元学习器进行组合。基础预测器包括一个用 Prophet 增强的 LSTM 用于趋势校正、一个基于 LSTM 的特征提取器，与聚类和随机森林配对以改进回归，以及一个用于局部和长期动态的 CNN-LSTM 模型。实验验证是在 Python 中使用从MATLAB/Simulink中实现的基线系统生成的数据进行的。结果表明，我们优化后的集成在可变时延和噪声下实现了与基线波变量系统相当的透明度，同时通过无源性约束确保了稳定性。"
    },
    {
        "title": "Learning Dynamics from Infrequent Output Measurements for Uncertainty-Aware Optimal Control",
        "summary": "Reliable optimal control is challenging when the dynamics of a nonlinear system are unknown and only infrequent, noisy output measurements are available. This work addresses this setting of limited sensing by formulating a Bayesian prior over the continuous-time dynamics and latent state trajectory in state-space form and updating it through a targeted marginal Metropolis-Hastings sampler equipped with a numerical ODE integrator. The resulting posterior samples are used to formulate a scenario-based optimal control problem that accounts for both model and measurement uncertainty and is solved using standard nonlinear programming methods. The approach is validated in a numerical case study on glucose regulation using a Type 1 diabetes model.",
        "url": "http://arxiv.org/abs/2512.08013v1",
        "published_date": "2025-12-08T20:10:37+00:00",
        "updated_date": "2025-12-08T20:10:37+00:00",
        "categories": [
            "eess.SY",
            "cs.LG",
            "math.OC"
        ],
        "authors": [
            "Robert Lefringhausen",
            "Theodor Springer",
            "Sandra Hirche"
        ],
        "tldr": "The paper presents a Bayesian approach for optimal control of nonlinear systems with unknown dynamics and infrequent, noisy output measurements, using a Metropolis-Hastings sampler and scenario-based optimal control. It validates the approach on glucose regulation for Type 1 diabetes.",
        "tldr_zh": "本文提出了一种贝叶斯方法，用于在非线性系统动力学未知且只有不频繁、有噪声的输出测量可用的情况下进行最优控制。该方法使用 Metropolis-Hastings 采样器和基于场景的最优控制，并通过 1 型糖尿病的葡萄糖调节进行了验证。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "当非线性系统的动力学未知且仅有少量、带噪声的输出测量可用时，可靠的最优控制极具挑战性。本文针对这种有限感知环境，通过在连续时间动力学和状态空间形式的隐状态轨迹上构建贝叶斯先验，并利用配备数值ODE积分器的目标边缘Metropolis-Hastings采样器对其进行更新来解决此问题。由此产生的后验样本被用于构建基于场景的最优控制问题，该问题考虑了模型和测量的不确定性，并使用标准的非线性规划方法进行求解。该方法在一个使用1型糖尿病模型的葡萄糖调节数值案例研究中得到了验证。"
    },
    {
        "title": "A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm",
        "summary": "Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.",
        "url": "http://arxiv.org/abs/2512.07997v1",
        "published_date": "2025-12-08T19:36:10+00:00",
        "updated_date": "2025-12-08T19:36:10+00:00",
        "categories": [
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Soroush Baghernezhad",
            "Elaheh Mohammadreza",
            "Vinicius Prado da Fonseca",
            "Ting Zou",
            "Xianta Jiang"
        ],
        "tldr": "This paper compares EMG and IMU signals for hand gesture recognition, finding that IMU signals alone can be sufficient for static gesture recognition by capturing tendon-induced micro-movements.",
        "tldr_zh": "本文比较了用于手势识别的肌电图（EMG）和惯性测量单元（IMU）信号，发现仅IMU信号就通过捕捉肌腱诱导的微小运动，足以用于静态手势识别。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "手势是我们日常与环境互动不可或缺的一部分。手势识别 (HGR) 是通过视觉数据（图像和视频）和生物信号等各种输入方式来解读人类意图的过程。生物信号因其能够通过放置在手臂上的传感器进行非侵入式采集而在 HGR 中得到广泛应用。其中，表面肌电图 (sEMG) 作为一种测量肌肉电活动的手段，是被研究最为广泛的方式。然而，惯性测量单元 (IMU) 等探索较少的替代方案可以提供关于细微肌肉运动的补充信息，这使其对于手势识别具有重要价值。在本研究中，我们调查了利用来自不同肌肉群的 IMU 信号来捕捉用户意图的潜力。我们的结果表明，IMU 信号包含足够的信息，可以作为静态手势识别的唯一输入传感器。此外，我们比较了不同的肌肉群，并检查了单个肌肉群上的模式识别质量。我们进一步发现，由肌腱引起的微动是静态手势识别的主要贡献者。我们相信利用肌肉微动信息可以增强截肢者使用假肢手臂的可用性。这种方法也为机器人、远程操作、手语翻译等领域的手势识别提供了新的可能性。"
    },
    {
        "title": "Decoupled Design of Time-Varying Control Barrier Functions via Equivariances",
        "summary": "This article presents a systematic method for designing time-varying Control Barrier Functions (CBF) composed of a time-invariant component and multiple time-dependent components, leveraging structural properties of the system dynamics. The method involves the construction of a specific class of time-invariant CBFs that encode the system's dynamic capabilities with respect to a given constraint, and augments them subsequently with appropriately designed time-dependent transformations. While transformations uniformly varying the time-invariant CBF can be applied to arbitrary systems, transformations exploiting structural properties in the dynamics - equivariances in particular - enable the handling of a broader and more expressive class of time-varying constraints. The article shows how to leverage such properties in the design of time-varying CBFs. The proposed method decouples the design of time variations from the computationally expensive construction of the underlying CBFs, thereby providing a computationally attractive method to the design of time-varying CBFs. The method accounts for input constraints and under-actuation, and requires only qualitative knowledge on the time-variation of the constraints making it suitable to the application in uncertain environments.",
        "url": "http://arxiv.org/abs/2512.08607v1",
        "published_date": "2025-12-09T13:51:07+00:00",
        "updated_date": "2025-12-09T13:51:07+00:00",
        "categories": [
            "eess.SY",
            "cs.RO"
        ],
        "authors": [
            "Adrian Wiltz",
            "Dimos V. Dimarogonas"
        ],
        "tldr": "This paper introduces a method for designing time-varying Control Barrier Functions (CBFs) by decoupling the design of time variations from the CBF construction, leveraging system dynamics' equivariances to handle complex time-varying constraints, even with input constraints and under-actuation.",
        "tldr_zh": "该论文提出了一种设计时变控制障碍函数（CBF）的方法，通过将时变设计与CBF构建分离，并利用系统动力学的等变性来处理复杂的时变约束，即使存在输入约束和欠驱动情况。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "本文提出了一种设计时变控制屏障函数（CBF）的系统方法，该函数由时不变分量和多个时变分量组成，利用了系统动力学的结构特性。该方法包括构建一类特定的时不变CBF，其编码了系统针对给定约束的动态能力，并随后用适当设计的时变变换对其进行增强。虽然均匀改变时不变CBF的变换可以应用于任意系统，但利用动力学中的结构特性（特别是等变性）的变换能够处理更广泛和更具表现力的时变约束。本文展示了如何在时变CBF的设计中利用这些特性。所提出的方法将时变设计的与底层CBF的计算密集型构建解耦，从而为时变CBF的设计提供了一种具有计算吸引力的方法。该方法考虑了输入约束和欠驱动，并且只需要关于约束时变的定性知识，使其适用于不确定环境中的应用。"
    },
    {
        "title": "A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles",
        "summary": "Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.",
        "url": "http://arxiv.org/abs/2512.08512v1",
        "published_date": "2025-12-09T11:54:09+00:00",
        "updated_date": "2025-12-09T11:54:09+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Jiang Liu",
            "Yan Qin",
            "Wei Dai",
            "Chau Yuen"
        ],
        "tldr": "The paper proposes a lightweight transfer learning method called CITL for accurate and fast SOH monitoring of Li-ion batteries in UAVs, achieving state-of-the-art results with significantly reduced computational cost.",
        "tldr_zh": "该论文提出了一种轻量级的迁移学习方法 CITL，用于无人机锂离子电池精确快速的 SOH 监测，以显著降低的计算成本实现了最先进的结果。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "精准且快速的健康状态（SOH）监测在指示锂离子电池供电的便携式移动设备的能量信息方面起着重要作用。为了应对其多变的工况，迁移学习（TL）作为一种有前景的技术脱颖而出，能够利用来自数据丰富的源工作条件的数据知识，显著减少目标工作条件下的SOH监测所需的训练数据。然而，传统的基于TL的SOH监测应用于便携式移动设备时是不可行的，因为TL阶段会消耗大量的计算资源，意外地降低设备的工作续航能力。为了应对这些挑战，本文提出了一种基于轻量级TL的SOH监测方法，即建设性增量迁移学习（CITL）。首先，利用目标域中的未标记数据，提出了一种半监督TL机制，通过在CITL中迭代地添加网络节点，以建设性的方式最小化监测残差。其次，通过结构风险最小化、迁移失配最小化和流形一致性最大化，全面保证了CITL节点参数的跨域学习能力。此外，给出了CITL的收敛性分析，从理论上保证了TL性能和网络紧凑性的有效性。最后，通过对从数十个飞行任务中收集的真实无人机（UAV）电池数据集进行的大量实验，验证了所提出的方法。具体而言，在SOH估计方面，CITL优于SS-TCA、MMD-LSTM-DA、DDAN、BO-CNN-TL和AS$^3$LSTM，均方根误差（RMSE）指标分别降低了83.73%、61.15%、28.24%、87.70%和57.34%。"
    },
    {
        "title": "Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery",
        "summary": "Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.",
        "url": "http://arxiv.org/abs/2512.08467v1",
        "published_date": "2025-12-09T10:40:17+00:00",
        "updated_date": "2025-12-09T10:40:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chamath Ranasinghe",
            "Uthayasanker Thayasivam"
        ],
        "tldr": "This paper presents a SAM-based football player tracking method that combines SAM with CSRT trackers and jersey color-based appearance models to improve occlusion recovery, particularly in crowded scenes. While effective for light to medium occlusions, it struggles with long-term occlusions where players leave the frame.",
        "tldr_zh": "本文提出了一种基于SAM的足球运动员跟踪方法，该方法将SAM与CSRT跟踪器和基于球衣颜色的外观模型相结合，以提高遮挡恢复能力，尤其是在拥挤的场景中。 虽然对于轻度到中度遮挡有效，但它在球员离开画面的长期遮挡方面存在困难。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "足球运动员跟踪面临着拥挤场景中频繁的遮挡、相似的外观和快速运动的挑战。本文提出了一种基于轻量级 SAM 的跟踪方法，该方法将 Segment Anything Model (SAM) 与 CSRT 跟踪器和基于球衣颜色的外观模型相结合。我们提出一个团队感知跟踪系统，该系统使用 SAM 进行精确定位初始化，并使用基于 HSV 直方图的重识别来提高遮挡恢复能力。我们的评估衡量了三个维度：处理速度（FPS 和内存）、跟踪精度（成功率和框稳定性）和鲁棒性（遮挡恢复和身份一致性）。在足球视频序列上的实验表明，该方法以稳定的内存使用率（约 1880 MB）实现了 7.6-7.7 FPS 的速度，在轻微遮挡下保持 100% 的跟踪成功率，在拥挤的罚球区场景（5 名或更多球员）中保持 90% 的跟踪成功率。基于外观的重识别恢复了 50% 的严重遮挡，证明了特定领域线索的价值。分析揭示了关键的权衡：SAM + CSRT 组合在不同人群密度下提供了稳定一致的性能，但在球员离开画面的长期遮挡情况下表现不佳，仅实现了 8.66% 的重新获取成功率。这些结果为在资源受限条件下部署足球跟踪系统提供了实用的指导，表明基于经典跟踪器的方法在连续可见性下运行良好，但对于长时间缺席需要更强的重识别机制。"
    },
    {
        "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries",
        "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git",
        "url": "http://arxiv.org/abs/2512.08400v1",
        "published_date": "2025-12-09T09:33:53+00:00",
        "updated_date": "2025-12-09T09:33:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Samitha Nuwan Thilakarathna",
            "Ercan Avsar",
            "Martin Mathias Nielsen",
            "Malte Pedersen"
        ],
        "tldr": "This paper presents a deep learning pipeline for fish re-identification in electronic monitoring systems, using a novel dataset and achieving improved performance with hard triplet mining, dataset-specific normalization, and a Swin-T architecture. They identify intra-species similarity and viewpoint inconsistency as key challenges.",
        "tldr_zh": "本文提出了一种用于电子监控系统中鱼类重识别的深度学习流程，使用了一个新的数据集，并通过硬三元组挖掘、数据集特定归一化和 Swin-T 架构提高了性能。 他们发现种内相似性和视点不一致是关键挑战。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "准确的渔业数据对于有效和可持续的海洋资源管理至关重要。随着电子监控（EM）系统的最新应用，现在收集到的视频数据量已超过人工审查的可行范围。本文通过开发用于自动鱼类重识别（Re-ID）的优化深度学习流程来解决这一挑战，该流程使用了新型的AutoFish数据集，该数据集通过传送带模拟EM系统，包含六种外观相似的鱼类。我们证明了通过使用硬三元组挖掘，并结合包含针对数据集的特定归一化的定制图像转换流程，关键的Re-ID指标（R1和mAP@k）显著提高。通过采用这些策略，我们证明了基于Vision Transformer的Swin-T架构始终优于基于卷积神经网络的ResNet-50，实现了41.65%的mAP@k和90.43%的Rank-1准确率的峰值性能。深入分析表明，主要挑战是区分同一物种中视觉上相似的个体（种内错误），其中视角不一致性比部分遮挡造成了更为显著的损害。源代码和文档可在以下网址获取：https://github.com/msamdk/Fish_Re_Identification.git"
    },
    {
        "title": "Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety",
        "summary": "Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring.",
        "url": "http://arxiv.org/abs/2512.08862v1",
        "published_date": "2025-12-09T17:53:19+00:00",
        "updated_date": "2025-12-09T17:53:19+00:00",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Mohamed Elmahallawy",
            "Sanjay Madria",
            "Samuel Frimpong"
        ],
        "tldr": "The paper introduces FedMining, a privacy-preserving federated learning framework tailored for underground mining, using decentralized functional encryption and a balancing aggregation mechanism to address security and data heterogeneity challenges.",
        "tldr_zh": "该论文介绍了 FedMining，一种专为地下矿井设计的隐私保护联邦学习框架，它采用去中心化功能加密和平衡聚合机制来应对安全性和数据异构性挑战。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "地下采矿作业依赖于传感器网络来监测关键参数，例如温度、气体浓度和矿工行动，从而能够及时进行危险检测和安全决策。然而，将原始传感器数据传输到集中式服务器以进行机器学习 (ML) 模型训练会引发严重的隐私和安全问题。联邦学习 (FL) 通过实现去中心化模型训练而无需暴露敏感本地数据，提供了一种很有前景的替代方案。然而，在地下采矿中应用联邦学习面临着独特的挑战：（i）攻击者可能窃听共享的模型更新，从而发起模型反演或成员推断攻击，危及数据隐私和运营安全；（ii）跨矿井的非独立同分布 (Non-IID) 数据分布和传感器噪声会阻碍模型收敛。为了解决这些问题，我们提出了 FedMining——一种为地下采矿量身定制的隐私保护联邦学习框架。FedMining 引入了两项核心创新：（1）一种去中心化函数加密 (DFE) 方案，该方案使本地模型保持加密状态，从而阻止未经授权的访问和推理攻击；以及 (2) 一种平衡聚合机制，用于缓解数据异构性并增强收敛性。在真实矿业数据集上的评估表明，FedMining 能够在保护隐私的同时保持高模型精度，并以更少的通信和计算开销实现快速收敛。这些优势使 FedMining 既安全又实用，可用于实时地下安全监测。"
    },
    {
        "title": "Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design",
        "summary": "Preliminary mission design of low-thrust spacecraft trajectories in the Circular Restricted Three-Body Problem is a global search characterized by a complex objective landscape and numerous local minima. Formulating the problem as sampling from an unnormalized distribution supported on neighborhoods of locally optimal solutions, provides the opportunity to deploy Markov chain Monte Carlo methods and generative machine learning. In this work, we extend our previous self-supervised diffusion model fine-tuning framework to employ gradient-informed Markov chain Monte Carlo. We compare two algorithms - the Metropolis-Adjusted Langevin Algorithm and Hamiltonian Monte Carlo - both initialized from a distribution learned by a diffusion model. Derivatives of an objective function that balances fuel consumption, time of flight and constraint violations are computed analytically using state transition matrices. We show that incorporating the gradient drift term accelerates mixing and improves convergence of the Markov chain for a multi-revolution transfer in the Saturn-Titan system. Among the evaluated methods, MALA provides the best trade-off between performance and computational cost. Starting from samples generated by a baseline diffusion model trained on a related transfer, MALA explicitly targets Pareto-optimal solutions. Compared to a random walk Metropolis algorithm, it increases the feasibility rate from 17.34% to 63.01% and produces a denser, more diverse coverage of the Pareto front. By fine-tuning a diffusion model on the generated samples and associated reward values with reward-weighted likelihood maximization, we learn the global solution structure of the problem and eliminate the need for a tedious separate data generation phase.",
        "url": "http://arxiv.org/abs/2512.08705v1",
        "published_date": "2025-12-09T15:21:11+00:00",
        "updated_date": "2025-12-09T15:21:11+00:00",
        "categories": [
            "eess.SY",
            "cs.LG",
            "math.OC"
        ],
        "authors": [
            "Jannik Graebner",
            "Ryne Beeson"
        ],
        "tldr": "This paper introduces a gradient-informed Monte Carlo fine-tuning method for diffusion models to optimize low-thrust spacecraft trajectories, demonstrating improved convergence and solution coverage compared to random walk methods. Further fine-tuning of the diffusion model eliminates the need for a separate data generation phase.",
        "tldr_zh": "本文提出了一种梯度信息蒙特卡洛微调扩散模型的方法，用于优化低推力航天器轨道，与随机游走方法相比，展示了更高的收敛速度和更好的解覆盖率。对扩散模型的进一步微调消除了对单独数据生成阶段的需求。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "低推力航天器在圆型限制性三体问题中的初步任务设计是一个全局搜索过程，其特点是目标函数形势复杂且存在大量局部极小值。将问题建模为从局部最优解邻域上支持的非归一化分布中采样，为部署马尔可夫链蒙特卡罗方法和生成式机器学习提供了机会。在本工作中，我们扩展了先前基于自监督扩散模型的微调框架，以采用梯度指导的马尔可夫链蒙特卡罗方法。我们比较了两种算法—— Metropolis-Adjusted Langevin Algorithm (MALA) 和 Hamiltonian Monte Carlo (HMC)——均从扩散模型学习到的分布中初始化。利用状态转移矩阵解析计算了平衡燃料消耗、飞行时间和约束违反的目标函数的导数。结果表明，在土星-泰坦系统中进行多次转移时，加入梯度漂移项可以加速混合并改善马尔可夫链的收敛性。在评估的方法中，MALA 在性能和计算成本之间提供了最佳的权衡。从在相关转移上训练的基线扩散模型生成的样本开始，MALA 明确地针对 Pareto 最优解。与随机游走 Metropolis 算法相比，它将可行率从 17.34% 提高到 63.01%，并产生了更密集、更多样化的 Pareto 前沿覆盖。通过使用奖励加权似然最大化方法，基于生成的样本和相关的奖励值微调扩散模型，我们学习了问题的全局解结构，从而消除了繁琐的单独数据生成阶段的需求。"
    },
    {
        "title": "Probabilistic Multi-Agent Aircraft Landing Time Prediction",
        "summary": "Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.",
        "url": "http://arxiv.org/abs/2512.08281v1",
        "published_date": "2025-12-09T06:27:26+00:00",
        "updated_date": "2025-12-09T06:27:26+00:00",
        "categories": [
            "cs.MA",
            "cs.LG"
        ],
        "authors": [
            "Kyungmin Kim",
            "Seokbin Yoon",
            "Keumjin Lee"
        ],
        "tldr": "This paper introduces a probabilistic multi-agent framework for aircraft landing time prediction, accounting for uncertainty and inter-aircraft interactions, demonstrated on real-world data from Incheon International Airport.",
        "tldr_zh": "本文介绍了一种用于飞机着陆时间预测的概率性多智能体框架，该框架考虑了不确定性和飞机间的相互作用，并在韩国仁川国际机场的真实数据上进行了演示。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "飞机着陆时间的准确和可靠预测对于空域交通管理中有效的资源分配至关重要。然而，飞机轨迹和交通流的内在不确定性给预测的准确性和可信度带来了重大挑战。因此，预测模型不仅应提供飞机着陆时间的点估计，还应提供与这些预测相关的uncertainties。此外，飞机轨迹经常受到附近飞机的影响，这是由于空中交通管制介入，如雷达引导。因此，着陆时间预测模型必须考虑空域中的多智能体交互。在这项工作中，我们提出了一个概率多智能体飞机着陆时间预测框架，该框架将多架飞机的着陆时间提供为分布。我们使用从韩国仁川国际机场终端空域收集的空中交通监视数据集评估了所提出的框架。结果表明，所提出的模型比基线模型实现了更高的预测精度，并量化了其结果的相关uncertainties。此外，该模型通过其注意力得分揭示了空中交通管制中的潜在模式，从而提高了可解释性。"
    }
]