[
    {
        "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
        "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
        "url": "http://arxiv.org/abs/2512.13177v2",
        "published_date": "2025-12-15T10:37:59+00:00",
        "updated_date": "2025-12-16T05:50:26+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Minghui Hou",
            "Wei-Hsing Huang",
            "Shaofeng Liang",
            "Daizong Liu",
            "Tai-Hao Wen",
            "Gang Wang",
            "Runwei Guan",
            "Weiping Ding"
        ],
        "tldr": "The paper introduces MMDrive, a multimodal vision-language model for autonomous driving that leverages occupancy maps, LiDAR, and text to enhance 3D scene understanding, achieving significant performance gains on DriveLM and NuScenes-QA benchmarks.",
        "tldr_zh": "该论文介绍了MMDrive，一种用于自动驾驶的多模态视觉-语言模型，该模型利用占用图、激光雷达和文本来增强3D场景理解，并在DriveLM和NuScenes-QA基准测试中取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "视觉-语言模型通过多源信息融合，实现了对复杂交通场景的理解和推理，使其成为自动驾驶的核心技术。然而，现有的视觉-语言模型受限于二维平面中的图像理解范式，这限制了它们感知三维空间信息和执行深度语义融合的能力，导致在复杂的自动驾驶环境中表现欠佳。本研究提出了一种多模态视觉-语言模型框架MMDrive，它将传统的图像理解扩展到广义的三维场景理解框架。MMDrive整合了三种互补的模态，包括占用栅格地图、激光雷达点云和文本场景描述。为此，它引入了两个用于自适应跨模态融合和关键信息提取的新组件。具体而言，面向文本的多模态调制器基于问题中的语义线索动态地加权每种模态的贡献，从而引导上下文感知的特征集成。跨模态抽象器采用可学习的抽象令牌来生成紧凑的跨模态摘要，突出显示关键区域和必要的语义。在DriveLM和NuScenes-QA基准上的全面评估表明，MMDrive在自动驾驶的视觉-语言模型上取得了显著的性能提升，在DriveLM上达到了54.56的BLEU-4评分和41.78的METEOR评分，在NuScenes-QA上达到了62.7%的准确率。MMDrive有效地打破了传统的仅图像理解的壁垒，实现了在复杂驾驶环境中强大的多模态推理，并为可解释的自动驾驶场景理解提供了新的基础。"
    },
    {
        "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning",
        "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.",
        "url": "http://arxiv.org/abs/2512.13100v1",
        "published_date": "2025-12-15T08:57:15+00:00",
        "updated_date": "2025-12-15T08:57:15+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Guanhua Ji",
            "Harsha Polavaram",
            "Lawrence Yunliang Chen",
            "Sandeep Bajamahal",
            "Zehan Ma",
            "Simeon Adebola",
            "Chenfeng Xu",
            "Ken Goldberg"
        ],
        "tldr": "This paper introduces OXE-AugE, a large-scale augmented dataset for cross-embodiment robot policy learning, showing significant improvements in policy performance on both seen and unseen robots in real-world tasks.",
        "tldr_zh": "本文介绍了 OXE-AugE，一个用于跨具身机器人策略学习的大规模增强数据集，展示了在真实世界任务中，策略在已见和未见机器人上的性能显著提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "为了训练能够控制各种机器人形态（机器人手臂和夹爪的组合）、执行多样任务以及适应不同环境的通用机器人策略，需要大型且多样化的数据集。由于为每个新的硬件平台重新收集演示数据和重新训练成本过于高昂，我们证明了现有的机器人数据可以被增强以实现迁移和泛化。开放X-形态（OXE）数据集汇集了来自超过60个机器人数据集的演示数据，已被广泛用作训练通用策略的基础。然而，它高度不平衡：前四种机器人类型占据了其真实数据的85%以上，这存在过度拟合机器人-场景组合的风险。我们提出了AugE-Toolkit，一个可扩展的机器人增强流程，以及OXE-AugE，一个高质量的开源数据集，它使用9种不同的机器人形态增强了OXE。OXE-AugE提供了超过440万条轨迹，是原始OXE的三倍多。我们对扩展机器人增强如何影响跨形态学习进行了系统研究。结果表明，使用不同的手臂和夹爪增强数据集不仅提高了增强机器人上的策略性能，而且提高了未见过机器人甚至原始机器人在分布偏移下的策略性能。在物理实验中，我们证明了诸如OpenVLA和$π_0$等最先进的通用策略可以通过在OXE-AugE上进行微调而受益，在四个真实世界操纵任务中，在先前未见过的机器人-夹爪组合上的成功率提高了24-45%。项目网站：https://OXE-AugE.github.io/。"
    },
    {
        "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos",
        "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.",
        "url": "http://arxiv.org/abs/2512.13080v1",
        "published_date": "2025-12-15T08:31:47+00:00",
        "updated_date": "2025-12-15T08:31:47+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yicheng Feng",
            "Wanpeng Zhang",
            "Ye Wang",
            "Hao Luo",
            "Haoqi Yuan",
            "Sipeng Zheng",
            "Zongqing Lu"
        ],
        "tldr": "This paper introduces VIPA-VLA, a Spatial-Aware VLA pretraining paradigm that uses human demonstration videos with 3D annotations to explicitly align visual and physical spaces, improving robot policy learning in downstream tasks.",
        "tldr_zh": "本文介绍了一种空间感知VLA预训练范式 VIPA-VLA，它利用带有 3D 注释的人类演示视频，显式对齐视觉和物理空间，从而改进了下游任务中的机器人策略学习。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "视觉-语言-动作(VLA)模型通过整合视觉感知与语言引导的策略学习，为机器人学习提供了一种有前景的范式。然而，大多数现有方法依赖于2D视觉输入在3D物理环境中执行动作，这在感知和动作定位之间造成了显著的差距。为了弥合这一差距，我们提出了一种空间感知VLA预训练范式，该范式在预训练期间执行视觉空间和物理空间之间的显式对齐，使模型能够在机器人策略学习之前获得3D空间理解。从预训练的视觉-语言模型出发，我们利用大规模人类示范视频提取3D视觉和3D动作标注，形成一种新的监督来源，将2D视觉观察与3D空间推理对齐。我们用VIPA-VLA实例化了这个范式，VIPA-VLA是一种双编码器架构，它包含一个3D视觉编码器，用3D感知特征增强语义视觉表示。当应用于下游机器人任务时，VIPA-VLA实现了2D视觉和3D动作之间显著改进的定位，从而产生了更稳健和更具泛化性的机器人策略。"
    },
    {
        "title": "Motus: A Unified Latent Action World Model",
        "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
        "url": "http://arxiv.org/abs/2512.13030v1",
        "published_date": "2025-12-15T06:58:40+00:00",
        "updated_date": "2025-12-15T06:58:40+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Hongzhe Bi",
            "Hengkai Tan",
            "Shenghao Xie",
            "Zeyuan Wang",
            "Shuhe Huang",
            "Haitian Liu",
            "Ruowen Zhao",
            "Yao Feng",
            "Chendong Xiang",
            "Yinze Rong",
            "Hongyan Zhao",
            "Hanyu Liu",
            "Zhizhong Su",
            "Lei Ma",
            "Hang Su",
            "Jun Zhu"
        ],
        "tldr": "Motus introduces a unified latent action world model using a Mixture-of-Transformer architecture and UniDiffuser-style scheduler to integrate understanding, video generation, and action, achieving SOTA performance in simulation and real-world robotic tasks by leveraging optical flow and a multi-phase training pipeline.",
        "tldr_zh": "Motus 提出了一个统一的潜在动作世界模型，利用混合Transformer架构和UniDiffuser风格的调度器来整合理解、视频生成和动作，通过利用光流和多阶段训练流水线，在模拟和真实世界机器人任务中实现了SOTA性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "摘要：\n虽然通用具身智能体必须作为一个统一系统运行，但目前的方法是基于孤立的理解、世界建模和控制模型构建的。这种碎片化阻碍了多模态生成能力的统一，并妨碍了从大规模异构数据中学习。本文提出了一种统一的潜在动作世界模型 Motus，它利用现有的通用预训练模型和丰富的、可共享的运动信息。Motus 引入了一种混合Transformer (MoT) 架构来整合三个专家（即，理解、视频生成和动作），并采用 UniDiffuser 风格的调度器，以实现不同建模模式之间的灵活切换（即，世界模型、视觉-语言-动作模型、逆动力学模型、视频生成模型和视频-动作联合预测模型）。Motus 进一步利用光流来学习潜在动作，并采用包含三阶段训练流程和六层数据金字塔的方案，从而提取像素级“delta action”并实现大规模的动作预训练。实验表明，在模拟（相比 X-VLA 性能提升 +15%，相比 Pi0.5 性能提升 +45%）和真实世界场景（性能提升 +11%~48%）中，Motus 都实现了优于最先进方法的性能，证明了所有功能和先验的统一建模能够显著提升下游机器人任务的性能。"
    },
    {
        "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding",
        "summary": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.",
        "url": "http://arxiv.org/abs/2512.12822v1",
        "published_date": "2025-12-14T20:02:43+00:00",
        "updated_date": "2025-12-14T20:02:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yongyuan Liang",
            "Xiyao Wang",
            "Yuanchen Ju",
            "Jianwei Yang",
            "Furong Huang"
        ],
        "tldr": "The paper introduces Lemon, a unified transformer architecture for 3D multimodal understanding that jointly processes point clouds and language, achieving state-of-the-art performance across various 3D tasks and demonstrating strong scaling properties.",
        "tldr_zh": "该论文介绍了Lemon，一个用于3D多模态理解的统一Transformer架构，它联合处理点云和语言，在各种3D任务上实现了最先进的性能，并展示了强大的扩展性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "将大型多模态模型(LMMs)扩展到三维理解带来了独特的挑战：点云数据稀疏且不规则，现有模型依赖于具有模态特定编码器的碎片化架构，并且训练流程通常存在不稳定性和较差的可扩展性。我们引入了Lemon，一个统一的Transformer架构，它通过将3D点云块和语言令牌作为单个序列进行联合处理来解决这些挑战。与以往依赖于模态特定编码器和跨模态对齐模块的工作不同，这种设计实现了早期的空间-语言融合，消除了冗余编码器，提高了参数效率，并支持更有效的模型扩展。为了处理3D数据的复杂性，我们开发了一种结构化的分块和令牌化方案，以保留空间上下文，以及一个三阶段训练课程，逐步构建从对象级识别到场景级空间推理的能力。Lemon在全面的3D理解和推理任务中建立了新的最先进性能，从对象识别和图像描述到3D场景中的空间推理，同时证明了随着模型大小和训练数据的增加，具有强大的扩展特性。我们的工作为推进现实世界应用中的3D空间智能提供了一个统一的基础。"
    },
    {
        "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
        "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
        "url": "http://arxiv.org/abs/2512.12799v1",
        "published_date": "2025-12-14T18:45:54+00:00",
        "updated_date": "2025-12-14T18:45:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Liu",
            "Runhui Huang",
            "Rui Yang",
            "Siming Yan",
            "Zining Wang",
            "Lu Hou",
            "Di Lin",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "tldr": "DrivePI is a novel spatial-aware 4D MLLM for autonomous driving, unifying understanding, perception, prediction, and planning in a single end-to-end framework, achieving state-of-the-art performance with a relatively small model.",
        "tldr_zh": "DrivePI是一种新颖的、具有空间感知的4D MLLM，用于自动驾驶，它在一个端到端的框架中统一了理解、感知、预测和规划，并以相对较小的模型实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "尽管多模态大型语言模型（MLLMs）已经在多个领域展现出强大的能力，但其在自动驾驶中生成细粒度三维感知和预测输出方面的应用仍未被充分探索。在本文中，我们提出了DrivePI，一种新型空间感知四维MLLM，可用作统一的视觉-语言-动作（VLA）框架，同时也兼容视觉-动作（VA）模型。我们的方法通过端到端优化并行地执行空间理解、三维感知（即三维占据栅格）、预测（即占据流）和规划（即动作输出）。为了获得精确的几何信息和丰富的视觉外观，我们的方法在一个统一的MLLM架构中集成了点云、多视角图像和语言指令。我们进一步开发了一个数据引擎，用于生成文本-占据栅格和文本-流QA对，以进行四维空间理解。值得注意的是，仅使用0.5B Qwen2.5模型作为MLLM主干，DrivePI作为一个单一的统一模型就能够媲美甚至超越现有的VLA模型和专门的VA模型。具体来说，与VLA模型相比，DrivePI在nuScenes-QA上超越了OpenDriveVLA-7B 2.5%的平均准确率，并在nuScenes上将碰撞率比ORION降低了70%（从0.37%降至0.11%）。与专门的VA模型相比，DrivePI在OpenOcc上，对三维占据栅格的RayIoU超越了FB-OCC 10.3，在OpenOcc上，对占据流的mAVE从0.591降至0.509，并在nuScenes上，对规划的L2误差比VAD降低了32%（从0.72米降至0.49米）。代码将在https://github.com/happinesslz/DrivePI上提供。"
    },
    {
        "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning",
        "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.",
        "url": "http://arxiv.org/abs/2512.13380v1",
        "published_date": "2025-12-15T14:32:03+00:00",
        "updated_date": "2025-12-15T14:32:03+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Chuan Mao",
            "Haoqi Yuan",
            "Ziye Huang",
            "Chaoyi Xu",
            "Kai Ma",
            "Zongqing Lu"
        ],
        "tldr": "The paper introduces DemoFunGrasp, a reinforcement learning framework for universal dexterous functional grasping that leverages demonstration editing to achieve strong sim-to-real transfer and autonomous instruction-following via a vision-language model.",
        "tldr_zh": "该论文介绍了DemoFunGrasp，一个用于通用灵巧功能抓取的强化学习框架，它利用演示编辑来实现强大的从仿真到现实的迁移，并通过视觉-语言模型实现自主指令跟随。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "强化学习(RL)在灵巧抓取方面取得了巨大成功，显著提高了抓取性能和从仿真到现实世界的泛化能力。然而，对于下游操作任务至关重要的精细化功能抓取，仍未得到充分探索，并且面临着几个挑战：跨不同对象指定功能抓取的目标和奖励函数的复杂性、多任务强化学习探索的难度，以及仿真到现实迁移的挑战。在这项工作中，我们提出了 DemoFunGrasp 用于通用灵巧功能抓取。我们将功能抓取条件分解为两个互补的组成部分——抓取风格和可供性——并将它们整合到一个强化学习框架中，该框架可以学习使用任何功能抓取条件来抓取任何对象。为了解决多任务优化挑战，我们利用一次抓取演示，并将强化学习问题重新定义为一步演示编辑，从而大大提高了样本效率和性能。仿真和现实世界中的实验结果表明，DemoFunGrasp 可以泛化到未见过的对象、可供性和抓取风格的组合，在成功率和功能抓取精度方面都优于基线方法。除了强大的仿真到现实能力外，通过结合视觉-语言模型 (VLM) 进行规划，我们的系统还可以实现自主指令跟随抓取执行。"
    },
    {
        "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles",
        "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.",
        "url": "http://arxiv.org/abs/2512.13359v1",
        "published_date": "2025-12-15T14:12:32+00:00",
        "updated_date": "2025-12-15T14:12:32+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Sümer Tunçay",
            "Alain Andres",
            "Ignacio Carlucho"
        ],
        "tldr": "This paper introduces a fast, GPU-accelerated RL training pipeline for 6-DOF AUV position control, achieving zero-shot sim-to-real transfer and demonstrating real-world results of RL-based AUV control across all six degrees of freedom.",
        "tldr_zh": "本文介绍了一种用于六自由度自主水下航行器位置控制的快速GPU加速的RL训练流水线，实现了零样本模拟到现实的转移，并展示了基于RL的AUV控制在所有六个自由度上的实际结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自主水下航行器（AUVs）需要在复杂和动态的海洋环境中有效运行，因此需要可靠的六自由度（6-DOF）位置控制。传统控制器在标称条件下有效，但在面对未建模的动力学或环境干扰时性能下降。强化学习（RL）提供了一种强大的替代方案，但训练通常很慢，且从模拟到现实的迁移仍然具有挑战性。本研究介绍了一种基于JAX和MuJoCo-XLA (MJX)构建的GPU加速RL训练流水线。通过联合JIT编译大规模并行物理仿真和学习更新，我们实现了低于两分钟的训练时间。通过对多种RL算法的系统评估，我们展示了在真实水下实验中强大的6-DOF轨迹跟踪和有效的干扰抑制能力，这些策略是从模拟中零样本迁移的。我们的结果首次明确地展示了基于RL的AUV位置控制在所有六个自由度上的真实世界应用。"
    },
    {
        "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration",
        "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.",
        "url": "http://arxiv.org/abs/2512.13293v2",
        "published_date": "2025-12-15T13:03:08+00:00",
        "updated_date": "2025-12-16T03:34:39+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Hao Fu",
            "Wei Liu",
            "Shuai Zhou"
        ],
        "tldr": "The paper introduces a new multi-robot reinforcement learning algorithm with intrinsic motivation for coordinated exploration in social formation navigation, showing superior performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了一种新的多机器人强化学习算法，该算法具有内在动机，用于社交队形导航中的协同探索，并在基准数据集上表现出卓越性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "本文研究了强化学习（RL）在多机器人社交编队导航中的应用，这是实现无缝人机共存的关键能力。尽管强化学习提供了一种有前景的范例， but 行人 inherently 不可预测 行人不 cooperative 动态 构成了实质性的挑战，尤其是在机器人之间协调探索的效率方面。为了解决这个问题，我们提出了一种新颖的协同探索多机器人强化学习算法，引入了内在激励探索。其核心组成部分是一种自学习内在奖励机制，旨在集体缓解策略保守性。此外，该算法在中心化训练和去中心化执行框架内融入了双重采样模式，以增强导航策略和内在奖励的表征，并利用双时间尺度更新规则来解耦参数更新。在社交编队导航基准测试中进行的实证结果表明，所提出的算法在关键指标上的性能优于现有的最先进方法。我们的代码和视频演示可在以下网址获取：https://github.com/czxhunzi/CEMRRL。"
    },
    {
        "title": "START: Traversing Sparse Footholds with Terrain Reconstruction",
        "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.",
        "url": "http://arxiv.org/abs/2512.13153v1",
        "published_date": "2025-12-15T10:02:41+00:00",
        "updated_date": "2025-12-15T10:02:41+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Ruiqi Yu",
            "Qianshi Wang",
            "Hongyi Li",
            "Zheng Jun",
            "Zhicheng Wang",
            "Jun Wu",
            "Qiuguo Zhu"
        ],
        "tldr": "START is a single-stage learning framework that uses onboard vision and proprioception to reconstruct terrain heightmaps, enabling agile and stable locomotion for quadruped robots on sparse footholds with zero-shot transfer to real-world scenarios.",
        "tldr_zh": "START是一个单阶段学习框架，利用机载视觉和本体感觉重建地形高度图，从而使四足机器人能够在稀疏落脚点上实现敏捷稳定的运动，并零样本迁移到真实世界场景。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "利用腿式动物的方式穿越具有稀疏支撑点的地形，对于四足机器人来说既是一种有前景又具有挑战的任务，因为它需要精确的环境感知和敏捷的控制，以确保安全的足端落点，同时保持动态稳定性。基于模型的层次化控制器在实验室环境中表现出色，但存在泛化能力有限和行为过于保守的问题。端到端学习方法具有更大的灵活性和适应性，但目前最先进的方法要么依赖于引入噪声以及复杂且昂贵的管道的高度图，要么从自我中心深度图像中隐式地推断地形特征，通常会遗漏准确的关键几何线索，导致低效的学习和僵硬的步态。为了克服这些限制，我们提出了一种名为 START 的单阶段学习框架，该框架能够在高度稀疏且随机的支撑点上实现敏捷稳定的运动。START 仅利用低成本的板载视觉和本体感觉来精确重建局部地形高度图，提供显式的中间表示来传递与稀疏支撑点区域相关的重要特征。这有助于全面地理解环境和精确地评估地形，从而降低探索成本并加速技能获取。实验结果表明，START 能够在各种真实场景中实现零样本迁移，展示了卓越的适应性、精确的足端落点和鲁棒的运动能力。"
    },
    {
        "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations",
        "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.",
        "url": "http://arxiv.org/abs/2512.13093v1",
        "published_date": "2025-12-15T08:50:20+00:00",
        "updated_date": "2025-12-15T08:50:20+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Mingqi Yuan",
            "Tao Yu",
            "Haolin Song",
            "Bo Li",
            "Xin Jin",
            "Hua Chen",
            "Wenjun Zeng"
        ],
        "tldr": "The paper presents PvP, a proprioceptive-privileged contrastive learning framework, and SRL4Humanoid, a unified framework, for data-efficient humanoid robot learning with improved sample efficiency and performance in WBC tasks.",
        "tldr_zh": "该论文提出了 PvP，一个本体感受特权对比学习框架，以及 SRL4Humanoid，一个统一框架，用于数据高效的人形机器人学习，并在 WBC 任务中提高了样本效率和性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "实现高效且稳健的全身控制（WBC）对于使人形机器人在动态环境中执行复杂任务至关重要。尽管强化学习（RL）在这方面取得了成功，但由于人形机器人复杂的动力学和部分可观测性，其采样效率低下仍然是一个重大挑战。为了解决这一局限性，我们提出了一种名为PvP的本体感知-特权对比学习框架，该框架利用了本体感知状态和特权状态之间的内在互补性。PvP学习紧凑且任务相关的潜在表示，无需手工设计的数据增强，从而实现更快更稳定的策略学习。为了支持系统评估，我们开发了SRL4Humanoid，这是第一个统一且模块化的框架，为人形机器人学习提供了具有代表性的状态表示学习（SRL）方法的高质量实现。在LimX Oli 机器人上进行的跨速度跟踪和运动模仿任务的广泛实验表明，与基线SRL方法相比，PvP 显著提高了采样效率和最终性能。我们的研究进一步提供了将SRL与RL集成以实现人形WBC的实践见解，为数据高效的人形机器人学习提供了有价值的指导。"
    },
    {
        "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion",
        "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.",
        "url": "http://arxiv.org/abs/2512.13090v1",
        "published_date": "2025-12-15T08:43:13+00:00",
        "updated_date": "2025-12-15T08:43:13+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jebeom Chae",
            "Junwoo Chang",
            "Seungho Yeom",
            "Yujin Kim",
            "Jongeun Choi"
        ],
        "tldr": "This paper introduces Language-Conditioned Heat-Inspired Diffusion (LCHD), a vision-based diffusion model for multi-robot motion planning that addresses limitations in existing diffusion-based methods by integrating semantic priors and a collision-avoiding kernel, demonstrating improved performance and reduced latency.",
        "tldr_zh": "本文介绍了一种名为语言条件热启发扩散 (LCHD) 的视觉扩散模型，用于多机器人运动规划。该模型通过整合语义先验和避碰核，克服了现有扩散方法的局限性，并展示了改进的性能和降低的延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "扩散模型作为捕获可行轨迹多模态分布的强大工具，最近开始应用于机器人运动规划。然而，将它们扩展到具有灵活的语言条件任务规范的多机器人环境仍然受到限制。此外，当前的基于扩散的方法在推理过程中会产生高昂的计算成本，并且由于它们需要显式构建环境表示且缺乏几何可达性推理机制，因此难以泛化。为了解决这些局限性，我们提出了 Language-Conditioned Heat-Inspired Diffusion (LCHD)，这是一个端到端的基于视觉的框架，用于生成具有语言条件的、无碰撞的轨迹。LCHD 将基于 CLIP 的语义先验与一个具有防撞扩散核相结合，该扩散核充当一种物理归纳偏置，使规划器能够在可到达的工作空间内严格解释语言指令。这通过引导机器人找到符合语义意图的可访问替代方案，自然地处理了超出分布（在可达性方面）的情况，同时消除了推理时对显式障碍物信息的需求。在各种受真实环境启发的地图上进行的广泛评估以及真实的机器人实验表明，LCHD 在成功率方面始终优于之前的基于扩散的规划器，同时降低了规划延迟。"
    },
    {
        "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations",
        "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.",
        "url": "http://arxiv.org/abs/2512.12993v1",
        "published_date": "2025-12-15T05:38:08+00:00",
        "updated_date": "2025-12-15T05:38:08+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Guillermo A. Castillo",
            "Himanshu Lodha",
            "Ayonga Hereid"
        ],
        "tldr": "This paper presents a hierarchical reinforcement learning framework for terrain-aware bipedal locomotion that utilizes reduced-dimensional perceptual representations learned from depth camera images to improve robustness and adaptability on a high-fidelity robotics simulator, demonstrating potential for hardware deployment.",
        "tldr_zh": "本文提出了一种分层强化学习框架，用于感知地形的双足运动，该框架利用从深度相机图像中学习的降维感知表征，从而提高了在高保真机器人模拟器上的鲁棒性和适应性，展示了硬件部署的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "这项工作提出了一种地形感知双足运动的分层策略，该策略集成了降维感知表示，以增强基于强化学习（RL）的高层（HL）策略，从而实现实时步态生成。与端到端方法不同，我们的框架利用卷积变分自编码器（CNN-VAE）生成潜在的地形编码，并结合降阶机器人动力学，从而以紧凑的状态优化运动决策过程。我们系统地分析了潜在空间维度对学习效率和策略鲁棒性的影响。此外，我们扩展了我们的方法使其具有历史感知能力，将最近地形观测序列纳入潜在表示中，以提高鲁棒性。为了解决现实可行性问题，我们引入了一种蒸馏方法，可以直接从深度相机图像中学习潜在表示，并通过比较模拟和真实传感器数据来提供初步的硬件验证。我们进一步使用高保真Agility Robotics (AR) 模拟器验证了我们的框架，其中包括逼真的传感器噪声、状态估计和执行器动力学。结果证实了我们方法的鲁棒性和适应性，突显了其硬件部署的潜力。"
    },
    {
        "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition",
        "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.",
        "url": "http://arxiv.org/abs/2512.12885v1",
        "published_date": "2025-12-14T23:56:34+00:00",
        "updated_date": "2025-12-14T23:56:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "cs.RO"
        ],
        "authors": [
            "Minghao Zhu",
            "Zhihao Zhang",
            "Anmol Sidhu",
            "Keith Redmill"
        ],
        "tldr": "The paper introduces SignRAG, a zero-shot road sign recognition framework using Retrieval-Augmented Generation. It combines VLMs and LLMs to achieve high accuracy in road sign recognition without task-specific training.",
        "tldr_zh": "本文介绍了一种名为SignRAG的零样本道路标志识别框架，该框架使用检索增强生成技术。它结合了视觉语言模型和大型语言模型，无需特定任务训练即可在道路标志识别方面实现高精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自动道路交通标志识别是智能交通系统的关键任务，但传统的深度学习方法难以应对类别数量庞大以及创建详尽标注数据集的不切实际。本文提出了一种新颖的零样本识别框架，该框架采用检索增强生成（RAG）范式来应对这一挑战。我们的方法首先使用视觉语言模型（VLM）从输入图像中生成交通标志的文本描述。此描述用于从参考设计的向量数据库中检索一小组最相关的候选标志。随后，大型语言模型（LLM）对检索到的候选标志进行推理，以进行最终的精细识别。我们在俄亥俄州 MUTCD 的一套全面的 303 个规则标志上验证了该方法。实验结果表明该框架的有效性，在理想参考图像上实现了 95.58% 的准确率，在具有挑战性的真实道路数据上实现了 82.45% 的准确率。这项工作证明了基于 RAG 的架构在创建可扩展且准确的道路交通标志识别系统方面的可行性，而无需特定于任务的训练。"
    },
    {
        "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding",
        "summary": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.",
        "url": "http://arxiv.org/abs/2512.12842v1",
        "published_date": "2025-12-14T21:13:56+00:00",
        "updated_date": "2025-12-14T21:13:56+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Kuan Fang",
            "Yuxin Chen",
            "Xinghao Zhu",
            "Farzad Niroui",
            "Lingfeng Sun",
            "Jiuguang Wang"
        ],
        "tldr": "The paper introduces SAGA, a framework for mobile manipulation that uses structured affordance grounding with multimodal foundation models to generalize across environments and tasks, demonstrating superior performance in real-world experiments.",
        "tldr_zh": "该论文介绍了SAGA，一个用于移动操作的框架，它使用带有模态基础模型的结构化可供性接地，以在各种环境和任务中推广，并在真实世界实验中展示出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了SAGA，一个多功能且自适应的视觉运动控制框架，可以在各种环境、任务目标和用户规格之间进行泛化。为了高效地学习这种能力，我们的核心思想是将高层语义意图与底层视觉运动控制解耦，通过将任务目标显式地锚定在观测环境中。通过使用基于可供性的任务表示，我们以统一的结构化形式表达了多样且复杂的行为。通过利用多模态基础模型，SAGA将提出的任务表示锚定到机器人的视觉观测，作为3D可供性热图，突出显示任务相关的实体，同时消除会阻碍泛化的虚假外观变化。这些被锚定的可供性使我们能够有效地在多任务演示数据上训练一个用于全身控制的条件策略。在一个统一的框架下，SAGA可以解决以不同形式指定的任务，包括语言指令、选定点和示例演示，从而实现零样本执行和少样本自适应。我们在一个四足操作机器人上实例化了SAGA，并在十一个真实世界的任务中进行了广泛的实验。SAGA始终以显著的优势优于端到端和模块化基线。总之，这些结果表明，结构化的可供性锚定为实现通用移动操作提供了一条可扩展且有效的途径。"
    },
    {
        "title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps",
        "summary": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.",
        "url": "http://arxiv.org/abs/2512.12793v1",
        "published_date": "2025-12-14T18:22:00+00:00",
        "updated_date": "2025-12-14T18:22:00+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Mizuho Aoki",
            "Kohei Honda",
            "Yasuhiro Yoshimura",
            "Takeshi Ishita",
            "Ryo Yonetani"
        ],
        "tldr": "The paper introduces VLG-Loc, a vision-language global localization method that utilizes labeled footprint maps and a vision-language model to localize robots by identifying visual landmarks, demonstrating robustness to environmental changes.",
        "tldr_zh": "该论文介绍了VLG-Loc，一种视觉-语言全局定位方法，它利用带有标签的足迹地图和视觉-语言模型，通过识别视觉地标来定位机器人，并且展示了对环境变化的鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文提出了一种视觉-语言全局定位（VLG-Loc）方法，这是一种新颖的全局定位方法，它使用人类可读的带有标签的足迹地图，该地图仅包含环境中独特视觉地标的名称和区域。 虽然人类自然利用此类地图进行自我定位，但由于在缺乏几何和外观细节的情况下，建立观测到的地标与地图中的地标之间的对应关系非常困难，因此将这种能力转化为机器人系统仍然极具挑战性。 为了解决这个挑战，VLG-Loc利用视觉-语言模型（VLM）在机器人的多方向图像观测中搜索地图中注释的地标。 该方法随后在蒙特卡洛定位框架内识别机器人位姿，其中找到的地标用于评估每个姿态假设的可能性。在模拟和真实零售环境中的实验验证表明，与现有的基于扫描的方法相比，该方法具有更强的鲁棒性，特别是在环境变化的情况下。 通过视觉和基于扫描的定位概率融合，可以进一步提高性能。"
    },
    {
        "title": "Memory in the Age of AI Agents",
        "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
        "url": "http://arxiv.org/abs/2512.13564v1",
        "published_date": "2025-12-15T17:22:34+00:00",
        "updated_date": "2025-12-15T17:22:34+00:00",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "authors": [
            "Yuyang Hu",
            "Shichun Liu",
            "Yanwei Yue",
            "Guibin Zhang",
            "Boyang Liu",
            "Fangyi Zhu",
            "Jiahang Lin",
            "Honglin Guo",
            "Shihan Dou",
            "Zhiheng Xi",
            "Senjie Jin",
            "Jiejun Tan",
            "Yanbin Yin",
            "Jiongnan Liu",
            "Zeyu Zhang",
            "Zhongxiang Sun",
            "Yutao Zhu",
            "Hao Sun",
            "Boci Peng",
            "Zhenrong Cheng",
            "Xuanbo Fan",
            "Jiaxin Guo",
            "Xinlei Yu",
            "Zhenhong Zhou",
            "Zewen Hu",
            "Jiahao Huo",
            "Junhao Wang",
            "Yuwei Niu",
            "Yu Wang",
            "Zhenfei Yin",
            "Xiaobin Hu",
            "Yue Liao",
            "Qiankun Li",
            "Kun Wang",
            "Wangchunshu Zhou",
            "Yixin Liu",
            "Dawei Cheng",
            "Qi Zhang",
            "Tao Gui",
            "Shirui Pan",
            "Yan Zhang",
            "Philip Torr",
            "Zhicheng Dou",
            "Ji-Rong Wen",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Shuicheng Yan"
        ],
        "tldr": "This paper surveys the rapidly expanding field of agent memory, offering a unified view through forms, functions, and dynamics, and outlines emerging research frontiers. It aims to provide a conceptual foundation for future agentic intelligence design.",
        "tldr_zh": "本文调研了快速发展的Agent记忆领域，通过形式、功能和动态三个维度提供了一个统一的视角，并概述了新兴的研究前沿。其目标是为未来智能Agent的设计提供一个概念基础。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "摘要：\n记忆已经成为，并将持续保持，基于基础模型智能体的核心能力。随着智能体记忆研究的迅速扩展并吸引了前所未有的关注，该领域也变得日益分散。现有归类于智能体记忆的工作在动机、实现和评估协议方面通常存在显著差异，而定义松散的记忆术语的泛滥进一步模糊了概念的清晰度。诸如长/短期记忆等传统分类已被证明不足以捕捉当代智能体记忆系统的多样性。本文旨在提供当前智能体记忆研究的最新概况。我们首先明确界定智能体记忆的范围，并将其与诸如LLM记忆、检索增强生成（RAG）和上下文工程等相关概念区分开来。然后，我们通过形式、功能和动态的统一视角来考察智能体记忆。从形式的角度来看，我们识别了智能体记忆的三种主要实现方式，即token级别记忆、参数式记忆和潜在记忆。从功能的角度来看，我们提出了一个更细粒度的分类，区分了事实记忆、经验记忆和工作记忆。从动态的角度来看，我们分析了记忆如何随时间形成、演化和检索。为了支持实际开发，我们汇总了记忆基准和开源框架的全面总结。除了整合之外，我们还阐明了对新兴研究前沿的前瞻性视角，包括记忆自动化、强化学习集成、多模态记忆、多智能体记忆和可信赖性问题。我们希望这份综述不仅可以作为现有工作的参考，还可以作为重新思考记忆，将其作为未来智能体智能设计中的一等公民概念基础。"
    },
    {
        "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
        "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
        "url": "http://arxiv.org/abs/2512.13438v1",
        "published_date": "2025-12-15T15:34:06+00:00",
        "updated_date": "2025-12-15T15:34:06+00:00",
        "categories": [
            "cs.SE",
            "cs.AI"
        ],
        "authors": [
            "Dezhi Ran",
            "Zhi Gong",
            "Yuzhe Guo",
            "Mengzhou Wu",
            "Yuan Cao",
            "Haochuan Lu",
            "Hengyu Zhang",
            "Xia Zeng",
            "Gang Cao",
            "Liangchao Yao",
            "Yuetang Deng",
            "Wei Yang",
            "Tao Xie"
        ],
        "tldr": "The paper introduces UIFormer, a framework that optimizes UI representations for LLM agents via program synthesis, achieving significant token reduction and improved efficiency without performance degradation.",
        "tldr_zh": "该论文介绍了UIFormer，一个通过程序合成优化LLM Agent 的UI表示的框架，实现了显著的token减少和效率提升，同时没有降低性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLM）智能体在自动化UI导航方面，例如自动化UI测试和AI助手，展现出巨大的潜力，但它们的效率在很大程度上被忽视了。我们的动机研究揭示了低效的UI表示方式会造成关键的性能瓶颈。然而，UI表示优化，被形式化为自动生成转换UI表示的程序的任务，面临着两个独特的挑战。首先，缺乏布尔预言机，而传统的程序合成依赖布尔预言机来果断验证语义正确性，这对token效率和完整性的协同优化构成了一个根本性的挑战。其次，需要处理大型、复杂的UI树作为输入，同时生成长的、组合式的转换程序，使得搜索空间巨大且容易出错。为了解决上述局限性，我们提出了UIFormer，这是第一个自动优化框架，它通过执行基于约束的优化和复杂合成任务的结构化分解，来合成UI转换程序。首先，UIFormer使用一种特定于领域的语言（DSL）来约束程序空间，该语言捕获特定于UI的操作。其次，UIFormer进行基于LLM的迭代精炼，并提供正确性和效率的回报，为实现效率-完整性协同优化提供指导。UIFormer作为一个轻量级插件运行，它应用转换程序以便与现有的LLM智能体无缝集成，只需要对它们的核心逻辑进行最小的修改。在跨越Android和Web平台的三个UI导航基准测试以及五个LLM上的评估表明，UIFormer实现了48.7%到55.8%的token减少，并且运行时开销最小，同时保持或提高了智能体的性能。在微信上的真实行业部署进一步验证了UIFormer的实际影响。"
    },
    {
        "title": "Differentiable Evolutionary Reinforcement Learning",
        "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.",
        "url": "http://arxiv.org/abs/2512.13399v1",
        "published_date": "2025-12-15T14:50:08+00:00",
        "updated_date": "2025-12-15T14:50:08+00:00",
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Sitao Cheng",
            "Tianle Li",
            "Xuhan Huang",
            "Xunjian Yin",
            "Difan Zou"
        ],
        "tldr": "This paper introduces Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that uses reinforcement learning to evolve reward functions, achieving state-of-the-art performance in complex reasoning tasks.",
        "tldr_zh": "本文介绍了一种可微分的进化强化学习 (DERL) 框架，该框架使用强化学习来进化奖励函数，在复杂的推理任务中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "有效奖励函数的设计是强化学习(RL)中的一个核心且往往艰巨的挑战，尤其是在为复杂的推理任务开发自主代理时。虽然存在自动奖励优化方法，但它们通常依赖于无导数的演化启发式方法，将奖励函数视为黑盒，从而未能捕获奖励结构与任务性能之间的因果关系。为了弥合这一差距，我们提出了可微分演化强化学习(DERL)，这是一种双层框架，能够自主发现最优奖励信号。在DERL中，一个元优化器通过组合结构化的原子原语来演化出一个奖励函数（即元奖励），从而指导内部循环策略的训练。至关重要的是，与先前的演化方法不同，DERL在其元优化中是可微分的：它将内部循环验证性能视为信号，通过强化学习更新元优化器。这使得DERL能够近似任务成功的“元梯度”，逐步学习生成更密集和更可操作的反馈。我们在三个不同的领域验证了DERL：机器人代理(ALFWorld)、科学模拟(ScienceWorld)和数学推理(GSM8k, MATH)。实验结果表明，DERL在ALFWorld和ScienceWorld上取得了最先进的性能，显著优于依赖启发式奖励的方法，尤其是在分布外场景中。对演化轨迹的分析表明，DERL成功地捕捉了任务的内在结构，从而能够在没有人为干预的情况下实现自我改进的代理对齐。"
    },
    {
        "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning",
        "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.",
        "url": "http://arxiv.org/abs/2512.13159v1",
        "published_date": "2025-12-15T10:08:53+00:00",
        "updated_date": "2025-12-15T10:08:53+00:00",
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Emre Can Acikgoz",
            "Jinoh Oh",
            "Jie Hao",
            "Joo Hyuk Jeon",
            "Heng Ji",
            "Dilek Hakkani-Tür",
            "Gokhan Tur",
            "Xiang Li",
            "Chengyuan Ma",
            "Xing Fan"
        ],
        "tldr": "The paper introduces SpeakRL, an RL method that enhances language models' conversational capabilities by rewarding proactive interactions for task completion. It also presents SpeakER, a synthetic dataset for training and evaluating such models and shows improved performance over baselines.",
        "tldr_zh": "该论文介绍了SpeakRL，一种通过奖励积极交互来增强语言模型对话能力的强化学习方法，以完成任务。它还提出了SpeakER，一个用于训练和评估此类模型的合成数据集，并显示出比基线模型更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在现实世界的应用中，有效的人-智能体协作越来越普遍。目前此类协作的趋势主要为单向，即用户向智能体提供指令或提出问题，而智能体直接回应，不寻求必要的澄清或确认。然而，智能体不断发展，它们需要更积极的参与，动态地参与对话以澄清用户意图、消除歧义并适应不断变化的情况。现有的先前工作未能充分利用语言模型 (LM) 的对话能力，从而将智能体优化为更好的跟随者，而不是有效的沟通者。 在这项工作中，我们提出了SpeakRL，一种强化学习 (RL) 方法，通过奖励与用户的积极互动来增强智能体的对话能力，例如在必要时提出正确的澄清问题。 为了支持这一点，我们整理了SpeakER，一个合成数据集，其中包含来自面向任务的对话的各种场景，这些任务通过交互式澄清问题得以解决。 我们对对话主动性的奖励设计进行了系统分析，并提出了一种有原则的奖励公式，用于教导智能体在提问和行动之间取得平衡。 实验评估表明，我们的方法在任务完成方面比基本模型绝对提高了 20.14%，并且没有增加对话轮数，甚至超越了更大的专有模型，展示了以澄清为中心的用户-智能体交互的前景。"
    },
    {
        "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.",
        "url": "http://arxiv.org/abs/2512.13106v1",
        "published_date": "2025-12-15T09:03:45+00:00",
        "updated_date": "2025-12-15T09:03:45+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Shenzhi Yang",
            "Guangcheng Zhu",
            "Xing Zheng",
            "Yingfan MA",
            "Zhongqi Chen",
            "Bowen Song",
            "Weiqiang Wang",
            "Junbo Zhao",
            "Gang Chen",
            "Haobo Wang"
        ],
        "tldr": "The paper introduces TraPO, a semi-supervised reinforcement learning framework for LLMs that leverages a small labeled dataset to guide RLVR training on unlabeled data, achieving strong performance with improved data efficiency and generalization in mathematical reasoning.",
        "tldr_zh": "该论文介绍了一种名为TraPO的半监督强化学习框架，用于提升大型语言模型（LLM）的推理能力。它利用少量标记数据指导在未标记数据上的RLVR训练，从而在数学推理中实现卓越的性能、数据效率和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "具有可验证奖励的强化学习 (RLVR) 已被证明可以通过利用答案可验证的信号来指导策略优化，从而有效地训练大型推理模型 (LRM)，然而，该方法面临着高昂的标注成本。为了缓解这个问题，最近的研究探索了无监督 RLVR 方法，该方法仅从模型的内部一致性中导出奖励，例如通过熵和多数投票。虽然这些方法看起来很有前景，但往往在训练后期遭受模型崩溃，这可能是由于在缺乏外部监督的情况下，错误的推理模式得到了强化。在这项工作中，我们研究了一种新的半监督 RLVR 范式，该范式利用一个小的标记数据集来指导对未标记样本的 RLVR 训练。我们的关键见解是，监督奖励对于稳定未标记样本上基于一致性的训练至关重要，确保只有在标记实例上验证过的推理模式才被纳入 RL 训练中。在技术上，我们提出了一种有效的策略优化算法 TraPO，该算法通过将其学习轨迹与标记样本的相似性相匹配来识别可靠的未标记样本。在此基础上，TraPO 在六个广泛使用的数学推理基准 (AIME24/25, AMC, MATH-500, Minerva, 和 Olympiad) 和三个分布外任务 (ARC-c, GPQA-diamond, 和 MMLU-pro) 上实现了卓越的数据效率和强大的泛化能力。 仅使用 1K 个标记样本和 3K 个未标记样本，TraPO 达到了 42.6% 的平均准确率，超过了在 45K 个未标记样本上训练的最佳无监督方法 (38.3%)。 值得注意的是，当使用 4K 个标记样本和 12K 个未标记样本时，TraPO 在所有基准测试中甚至优于在完整的 45K 个标记样本上训练的完全监督模型，同时仅使用了 10% 的标记数据。代码可通过 https://github.com/ShenzhiYang2000/TRAPO 获取。"
    },
    {
        "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization",
        "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2512.13070v1",
        "published_date": "2025-12-15T08:07:23+00:00",
        "updated_date": "2025-12-15T08:07:23+00:00",
        "categories": [
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Bizhe Bai",
            "Hongming Wu",
            "Peng Ye",
            "Tao Chen"
        ],
        "tldr": "This paper introduces M-GRPO, a framework to stabilize self-supervised RL training for LLMs by using a momentum model as a stable target and an IQR filter to maintain policy diversity, achieving state-of-the-art performance on reasoning benchmarks.",
        "tldr_zh": "该论文介绍了M-GRPO，一个通过使用动量模型作为稳定目标和IQR滤波器来维持策略多样性，从而稳定LLM的自监督RL训练的框架，在推理基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自监督强化学习（RL）为提升大型语言模型（LLM）的推理能力提供了一种有前景的方法，并且无需依赖昂贵的人工标注数据。然而，我们发现现有方法在长时程训练下存在一个关键的失效模式：“策略崩溃”，即性能急剧下降。我们诊断出这种不稳定性，并证明简单地增加rollout的数量——一种常见的提升性能策略——只能延缓，而不能阻止这种崩溃。为了对抗这种不稳定性，我们首先引入了M-GRPO（动量锚定分组相对策略优化），该框架利用一个缓慢演化的动量模型来提供稳定的训练目标。此外，我们发现该过程通常伴随着策略熵的快速崩溃，导致过早自信且次优的策略。为了专门解决这个问题，我们提出了第二个贡献：一种基于四分位距（IQR）的自适应过滤方法，该方法动态地剪除低熵轨迹，从而保留必要的策略多样性。我们在多个推理基准上的大量实验表明，M-GRPO稳定了训练过程，而IQR过滤器则防止了过早收敛。这两个创新的结合带来了卓越的训练稳定性和最先进的性能。"
    },
    {
        "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
        "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
        "url": "http://arxiv.org/abs/2512.13043v1",
        "published_date": "2025-12-15T07:11:56+00:00",
        "updated_date": "2025-12-15T07:11:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tong Wei",
            "Yijun Yang",
            "Changhao Zhang",
            "Junliang Xing",
            "Yuanchun Shi",
            "Zongqing Lu",
            "Deheng Ye"
        ],
        "tldr": "GTR-Turbo enhances vision-language model RL training by using a merged checkpoint as a free teacher, improving performance and reducing computational cost compared to methods using external teacher models.",
        "tldr_zh": "GTR-Turbo通过使用合并的检查点作为免费教师，增强了视觉语言模型强化学习训练，与使用外部教师模型的方法相比，提高了性能并降低了计算成本。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于视觉-语言模型(VLM)构建的多模态智能体的多轮强化学习(RL)受到稀疏奖励和长程信用分配的阻礙。 近期的方法通过查询提供步级反馈的教师来密集化奖励，例如引导式思维强化(GTR)和在线策略蒸馏，但依赖于成本高昂且通常具有特权的教师模型，从而限制了实用性和可复现性。 我们引入了GTR-Turbo，这是GTR的一个高效升级版本，无需训练或查询昂贵的教师模型即可达到相同的性能。 具体来说，GTR-Turbo合并了正在进行的RL训练期间产生的检查点的权重，然后使用这个合并后的模型作为“免费”教师，通过监督微调或软logit蒸馏来指导后续的RL。 这种设计消除了对特权VLM(例如GPT或Gemini)的依赖，减轻了先前工作中观察到的“熵塌陷”现象，并保持了训练的稳定性。 在各种视觉智能体任务中，GTR-Turbo将基线模型的准确率提高了10-30%，同时相对于GTR，减少了50%的运行时间和60%的计算成本。"
    },
    {
        "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution",
        "summary": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.",
        "url": "http://arxiv.org/abs/2512.12806v1",
        "published_date": "2025-12-14T19:03:59+00:00",
        "updated_date": "2025-12-14T19:03:59+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Boyang Yan"
        ],
        "tldr": "This paper introduces a fault-tolerant sandboxing framework using transactional filesystems to safely execute autonomous AI coding agents, achieving high interception and rollback rates with moderate performance overhead, unlike interactive commercial solutions.",
        "tldr_zh": "本文介绍了一个容错沙箱框架，使用事务文件系统来安全地执行自主AI编码代理，实现了高拦截率和回滚率，且性能开销适中，这与交互式的商业解决方案不同。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型（LLM）从被动代码生成器向自主代理的转变引入了显著的安全风险，特别是关于破坏性命令和不一致的系统状态。现有的商业解决方案通常优先考虑交互式用户安全，强制执行身份验证屏障，这会破坏真正自主运行所需的无头循环。本文提出了一种容错沙盒框架，旨在通过基于策略的拦截层和事务性文件系统快照机制来缓解这些风险。我们假设将代理行为封装在原子事务中可以在可接受的延迟下保证安全，优于容器的笨重初始化开销或商业CLI的交互式摩擦。我们通过在基于自定义Proxmox的测试床上部署经由nano-vllm提供服务的Minimind-MoE LLM，并利用EVPN/VXLAN隔离来验证该方法。实验结果表明，高风险命令的拦截率达到100%，并且回滚失败状态的成功率也达到100%。更关键的是，我们的原型每次事务仅产生14.5%的性能开销（约1.8秒）。相比之下，针对Gemini CLI沙盒的基准测试显示，它需要交互式身份验证（“登录”），使其无法用于无头、自主代理工作流程。"
    },
    {
        "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
        "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
        "url": "http://arxiv.org/abs/2512.13604v1",
        "published_date": "2025-12-15T17:59:58+00:00",
        "updated_date": "2025-12-15T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianxiong Gao",
            "Zhaoxi Chen",
            "Xian Liu",
            "Junhao Zhuang",
            "Chengming Xu",
            "Jianfeng Feng",
            "Yu Qiao",
            "Yanwei Fu",
            "Chenyang Si",
            "Ziwei Liu"
        ],
        "tldr": "LongVie 2 is a new autoregressive framework for building controllable, long-term, high-quality, and temporally consistent video world models, achieving state-of-the-art performance and generating videos up to five minutes long.",
        "tldr_zh": "LongVie 2是一个新的自回归框架，用于构建可控、长期、高质量且时间一致的视频世界模型，实现了最先进的性能，并能生成长达五分钟的视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在预训练的视频生成系统之上构建视频世界模型，是通向通用时空智能的重要但具有挑战性的一步。一个世界模型应具备三个基本属性：可控性、长期视觉质量和时间一致性。为此，我们采取渐进式方法——首先增强可控性，然后扩展到长期、高质量的生成。我们提出了 LongVie 2，一个端到端的自回归框架，通过三个阶段进行训练：（1）多模态引导，它整合密集和稀疏的控制信号，以提供隐式的世界级监督并提高可控性；（2）输入帧的降级感知训练，弥合训练和长期推理之间的差距，以保持高视觉质量；以及（3）历史上下文引导，它对齐相邻片段之间的上下文信息，以确保时间一致性。我们还推出了 LongVGenBench，一个包含 100 个高分辨率一分钟视频的全面基准，涵盖了多样化的真实世界和合成环境。大量实验表明，LongVie 2 在远距离可控性、时间连贯性和视觉保真度方面实现了最先进的性能，并支持长达五分钟的连续视频生成，标志着迈向统一视频世界建模的重要一步。"
    },
    {
        "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
        "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
        "url": "http://arxiv.org/abs/2512.13411v1",
        "published_date": "2025-12-15T15:00:17+00:00",
        "updated_date": "2025-12-15T15:00:17+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Patryk Niżeniec",
            "Marcin Iwanowski"
        ],
        "tldr": "This paper presents a pipeline using 3D Gaussian Splatting to generate realistic, automatically labeled datasets for training computer vision models in robotics, addressing the domain gap and annotation bottleneck.",
        "tldr_zh": "本文提出了一种使用3D高斯溅射（3DGS）生成逼真且自动标记的数据集的流程，用于训练机器人领域的计算机视觉模型，旨在解决领域差距和标注瓶颈问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文介绍了一种新颖的流程，用于生成大规模、高度真实且自动标注的机器人环境计算机视觉任务数据集。 我们的方法解决了合成图像与真实世界图像之间存在的领域鸿沟以及耗时的人工标注瓶颈等关键挑战。 我们利用 3D 高斯溅射 (3DGS) 来创建操作环境和物体的逼真表示。 然后，这些资产在游戏引擎中使用，通过物理模拟创建自然的排列。 一种新颖的两遍渲染技术将溅射的真实感与代理网格生成的阴影图相结合。 然后，将该阴影图通过算法与图像合成，以添加符合物理规律的阴影和细微的高光，从而显著增强真实感。 像素完美的分割掩码被自动生成并格式化，可直接用于诸如 YOLO 等目标检测模型。 我们的实验表明，将少量真实图像与大量合成数据相结合的混合训练策略可以产生最佳的检测和分割性能，证实这是一种有效实现稳健而准确模型的最佳策略。"
    },
    {
        "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
        "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
        "url": "http://arxiv.org/abs/2512.13250v1",
        "published_date": "2025-12-15T12:04:26+00:00",
        "updated_date": "2025-12-15T12:04:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juil Koo",
            "Daehyeon Choi",
            "Sangwoo Youn",
            "Phillip Y. Lee",
            "Minhyuk Sung"
        ],
        "tldr": "The paper introduces Visually Grounded Active View Selection (VG-AVS), a task and framework for enabling VLMs to actively select informative viewpoints for VQA using only visual information, demonstrating improved performance in synthetic and real scenes.",
        "tldr_zh": "这篇论文介绍了视觉引导的主动视角选择（VG-AVS），一个任务和框架，旨在使视觉语言模型能够仅使用视觉信息主动选择信息量大的视角来进行视觉问答（VQA），并在合成和真实场景中表现出更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "视觉语言模型（VLM）在视觉问答（VQA）方面表现出色，但仍然局限于快照视觉，即基于静态图像进行推理。相比之下，具身智能体需要移动视觉，主动移动以获取更具信息量的视角。我们引入了视觉上下文主动视角选择（VG-AVS）任务，该任务仅利用当前图像中的视觉信息来选择最具信息量的下一个视角，而不依赖于场景记忆或外部知识。为了支持这项任务，我们构建了一个合成数据集，其中包含自动生成的配对查询-目标视图和问答提示。我们还提出一种框架，通过监督微调（SFT）和基于强化学习（RL）的策略优化来微调预训练的VLM。我们的方法基于视角选择实现了强大的问答性能，并能够稳健地泛化到未见过的合成场景和真实场景。此外，将我们学习的VG-AVS框架整合到现有的基于场景探索的具身问答（EQA）系统中，可以提高下游问答的准确性。"
    },
    {
        "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance",
        "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.",
        "url": "http://arxiv.org/abs/2512.13238v1",
        "published_date": "2025-12-15T11:53:35+00:00",
        "updated_date": "2025-12-15T11:53:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Francesco Ragusa",
            "Michele Mazzamuto",
            "Rosario Forte",
            "Irene D'Ambra",
            "James Fort",
            "Jakob Engel",
            "Antonino Furnari",
            "Giovanni Maria Farinella"
        ],
        "tldr": "Ego-EXTRA is a new egocentric video-language dataset featuring expert-trainee interactions in procedural tasks. It contains 50 hours of video and 15k VQA pairs designed to benchmark multimodal LLMs for expert assistance.",
        "tldr_zh": "Ego-EXTRA是一个新的以自我为中心的视频语言数据集，包含专家-学员在程序性任务中的互动。 它包含50小时的视频和15k个VQA对，旨在用于评估多模态LLM在专家辅助方面的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了Ego-EXTRA，一个用于专家-学员辅助的以自我为中心的视频-语言数据集。Ego-EXTRA 包含 50 小时的非脚本化以自我为中心的视频，记录了受试者（学员）在真实世界专家的指导下执行程序性活动的过程，专家使用自然语言提供指导并回答具体问题。遵循“绿野仙踪”数据收集范式，专家扮演可穿戴智能助手，完全从学员的自我中心视角观察其活动，在学员提问时回答问题，或在程序过程中主动提出建议进行互动。这种独特的数据收集协议使 Ego-EXTRA 能够捕捉到高质量的对话，其中包含专家级别的反馈提供给学员。专家和学员之间的双向对话被记录、转录，并用于创建一个新的基准，该基准包含超过 15k 个高质量的视觉问答集，我们利用这些问答集来评估多模态大型语言模型。结果表明，Ego-EXTRA 具有挑战性，并突出了当前模型在用于向用户提供专家级别辅助时存在的局限性。Ego-EXTRA 数据集已公开，旨在支持以自我为中心的视频-语言助手的基准测试：https://fpv-iplab.github.io/Ego-EXTRA/。"
    },
    {
        "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning",
        "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.",
        "url": "http://arxiv.org/abs/2512.13095v1",
        "published_date": "2025-12-15T08:53:47+00:00",
        "updated_date": "2025-12-15T08:53:47+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Feng Zhang",
            "Zezhong Tan",
            "Xinhong Ma",
            "Ziqiang Dong",
            "Xi Leng",
            "Jianfei Zhao",
            "Xin Sun",
            "Yang Yang"
        ],
        "tldr": "The paper introduces ADHint, a new reinforcement learning method that adaptively adjusts hint ratios based on sample difficulty and modulates gradients to improve reasoning generalization and stability during training. It outperforms existing hint-based RL methods across diverse tasks.",
        "tldr_zh": "该论文介绍了一种新的强化学习方法ADHint，该方法基于样本难度自适应调整提示比例，并调整梯度以提高训练期间的推理泛化和稳定性。它在各种任务中优于现有的基于提示的强化学习方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "为了结合监督式微调（SFT）和强化学习（RL）的优势，最近的方法已将“提示”（即完整推理轨迹的前缀片段）整合到后训练中，旨在实现强大的知识扩展和推理泛化。然而，现有的基于提示的强化学习方法通常忽略了在调度提示比例和估计相对优势时的难度，导致不稳定的学习和过度模仿离策略提示。在这项工作中，我们提出了ADHint，它将难度作为提示比例调度和相对优势估计的关键因素，以实现探索和模仿之间更好的权衡。具体来说，我们提出了带有样本难度先验的自适应提示，它评估策略模型下每个样本的难度，并相应地调度适当的提示比例来指导其推出。我们还引入了基于一致性的梯度调制和提示保留的选择性掩码，以调制提示内的令牌级梯度，防止有偏差和破坏性的更新。此外，我们提出了带有推出难度后验的优势估计，它利用有提示和无提示的推出的相对难度来估计它们各自的优势，从而实现更平衡的更新。在不同模态、模型规模和领域的大量实验表明，ADHint提供了卓越的推理能力和分布外泛化能力，在pass@1和avg@8上始终优于现有方法。我们的代码和数据集将在论文被接受后公开。"
    },
    {
        "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference",
        "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.",
        "url": "http://arxiv.org/abs/2512.12977v1",
        "published_date": "2025-12-15T04:45:47+00:00",
        "updated_date": "2025-12-15T04:45:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shengling Qin",
            "Hao Yu",
            "Chenxin Wu",
            "Zheng Li",
            "Yizhong Cao",
            "Zhengyang Zhuge",
            "Yuxin Zhou",
            "Wentao Yao",
            "Yi Zhang",
            "Zhengheng Wang",
            "Shuai Bai",
            "Jianwei Zhang",
            "Junyang Lin"
        ],
        "tldr": "VLCache optimizes vision-language inference by caching and reusing computations from prior multimodal inputs, achieving significant speedups with minimal accuracy loss by only recomputing 2-5% of tokens.",
        "tldr_zh": "VLCache通过缓存和重用先前多模态输入的计算来优化视觉-语言推理，通过仅重新计算2-5%的token，在几乎不损失精度的情况下实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文提出了一种缓存复用框架VLCache，其利用先前多模态输入的键值（KV）缓存和编码器缓存，以消除相同多模态输入重复出现时代价高昂的重新计算。与以往的启发式方法不同，我们正式地识别了累积复用误差效应，并展示了如何有效地最小化非前缀缓存复用误差。我们进一步分析了模型各层的不同重要性，并提出了一种动态的、层感知的重新计算策略，以平衡准确率和效率。实验结果表明，VLCache实现了与完全重新计算相当的准确率，同时只需要计算2-5%的token，从而产生1.2倍-16倍的TTFT加速。所提出的VLCache流水线已集成到SGLang中，从而在实际部署中实现显著更快的推理速度。"
    },
    {
        "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
        "summary": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
        "url": "http://arxiv.org/abs/2512.13278v1",
        "published_date": "2025-12-15T12:38:04+00:00",
        "updated_date": "2025-12-15T12:38:04+00:00",
        "categories": [
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Jiaru Zou",
            "Ling Yang",
            "Yunzhe Qi",
            "Sirui Chen",
            "Mengting Ai",
            "Ke Shen",
            "Jingrui He",
            "Mengdi Wang"
        ],
        "tldr": "AutoTool introduces a framework for LLM agents to dynamically select and integrate tools throughout their reasoning process, demonstrating improved performance and generalization across various tasks compared to fixed-tool approaches.",
        "tldr_zh": "AutoTool 提出了一种框架，使 LLM 代理能够在推理过程中动态选择和集成工具，与固定工具方法相比，在各种任务中表现出更好的性能和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "代理强化学习推动了大型语言模型(LLMs)通过交错使用外部工具进行长链式思维轨迹的推理。现有方法假设工具库存是固定的，限制了LLM代理对新的或演化工具集的适应性。我们提出了AutoTool，一个赋予LLM代理在推理轨迹中动态工具选择能力的框架。 我们首先构建了一个包含20万个样本的数据集，其中包含跨越1000+个工具和100+个任务（涵盖数学、科学、代码生成和多模态推理）的明确的工具选择理由。基于这个数据基础，AutoTool采用一个双阶段优化流程：（i）基于监督和强化学习的轨迹稳定化，以实现连贯的推理，以及（ii）KL正则化的Plackett-Luce排名，以完善一致的多步工具选择。在十个不同的基准测试中，我们使用AutoTool训练了两个基础模型，Qwen3-8B和Qwen2.5-VL-7B。在参数更少的情况下，AutoTool始终优于高级LLM代理和工具集成方法，在数学与科学推理中平均获得6.4%的提升，在基于搜索的问答中获得4.5%的提升，在代码生成中获得7.7%的提升，在多模态理解中获得6.9%的提升。此外，AutoTool通过在推理过程中动态利用演化工具集中未见过的工具，展现出更强的泛化能力。"
    },
    {
        "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM",
        "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.",
        "url": "http://arxiv.org/abs/2512.13514v1",
        "published_date": "2025-12-15T16:42:48+00:00",
        "updated_date": "2025-12-15T16:42:48+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Aman Arora",
            "Matteo El-Hariry",
            "Miguel Olivares-Mendez"
        ],
        "tldr": "This paper presents a reinforcement learning framework for 6-DoF docking of the Int-Ball2 robot inside the ISS-JEM using PPO, demonstrating stable and reliable docking in simulation under realistic conditions.",
        "tldr_zh": "本文提出了一个强化学习框架，用于在国际空间站JEM中Int-Ball2机器人的六自由度对接，使用PPO算法在逼真的模拟条件下展示了稳定可靠的对接。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "自主式自由飞行器在国际空间站（ISS）的舱内任务中发挥着关键作用，但在存在传感噪声、微小驱动不匹配和环境变化的情况下，其精确对接仍然是一项具有挑战性的任务。 本文提出了一种强化学习（RL）框架，用于日本宇宙航空研究开发机构（JAXA）Int-Ball2机器人在日本实验舱（JEM）的高保真Isaac Sim模型内部的六自由度（6-DoF）对接。 我们使用近端策略优化（PPO），在域随机动力学和有界观测噪声下训练和评估控制器，同时显式地建模了螺旋桨阻力-扭矩效应和极性结构。 这使得我们能够对Int-Ball2的推进物理特性如何影响基于强化学习的微重力约束空间内对接性能进行受控研究。 所学习的策略在各种条件下实现了稳定可靠的对接，并为未来在Int-Ball2的避碰导航、安全强化学习、精确推进器的模拟到现实迁移，以及基于视觉的端到端对接方面的扩展奠定了基础。"
    },
    {
        "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)",
        "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.",
        "url": "http://arxiv.org/abs/2512.13356v1",
        "published_date": "2025-12-15T14:10:04+00:00",
        "updated_date": "2025-12-15T14:10:04+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zeyad Gamal",
            "Youssef Mahran",
            "Ayman El-Badawy"
        ],
        "tldr": "This paper presents a reinforcement learning (RL) framework using the TD3 algorithm to control a Twin Rotor Aerodynamic System (TRAS), demonstrating its effectiveness in simulation with wind disturbances and in real-world experiments compared to PID controllers.",
        "tldr_zh": "本文提出了一种使用TD3算法的强化学习框架，用于控制双旋翼气动系统（TRAS），并通过仿真（包括风扰）和实际实验验证了其有效性，并与PID控制器进行了比较。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文提出了一种强化学习（RL）框架，用于控制和稳定双旋翼气动系统（TRAS）在特定俯仰角和方位角，并跟踪给定的轨迹。TRAS的复杂动力学和非线性特性使其难以使用传统控制算法进行控制。然而，强化学习的最新发展因其在多旋翼飞行器控制中的潜在应用而引起了人们的兴趣。本文采用双延迟深度确定性策略梯度（TD3）算法来训练强化学习代理。该算法适用于具有连续状态和动作空间的环境，与TRAS类似，因为它不需要系统模型。仿真结果表明了强化学习控制方法的有效性。接着，采用风扰动的形式引入外部干扰，以测试控制器与传统PID控制器相比的有效性。最后，在实验室装置上进行了实验，以验证控制器在实际应用中的有效性。"
    },
    {
        "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving",
        "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.",
        "url": "http://arxiv.org/abs/2512.13262v1",
        "published_date": "2025-12-15T12:18:50+00:00",
        "updated_date": "2025-12-15T12:18:50+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Hyunki Seong",
            "Jeong-Kyun Lee",
            "Heesoo Myeong",
            "Yongho Shin",
            "Hyun-Mook Cho",
            "Duck Hoon Kim",
            "Pranav Desai",
            "Monu Surana"
        ],
        "tldr": "The paper introduces GRBO, a post-training RL method for improving safety in autonomous driving, and Warm-K, a test-time scaling strategy to enhance behavioral consistency and reactivity. They address the limitations of imitation learning's bias towards safe demonstrations and open-loop evaluation.",
        "tldr_zh": "本文提出了一种后训练强化学习方法GRBO，用于提高自动驾驶的安全性，以及一种测试时缩放策略Warm-K，以增强行为一致性和反应性。他们解决了模仿学习对安全演示的偏见以及开环评估的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "学习多智能体间的交互式运动行为是自动驾驶中的核心挑战。虽然模仿学习模型可以生成逼真的轨迹，但它们常常会继承数据集中由安全演示主导而产生的偏差，从而限制了在安全攸关情况下的鲁棒性。此外，大多数研究依赖于开环评估，忽略了闭环执行中的累积误差。我们通过两种互补的策略来解决这些局限性。首先，我们提出群体相对行为优化（GRBO），这是一种强化学习的后训练方法，通过以人类正则化进行群体相对优势最大化来微调预训练的行为模型。仅使用10%的训练数据集，GRBO即可在保持行为真实性的同时，将安全性能提升超过40%。其次，我们引入Warm-K，一种热启动的Top-K采样策略，用于平衡运动选择中的一致性和多样性。我们基于Warm-K方法的测试时尺度调整增强了测试时的行为一致性和反应性，无需重新训练，从而减轻了协变量偏移并减少了性能差异。演示视频可在补充材料中找到。"
    },
    {
        "title": "Differentiable Material Point Method for the Control of Deformable Objects",
        "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.",
        "url": "http://arxiv.org/abs/2512.13214v1",
        "published_date": "2025-12-15T11:26:51+00:00",
        "updated_date": "2025-12-15T11:26:51+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Diego Bolliger",
            "Gabriele Fadini",
            "Markus Bambach",
            "Alisa Rupenyan"
        ],
        "tldr": "This paper introduces a differentiable Material Point Method (MPM) simulator for controlling deformable objects, demonstrating its effectiveness in an active damping problem with significant speed and energy efficiency improvements over a baseline method.",
        "tldr_zh": "本文介绍了一种用于控制可变形物体的可微分材料点法（MPM）模拟器，并在主动阻尼问题中展示了其有效性，在速度和能量效率方面比基线方法有显著改进。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "控制柔性物体的形变极具挑战，原因在于它们非线性的动力学特性和高维的构型空间。本文提出了一种可微分的物质点法（MPM）模拟器，专用于控制应用。我们利用该模拟器的可微分性，为一条超弹性绳索在主动阻尼问题中优化控制轨迹。该模拟器能够有效地最小化绳索的动能，比基线MPPI方法快约2倍，并将能量水平降低20%，同时仅消耗约3%的计算时间。"
    },
    {
        "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks",
        "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.",
        "url": "http://arxiv.org/abs/2512.13170v1",
        "published_date": "2025-12-15T10:30:40+00:00",
        "updated_date": "2025-12-15T10:30:40+00:00",
        "categories": [
            "cs.RO",
            "cs.LG",
            "eess.SY",
            "math.OC"
        ],
        "authors": [
            "Deepak Ingole",
            "Valentin Bhend",
            "Shiva Ganesh Murali",
            "Oliver Dobrich",
            "Alisa Rupenayan"
        ],
        "tldr": "This paper introduces an iterative learning method for automatically tuning NMPC weighting matrices on robots performing repetitive tasks, achieving near-optimal performance with only a few online iterations.",
        "tldr_zh": "本文介绍了一种迭代学习方法，用于自动调整机器人在重复任务中非线性模型预测控制（NMPC）的权重矩阵，仅需几次在线迭代即可实现接近最佳性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "制造过程常常受到环境漂移和系统磨损的扰动，即使存在重复操作，也需要重新调整控制参数。本文提出了一种迭代学习框架，用于基于任务级性能反馈自动调整非线性模型预测控制（NMPC）的权重矩阵。受范数最优迭代学习控制（ILC）的启发，该方法在任务重复过程中自适应地调整NMPC权重Q和R，以最小化与跟踪精度、控制能量和饱和度相关的关键性能指标（KPI）。与需要通过NMPC求解器进行微分的基于梯度的方法不同，我们构建了一个经验灵敏度矩阵，从而能够在没有解析导数的情况下进行结构化的权重更新。该框架通过在UR10e机器人上进行四面体碳纤维缠绕的仿真得到验证。结果表明，与贝叶斯优化（BO）算法所需的100次离线评估相比，所提出的方法仅需4次在线重复就收敛到接近最优的跟踪性能（RMSE在离线贝叶斯优化（BO）的0.3%以内）。该方法为重复性机器人任务中的自适应NMPC整定提供了一种实用的解决方案，将精心优化的控制器的精度与在线自适应的灵活性相结合。"
    },
    {
        "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation",
        "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.",
        "url": "http://arxiv.org/abs/2512.13094v1",
        "published_date": "2025-12-15T08:50:23+00:00",
        "updated_date": "2025-12-15T08:50:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Xiang Li",
            "Gang Liu",
            "Weitao Zhou",
            "Hongyi Zhu",
            "Zhong Cao"
        ],
        "tldr": "The paper introduces Sequence of Experts (SoE), a temporal alternation policy for imitation learning in autonomous driving, which improves closed-loop performance without increasing model size or data requirements and achieves state-of-the-art results on nuPlan.",
        "tldr_zh": "本文提出了一种名为专家序列（SoE）的时序交替策略，用于自动驾驶中的模仿学习，该策略可在不增加模型大小或数据需求的情况下提高闭环性能，并在nuPlan基准测试中实现最先进的结果。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "模仿学习（IL）已经成为自动驾驶领域的核心范式。尽管IL通过最小化每一步的预测误差，在开环环境中匹配专家行为方面表现出色，但由于随着时间的推移，微小且不易察觉的误差逐渐累积，其在闭环环境中的性能会出乎意料地降低。在连续的规划周期中，这些误差会复合，并可能导致严重的失败。目前的研究工作主要依赖于日益复杂的网络架构或高保真度的训练数据集来增强IL规划器对抗误差累积的鲁棒性，侧重于单时间点的状态层面鲁棒性。然而，自动驾驶本质上是一个连续时间过程，利用时间尺度来增强鲁棒性可能为解决这个问题提供一个新的视角。为此，我们提出了一种名为专家序列（SoE）的方法，这是一种时间交替策略，它在不增加模型大小或数据需求的情况下，提高了闭环性能。我们在大规模自动驾驶基准 nuPlan 上的实验表明，SoE方法能够始终如一且显著地提高所有评估模型的性能，并达到最先进的水平。该模块可能为提高自动驾驶模型的训练效率提供关键且广泛适用的支持。"
    },
    {
        "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning",
        "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.",
        "url": "http://arxiv.org/abs/2512.12987v1",
        "published_date": "2025-12-15T05:23:23+00:00",
        "updated_date": "2025-12-15T05:23:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Amin Jalal Aghdasian",
            "Farzaneh Abdollahi",
            "Ali Kamali Iglie"
        ],
        "tldr": "This paper introduces two novel action-robust DRL-based lane keeping systems for autonomous vehicles in snowy conditions, demonstrating superior performance in simulation and real-world experiments using a Jetson Nano-based vehicle.",
        "tldr_zh": "本文提出了两种新的基于动作鲁棒深度强化学习的车道保持系统，用于在雪地条件下自动驾驶车辆，并在仿真和基于Jetson Nano的真实车辆实验中表现出卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文提出了两种新的算法，用于在雪地条件下运行的自动驾驶车辆(AVs)的车道保持系统(LKS)。这些算法利用深度强化学习(DRL)来处理不确定性和滑移现象。它们包括动作鲁棒循环深度确定性策略梯度(AR-RDPG)和端到端动作鲁棒卷积神经网络注意力机制确定性策略梯度(AR-CADPG)，这两种方法都是动作鲁棒的决策方法。在AR-RDPG方法中，在感知层内，首先使用多尺度神经网络对相机图像进行去噪。然后，通过预训练的深度卷积神经网络(DCNN)提取中心线系数。这些系数与驾驶特性串联起来，作为控制层的输入。AR-CADPG方法提出了一种端到端方法，其中卷积神经网络(CNN)和注意力机制被集成到DRL框架中。这两种方法首先在CARLA模拟器中进行训练，并在各种雪地场景下进行验证。基于Jetson Nano的自动驾驶车辆的真实世界实验证实了所学策略的可行性和稳定性。在两种模型中，AR-CADPG方法表现出卓越的路径跟踪精度和鲁棒性，突显了在自动驾驶车辆中结合时间记忆、对抗弹性以及注意力机制的有效性。"
    },
    {
        "title": "MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems",
        "summary": "Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.",
        "url": "http://arxiv.org/abs/2512.12855v1",
        "published_date": "2025-12-14T21:39:16+00:00",
        "updated_date": "2025-12-14T21:39:16+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Patrick Kostelac",
            "Xuerui Wang",
            "Anahita Jamshidnejad"
        ],
        "tldr": "This paper proposes an integrated MPC-RL framework that combines the safety of MPC with the adaptability of RL for controlling nonlinear systems, using MPC to guide RL training and a Lipschitz-based filter for runtime safety, demonstrated on an aeroelastic wing system.",
        "tldr_zh": "本文提出了一种集成的MPC-RL框架，该框架结合了MPC的安全性与RL的适应性，用于控制非线性系统。该框架使用MPC指导RL训练，并采用基于Lipschitz的滤波器确保运行时安全，已在气动弹性机翼系统上进行了演示。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "现代工程系统，如自动驾驶车辆、柔性机器人和智能航空航天平台，需要控制器能够对不确定性具有鲁棒性、能够适应环境变化并在实时约束下具有安全感知能力。强化学习(RL)为具有非线性动力学并与不确定环境交互的系统提供了强大的数据驱动适应性。然而，强化学习缺乏用于在探索过程中满足动态约束的内置机制。模型预测控制(MPC)提供了结构化的约束处理和鲁棒性，但其对精确模型的依赖和计算密集型的在线优化可能会带来重大挑战。本文提出了一种集成的MPC-RL框架，该框架结合了MPC的稳定性和安全性保证以及RL的适应性。在训练期间，MPC定义了安全控制边界，用于指导RL组件并实现具有约束感知的策略学习。在部署时，学习到的策略实时运行，并具有基于Lipschitz连续性的轻量级安全滤波器，以确保约束满足，而无需进行繁重的在线优化。该方法在一个非线性气动弹性机翼系统上进行了验证，证明其提高了抗扰能力、减少了作动器工作量并在湍流下具有鲁棒性。该架构可推广到具有结构化非线性和有界扰动的其他领域，为工程应用中安全的人工智能驱动控制提供了一种可扩展的解决方案。"
    },
    {
        "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings",
        "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.",
        "url": "http://arxiv.org/abs/2512.13481v1",
        "published_date": "2025-12-15T16:17:12+00:00",
        "updated_date": "2025-12-15T16:17:12+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CY"
        ],
        "authors": [
            "Ojas Pungalia",
            "Rashi Upadhyay",
            "Abhishek Mishra",
            "Abhiram H",
            "Tejasvi Alladi",
            "Sujan Yenuganti",
            "Dhruv Kumar"
        ],
        "tldr": "This paper investigates whether LLMs exhibit envy-like behavior in multi-agent settings, finding that some models do attempt to equalize outcomes by hindering peers, suggesting implications for LLM-based multi-agent system design.",
        "tldr_zh": "该论文研究了大型语言模型是否在多智能体环境中表现出类似嫉妒的行为，发现某些模型确实试图通过阻碍其他模型来均衡结果，这对基于大型语言模型的的多智能体系统设计具有重要意义。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "嫉妒是一种常见的人类行为，它塑造了竞争性，并能改变团队环境中的结果。随着大型语言模型（LLMs）在协作和竞争性工作流程中越来越多地代表人类行事，迫切需要评估它们是否以及在何种条件下表现出类似嫉妒的偏好。在本文中，我们测试了LLM之间是否表现出类似嫉妒的行为。我们考虑了两种情景：（1）一个点数分配游戏，测试模型是否试图战胜其同伴。（2）一个工作场所环境，观察当认可不公平时模型的行为。我们的研究结果表明，某些LLM中存在一致的类似嫉妒模式的证据，并且模型和情境之间存在很大差异。例如，GPT-5-mini和Claude-3.7-Sonnet表现出明显的拉低同伴模型以实现结果均等的倾向，而Mistral-Small-3.2-24B则更专注于最大化其自身的个人收益。这些结果强调了在基于LLM的多智能体系统中，将竞争倾向作为安全和设计因素进行考虑的必要性。"
    },
    {
        "title": "SACn: Soft Actor-Critic with n-step Returns",
        "summary": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.",
        "url": "http://arxiv.org/abs/2512.13165v1",
        "published_date": "2025-12-15T10:23:13+00:00",
        "updated_date": "2025-12-15T10:23:13+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Jakub Łyskawa",
            "Jakub Lewandowski",
            "Paweł Wawrzyński"
        ],
        "tldr": "The paper proposes SACn, a novel extension of Soft Actor-Critic (SAC) that incorporates n-step returns with a numerically stable importance sampling method and a variance-reduced entropy estimation technique, demonstrating its effectiveness in MuJoCo environments.",
        "tldr_zh": "本文提出了一种新颖的Soft Actor-Critic (SAC)扩展方法SACn，它结合了n步回报，并采用数值稳定的重要性采样方法和方差减少的熵估计技术，并在MuJoCo环境中验证了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "软演员-评论家（SAC）在实际应用中被广泛使用，并且是目前最相关的离线在线无模型强化学习（RL）方法之一。众所周知，与基于单步回报的版本相比，n步回报技术可以提高RL算法的收敛速度。然而，SAC很难与n步回报结合，因为它们通常的结合会在离策略算法中引入偏差，这是由于动作分布的变化造成的。虽然这个问题可以通过重要性采样来解决，重要性采样是一种使用来自另一个分布的样本来估计一个分布的期望值的方法，但重要性采样可能会导致数值不稳定。在这项工作中，我们以一种克服此问题的方式将SAC与n步回报相结合。我们提出了一种应用数值稳定的重要性采样的方法，并简化了超参数的选择。此外，我们分析了软演员-评论家在n步最大熵框架下的熵估计方法，并提出了$τ$-采样的熵估计，以减少学习目标的方差。最后，我们提出了带有n步回报的软演员-评论家（SAC$n$）算法，并在MuJoCo模拟环境中进行了实验验证。"
    },
    {
        "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass",
        "summary": "Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R",
        "url": "http://arxiv.org/abs/2512.13122v1",
        "published_date": "2025-12-15T09:21:28+00:00",
        "updated_date": "2025-12-15T09:21:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Vivek Alumootil",
            "Tuan-Anh Vu",
            "M. Khalid Jawed"
        ],
        "tldr": "DePT3R is a new framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass, without requiring camera poses, offering improved memory efficiency.",
        "tldr_zh": "DePT3R是一个新框架，它能够在一次前向传播中同时执行动态场景的密集点跟踪和3D重建，并且不需要相机姿势，从而提供了更高的内存效率。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "当前在动态场景中进行密集3D点跟踪的方法通常依赖于成对处理、需要已知的相机姿态或假设输入帧的时间顺序，从而限制了它们的灵活性和适用性。此外，近期的进展已经成功实现了从大规模、未定位姿态图像集合中进行高效的3D重建，这突出了统一方法来理解动态场景的机遇。受此启发，我们提出了DePT3R，一种新颖的框架，它在一次前向传播中同时执行来自多个图像的动态场景的密集点跟踪和3D重建。这种多任务学习通过使用强大的主干网络提取深度时空特征并使用密集预测头回归像素级映射来实现。至关重要的是，DePT3R无需相机姿态即可运行，从而显著提高了其适应性和效率——这在快速变化的动态环境中尤为重要。我们在几个涉及动态场景的具有挑战性的基准上验证了DePT3R，证明了强大的性能，并且在内存效率方面比现有最先进的方法有了显著提高。数据和代码可通过开源库获得：https://github.com/StructuresComp/DePT3R"
    },
    {
        "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence",
        "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.",
        "url": "http://arxiv.org/abs/2512.13465v1",
        "published_date": "2025-12-15T16:03:26+00:00",
        "updated_date": "2025-12-15T16:03:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiyan Wang",
            "Teng Hu",
            "Kaihui Huang",
            "Zihan Su",
            "Ran Yi",
            "Lizhuang Ma"
        ],
        "tldr": "The paper introduces PoseAnything, a novel framework for universal pose-guided video generation that supports both human and non-human characters, along with independent camera control, and a new dataset called XPose.",
        "tldr_zh": "该论文介绍了PoseAnything，一个用于通用姿势引导视频生成的新框架，它支持人类和非人类角色，以及独立的摄像机控制，并提出了一个新的名为XPose的数据集。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "基于姿态引导的视频生成指的是通过一系列姿态来控制生成视频中对象物体的运动。它实现了对对象物体运动的精确控制，并在动画制作中有着重要的应用。然而，目前基于姿态引导的视频生成方法仅限于接受人体姿态作为输入，因此在其他对象物体的姿态上的泛化能力较差。为了解决这个问题，我们提出了PoseAnything，这是第一个通用的基于姿态引导的视频生成框架，能够处理人类和非人类角色，并支持任意骨骼输入。为了增强运动过程中的一致性保持，我们引入了Part-aware Temporal Coherence Module (部件感知的时间连贯性模块)，该模块将对象物体划分为不同的部分，建立部件对应关系，并计算跨帧对应部件之间的交叉注意力，以实现精细的部件级别一致性。此外，我们提出了Subject and Camera Motion Decoupled CFG (主体和相机运动解耦的CFG)，这是一种新颖的引导策略，它首次实现了在基于姿态引导的视频生成中独立控制相机运动，通过将主体和相机运动控制信息分别注入CFG（Classifier-Free Guidance）的正负锚点。此外，我们还提出了XPose，这是一个高质量的公共数据集，包含50,000个非人类姿态-视频对，以及一个用于标注和过滤的自动化流程。大量的实验表明，Pose-Anything在有效性和泛化性方面都显著优于最先进的方法。"
    },
    {
        "title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs",
        "summary": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyond-visible.github.io/",
        "url": "http://arxiv.org/abs/2512.13392v2",
        "published_date": "2025-12-15T14:45:05+00:00",
        "updated_date": "2025-12-16T07:08:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anran Qi",
            "Changjian Li",
            "Adrien Bousseau",
            "Niloy J. Mitra"
        ],
        "tldr": "This paper introduces a training-free image-to-video generation pipeline that allows users to control motion via a Proxy Dynamic Graph (PDG) and edit disoccluded regions in the final frame, offering improved articulation and control.",
        "tldr_zh": "本文介绍了一种无需训练的图像到视频生成流程，用户可以通过代理动态图（PDG）控制运动，并编辑最终帧中新出现的遮挡区域，从而提供更好的关节运动控制。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们致力于解决图像到视频生成的问题，特别是要实现用户对最终帧中未遮挡区域的显式控制。目前的图像到视频生成流程能够产生看似合理的运动，但难以生成可预测的、有关节的运动，同时在新增露出的区域中强制施加用户指定的内容。我们的核心思想是将运动规范与外观合成分离：我们引入了一个轻量级的、用户可编辑的代理动态图（PDG），该图确定性且近似地驱动部件运动，而冻结的扩散先验则用于合成符合该运动的合理外观。在我们无需训练的流程中，用户对PDG进行粗略标注和重新姿态调整，然后我们从中计算出密集运动流，以利用扩散模型作为运动引导着色器。接着，用户可以编辑图像中未遮挡区域的外观，并利用PDG编码的可见性信息执行潜在空间混合，从而在这些区域协调运动和用户意图。这种设计无需微调即可实现可控的关节运动以及用户对未遮挡区域的控制。我们展示了相对于最先进的替代方案，在将图像转换为有关节物体、家具、车辆和可变形物体的短视频方面，我们的方法具有明显的优势。我们的方法将生成控制（以粗略的姿态和结构的形式）与可预测的控制（以未遮挡区域最终帧中的外观规范的形式）相结合，开启了一个全新的图像到视频工作流程。代码将在接收后发布。项目主页：https://anranqi.github.io/beyond-visible.github.io/"
    },
    {
        "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception",
        "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.",
        "url": "http://arxiv.org/abs/2512.13191v1",
        "published_date": "2025-12-15T11:00:38+00:00",
        "updated_date": "2025-12-15T11:00:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Gong Chen",
            "Chaokun Zhang",
            "Pengcheng Lv",
            "Xiaohui Xie"
        ],
        "tldr": "The paper introduces CoRA, a collaborative perception architecture that uses a hybrid fusion approach combining feature-level and object-level fusion to achieve robust performance under adverse communication conditions with low communication volume.",
        "tldr_zh": "该论文介绍了CoRA，一种协作感知架构，它使用混合融合方法，结合特征级别和对象级别的融合，以在不利的通信条件下实现强大的性能，同时保持较低的通信量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "协作感知作为克服单智能体系统感知局限性的关键技术，已受到广泛关注。 许多最先进（SOTA）的方法通过中间融合实现了通信效率和高性能。 然而，它们都存在一个关键的脆弱性：由于数据传输引起的对齐误差，其性能在不利的通信条件下会降低，这严重阻碍了它们的实际部署。 为了弥合这一差距，我们重新审视了不同的融合范式，并发现中间融合和后期融合的优势并非一种权衡，而是一种互补配对。 基于这一关键见解，我们提出了一种新颖的协作鲁棒架构CoRA，它采用混合方法将性能与低通信下的鲁棒性解耦。 它由两个组件组成：一个特征级融合分支和一个目标级校正分支。 其第一个分支选择关键特征并高效融合它们，以确保性能和可扩展性。 第二个分支利用语义相关性来校正空间位移，从而保证了对位姿误差的弹性。 实验表明了CoRA的优越性。 在极端场景下，CoRA使其基线性能在AP@0.7指标上提高了约19%，同时通信量减少了5倍以上，这使其成为一种有前景的鲁棒协作感知解决方案。"
    },
    {
        "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects",
        "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.",
        "url": "http://arxiv.org/abs/2512.13007v1",
        "published_date": "2025-12-15T06:04:49+00:00",
        "updated_date": "2025-12-15T06:04:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikolai Goncharov",
            "James L. Gray",
            "Donald G. Dansereau"
        ],
        "tldr": "The paper introduces a novel object tracking method using light fields and vision foundation models to track previously unobserved objects with complex visual properties, achieving competitive performance against state-of-the-art model-based trackers.",
        "tldr_zh": "本文提出了一种新的物体追踪方法，该方法利用光场和视觉基础模型来追踪以前未观察到的具有复杂视觉属性的物体，并在高精度追踪器中具有竞争优势。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "目标跟踪是机器人学和重新自主驾驶流程中的重要步骤，它必须能够泛化到之前未见过且复杂的物体。现有高性能方法通常依赖于预先捕获的物体视图来构建显式参考模型，这限制了它们只能跟踪固定的已知物体集合。然而，此类参考模型在面对视觉上复杂的外观时可能会遇到困难，从而降低跟踪质量。 在这项工作中，我们提出了一种基于光场图像的物体跟踪方法，该方法不依赖于预训练模型，同时对复杂的视觉行为（如反射）具有鲁棒性。我们利用视觉基础模型从光场输入中提取语义和几何特征，并将它们转换为视角相关的Gaussian splats。这些splats作为统一的物体表示，支持可微分渲染和姿态优化。我们进一步引入了一个光场物体跟踪数据集，其中包含具有精确ground truth姿态的、具有挑战性的反射物体。实验表明，我们的方法在这些困难情况下与最先进的基于模型的跟踪器具有竞争力，为机器人系统中的通用物体跟踪铺平了道路。代码/数据可在https://github.com/nagonch/LiFT-6DoF上获取。"
    },
    {
        "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
        "summary": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
        "url": "http://arxiv.org/abs/2512.13526v1",
        "published_date": "2025-12-15T16:56:28+00:00",
        "updated_date": "2025-12-15T16:56:28+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Asa Cooper Stickland",
            "Jan Michelfeit",
            "Arathi Mani",
            "Charlie Griffin",
            "Ollie Matthews",
            "Tomek Korbak",
            "Rogan Inglis",
            "Oliver Makins",
            "Alan Cooney"
        ],
        "tldr": "This paper explores asynchronous monitoring of LLM-based software engineering agents to prevent intentional sabotage, using an adversarial game framework and a deployed monitor with a 6% false negative rate at 1% false positive rate in simulated software engineering environments.",
        "tldr_zh": "本文研究了基于LLM的软件工程代理的异步监控，以防止故意破坏。作者使用对抗性博弈框架，并在模拟的软件工程环境中部署了一个监控系统，该系统以1%的假阳性率实现了6%的假阴性率。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于大型语言模型的软件工程智能体越来越多地应用于实际开发任务中，并且常常能够访问敏感数据或安全关键的代码库。 如果这些智能体偏差过大，可能会蓄意破坏这些代码库。 我们研究了异步监控，即监控系统在事后审查智能体的行为。 与同步监控不同，这种方法不会引入运行时延迟，同时仍然试图在造成不可逆损害之前阻止攻击。 我们将监控器的开发视为一个蓝队（负责设计监控器）和红队（负责创建破坏性智能体）之间的对抗性博弈。 我们试图设置博弈规则，使其能够限定基于 Claude 4.1 Opus 的智能体的破坏潜力上限。 为了将这场博弈置于一个真实且高风险的部署场景中，我们开发了一套包含 5 个多样化软件工程环境的套件，用于模拟智能体可能在人工智能开发者内部基础设施中执行的任务。 在博弈过程中，我们开发了一个集成监控器，在预留的测试环境中达到了 6% 的假阴性率和 1% 的假阳性率。 然后，我们通过从监控器的假阴性率中推断，来估计部署时破坏的风险。 我们描述了一种简单的用于推断的模型，提出了一个敏感性分析，并描述了该模型失效的情况。 代码可在以下位置获得：https://github.com/UKGovernmentBEIS/async-control。"
    },
    {
        "title": "A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments",
        "summary": "Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.",
        "url": "http://arxiv.org/abs/2512.13517v1",
        "published_date": "2025-12-15T16:43:50+00:00",
        "updated_date": "2025-12-15T16:43:50+00:00",
        "categories": [
            "q-bio.NC",
            "cs.LG"
        ],
        "authors": [
            "Raymond Khazoum",
            "Daniela Fernandes",
            "Aleksandr Krylov",
            "Qin Li",
            "Stephane Deny"
        ],
        "tldr": "This paper proposes a deep learning model of human mental rotation, guided by VR experiments and incorporating equivariant, neuro-symbolic, and recurrent components to capture human performance and response times.",
        "tldr_zh": "本文提出了一个人类心理旋转的深度学习模型，该模型以VR实验为指导，并结合了等变、神经符号和循环组件，以捕捉人类的表现和反应时间。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "心理旋转——比较从不同视角观察到的物体的能力——是人类心理模拟和空间世界建模的一个基本例子。在此，我们提出一种人类心理旋转的机制模型，利用了深度学习、等变学习和神经符号学习的进展。我们的模型由三个堆叠的组件组成：(1) 一个等变神经编码器，以图像作为输入，生成物体的3D空间表示；(2) 一个神经符号对象编码器，从这些空间表示中导出物体的符号描述；以及(3) 一个神经决策代理，比较这些符号描述，并通过循环通路在3D潜在空间中规定旋转模拟。我们的模型设计受到大量关于心理旋转的实验文献的指导，我们通过在VR中进行的实验对其进行了补充，参与者有时可以操纵物体进行比较，这为我们提供了对心理旋转认知过程的额外见解。我们的模型很好地捕捉了参与者在我们和其他人的实验中的表现、反应时间和行为。通过系统的消融实验，表明了每个模型组件的必要性。我们的工作增加了近期关于人类空间推理的深度神经模型集合，进一步证明了整合深度表示、等变表示和符号表示来建模人类思维的强大能力。"
    },
    {
        "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation",
        "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.",
        "url": "http://arxiv.org/abs/2512.13271v1",
        "published_date": "2025-12-15T12:30:24+00:00",
        "updated_date": "2025-12-15T12:30:24+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Fangju Yang",
            "Hang Yang",
            "Ibrahim Alsarraj",
            "Yuhao Wang",
            "Ke Wu"
        ],
        "tldr": "This paper introduces a lightweight dynamic modeling framework (LASEM) for cable-driven continuum robots, achieving computational speedup and supporting both force-input and displacement-input actuation modes, making it suitable for real-time dynamics prediction and model-based control.",
        "tldr_zh": "该论文介绍了一种用于缆索驱动连续机器人的的轻量级动态建模框架（LASEM），该框架实现了计算加速，并支持力输入和位移输入驱动模式，使其适用于实时动力学预测和基于模型的控制。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "绳索驱动连续体机器人(CDCRs)需要精确的、实时的动力学模型来进行高速动力学预测或基于模型的控制，使得这种能力成为亟待解决的需求。在本文中，我们提出了一种用于CDCRs的轻量化驱动空间能量建模(LASEM)框架，该框架直接在驱动空间中建立驱动势能模型，从而实现轻量化但精确的动力学建模。通过统一的变分推导，控制动力学简化为一个单一的偏微分方程(PDE)，仅需考虑欧拉力矩平衡，同时隐式地包含了牛顿力平衡。此外，该公式避免了对绳索-骨架接触力的显式计算，简化了模型结构，提高了计算效率，同时保留了几何精度和物理一致性。重要的是，所提出的动力学建模框架原生支持力输入和位移输入两种驱动模式，这是现有动力学公式中很少实现的功能。利用这种轻量化的结构，结合具有还原状态解析时域导数的伽辽金时空模态离散化方法，与最先进的实时动态建模方法相比，实现了平均62.3%的计算速度提升。"
    },
    {
        "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment",
        "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).",
        "url": "http://arxiv.org/abs/2512.13215v1",
        "published_date": "2025-12-15T11:28:04+00:00",
        "updated_date": "2025-12-15T11:28:04+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yinsong Qu",
            "Yunxiang Li",
            "Shanlin Zhong"
        ],
        "tldr": "This paper presents an Improved Sequential Model Predictive Control (ISMPC) navigation framework for AMRs in cluttered environments, using Multi-Directional Safety Rectangular Corridors (MDSRC) and barrier functions for obstacle avoidance, achieving better space utilization and real-time performance.",
        "tldr_zh": "本文提出了一种改进的序列模型预测控制（ISMPC）导航框架，用于在杂乱环境中自主移动机器人（AMR）的导航。该框架使用多方向安全矩形走廊（MDSRC）和障碍函数来避免障碍，从而实现更好的空间利用率和实时性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "自主移动机器人（AMR）因其操作灵活性和效率已成为工业应用中不可或缺的部分。导航是完成复杂任务的关键技术基础。然而，在密集、杂乱和半结构化环境中导航AMR仍然具有挑战性，这主要是由于非完整车辆动力学、与混合静态/动态障碍物的交互以及此类操作空间的非凸约束性质。为了解决这些问题，本文提出了一种改进的序列模型预测控制（ISMPC）导航框架，该框架系统地将导航任务重新表述为序列切换最优控制问题。该框架通过两项关键创新解决了上述挑战：1）实施多方向安全矩形走廊（MDSRC）算法，该算法通过矩形凸区域编码自由空间以避免与静态障碍物碰撞，从而消除冗余计算负担并加速求解器收敛；2）提出了一种将走廊约束与障碍函数约束相结合的序列MPC导航框架，以实现静态和动态障碍物规避。ISMPC导航框架能够为AMR直接生成速度，简化了传统的导航算法架构。对比实验表明，该框架在自由空间利用率方面具有优越性（平均走廊面积增加41.05$\\%$），同时保持了实时的计算性能（平均走廊生成延迟为3毫秒）。"
    },
    {
        "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots",
        "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.",
        "url": "http://arxiv.org/abs/2512.13009v1",
        "published_date": "2025-12-15T06:11:14+00:00",
        "updated_date": "2025-12-15T06:11:14+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Oğuzhan Akbıyık",
            "Naseem Alhousani",
            "Fares J. Abu-Dakka"
        ],
        "tldr": "This paper introduces K-VARK, a novel kernelized variance-aware residual Kalman filter, for improved sensorless force estimation in collaborative robots, demonstrating a 20% RMSE reduction compared to existing methods.",
        "tldr_zh": "本文提出了一种名为 K-VARK 的新型核化方差感知残差卡尔曼滤波器，用于改进协作机器人中的无传感器力估计，与现有方法相比，均方根误差降低了 20%。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "接触力可靠估计对于确保机器人在非结构化环境中安全和精准的交互至关重要。然而，由于固有的建模误差以及复杂的残余动力学和摩擦力，精确的无传感器力估计仍然具有挑战性。为了解决这一挑战，本文提出了一种新颖的方法K-VARK (核化方差感知残差卡尔曼滤波器)，该方法将关节残余扭矩的核化概率模型集成到自适应卡尔曼滤波框架中。通过在优化激励轨迹上训练的核化运动基元 (Kernelized Movement Primitives)，K-VARK捕获了残余扭矩的预测均值和输入相关的异方差方差，反映了数据变异性和与训练数据的距离效应。这些统计量通过增加测量噪声的协方差来告知方差感知的虚拟测量更新，同时过程噪声的协方差通过变分贝叶斯优化在线自适应，以处理动态扰动。在6自由度协作机械臂上的实验验证表明，与最先进的无传感器力估计方法相比，K-VARK的均方根误差降低了20%以上，从而实现了稳健和准确的外部力/扭矩估计，适用于诸如抛光和装配等高级任务。"
    },
    {
        "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework",
        "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.",
        "url": "http://arxiv.org/abs/2512.12945v1",
        "published_date": "2025-12-15T03:16:04+00:00",
        "updated_date": "2025-12-15T03:16:04+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Anja Sheppard",
            "Parker Ewen",
            "Joey Wilson",
            "Advaith V. Sethuraman",
            "Benard Adewole",
            "Anran Li",
            "Yuzhen Chen",
            "Ram Vasudevan",
            "Katherine A. Skinner"
        ],
        "tldr": "The paper introduces SLIM-VDB, a novel 3D semantic mapping system leveraging OpenVDB for efficient probabilistic semantic fusion, supporting both fixed-category and open-language labels. It achieves significant memory and integration time reduction while maintaining comparable accuracy.",
        "tldr_zh": "本文介绍 SLIM-VDB，一种新型 3D 语义地图系统，它利用 OpenVDB 进行有效的概率语义融合，支持固定类别和开放语言标签。它在保持相当精度的同时，显著减少了内存和集成时间。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "本文介绍了一种新的轻量级语义地图构建系统SLIM-VDB，该系统采用概率语义融合，适用于封闭或开放集词典。计算机图形学领域的数据结构进展，如OpenVDB，已证明在体场景表示中显著提高了计算和内存效率。虽然OpenVDB已被用于机器人应用中的几何地图构建，但使用OpenVDB进行语义地图构建以实现场景理解仍未被探索。此外，现有的语义地图构建系统缺乏对在单个框架内集成固定类别和开放语言标签预测的支持。在本文中，我们提出了一种新颖的3D语义地图构建系统，该系统利用OpenVDB数据结构，并集成了用于封闭和开放集语义融合的统一贝叶斯更新框架。与当前最先进的语义地图构建方法相比，我们提出的框架SLIM-VDB在保持相当的地图构建精度的情况下，显著减少了内存占用和融合时间。一个带有Python接口的开源C++代码库可在https://github.com/umfieldrobotics/slim-vdb获取。"
    },
    {
        "title": "Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection",
        "summary": "In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.",
        "url": "http://arxiv.org/abs/2512.12884v1",
        "published_date": "2025-12-14T23:56:16+00:00",
        "updated_date": "2025-12-14T23:56:16+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xiangzhong Liu",
            "Jiajie Zhang",
            "Hao Shen"
        ],
        "tldr": "This paper proposes a cross-level sensor fusion approach for 3D object detection, integrating object lists and raw camera images via a Transformer, and introduces a method for generating pseudo object lists for training.",
        "tldr_zh": "本文提出了一种用于3D物体检测的跨层传感器融合方法，通过Transformer整合物体列表和原始相机图像，并引入了一种生成伪物体列表的训练方法。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "在汽车传感器融合系统中，通常会用到智能传感器和车联网(V2X)模块。这些系统中的传感器数据通常以处理后的目标列表形式提供，而不是传统传感器的原始传感器数据。我们提出了一种基于Transformer的端到端跨层融合概念，它将高度抽象的目标列表信息与原始相机图像集成用于3D目标检测，而不是单独处理其他原始数据，然后在目标级别进行融合。目标列表作为去噪查询被馈送到Transformer中，并与可学习查询一起在后续特征聚合过程中传播。此外，一个由目标列表中的位置和尺寸先验导出的可变形高斯掩码被显式地集成到Transformer解码器中。这可以将注意力引导到感兴趣的目标区域，并加速模型训练收敛。此外，由于没有包含目标列表作为独立模态的公共数据集，我们提出了一种从真实边界框生成伪目标列表的方法，通过模拟状态噪声以及假正例和假负例。作为首个进行跨层融合的工作，我们的方法在nuScenes数据集上显示出相对于基于视觉的基线的显著性能提升。它展示了其在模拟目标列表和真实检测器的不同噪声水平下的泛化能力。"
    },
    {
        "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather",
        "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.",
        "url": "http://arxiv.org/abs/2512.13107v1",
        "published_date": "2025-12-15T09:03:46+00:00",
        "updated_date": "2025-12-15T09:03:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhijian He",
            "Feifei Liu",
            "Yuwei Li",
            "Zhanpeng Liu",
            "Jintao Cheng",
            "Xieyuanli Chen",
            "Xiaoyu Tang"
        ],
        "tldr": "This paper introduces DiffFusion, a diffusion-based framework for robust multi-modal 3D object detection in adverse weather, using restoration techniques and adaptive cross-modal fusion to improve performance and generalization.",
        "tldr_zh": "本文介绍DiffFusion，一个基于扩散的多模态3D目标检测框架，用于在恶劣天气下实现鲁棒性。它通过使用复原技术和自适应跨模态融合来提高性能和泛化能力。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "多模态3D目标检测对于机器人和自动驾驶领域中可靠的感知至关重要。然而，由于天气引起的数据失真以及不同模态数据之间的未对齐，其有效性在恶劣天气条件下仍然受到限制。在这项工作中，我们提出DiffFusion，一种新颖的框架，旨在通过基于扩散的恢复和自适应跨模态融合来增强在具有挑战性天气条件下的鲁棒性。我们的核心思想是扩散模型具有强大的去噪和生成能够适应各种天气条件的数据的能力。在此基础上，DiffFusion引入Diffusion-IR，用于恢复受天气影响而退化的图像，以及点云恢复（PCR），利用图像目标线索来补偿损坏的激光雷达数据。为了解决两种模态之间的未对齐问题，我们开发了双向自适应融合与对齐模块（BAFAM）。它支持动态多模态融合和双向鸟瞰图（BEV）对齐，以保持一致的空间对应关系。在三个公共数据集上进行的大量实验表明，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时保留了强大的清晰数据性能。在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力。我们的DiffFusion实现将作为开源发布。"
    },
    {
        "title": "Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm",
        "summary": "In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.",
        "url": "http://arxiv.org/abs/2512.12901v1",
        "published_date": "2025-12-15T00:59:44+00:00",
        "updated_date": "2025-12-15T00:59:44+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Parthasarathy Nadarajan",
            "Michael Botsch",
            "Sebastian Sardina"
        ],
        "tldr": "This paper proposes a system using Stacked Denoising Autoencoders and Random Forests to predict future probabilistic occupancy grids for vehicle safety, demonstrating its effectiveness in simulations and real-world experiments.",
        "tldr_zh": "本文提出了一种使用堆叠降噪自编码器和随机森林的系统，用于预测车辆安全方面的未来概率占用栅格，并在模拟和真实实验中展示了其有效性。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "本文提出了一种利用机器学习算法预测复杂交通场景的概率时空表示方法。这种表示方法对于所有主动车辆安全应用都至关重要，尤其是在复杂交通场景中执行动态操作时。首先，使用分层情境分类器来区分不同类型的交通场景。该分类器负责识别道路基础设施类型和驾驶环境中与安全相关的交通参与者。对于每个代表相似交通场景的类别，分别训练一组随机森林（RF）算法来预测概率时空表示，该表示描述了交通参与者的未来行为。这种表示被称为预测占用栅格（POG）。随机森林的输入是增强占用栅格（AOG）。为了提高随机森林的学习精度并进行更好的预测，使用堆叠降噪自编码器（SDA）将增强占用栅格降维为低维特征。通过仿真和实车实验表明，所提出的由堆叠降噪自编码器和随机森林组成的机器学习方法具有卓越的性能。此外，还展示了预测占用栅格在评估交通场景的严重程度以及确定安全轨迹方面的应用。"
    },
    {
        "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments",
        "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.",
        "url": "http://arxiv.org/abs/2512.13561v1",
        "published_date": "2025-12-15T17:18:04+00:00",
        "updated_date": "2025-12-15T17:18:04+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Li-Wei Shih",
            "Ruo-Syuan Mei",
            "Jesse Heidrich",
            "Hui-Ping Wang",
            "Joel Hooton",
            "Joshua Solomon",
            "Jorge Arinez",
            "Guangze Li",
            "Chenhui Shao"
        ],
        "tldr": "This paper presents a three-tier near-field perception system for autonomous mobile robots using laser stripe projection and computer vision to detect and classify nearby obstacles, improving safety in manufacturing environments.",
        "tldr_zh": "本文提出了一种用于自主移动机器人的三层近场感知系统，该系统利用激光条纹投影和计算机视觉来检测和分类附近的障碍物，从而提高制造环境中的安全性。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "近场感知对于自主移动机器人(AMR)在制造环境中安全运行至关重要。传统的测距传感器，例如激光雷达(LiDAR)和超声波设备，虽然能提供广阔的态势感知能力，但往往无法检测到机器人底座附近的小物体。为了解决这一局限性，本文提出了一种三层近场感知框架。第一种方法采用光线不连续性检测，将激光条纹投射到近场区域，并通过识别条纹中的中断来执行快速的二元截止感应，以检测障碍物是否存在。第二种方法利用光线位移测量，通过分析相机图像中投射条纹的几何位移来估计物体高度，从而以最小的计算开销提供定量的障碍物高度信息。第三种方法在嵌入式AI硬件上采用基于计算机视觉的物体检测模型来对物体进行分类，从而实现语义感知和上下文感知的安全决策。所有方法都在Raspberry Pi 5系统上实现，并以每秒25或50帧的速度实现实时性能。实验评估和对比分析表明，所提出的分层结构平衡了精度、计算量和成本，从而为在制造环境中实现AMR的安全运行提供了一种可扩展的感知解决方案。"
    },
    {
        "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification",
        "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.",
        "url": "http://arxiv.org/abs/2512.13183v1",
        "published_date": "2025-12-15T10:48:42+00:00",
        "updated_date": "2025-12-15T10:48:42+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Alfredo González-Calvin",
            "Juan F. Jiménez",
            "Héctor García de Marina"
        ],
        "tldr": "This paper presents a computationally efficient mollification method for smoothing non-differentiable paths with curvature guarantees, enabling real-time trajectory generation for robots with nonholonomic constraints.",
        "tldr_zh": "本文提出了一种计算高效的磨光方法，用于平滑具有曲率保证的非可微路径，从而能够为具有非完整约束的机器人进行实时轨迹生成。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "在移动机器人领域，大多数路径跟踪和轨迹跟踪算法都要求期望的路径或轨迹至少由二阶连续可微函数定义，以保证全局收敛等关键特性，特别是对于具有速度约束的非完整约束机器人，例如单轮车。因此，这些算法通常排除连续但不可微的路径，例如分段函数。尽管如此，这种路径为描述机器人任务或行为提供了方便的高层输入。虽然样条插值或基于优化的方法通常用于平滑不可微路径或从航路点序列创建可行路径，但它们要么会产生不必要地复杂的轨迹，要么计算成本高昂。在这项工作中，我们提出了一种正则化不可微函数并通过磨光来生成可行路径的方法。具体来说，我们使用一个可微函数来逼近任意路径，该函数可以以任意精度收敛到该路径。此外，我们提供了一种系统的方法来限制所生成路径的曲率，并通过将其应用于由线段连接一系列航路点所产生的路径来证明这一点。所提出的方法计算效率高，可在微控制器上实现实时实现，并与标准的轨迹跟踪和路径跟踪算法兼容。"
    },
    {
        "title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery",
        "summary": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.",
        "url": "http://arxiv.org/abs/2512.13402v1",
        "published_date": "2025-12-15T14:53:20+00:00",
        "updated_date": "2025-12-15T14:53:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Lorenzo Pettinari",
            "Sidaty El Hadramy",
            "Michael Wehrli",
            "Philippe C. Cattin",
            "Daniel Studer",
            "Carol C. Hasler",
            "Maria Licci"
        ],
        "tldr": "The paper introduces End2Reg, an end-to-end deep learning framework for markerless RGB-D registration in spine surgery, which jointly optimizes segmentation and registration to achieve state-of-the-art performance without relying on weak segmentation labels.",
        "tldr_zh": "本文介绍了一种名为End2Reg的端到端深度学习框架，用于脊柱手术中的无标记RGB-D配准，该框架联合优化分割和配准，无需依赖弱分割标签即可实现最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "目的：脊柱手术中的术中导航要求达到毫米级的精度。目前基于术中放射成像和骨锚定标记物的系统具有侵入性、辐射强度大且扰乱工作流程等缺点。最近的无标记RGB-D配准方法提供了一种有前景的替代方案，但现有方法依赖于弱分割标签来隔离相关的解剖结构，这会将误差传播到整个配准过程中。 方法：我们提出了一种名为End2Reg的端到端深度学习框架，该框架联合优化分割和配准，消除了对弱分割标签和手动步骤的需求。该网络学习专门为配准而优化的分割掩码，仅受配准目标的指导，而无需直接的分割监督。 结果：所提出的框架在离体和在体基准测试中均实现了最先进的性能，中值目标配准误差 (Target Registration Error) 降低了 32% 至 1.83 毫米，平均均方根误差 (Root Mean Square Error) 降低了 45% 至 3.95 毫米。一项消融研究证实，端到端优化显著提高了配准精度。 结论：所提出的端到端RGB-D配准流程消除了对弱标签和手动步骤的依赖，朝着全自动、无标记的术中导航迈进了一步。 代码和交互式可视化可在以下网址获取：https://lorenzopettinari.github.io/end-2-reg/。"
    },
    {
        "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning",
        "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.",
        "url": "http://arxiv.org/abs/2512.13131v1",
        "published_date": "2025-12-15T09:43:08+00:00",
        "updated_date": "2025-12-15T09:43:08+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.GR",
            "cs.MM",
            "cs.SD"
        ],
        "authors": [
            "Xin Guo",
            "Yifan Zhao",
            "Jia Li"
        ],
        "tldr": "This paper introduces a Hierarchical Implicit Periodicity (HIP) learning approach for generating realistic 3D co-speech gestures by modeling inter- and intra-correlations between different body parts, outperforming existing methods.",
        "tldr_zh": "本文提出了一种分层隐式周期性（HIP）学习方法，通过建模不同身体部位之间的相互关系，生成逼真的3D语音同步手势，优于现有方法。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "从语音生成基于3D的身体运动在广泛的下游应用中展现出巨大的潜力，但仍然面临着模仿逼真人类运动的挑战。目前的研究工作主要集中于端到端的生成方案来生成伴随语音的手势，包括GAN、VQ-VAE和近期的扩散模型。作为一个欠定问题，本文认为这些主流的学习方案未能建模不同运动单元（如头部、身体和手部）之间至关重要的相互和内部关联，从而导致不自然的运动和较差的协调性。为了深入研究这些内在关联，我们提出了一种统一的分层隐式周期性（HIP）学习方法，用于受音频启发的3D手势生成。与现有的主要研究不同，我们的方法通过两种显式的技术见解来建模这种多模态的隐式关系：i) 为了解耦复杂的手势运动，我们首先探索具有周期性自编码器的手势运动相位流形，以从真实的分布中模仿人类的自然行为，同时结合来自当前潜在状态的非周期性行为，以实现实例级别的多样性。ii) 为了建模面部运动、身体姿势和手部运动的层次关系，在学习期间采用级联引导来驱动动画。我们在3D虚拟形象上展示了我们提出的方法，并通过广泛的实验表明，我们的方法在定量和定性评估方面均优于最先进的语音伴随手势生成方法。代码和模型将公开提供。"
    },
    {
        "title": "Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels",
        "summary": "Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.",
        "url": "http://arxiv.org/abs/2512.12870v1",
        "published_date": "2025-12-14T23:06:37+00:00",
        "updated_date": "2025-12-14T23:06:37+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.OC"
        ],
        "authors": [
            "Pouya Ahadi",
            "Blair Winograd",
            "Camille Zaug",
            "Karunesh Arora",
            "Lijun Wang",
            "Kamran Paynabar"
        ],
        "tldr": "This paper introduces an active learning framework that minimizes label noise by optimally assigning query points to labelers and employing a novel sampling method to improve classification performance when dealing with imperfect labels.",
        "tldr_zh": "本文提出了一种主动学习框架，通过优化查询点到标注者的分配，并采用一种新的采样方法，在处理不完美标签时，最大限度地减少标签噪声，从而提高分类性能。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "主动学习(AL)在标注训练数据成本高昂的诸多应用领域中引起了广泛关注。AL提供了一种框架，帮助从业者查询信息丰富的样本，以供预言机(标注者)进行标注。然而，由于标注者准确度水平的差异，这些标签通常包含噪声。此外，由于其复杂性，不确定样本更容易收到不正确的标签。从不完美标注的数据中学习会导致分类器不准确。我们提出了一种新的AL框架，通过最小化噪声水平来构建一个鲁棒的分类模型。我们的方法包括一个分配模型，该模型以最优方式将查询点分配给标注者，旨在最小化每个周期内的最大可能噪声。此外，我们引入了一种新的采样方法来识别最佳查询点，从而减少标签噪声对分类器性能的影响。我们的实验表明，与几种基准方法相比，我们的方法能够显著提高分类性能。"
    },
    {
        "title": "3D Human-Human Interaction Anomaly Detection",
        "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.",
        "url": "http://arxiv.org/abs/2512.13560v1",
        "published_date": "2025-12-15T17:17:55+00:00",
        "updated_date": "2025-12-15T17:17:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shun Maeda",
            "Chunzhi Gu",
            "Koichiro Kamide",
            "Katsuya Hotta",
            "Shangce Gao",
            "Chao Zhang"
        ],
        "tldr": "This paper introduces the task of Human-Human Interaction Anomaly Detection (H2IAD) and proposes a network (IADNet) leveraging temporal attention sharing and distance-based relational encoding to identify anomalous collaborative behaviors in 3D human actions.",
        "tldr_zh": "本文提出了人-人交互异常检测 (H2IAD) 任务，并提出了一种利用时间注意力共享和基于距离的关系编码的网络 (IADNet)，以识别 3D 人体动作中的异常协同行为。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "以人为中心的异常检测（AD）主要研究的是指定单个人中的异常行为。然而，由于人类本质上倾向于以协作方式行动，因此行为异常也可能源于人与人之间的互动。 使用现有单人AD模型检测此类异常容易导致准确率低下，因为这些方法通常并非旨在捕获互动中复杂且不对称的动态特性。 在本文中，我们引入了一项新任务，即人与人互动异常检测（H2IAD），旨在识别协作式3D人体动作中异常的互动行为。 为了解决H2IAD，我们提出了一种交互异常检测网络（IADNet），它形式化了一个时间注意力共享模块（TASM）。 具体来说，在设计TASM时，我们共享了两个人之间编码的运动嵌入，从而可以有效地同步协作运动相关性。 此外，我们注意到，除了时间动态之外，人际互动还以两个人之间的空间配置为特征。 因此，我们引入了基于距离的关系编码模块（DREM），以更好地反映H2IAD中的社会线索。 最终，采用归一化流进行异常评分。 在人与人运动基准数据集上的大量实验表明，IADNet在H2IAD中优于现有的以人为中心的AD基线。"
    },
    {
        "title": "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing",
        "summary": "In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.",
        "url": "http://arxiv.org/abs/2512.13497v1",
        "published_date": "2025-12-15T16:27:23+00:00",
        "updated_date": "2025-12-15T16:27:23+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Haoyu Ren",
            "Kay Koehle",
            "Kirill Dorofeev",
            "Darko Anicic"
        ],
        "tldr": "This paper introduces an on-device continual learning approach for unsupervised visual anomaly detection in dynamic manufacturing environments, using a modified PatchCore to achieve improved accuracy, memory efficiency, and adaptation speed compared to baseline methods.",
        "tldr_zh": "本文介绍了一种用于动态制造环境中无监督视觉异常检测的设备端持续学习方法，该方法改进了PatchCore，与基线方法相比，实现了更高的准确性、内存效率和适应速度。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "在现代制造业中，视觉异常检测（VAD）对于自动化检测和一致的产品质量至关重要。然而，日益动态和灵活的生产环境带来了关键挑战：首先，小批量和按需制造中频繁的产品变更需要快速的模型更新。其次，传统边缘硬件缺乏训练和运行大型人工智能模型的资源。最后，异常和正常的训练数据通常都很稀缺，特别是对于新引入的产品变体。我们研究了用于无监督 VAD 的片上持续学习，并将其扩展到 PatchCore 以融入在线学习，从而应对实际的工业场景。所提出的方法利用了轻量级特征提取器和基于 k-中心选择的增量式核心集更新机制，从而能够在有限数据下进行快速、内存高效的自适应，同时避免了昂贵的云端重新训练。我们在工业用例上进行了评估，使用了一个旨在模拟灵活生产的测试平台，该平台在受控环境中模拟频繁的变体更改。与基线相比，我们的方法实现了 12% 的 AUROC 提升、80% 的内存使用量减少以及更快的训练速度（与批量重新训练相比）。这些结果证实，我们的方法提供了准确、资源高效且自适应的 VAD，适用于动态和智能制造。"
    },
    {
        "title": "rNCA: Self-Repairing Segmentation Masks",
        "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.",
        "url": "http://arxiv.org/abs/2512.13397v1",
        "published_date": "2025-12-15T14:49:55+00:00",
        "updated_date": "2025-12-15T14:49:55+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Malte Silbernagel",
            "Albert Alonso",
            "Jens Petersen",
            "Bulat Ibragimov",
            "Marleen de Bruijne",
            "Madeleine K. Wyburd"
        ],
        "tldr": "The paper introduces rNCA, a method that repurposes Neural Cellular Automata for refining segmentation masks by iteratively repairing topological errors using local information and image context, achieving improvements in metrics like Dice, clDice, and Betti errors on tasks such as retinal vessel and myocardium segmentation.",
        "tldr_zh": "该论文介绍了rNCA，一种利用神经元胞自动机通过局部信息和图像上下文迭代修复拓扑错误来优化分割掩码的方法。在视网膜血管和心肌分割等任务上，该方法在Dice、clDice和Betti错误等指标上取得了改进。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "准确预测拓扑结构正确的掩膜对于通用分割模型来说仍然是一项艰巨的任务，这些模型经常产生碎片化或断连的输出。修复这些伪影通常需要手工设计的精细化规则或专门针对特定任务的架构。在这里，我们展示了神经元胞自动机 (NCA) 可以被直接重新用于作为一种有效的精细化机制，利用由图像上下文引导的局部、迭代更新来修复分割掩膜。通过在不完美的掩膜和真实标签上进行训练，自动机学习目标形状的结构属性，同时仅依赖于局部信息。当应用于粗糙的、全局预测的掩膜时，学习到的动力学逐步重新连接断裂区域，剪除松散的碎片，并收敛到稳定、拓扑结构一致的结果。我们展示了精细化 NCA (rNCA) 如何能被轻松地应用于修复由不同基础分割模型和任务产生的常见拓扑错误：对于碎片化的视网膜血管，它在 Dice/clDice 上产生了 2-3% 的增益，并改善了 Betti 误差，将 $β_0$ 误差降低了 60%，$β_1$ 降低了 20%；对于心肌，它在零样本设置中修复了 61.5% 的断裂病例，同时分别降低了 ASSD 和 HD 19% 和 16%。这展示了 NCA 作为一种有效且广泛适用的精细化器。"
    },
    {
        "title": "Improving the Plausibility of Pressure Distributions Synthesized from Depth through Generative Modeling",
        "summary": "Monitoring contact pressure in hospital beds is essential for preventing pressure ulcers and enabling real-time patient assessment. Current methods can predict pressure maps but often lack physical plausibility, limiting clinical reliability. This work proposes a framework that enhances plausibility via Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with generative modeling to produce high-fidelity, physically consistent pressure estimates. This study also applies diffusion based conditional Brownian Bridge Diffusion Model (BBDM) and proposes training strategy for its latent counterpart Latent Brownian Bridge Diffusion Model (LBBDM) tailored for pressure synthesis in lying postures. Experiment results shows proposed method improves physical plausibility and performance over baselines: BBDM with ILS delivers highly detailed maps at higher computational cost and large inference time, whereas LBBDM provides faster inference with competitive performance. Overall, the approach supports non-invasive, vision-based, real-time patient monitoring in clinical environments.",
        "url": "http://arxiv.org/abs/2512.13757v1",
        "published_date": "2025-12-15T11:08:36+00:00",
        "updated_date": "2025-12-15T11:08:36+00:00",
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Neevkumar Manavar",
            "Hanno Gerd Meyer",
            "Joachim Waßmuth",
            "Barbara Hammer",
            "Axel Schneider"
        ],
        "tldr": "This paper proposes a generative modeling framework using Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with diffusion models to synthesize physically plausible pressure distributions from depth images for real-time patient monitoring, specifically for preventing pressure ulcers.",
        "tldr_zh": "本文提出了一种生成建模框架，使用知情潜在空间（ILS）和权重优化损失（WOL）以及扩散模型，从深度图像合成物理上合理的压力分布，用于实时患者监测，特别是用于预防压疮。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "监测医院病床上的接触压力对于预防压疮和实现患者实时评估至关重要。目前的方法可以预测压力图，但通常缺乏物理合理性，从而限制了临床可靠性。本研究提出了一个框架，通过知情隐空间(ILS)和权重优化损失(WOL)增强合理性，并结合生成式建模来产生高保真、物理一致的压力估计。此外，本研究应用了基于扩散的条件布朗桥扩散模型(BBDM)，并为针对卧姿压力合成的其潜在对应模型：潜在布朗桥扩散模型(LBBDM) 提出了训练策略。实验结果表明，所提出的方法提高了物理合理性和性能，优于基线方法：带有ILS的BBDM能以更高的计算成本和更长的推理时间提供高度精细的压力图，而LBBDM则以具有竞争力的表现提供更快的推理速度。总而言之，该方法支持临床环境中的非侵入性、基于视觉的实时患者监测。"
    },
    {
        "title": "Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing",
        "summary": "Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR",
        "url": "http://arxiv.org/abs/2512.13055v1",
        "published_date": "2025-12-15T07:30:17+00:00",
        "updated_date": "2025-12-15T07:30:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jaeyoon Kim",
            "Yoonki Cho",
            "Sung-Eui Yoon"
        ],
        "tldr": "This paper introduces an efficient asymmetric Visual Place Recognition (VPR) framework using a high-capacity gallery model and a lightweight query network, enhanced by a geographical memory bank and implicit embedding augmentation, to reduce computational costs in resource-limited environments.",
        "tldr_zh": "本文介绍了一种高效的非对称视觉地点识别(VPR)框架，该框架利用高容量的图库模型和轻量级的查询网络，并通过地理记忆库和隐式嵌入增强，以降低资源受限环境下的计算成本。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "视觉场景识别（VPR）在DINOv2等高容量基础模型的推动下取得了显著进展，实现了卓越的性能。然而，其巨大的计算成本使得在资源受限设备上的部署变得不切实际。本文提出了一种高效的非对称VPR框架，该框架包含一个高容量的图库模型用于离线特征提取，以及一个轻量级的查询网络用于在线处理。这种设置中的一个关键挑战是确保这些异构网络之间的兼容性，传统的解决方法是通过计算成本高昂的基于k近邻的兼容性训练来实现。为了克服这一困难，我们提出了一种地理记忆库，该记忆库利用VPR数据库中固有的地理位置元数据来组织图库特征，从而消除了对详尽的k近邻计算的需求。此外，我们引入了一种隐式嵌入增强技术，该技术增强了查询网络对特征变化的建模能力，尽管其容量有限。大量的实验表明，我们的方法不仅显著降低了计算成本，而且优于现有的非对称检索技术，为资源受限环境下的VPR建立了一个新的维度。代码可在https://github.com/jaeyoon1603/AsymVPR 获取。"
    },
    {
        "title": "Scalable Formal Verification via Autoencoder Latent Space Abstraction",
        "summary": "Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.",
        "url": "http://arxiv.org/abs/2512.13593v2",
        "published_date": "2025-12-15T17:48:07+00:00",
        "updated_date": "2025-12-16T16:58:02+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Robert Reed",
            "Luca Laurenti",
            "Morteza Lahijanian"
        ],
        "tldr": "This paper presents a formal verification method using autoencoders for dimensionality reduction to improve scalability of finite abstraction techniques, guaranteeing correctness by mapping verification results back to the original system.",
        "tldr_zh": "本文提出了一种使用自编码器进行降维的形式化验证方法，以提高有限抽象技术的可扩展性，并通过将验证结果映射回原始系统来保证正确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "有限抽象方法提供了一个强大的形式化框架，用于证明系统满足其规范。然而，这些技术在高维系统方面面临可扩展性挑战，因为它们依赖于状态空间离散化，而该离散化随着维度呈指数增长。利用神经网络和自编码器等基于学习的降维方法，已显示出缓解此问题的巨大潜力。然而，确保由此产生的验证结果的正确性仍然是一个悬而未决的问题。在这项工作中，我们提供了一种形式化方法，通过凸自编码器来降低系统维度，并通过基于核的方法来学习潜在空间中的动力学。然后，我们从潜在空间中学习到的模型构建一个有限抽象，并保证该抽象包含原始系统的真实行为。我们证明了潜在空间中的验证结果可以映射回原始系统。最后，我们在多个系统上展示了我们方法的有效性，包括一个由神经网络控制的26维系统，显示出显着的可扩展性改进，而没有损失严谨性。"
    },
    {
        "title": "Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency",
        "summary": "Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.",
        "url": "http://arxiv.org/abs/2512.13482v1",
        "published_date": "2025-12-15T16:18:36+00:00",
        "updated_date": "2025-12-15T16:18:36+00:00",
        "categories": [
            "eess.SY",
            "cs.LG"
        ],
        "authors": [
            "Wenyi Liu",
            "R. Sharma",
            "W. \"Grace\" Guo",
            "J. Yi",
            "Y. B. Guo"
        ],
        "tldr": "This paper presents a real-time AI-driven digital twin for milling, enabling smart manufacturing with live data streaming and machine learning models. It explores virtual models, data flow, and feedback mechanisms in milling processes, highlighting a case study and outlining future research for Industry 4.0.",
        "tldr_zh": "本文介绍了一种用于铣削的实时AI驱动的数字孪生，通过实时数据流和机器学习模型实现智能制造。它探讨了铣削过程中虚拟模型、数据流和反馈机制，重点介绍了一个案例研究，并概述了工业4.0的未来研究方向。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "数字孪生(DT)通过利用实时数据、人工智能模型和智能控制系统，赋能智能制造。本文针对铣削领域中新兴的数字孪生技术进行了最先进的分析。通过物理铣削的虚拟模型、从物理铣削到虚拟模型的数据流，以及从虚拟模型到物理铣削的反馈，探讨了数字孪生的关键方面。重点阐述了实时数据流协议和虚拟建模方法。一个案例研究展示了在铣削过程中，基于实时机器学习驱动的刀具-工件接触的实时数字孪生所带来的变革能力。本文还概述了未来研究方向，旨在实现工业4.0及更远的目标。"
    },
    {
        "title": "Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning",
        "summary": "Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.",
        "url": "http://arxiv.org/abs/2512.13196v1",
        "published_date": "2025-12-15T11:10:40+00:00",
        "updated_date": "2025-12-15T11:10:40+00:00",
        "categories": [
            "cs.LG",
            "cs.AR"
        ],
        "authors": [
            "Chethana Prasad Kabgere",
            "Sudarshan T S B"
        ],
        "tldr": "This paper introduces a Noise-Resilient Quantum Federated Learning (NR-QFL) framework using variational quantum circuits for ADAS, aiming for secure, low-latency, and noise-tolerant model aggregation in vehicular networks.",
        "tldr_zh": "本文介绍了一种噪声鲁棒的量子联邦学习（NR-QFL）框架，该框架使用可变量子电路用于ADAS，旨在实现车辆网络中安全、低延迟和抗噪声的模型聚合。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "高级驾驶辅助系统(ADAS)越来越多地采用联邦学习(FL)，以便在分布式车辆节点上协同训练模型，同时保护数据隐私。然而，传统的FL聚合仍然容易受到实时车辆网络固有的噪声、延迟和安全约束的影响。本文介绍了一种抗噪声量子联邦学习(NR-QFL)，这是一种混合量子-经典框架，它可以通过在噪声中等规模量子(NISQ)条件下运行的变分量子电路(VQC)实现安全、低延迟的聚合。该框架将模型参数编码为具有自适应门重参数化的量子态，从而确保有界误差收敛和在完全正迹保留(CPTP)动力学下的可证明的弹性。NR-QFL采用基于量子熵的客户端选择和多服务器协调，以实现公平性和稳定性。经验验证表明，在受限的边缘条件下，该框架具有一致的收敛性，同时降低了梯度方差、降低了通信开销并增强了抗噪声能力。该框架为量子增强联邦学习奠定了可扩展的基础，从而在车辆边缘实现安全、高效和动态稳定的 ADAS 智能。"
    },
    {
        "title": "Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic",
        "summary": "This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.",
        "url": "http://arxiv.org/abs/2512.12907v1",
        "published_date": "2025-12-15T01:24:02+00:00",
        "updated_date": "2025-12-15T01:24:02+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Parthasarathy Nadarajan",
            "Michael Botsch",
            "Sebastian Sardina"
        ],
        "tldr": "The paper proposes a novel machine learning architecture consisting of stacked denoising autoencoders and random forests to estimate predicted occupancy grids (POGs) for traffic scenarios, aiming to improve autonomous driving and active safety systems.",
        "tldr_zh": "本文提出了一种新颖的机器学习架构，该架构由堆叠式去噪自动编码器和随机森林组成，用于估计交通场景的预测占用栅格（POG），旨在改进自动驾驶和主动安全系统。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "本文介绍了一种新颖的机器学习架构，用于高效估计复杂交通场景的概率时空表示。对未来交通场景的详细表示对于自动驾驶和所有主动安全系统都至关重要。为了预测交通场景的未来时空表示，首先识别交通场景的类型，然后机器学习算法将当前场景状态映射到可能的未来状态。机器学习算法的输入是交通场景的当前状态表示，称为增强占用网格 (AOG)。输出是概率时空表示，包括交通参与者行为的不确定性，称为预测占用网格 (POG)。这种新颖的架构由两个堆叠式去噪自编码器 (SDAs) 和一组随机森林组成。然后将其与由 SDAs 和反卷积网络 (DeconvNet) 组成的另外两种现有架构进行比较。通过仿真验证了这些架构，并从准确性和计算时间两方面进行了比较。此外，本文还简要概述了 POG 在主动安全领域的应用。"
    },
    {
        "title": "Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning",
        "summary": "This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.",
        "url": "http://arxiv.org/abs/2512.12896v1",
        "published_date": "2025-12-15T00:45:26+00:00",
        "updated_date": "2025-12-15T00:45:26+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Parthasarathy Nadarajan",
            "Michael Botsch"
        ],
        "tldr": "This paper proposes a machine learning approach, specifically using Random Forests, to efficiently compute Predicted-Occupancy Grids (POGs) for vehicle safety applications, aiming to replace computationally expensive model-based methods.",
        "tldr_zh": "该论文提出了一种基于机器学习的方法，特别是使用随机森林，来高效计算车辆安全应用中的预测占用栅格（POG），旨在取代计算量大的基于模型的方法。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "本文提出了一种预测包含多个对象的复杂交通场景演变的方法。假设可从传感器获取场景的当前状态，并且预测考虑了关于交通参与者行为的各种假设。通过这种方式，可以详细建模关于交通参与者行为的不确定性。本文的第一部分提出了一种基于模型的方法来计算预测占用栅格（POG），POG作为未来场景假设的基于栅格的概率表示被引入。然而，由于每个交通参与者可能存在大量轨迹，基于模型的方法带来了非常高的计算负担。因此，采用了一种机器学习方法来计算POG。这项工作使用了一种新颖的基于栅格的交通场景当前状态表示，并执行到POG的映射。这种表示由占用栅格中的增强单元组成。所采用的机器学习方法基于随机森林算法。通过交通场景的仿真，比较了机器学习方法和基于模型的方法。结果令人满意，并可能实现车辆安全应用中POG的实时计算。通过对不确定性的这种详细建模，可以改进车辆安全系统中的关键组件，如临界性估计和轨迹规划。"
    },
    {
        "title": "KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation",
        "summary": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.",
        "url": "http://arxiv.org/abs/2512.12850v1",
        "published_date": "2025-12-14T21:29:10+00:00",
        "updated_date": "2025-12-14T21:29:10+00:00",
        "categories": [
            "cs.AR",
            "cs.LG",
            "eess.SY",
            "hep-ex"
        ],
        "authors": [
            "Duc Hoang",
            "Aarush Gupta",
            "Philip Harris"
        ],
        "tldr": "The paper introduces KANELÉ, a framework for implementing Kolmogorov-Arnold Networks (KANs) on FPGAs for efficient and low-latency neural network inference, showing significant speedups and resource savings compared to existing methods, particularly for symbolic or physical formula tasks.",
        "tldr_zh": "该论文介绍了KANELÉ，一个在FPGA上实现Kolmogorov-Arnold网络（KANs）的框架，用于高效和低延迟的神经网络推理。与现有方法相比，KANELÉ在速度和资源节省方面表现出显著优势，尤其是在处理符号或物理公式相关任务时。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "在FPGA上实现低延迟、资源高效的神经网络推理对于需要实时能力和低功耗的应用至关重要。基于查找表（LUT）的神经网络是一种常见的解决方案，它将强大的表征能力与高效的FPGA实现相结合。本文介绍KANELÉ，一个利用柯尔莫哥洛夫-阿诺德网络（KANs）独特属性进行FPGA部署的框架。与传统的多层感知器（MLPs）不同，KANs采用可学习的一维样条函数作为边激活函数，并具有固定的域，这种结构天然适合离散化和高效的LUT映射。我们提出了首个用于在FPGA上实现KANs的系统化设计流程，通过量化和剪枝协同优化训练过程，以实现紧凑、高吞吐量和低延迟的KAN架构。我们的结果表明，相比之前的KAN-on-FPGA方法，速度提升高达2700倍，并节省了数个数量级的资源。此外，在广泛使用的基准测试中，KANELÉ在特别是涉及符号或物理公式的任务上，匹配或超过了其他基于LUT的架构，同时平衡了FPGA硬件上的资源利用率。最后，我们通过将其扩展到实时、节能的控制系统，展示了该框架的多功能性。"
    },
    {
        "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model",
        "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.",
        "url": "http://arxiv.org/abs/2512.13477v1",
        "published_date": "2025-12-15T16:14:22+00:00",
        "updated_date": "2025-12-15T16:14:22+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Timothy A. Brumfiel",
            "Revanth Konda",
            "Drew Elliott",
            "Jaydev P. Desai"
        ],
        "tldr": "This paper evaluates the navigation capabilities of a simplified, two-tube version of the COAST guidewire robot in a vascular phantom model with pulsatile flow, demonstrating its effectiveness in tortous vasculature.",
        "tldr_zh": "本文评估了一个简化的双管COAST导丝机器人在脉动流动的血管模型中的导航能力，证明了其在复杂的血管结构中的有效性。",
        "relevance_score": 3,
        "novelty_claim_score": 4,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 4,
        "summary_zh": "为了解决血管内介入手术中导丝人工导航所产生的问题，医疗机器人领域的研究对开发机器人可控导丝产生了浓厚的兴趣。这类导丝具有增强机动性和导航能力的可能性，因为导丝顶端可以被主动控制。同轴对准可控 (COAST) 导丝机器人能够产生多种运动，包括具有不同弯曲长度的弯曲运动、跟随者运动以及前馈运动。在过去的 研究中，我们探索了 COAST 导丝机器人的不同设计，并开发了 COAST 导丝机器人的建模、控制和传感策略。在本研究中，我们通过在具有搏动血流的解剖模型中进行导航实验来评估改进型 COAST 导丝机器人的性能。改进型 COAST 导丝机器人是 COAST 导丝机器人的简化版本，由两个管子组成，而不是三个管子。通过这项研究，我们证明了改进型 COAST 导丝机器人在导航迂曲的体外血管模型中的有效性。"
    },
    {
        "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories",
        "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.",
        "url": "http://arxiv.org/abs/2512.13304v1",
        "published_date": "2025-12-15T13:22:16+00:00",
        "updated_date": "2025-12-15T13:22:16+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Sait Sovukluk",
            "Johannes Englsberger",
            "Christian Ott"
        ],
        "tldr": "The paper presents a step adaptation framework for humanoid robots to achieve agile locomotion, including running through stepping stones and jumping over obstacles, using spring-mass trajectories and deadbeat control with a single library and control parameter set.",
        "tldr_zh": "本文提出了一种用于类人机器人的步态适应框架，通过使用弹簧-质量轨迹和零拍控制，实现了敏捷的运动，包括在垫脚石上奔跑和跳过障碍物，所有运动都使用了单一的库和控制参数集。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "本研究提出了一种步态自适应框架，用于通过弹簧-质量轨迹和无差拍控制增益库实现奔跑。它包括四个主要部分：（1）自动生成弹簧-质量轨迹库；（2）通过一个能够很好地模拟全身动力学的主动控制模板模型生成无差拍控制增益库；（3）开发用于步态自适应的轨迹选择策略；（4）通过全身控制（WBC）框架将弹簧-质量轨迹映射到人形机器人模型，同时考虑闭链运动机构、自碰撞和反应式肢体摆动。我们通过各种具有挑战性的敏捷行为，例如，在随机生成的踏脚石上奔跑、跳过随机障碍物、执行蛇形运动、用随机一条腿突然改变奔跑方向，以及通过 MuJoCo 物理模拟器拒绝显著的扰动和不确定性，来展示所提出框架的包容性和鲁棒性。我们还在包含一系列不确定性和噪声的环境下进行了额外的模拟，以更好地证明所提出方法对现实世界挑战的鲁棒性，包括信号噪声、不精确性、建模误差和延迟。所有上述行为都是使用单个库和同一组 WBC 控制参数执行的，无需额外调整。弹簧-质量和无差拍控制增益库总共在 4.5 秒内自动计算出 315 条不同的轨迹。"
    },
    {
        "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation",
        "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.",
        "url": "http://arxiv.org/abs/2512.13198v1",
        "published_date": "2025-12-15T11:13:30+00:00",
        "updated_date": "2025-12-15T11:13:30+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Hyun-Gi Lee",
            "Jaekyeong Han",
            "Minjun Kwon",
            "Hyeonuk Kwon",
            "Jooha Park",
            "Hoe Jin Ha",
            "Dong-Hwa Seo"
        ],
        "tldr": "The paper introduces ALBATROSS, an automated robotic system for high-throughput electrolyte screening via automated coin-cell assembly and electrochemical evaluation, achieving high reliability and accelerating electrolyte development.",
        "tldr_zh": "该论文介绍了一种名为ALBATROSS的自动化机器人系统，用于通过自动化硬币电池组装和电化学评估进行高通量电解质筛选，实现了高可靠性并加速了电解质开发。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "随着电池技术向更高的稳定性和能量密度发展，对各种组件配置进行广泛的电池级测试变得至关重要。在实验室规模下评估电池性能并理解其工作原理，纽扣电池的制造和评估是必不可少的流程。然而，传统的纽扣电池组装和测试过程需要研究人员花费大量时间和精力，给高通量筛选研究带来了挑战。在本研究中，我们介绍了一种自动化锂离子电池测试机器人系统（ALBATROSS），该自动化系统能够进行电解液配方、纽扣电池组装和电化学评估。该系统集成在充满氩气的手套箱内，能够全自动组装和测试多达48个电池，无需研究人员干预。通过采用定制设计的机器人夹持器和为精确的电池处理而优化的3D打印结构，ALBATROSS实现了高组装可靠性，对于NCM811||Li半电池，其放电容量的相对标准偏差（RSD）小于1.2%，EIS测量的标准偏差小于3 Ω。由于其高可靠性和自动化能力，ALBATROSS能够获取高质量的纽扣电池数据集，有望加速下一代电解液的开发。"
    },
    {
        "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition",
        "summary": "Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM",
        "url": "http://arxiv.org/abs/2512.13415v1",
        "published_date": "2025-12-15T15:05:16+00:00",
        "updated_date": "2025-12-15T15:05:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahmed Abul Hasanaath",
            "Hamzah Luqman"
        ],
        "tldr": "The paper introduces USTM, a Unified Spatio-Temporal Modeling framework for Continuous Sign Language Recognition (CSLR) that uses a Swin Transformer backbone with lightweight temporal adapters to achieve state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "该论文介绍了一种用于连续手语识别 (CSLR) 的统一时空建模框架 USTM，该框架使用 Swin Transformer 主干网络和轻量级时间适配器，在基准数据集上实现了最先进的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "连续手语识别（CSLR）需要精确的时空建模，以准确识别视频中的手势序列。现有框架通常依赖于基于CNN的空间骨干网络，并结合时间卷积或循环模块。这些技术无法捕获细粒度的手部和面部线索，也无法建模长距离的时间依赖性。为了解决这些局限性，我们提出了一种统一时空建模（USTM）框架，这是一种时空编码器，它通过结合使用Swin Transformer骨干网络，并使用轻量级的时间适配器与位置嵌入（TAPE）进行增强，从而有效地建模复杂模式。我们的框架能够捕获细粒度的空间特征以及短时和长时的时间上下文，从而能够从RGB视频中进行鲁棒的手语识别，而无需依赖多流输入或辅助模态。在包括PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上进行的大量实验表明，USTM在基于RGB和多模态CSLR方法方面均实现了最先进的性能，同时在多流方法方面保持了具有竞争力的性能。这些结果突出了USTM框架在CSLR方面的优势和有效性。代码可在https://github.com/gufranSabri/USTM上获取。"
    },
    {
        "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection",
        "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.",
        "url": "http://arxiv.org/abs/2512.13104v1",
        "published_date": "2025-12-15T09:01:10+00:00",
        "updated_date": "2025-12-15T09:01:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Zhang",
            "Baoxin Li",
            "Han Sun",
            "Yuhang Gao",
            "Mingtai Zhang",
            "Pei Wang"
        ],
        "tldr": "The paper introduces FID-Net, a deep learning model improving on YOLOv8n, for detecting pest-affected trees from UAV imagery using feature enhancement, multi-scale feature fusion, and channel attention, achieving improved performance in forest infestation detection and analysis.",
        "tldr_zh": "该论文介绍了FID-Net，一种改进自YOLOv8n的深度学习模型，它通过使用特征增强、多尺度特征融合和通道注意力机制，从无人机图像中检测受病虫害影响的树木，从而提高森林病虫害检测和分析的性能。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "森林病虫害威胁着生态系统的稳定性，因此需要高效的监测。为了克服传统方法在大尺度、细粒度检测方面的局限性，本研究侧重于精确识别受感染的树木并分析其侵染模式。我们提出了一种深度学习模型FID-Net，该模型能够从无人机可见光图像中检测受病虫害影响的树木，并通过三个空间度量实现侵染分析。FID-Net基于YOLOv8n，引入了轻量级的特征增强模块（FEM）来提取对病害敏感的线索，自适应多尺度特征融合模块（AMFM）来对齐和融合双分支特征（RGB和FEM增强的特征），以及高效通道注意力（ECA）机制来高效地增强判别信息。根据检测结果，我们构建了一个病虫害态势分析框架，该框架使用：（1）核密度估计来定位感染热点；（2）邻域评估来评估健康树木的感染风险；（3）DBSCAN聚类来识别高密度健康集群作为优先保护区。在中国东天山32个林地的无人机图像实验表明，FID-Net实现了86.10%的精确率、75.44%的召回率、82.29%的mAP@0.5和64.30%的mAP@0.5:0.95，性能优于主流的YOLO模型。分析证实，受感染的树木表现出明显的聚类现象，支持有针对性的森林保护。FID-Net能够准确区分树木健康状况，并结合空间度量，为智能病虫害监测、早期预警和精确管理提供可靠的数据。"
    },
    {
        "title": "DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems",
        "summary": "Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.",
        "url": "http://arxiv.org/abs/2512.13460v1",
        "published_date": "2025-12-15T15:56:58+00:00",
        "updated_date": "2025-12-15T15:56:58+00:00",
        "categories": [
            "cs.LG",
            "math.NA"
        ],
        "authors": [
            "Chethana Prasad Kabgere",
            "Shylaja S S"
        ],
        "tldr": "The paper introduces DP-EMAR, a differentially private framework for detecting and correcting transmission errors in federated learning on IoT devices, enhancing robustness and privacy in resource-constrained environments.",
        "tldr_zh": "该论文介绍了DP-EMAR，一个差分隐私框架，用于检测和纠正物联网设备上联邦学习中的传输错误，从而增强资源受限环境中的鲁棒性和隐私性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "联合学习 (FL) 能够在不共享原始数据的情况下实现去中心化的模型训练，但模型权重失真仍然是资源受限的物联网网络中的一个主要挑战。在多层联合物联网 (Fed-IoT) 系统中，不稳定的连接和对抗性干扰会悄无声息地改变传输的参数，从而降低收敛性。我们提出了一种差分隐私的、基于误差模型的自主修复框架 DP-EMAR，该框架能够检测和重构 FL 聚合期间由传输引起的失真。 DP-EMAR 在添加隐私噪声之前估计损坏模式并应用自适应校正，从而能够在不违反机密性的情况下进行可靠的网络内修复。通过将差分隐私 (DP) 与安全聚合 (SA) 相结合，该框架能够区分 DP 噪声与真实的传输错误。在异构物联网传感器和图数据集上的实验表明，DP-EMAR 在通信发生损坏的情况下，既能保持收敛稳定性，又能维持接近基线性能，同时确保严格的 (epsilon, delta)-DP 保证。该框架增强了在保护隐私的联邦物联网学习中的鲁棒性、通信效率和信任。"
    },
    {
        "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization",
        "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.",
        "url": "http://arxiv.org/abs/2512.13219v1",
        "published_date": "2025-12-15T11:32:45+00:00",
        "updated_date": "2025-12-15T11:32:45+00:00",
        "categories": [
            "cs.RO",
            "cs.MS"
        ],
        "authors": [
            "Christoph Hartmann",
            "Marios Demetriades",
            "Kevin Prüfer",
            "Zichen Zhang",
            "Klaus Spindler",
            "Stefan Weltge"
        ],
        "tldr": "The paper introduces PyCAALP, an open-source Python framework for automated assembly sequence and production line planning using graph-based optimization, addressing kinematic constraints and manufacturing station balancing.",
        "tldr_zh": "本文介绍了一个名为PyCAALP的开源Python框架，该框架使用基于图的优化方法自动进行装配序列和生产线规划，解决了运动学约束和制造站平衡问题。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 3,
        "summary_zh": "本文介绍了一个名为PyCAALP（基于Python的计算机辅助装配线规划）的框架，用于自动装配序列规划（ASP）和生产线规划（PLP），其采用基于图的方法来建模生产模块内的零部件和连接点。该框架集成了运动学边界条件，例如潜在的零件碰撞，以保证自动装配规划的可行性。所开发的算法计算所有可行的生产序列，集成了用于检测空间关系和制定几何约束的模块。该算法结合了额外的属性，包括搬运可行性、公差匹配和连接兼容性，以管理装配序列生成中固有的高组合复杂性。诸如单件流装配和几何约束强制执行等的启发式方法被用于进一步优化解空间，从而促进复杂装配的更高效规划。PLP阶段被表述为一个混合整数规划（MIP），以平衡固定数量的制造站的总时间。 虽然一些复杂度降低技术可能会牺牲最优性，但它们可以显著减少MIP的计算时间。 此外，该框架支持工程约束的定制，并支持ASP和PLP之间的灵活权衡。该框架的开源性质，可在https://github.com/TUM-utg/PyCAALP 获取，促进了工业和生产研究应用中的进一步合作和采用。"
    }
]