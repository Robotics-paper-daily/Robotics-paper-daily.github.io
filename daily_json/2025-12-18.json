[
    {
        "title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
        "summary": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.",
        "url": "http://arxiv.org/abs/2512.14666v1",
        "published_date": "2025-12-16T18:26:38+00:00",
        "updated_date": "2025-12-16T18:26:38+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zechen Bai",
            "Chen Gao",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces EVOLVE-VLA, a test-time training framework that allows Vision-Language-Action models to continuously adapt through interaction with the environment using learned progress estimators, enabling adaptation and generalization with minimal task-specific demonstrations.",
        "tldr_zh": "该论文介绍了一种名为EVOLVE-VLA的测试时训练框架，它允许视觉-语言-动作模型通过与环境交互并使用学习到的进度估计器来持续适应环境，从而以最少的任务特定演示实现适应性和泛化。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "实现真正的自适应具身智能要求智能体不仅能通过模仿静态演示进行学习，还能通过与环境交互不断改进，这类似于人类通过实践掌握技能的方式。视觉-语言-动作 (VLA) 模型通过利用大型语言模型推进了机器人操作，但仍受到监督微调 (SFT) 的根本限制：每个任务需要数百个演示，刚性地记忆轨迹，并且在部署条件偏离训练时无法适应。我们引入了 EVOLVE-VLA，这是一个测试时训练框架，使 VLA 能够通过与环境交互持续适应，且只需要极少或零任务特定的演示。关键的技术挑战是用自主反馈取代预言机奖励信号（在测试时不可用）。我们通过一个提供密集反馈的学习型进度估计器来解决这个问题，并且至关重要的是，我们设计我们的框架通过两种机制来“驯服”这种本质上嘈杂的信号：（1）一种累积进度估计机制，平滑噪声的点状估计，以及（2）一种渐进式时间范围扩展策略，实现逐渐的策略演变。EVOLVE-VLA 获得了显著的提升：在长时域任务上 +8.6%，在一次学习中 +22.0%，并实现了跨任务泛化——在没有任务特定的演示训练的情况下，在未见任务上实现了 20.8% 的成功率（纯 SFT 为 0%）。定性分析揭示了演示中没有的涌现能力，包括错误恢复和新颖策略。这项工作代表了通向真正学习和适应的 VLA 的关键一步，从静态模仿转向持续的自我改进。"
    },
    {
        "title": "Step-GUI Technical Report",
        "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.",
        "url": "http://arxiv.org/abs/2512.15431v1",
        "published_date": "2025-12-17T13:26:30+00:00",
        "updated_date": "2025-12-17T13:26:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haolong Yan",
            "Jia Wang",
            "Xin Huang",
            "Yeqing Shen",
            "Ziyang Meng",
            "Zhimin Fan",
            "Kaijun Tan",
            "Jin Gao",
            "Lieyu Shi",
            "Mi Yang",
            "Shiliang Yang",
            "Zhirui Wang",
            "Brian Li",
            "Kang An",
            "Chenyang Li",
            "Lei Lei",
            "Mengmeng Duan",
            "Danxun Liang",
            "Guodong Liu",
            "Hang Cheng",
            "Hao Wu",
            "Jie Dong",
            "Junhao Huang",
            "Mei Chen",
            "Renjie Yu",
            "Shunshan Li",
            "Xu Zhou",
            "Yiting Dai",
            "Yineng Deng",
            "Yingdan Liang",
            "Zelin Chen",
            "Wen Sun",
            "Chengxu Yan",
            "Chunqin Xu",
            "Dong Li",
            "Fengqiong Xiao",
            "Guanghao Fan",
            "Guopeng Li",
            "Guozhen Peng",
            "Hongbing Li",
            "Hang Li",
            "Hongming Chen",
            "Jingjing Xie",
            "Jianyong Li",
            "Jingyang Zhang",
            "Jiaju Ren",
            "Jiayu Yuan",
            "Jianpeng Yin",
            "Kai Cao",
            "Liang Zhao",
            "Liguo Tan",
            "Liying Shi",
            "Mengqiang Ren",
            "Min Xu",
            "Manjiao Liu",
            "Mao Luo",
            "Mingxin Wan",
            "Na Wang",
            "Nan Wu",
            "Ning Wang",
            "Peiyao Ma",
            "Qingzhou Zhang",
            "Qiao Wang",
            "Qinlin Zeng",
            "Qiong Gao",
            "Qiongyao Li",
            "Shangwu Zhong",
            "Shuli Gao",
            "Shaofan Liu",
            "Shisi Gao",
            "Shuang Luo",
            "Xingbin Liu",
            "Xiaojia Liu",
            "Xiaojie Hou",
            "Xin Liu",
            "Xuanti Feng",
            "Xuedan Cai",
            "Xuan Wen",
            "Xianwei Zhu",
            "Xin Liang",
            "Xin Liu",
            "Xin Zhou",
            "Yingxiu Zhao",
            "Yukang Shi",
            "Yunfang Xu",
            "Yuqing Zeng",
            "Yixun Zhang",
            "Zejia Weng",
            "Zhonghao Yan",
            "Zhiguo Huang",
            "Zhuoyu Wang",
            "Zheng Ge",
            "Jing Li",
            "Yibo Zhu",
            "Binxing Jiao",
            "Xiangyu Zhang",
            "Daxin Jiang"
        ],
        "tldr": "The paper introduces Step-GUI, a family of GUI automation models trained using a self-evolving pipeline with calibrated rewards, along with GUI-MCP for privacy-preserving deployment and AndroidDaily, a new benchmark for real-world mobile usage.",
        "tldr_zh": "该论文介绍了Step-GUI，一系列使用具有校准奖励的自进化管道训练的GUI自动化模型，以及用于保护隐私部署的GUI-MCP和AndroidDaily，这是一个针对真实世界移动使用的新基准测试。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "多模态大型语言模型的最新进展为图形用户界面（GUI）自动化解锁了前所未有的机遇。然而，一个根本性的挑战依然存在：如何在保持标注可靠性的同时，高效地获取高质量的训练数据？我们引入了一个由校准步骤奖励系统驱动的自进化训练流水线，该流水线通过轨迹级别的校准将模型生成的轨迹转化为可靠的训练信号，以降低10-100倍成本的代价实现>90%的标注准确率。利用这个流水线，我们推出了Step-GUI，一个模型家族（4B/8B），实现了最先进的GUI性能（8B：AndroidWorld为80.2%，OSWorld为48.5%，ScreenShot-Pro为62.6%），同时保持了强大的通用能力。随着GUI代理能力的提高，实际部署要求跨异构设备的标准化接口，同时保护用户隐私。为此，我们提出了GUI-MCP，这是首个用于GUI自动化的模型上下文协议，它采用分层架构，结合了低级别的原子操作和对本地专家模型的高级别任务委托，从而实现高隐私执行，将敏感数据保留在设备上。最后，为了评估代理是否能够处理真实的日常使用场景，我们推出了AndroidDaily，这是一个基于真实世界移动使用模式的基准，包含3146个静态动作和235个跨高频日常场景的端到端任务（8B：静态89.91%，端到端52.50%）。我们的工作推进了实用GUI代理的开发，并展示了在日常数字交互中进行实际部署的强大潜力。"
    },
    {
        "title": "MMGR: Multi-Modal Generative Reasoning",
        "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.",
        "url": "http://arxiv.org/abs/2512.14691v2",
        "published_date": "2025-12-16T18:58:04+00:00",
        "updated_date": "2025-12-17T18:42:37+00:00",
        "categories": [
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Zefan Cai",
            "Haoyi Qiu",
            "Tianyi Ma",
            "Haozhe Zhao",
            "Gengze Zhou",
            "Kung-Hsiang Huang",
            "Parisa Kordjamshidi",
            "Minjia Zhang",
            "Wen Xiao",
            "Jiuxiang Gu",
            "Nanyun Peng",
            "Junjie Hu"
        ],
        "tldr": "The paper introduces MMGR, a multi-modal benchmark for evaluating the reasoning abilities (physical, logical, spatial, temporal) of video and image generative models across various domains, revealing significant performance gaps, especially in abstract reasoning and long-horizon spatial planning.",
        "tldr_zh": "该论文介绍了MMGR，一个用于评估视频和图像生成模型在不同领域中推理能力（物理，逻辑，空间，时间）的多模态基准。结果显示，这些模型在抽象推理和长期空间规划方面存在显著的性能差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "视频基础模型能够生成视觉逼真且时间连贯的内容，但其作为世界模拟器的可靠性取决于它们是否能捕捉物理、逻辑和空间约束。现有指标，如Frechet视频距离（FVD），侧重于感知质量，而忽略了推理失败，包括因果关系、物理学和全局一致性的违反。我们引入了MMGR（多模态生成推理评估和基准测试），这是一个基于五种推理能力的原则性评估框架：物理、逻辑、3D空间、2D空间和时间。MMGR评估跨越三个领域的生成推理：抽象推理（ARC-AGI, 数独）、具身导航（真实世界3D导航和定位）和物理常识（体育和组合交互）。MMGR运用细粒度的指标，这些指标要求视频和图像生成在整体上都是正确的。我们对领先的视频模型（Veo-3, Sora-2, Wan-2.2）和图像模型（Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image）进行了基准测试，揭示了跨领域的显著性能差距。模型在物理常识任务上表现出一定程度的成功，但在抽象推理方面表现不佳（ARC-AGI的准确率低于10%），并且在具身环境中的长程空间规划方面存在困难。我们的分析突出了当前模型的主要局限性，包括过度依赖感知数据、全局状态一致性薄弱以及奖励视觉合理性而非因果正确性的目标。MMGR提供了一个统一的诊断基准测试和一个通往推理感知型生成世界模型的途径。"
    },
    {
        "title": "OMCL: Open-vocabulary Monte Carlo Localization",
        "summary": "Robust robot localization is an important prerequisite for navigation planning. If the environment map was created from different sensors, robot measurements must be robustly associated with map features. In this work, we extend Monte Carlo Localization using vision-language features. These open-vocabulary features enable to robustly compute the likelihood of visual observations, given a camera pose and a 3D map created from posed RGB-D images or aligned point clouds. The abstract vision-language features enable to associate observations and map elements from different modalities. Global localization can be initialized by natural language descriptions of the objects present in the vicinity of locations. We evaluate our approach using Matterport3D and Replica for indoor scenes and demonstrate generalization on SemanticKITTI for outdoor scenes.",
        "url": "http://arxiv.org/abs/2512.15557v1",
        "published_date": "2025-12-17T16:08:53+00:00",
        "updated_date": "2025-12-17T16:08:53+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Evgenii Kruzhkov",
            "Raphael Memmesheimer",
            "Sven Behnke"
        ],
        "tldr": "This paper introduces Open-vocabulary Monte Carlo Localization (OMCL) using vision-language features for robust robot localization, enabling association of observations and map elements from different modalities, even with maps created from different sensors or modalities. They demonstrate generalization across various datasets.",
        "tldr_zh": "本文介绍了一种使用视觉-语言特征的开放词汇蒙特卡洛定位（OMCL）方法，用于实现鲁棒的机器人定位，即使在地图由不同传感器或模态创建的情况下，也能够关联观察结果和地图元素。作者在多个数据集上验证了其泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "稳健的机器人定位是导航规划的重要前提。如果环境地图是由不同的传感器创建的，那么机器人测量必须与地图特征进行稳健关联。在这项工作中，我们扩展了使用视觉-语言特征的蒙特卡洛定位方法。这些开放词汇特征能够稳健地计算视觉观测的似然性，给定一个相机位姿以及一个由带位姿RGB-D图像或对齐点云创建的3D地图。抽象的视觉-语言特征能够关联来自不同模态的观测和地图元素。全局定位可以通过对位置附近存在的物体的自然语言描述来进行初始化。我们使用 Matterport3D 和 Replica 对室内场景评估了我们的方法，并展示了在 SemanticKITTI 上对户外场景的泛化能力。"
    },
    {
        "title": "MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training",
        "summary": "While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.",
        "url": "http://arxiv.org/abs/2512.15411v1",
        "published_date": "2025-12-17T12:59:41+00:00",
        "updated_date": "2025-12-17T12:59:41+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhenhan Yin",
            "Xuanhan Wang",
            "Jiahao Jiang",
            "Kaiyuan Deng",
            "Pengqi Chen",
            "Shuangle Li",
            "Chong Liu",
            "Xing Xu",
            "ingkuan Song",
            "Lianli Gao",
            "Heng Tao Shen"
        ],
        "tldr": "MiVLA proposes a vision-language-action model pre-trained with human-robot mutual imitation, leveraging kinematic rules for behavioral alignment to improve generalization in robot control tasks, achieving significant improvements over SOTA VLA models.",
        "tldr_zh": "MiVLA 提出了一种基于人机互模仿预训练的视觉-语言-动作模型，利用运动学规则进行行为对齐，以提高机器人控制任务的泛化能力，与最先进的 VLA 模型相比取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "摘要：\n虽然利用丰富的人类视频和模拟机器人数据为解决真实机器人数据的稀缺问题提供了一个可扩展的方案，但现有视觉-语言-动作模型(VLAs)的泛化能力仍然受到相机视角、视觉外观和具身形态失配的限制。为了克服这一限制，我们提出了MiVLA，一个由人-机互模仿预训练赋能的可泛化VLA，它利用人手和机械臂之间固有的行为相似性，为人类动作和机器人控制建立强大的行为先验基础。具体而言，我们的方法利用带有左/右手坐标系的运动学规则来实现人与机器人动作空间之间的双向对齐。给定人类或模拟机器人演示，MiVLA被训练来预测一种具身的行为轨迹，并模仿另一种在演示中未见过的具身的行为。基于这种互模仿，它将真实世界人类数据的行为保真度与模拟机器人数据的操作多样性集成到一个统一的模型中，从而增强了下游任务的泛化能力。在包含三种机器人（ARX、PiPer和LocoMan）的模拟和真实世界平台上的大量实验表明，MiVLA实现了显著提升的泛化能力，在模拟环境中优于最先进的VLAs（例如，$\\boldsymbolπ_{0}$, $\\boldsymbolπ_{0.5}$和H-RDT）25%，在真实世界机器人控制任务中优于14%。"
    },
    {
        "title": "VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments",
        "summary": "This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.",
        "url": "http://arxiv.org/abs/2512.15258v1",
        "published_date": "2025-12-17T10:02:55+00:00",
        "updated_date": "2025-12-17T10:02:55+00:00",
        "categories": [
            "cs.RO",
            "cs.AI"
        ],
        "authors": [
            "Yuze Wu",
            "Mo Zhu",
            "Xingxing Li",
            "Yuheng Du",
            "Yuxin Fan",
            "Wenjun Li",
            "Xin Zhou",
            "Fei Gao"
        ],
        "tldr": "The paper introduces VLA-AN, an efficient vision-language-action framework for autonomous drone navigation, addressing limitations of existing models through a new dataset, training framework, and optimized onboard deployment, achieving high success rates in complex environments.",
        "tldr_zh": "该论文介绍了VLA-AN，一种用于无人机自主导航的高效视觉-语言-动作框架。它通过新的数据集，训练框架和优化的板载部署来解决现有模型的局限性，并在复杂环境中实现了很高的成功率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文提出了一种高效的、适用于机载的视觉-语言-动作（VLA）框架VLA-AN，致力于复杂环境中无人机自主导航。VLA-AN解决了现有大型空中导航模型的四大局限性：数据域差距、基于推理的不足时序导航、生成式动作策略的安全问题以及机载部署约束。首先，我们构建了一个利用3D高斯溅射（3D-GS）的高保真数据集，有效弥合了领域差距。其次，我们引入了一个渐进式的三阶段训练框架，依次强化场景理解、核心飞行技能和复杂导航能力。第三，我们设计了一个轻量级的实时动作模块，并结合了几何安全校正。该模块确保快速、无碰撞和稳定的指令生成，从而减轻了随机生成策略中固有的安全风险。最后，通过对机载部署流水线的深度优化，VLA-AN在资源受限的无人机上实现了强大的实时推理吞吐量，提高了8.3倍。大量实验表明，VLA-AN显著提高了空间定位、场景推理和长程导航能力，实现了高达98.1%的单任务成功率，并为在轻型空中机器人中实现全链闭环自主性提供了一种高效实用的解决方案。"
    },
    {
        "title": "HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles",
        "summary": "3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.",
        "url": "http://arxiv.org/abs/2512.15047v1",
        "published_date": "2025-12-17T03:22:27+00:00",
        "updated_date": "2025-12-17T03:22:27+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Yunheng Wang",
            "Yixiao Feng",
            "Yuetong Fang",
            "Shuning Zhang",
            "Tan Jing",
            "Jian Li",
            "Xiangrui Jiang",
            "Renjing Xu"
        ],
        "tldr": "This paper introduces HERO, a novel framework for building hierarchical traversable 3D scene graphs that accounts for movable obstacles, enabling more efficient and reachable embodied navigation in complex environments.",
        "tldr_zh": "该论文介绍了一种名为HERO的新框架，用于构建分层可遍历的3D场景图，该框架考虑了可移动障碍物，从而能够在复杂环境中实现更高效和可达的具身导航。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "3D场景图(3DSG)构成了物理世界的一种强大表示形式，其突出特点在于能够显式地建模实体之间复杂的空间、语义和功能关系，从而奠定基础性的理解，使得智能体能够与其环境进行智能交互并执行多功能的行为。具身导航作为这种能力的关键组成部分，利用3DSG的紧凑性和表达性，从而能够在复杂、大规模环境中进行长时程的推理和规划。然而，先前的工作依赖于静态世界的假设，仅基于静态空间布局来定义可通行空间，因此将可交互的障碍物视为不可通行。这种根本性的限制严重削弱了它们在现实世界场景中的有效性，导致可达性有限、效率低下和可扩展性差。为了解决这些问题，我们提出了HERO，一种用于构建分层可通行3DSG的新颖框架，它通过将可操作的障碍物建模为路径来重新定义可通行性，捕获了它们的物理交互性、功能语义以及场景的关系层次结构。结果表明，相对于其基线，HERO在部分受阻环境中将路径长度（PL）降低了35.1%，在完全受阻环境中将成功率（SR）提高了79.4%，证明了其显著更高的效率和可达性。"
    },
    {
        "title": "ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision",
        "summary": "Vision-based imitation learning has enabled impressive robotic manipulation skills, but its reliance on object appearance while ignoring the underlying 3D scene structure leads to low training efficiency and poor generalization. To address these challenges, we introduce \\emph{Implicit Scene Supervision (ISS) Policy}, a 3D visuomotor DiT-based diffusion policy that predicts sequences of continuous actions from point cloud observations. We extend DiT with a novel implicit scene supervision module that encourages the model to produce outputs consistent with the scene's geometric evolution, thereby improving the performance and robustness of the policy. Notably, ISS Policy achieves state-of-the-art performance on both single-arm manipulation tasks (MetaWorld) and dexterous hand manipulation (Adroit). In real-world experiments, it also demonstrates strong generalization and robustness. Additional ablation studies show that our method scales effectively with both data and parameters. Code and videos will be released.",
        "url": "http://arxiv.org/abs/2512.15020v1",
        "published_date": "2025-12-17T02:20:21+00:00",
        "updated_date": "2025-12-17T02:20:21+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Wenlong Xia",
            "Jinhao Zhang",
            "Ce Zhang",
            "Yaojia Wang",
            "Youmin Gong",
            "Jie Mei"
        ],
        "tldr": "The paper introduces Implicit Scene Supervision (ISS) Policy, a diffusion policy for robotic manipulation that uses point cloud observations and implicitly supervises the model with scene geometry to improve training efficiency and generalization, achieving state-of-the-art results on several manipulation tasks.",
        "tldr_zh": "该论文介绍了一种名为隐式场景监督 (ISS) Policy 的扩散策略，用于机器人操作。该策略使用点云观测，并通过场景几何隐式地监督模型，以提高训练效率和泛化能力，在多个操作任务上取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于视觉的模仿学习已经实现了令人瞩目的机器人操作技能，但它依赖于物体外观，而忽略了底层3D场景结构，导致训练效率低下和泛化能力差。为了解决这些挑战，我们引入了\\emph{隐式场景监督 (ISS) 策略}，这是一种基于3D视觉运动扩散Transformer (DiT) 的扩散策略，它从点云观测中预测连续动作序列。我们使用一种新颖的隐式场景监督模块扩展了 DiT，该模块鼓励模型产生与场景几何演化一致的输出，从而提高策略的性能和鲁棒性。值得注意的是，ISS 策略在单臂操作任务 (MetaWorld) 和灵巧手操作 (Adroit) 上都达到了最先进的性能。在真实世界的实验中，它还表现出强大的泛化能力和鲁棒性。额外的消融研究表明，我们的方法可以有效地扩展数据和参数。代码和视频将会发布。"
    },
    {
        "title": "CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives",
        "summary": "We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\\% to 6.9\\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.",
        "url": "http://arxiv.org/abs/2512.14696v1",
        "published_date": "2025-12-16T18:59:50+00:00",
        "updated_date": "2025-12-16T18:59:50+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.RO"
        ],
        "authors": [
            "Zihan Wang",
            "Jiashun Wang",
            "Jeff Tan",
            "Yiwen Zhao",
            "Jessica Hodgins",
            "Shubham Tulsiani",
            "Deva Ramanan"
        ],
        "tldr": "CRISP recovers simulatable human motion and scene geometry from monocular video by fitting planar primitives to a point cloud reconstruction and using human-scene contact modeling to improve physics-based simulation, outperforming previous methods on motion tracking and simulation throughput.",
        "tldr_zh": "CRISP通过将平面基元拟合到点云重建，并使用人与场景的接触建模来改进基于物理的模拟，从单目视频中恢复可模拟的人体运动和场景几何，在运动跟踪和模拟吞吐量方面优于以前的方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们介绍了CRISP，一种从单目视频中恢复可模拟的人体运动和场景几何的方法。先前关于人体-场景联合重建的工作依赖于数据驱动的先验知识和不包含物理引擎的联合优化，或者恢复具有伪影的噪声几何体，导致带有场景交互的运动跟踪策略失败。相比之下，我们的关键洞察在于通过将平面基元拟合到场景的点云重建中，来恢复凸的、干净的、且可用于模拟的几何体，这通过一个基于深度、法线和光流的简单聚类流程实现。为了重建在交互过程中可能被遮挡的场景几何，我们利用了人体-场景接触建模（例如，我们使用人体姿势来重建椅子的被遮挡的座位）。最后，我们通过使用人体和场景重建来驱动由强化学习驱动的类人控制者，从而确保人和场景的重建在物理上是合理的。我们的方法在以人为中心的视频基准（EMDB、PROX）上将运动跟踪失败率从 55.2% 降低到 6.9%，同时实现了快 43% 的强化学习模拟吞吐量。我们在真实世界的视频上进一步验证了它，包括随手拍摄的视频、互联网视频，甚至是由Sora生成的视频。这证明了CRISP大规模生成物理上有效的人体运动和交互环境的能力，极大地推进了机器人和AR/VR的实到虚应用。"
    },
    {
        "title": "Soft Geometric Inductive Bias for Object Centric Dynamics",
        "summary": "Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.",
        "url": "http://arxiv.org/abs/2512.15493v1",
        "published_date": "2025-12-17T14:40:37+00:00",
        "updated_date": "2025-12-17T14:40:37+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "stat.ML"
        ],
        "authors": [
            "Hampus Linander",
            "Conor Heins",
            "Alexander Tschantz",
            "Marco Perin",
            "Christopher Buckley"
        ],
        "tldr": "This paper introduces object-centric world models using geometric algebra neural networks for learning 2D rigid body dynamics, demonstrating improved long-horizon prediction accuracy due to soft geometric inductive bias compared to non-equivariant baselines.",
        "tldr_zh": "本文提出了一种基于几何代数神经网络的以对象为中心的世界模型，用于学习二维刚体动力学。实验表明，与非等变基线模型相比，该模型由于软几何归纳偏置，在长时程预测中表现出更高的精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "等变性是学习物理动力学的强大先验，但如果对称性被打破，精确群等变性会降低性能。我们提出了一种以对象为中心的世界模型，该模型构建于几何代数神经网络之上，提供了一种软几何归纳偏置。我们的模型在具有静态障碍物的二维刚体动力学模拟环境中进行评估，我们在其中自回归地训练下一步预测。对于长程展开，我们表明，与非等变基线模型相比，我们模型的软归纳偏置在物理保真度方面产生了更好的性能。该方法补充了最近的软等变性思想，并与以下观点一致：简单且经过精心选择的先验可以产生稳健的泛化能力。这些结果表明，几何代数在手工制作的物理引擎和非结构化深度网络之间提供了一种有效的中间地带，为多对象场景提供了样本高效的动力学模型。"
    },
    {
        "title": "Double Horizon Model-Based Policy Optimization",
        "summary": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long \"distribution rollout\" (DR) and a short \"training rollout\" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.",
        "url": "http://arxiv.org/abs/2512.15439v1",
        "published_date": "2025-12-17T13:37:23+00:00",
        "updated_date": "2025-12-17T13:37:23+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Akihiro Kubo",
            "Paavo Parmas",
            "Shin Ishii"
        ],
        "tldr": "This paper introduces Double Horizon Model-Based Policy Optimization (DHMBPO), which uses a long rollout for distribution shift mitigation and a short rollout for stable gradient estimation, improving sample efficiency and runtime in MBRL.",
        "tldr_zh": "本文提出了双视界模型策略优化（DHMBPO），它使用长程rollout来缓解分布偏移，并使用短程rollout来实现稳定的梯度估计，从而提高MBRL中的样本效率和运行时间。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于模型的强化学习（MBRL）通过从学习到的动力学模型中生成合成轨迹（称为rollouts）来降低真实环境采样的成本。然而，选择 rollouts 的长度会带来两个困境：（1）较长的 rollouts 更好地保留了 on-policy 训练，但会放大模型偏差，表明需要一个中间范围来缓解分布偏移（即 on-policy 和过去 off-policy 样本之间的差距）。（2）此外，较长的模型 rollout 可能会降低价值估计的偏差，但由于通过多个步骤的反向传播会提高策略梯度的方差，这意味着需要另一个中间范围以获得稳定的梯度估计。然而，这两个最佳范围可能不同。为了解决这个冲突，我们提出了双范围基于模型的策略优化（DHMBPO），它将 rollout 过程分为一个长的“分布 rollout”（DR）和一个短的“训练 rollout”（TR）。DR 生成 on-policy 状态样本，以减轻分布偏移。相比之下，短的 TR 利用可微的转换来提供准确的价值梯度估计以及稳定的梯度更新，从而减少所需的更新次数并缩短整体运行时间。我们证明了双范围方法有效地平衡了分布偏移、模型偏差和梯度不稳定，并且在样本效率和运行时间方面都优于连续控制基准测试上的现有 MBRL 方法。"
    },
    {
        "title": "FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments",
        "summary": "Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.",
        "url": "http://arxiv.org/abs/2512.15430v1",
        "published_date": "2025-12-17T13:26:17+00:00",
        "updated_date": "2025-12-17T13:26:17+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Quanxi Zhou",
            "Wencan Mao",
            "Manabu Tsukada",
            "John C. S. Lui",
            "Yusheng Ji"
        ],
        "tldr": "The paper introduces FM-EAC, a novel reinforcement learning algorithm combining model-based and model-free approaches with feature-based models and an enhanced actor-critic framework for improved generalization in multi-task control, demonstrating superior performance in urban and agricultural simulations.",
        "tldr_zh": "该论文介绍了一种新的强化学习算法FM-EAC，它结合了基于模型和无模型的方法，利用基于特征的模型和增强的Actor-Critic框架，以提高多任务控制中的泛化能力，并在城市和农业模拟中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "基于模型的强化学习（MBRL）和无模型的强化学习（MFRL）沿着不同的路径演进，但在Dyna-Q [1]的设计中趋同。然而，现代强化学习方法仍然难以在不同任务和场景中实现有效的可迁移性。受限于此，我们提出一种广义算法，即基于特征模型的增强型Actor-Critic算法（FM-EAC），它集成了规划、行动和学习，用于动态环境中的多任务控制。FM-EAC结合了MBRL和MFRL的优势，并通过使用新型的基于特征的模型和增强型的Actor-Critic框架来提高泛化能力。在城市和农业应用中的仿真结果表明，FM-EAC始终优于许多最先进的MBRL和MFRL方法。更重要的是，可以在FM-EAC内部定制不同的子网络，以满足用户的特定需求。"
    },
    {
        "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
        "summary": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
        "url": "http://arxiv.org/abs/2512.15374v1",
        "published_date": "2025-12-17T12:25:05+00:00",
        "updated_date": "2025-12-17T12:25:05+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Zehua Pei",
            "Hui-Ling Zhen",
            "Shixiong Kai",
            "Sinno Jialin Pan",
            "Yunhe Wang",
            "Mingxuan Yuan",
            "Bei Yu"
        ],
        "tldr": "The paper introduces SCOPE, a method for automatically evolving LLM agent prompts based on execution traces to improve context management and task success in dynamic environments. It outperforms static prompts on the HLE benchmark.",
        "tldr_zh": "该论文介绍了SCOPE，一种基于执行轨迹自动演化LLM代理提示的方法，以提高动态环境中的上下文管理和任务成功率。在HLE基准测试中，它优于静态提示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "大型语言模型(LLM)代理越来越多地部署在生成大规模、动态上下文的环境中。然而，一个关键的瓶颈仍然存在：虽然代理可以访问这些上下文，但它们静态的提示缺乏有效地管理上下文的机制，导致反复出现纠错和增强失败。为了解决这种能力缺口，我们引入了\\textbf{SCOPE}（通过提示演化的自演化上下文优化）。SCOPE将上下文管理构建为一个\\textit{在线优化}问题，从执行轨迹中综合指导原则，以自动演化代理的提示。我们提出了一种双流机制，该机制平衡了战术上的具体性（解决即时错误）和战略上的通用性（演化长期原则）。此外，我们引入视角驱动探索来最大化策略覆盖范围，从而增加代理针对任何给定任务都能拥有正确策略的可能性。在HLE基准测试上的实验表明，SCOPE在无需人工干预的情况下，将任务成功率从14.23%提高到38.64%。我们的代码已在https://github.com/JarvisPei/SCOPE上公开。"
    },
    {
        "title": "Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) significantly enhances the reasoning capability of Large Language Models (LLMs). Current RLVR approaches typically conduct training across all generated tokens, but neglect to explore which tokens (e.g., prefix tokens) actually contribute to reasoning. This uniform training strategy spends substantial effort on optimizing low-return tokens, which in turn impedes the potential improvement from high-return tokens and reduces overall training effectiveness. To address this issue, we propose a novel RLVR approach called Progressive Prefix-token Policy Optimization (PPPO), which highlights the significance of the prefix segment of generated outputs. Specifically, inspired by the well-established human thinking theory of Path Dependence, where early-stage thoughts substantially constrain subsequent thinking trajectory, we identify an analogous phenomenon in LLM reasoning termed Beginning Lock-in Effect (BLE). PPPO leverages this finding by focusing its optimization objective on the prefix reasoning process of LLMs. This targeted optimization strategy can positively influence subsequent reasoning processes, and ultimately improve final results. To improve the learning effectiveness of LLMs on how to start reasoning with high quality, PPPO introduces two training strategies: (a) Progressive Prefix Retention, which shapes a progressive learning process by increasing the proportion of retained prefix tokens during training; (b) Continuation Accumulated Reward, which mitigates reward bias by sampling multiple continuations for one prefix token sequence, and accumulating their scores as the reward signal. Extensive experimental results on various reasoning tasks demonstrate that our proposed PPPO outperforms representative RLVR methods, with the accuracy improvements of 18.02% on only 26.17% training tokens.",
        "url": "http://arxiv.org/abs/2512.15274v1",
        "published_date": "2025-12-17T10:26:11+00:00",
        "updated_date": "2025-12-17T10:26:11+00:00",
        "categories": [
            "cs.CL",
            "cs.AI"
        ],
        "authors": [
            "Yiliu Sun",
            "Zicheng Zhao",
            "Yang Wei",
            "Yanfang Zhang",
            "Chen Gong"
        ],
        "tldr": "This paper introduces Progressive Prefix-token Policy Optimization (PPPO), a Reinforcement Learning with Verifiable Rewards (RLVR) approach that focuses optimization on the prefix segment of LLM generated outputs to improve reasoning accuracy by addressing the Beginning Lock-in Effect (BLE). Experiments show accuracy improvements of 18.02% with only 26.17% training tokens.",
        "tldr_zh": "本文提出了一种名为渐进式前缀标记策略优化 (PPPO) 的方法，这是一种基于可验证奖励的强化学习 (RLVR) 方法，其优化重点放在 LLM 生成输出的前缀部分，通过解决起始锁定效应 (BLE) 来提高推理准确性。实验表明，仅使用 26.17% 的训练 tokens 即可提高 18.02% 的准确率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "具有可验证奖励的强化学习（RLVR）显著增强了大型语言模型（LLM）的推理能力。目前的RLVR方法通常对所有生成的词元进行训练，但忽略了探索哪些词元（例如，前缀词元）实际有助于推理。这种统一的训练策略花费大量精力来优化低回报词元，反过来又阻碍了高回报词元的潜在改进，并降低了整体训练效率。为了解决这个问题，我们提出了一种新颖的RLVR方法，称为渐进式前缀词元策略优化（PPPO），它突出了生成输出的前缀片段的重要性。具体来说，受到成熟的“路径依赖”人类思维理论的启发，该理论认为早期思维会极大地约束后续思维轨迹，我们在LLM推理中发现了一种类似的现象，称为“开端锁定效应”（BLE）。 PPPO通过将优化目标集中在LLM的前缀推理过程上来利用这一发现。这种有针对性的优化策略可以积极影响后续推理过程，并最终改善最终结果。为了提高LLM学习如何高质量地启动推理的学习效率，PPPO引入了两种训练策略：（a）渐进式前缀保留，通过在训练期间增加保留的前缀词元的比例来形成渐进式学习过程；（b）延续累积奖励，通过为一个前缀词元序列采样多个延续，并将其分数累加作为奖励信号来减轻奖励偏差。在各种推理任务上的大量实验结果表明，我们提出的PPPO优于具有代表性的RLVR方法，仅使用26.17%的训练词元，准确率提高了18.02%。"
    },
    {
        "title": "Automatic Reward Shaping from Multi-Objective Human Heuristics",
        "summary": "Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.",
        "url": "http://arxiv.org/abs/2512.15120v1",
        "published_date": "2025-12-17T06:24:38+00:00",
        "updated_date": "2025-12-17T06:24:38+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Yuqing Xie",
            "Jiayu Chen",
            "Wenhao Tang",
            "Ya Zhang",
            "Chao Yu",
            "Yu Wang"
        ],
        "tldr": "The paper introduces MORSE, a framework for automatically shaping reward functions in multi-objective RL by combining human heuristics through bi-level optimization, achieving results comparable to manual tuning in robotic tasks.",
        "tldr_zh": "该论文介绍了MORSE，一个自动塑造多目标强化学习中奖励函数的框架，通过双层优化结合人类启发式算法，并在机器人任务中实现了与手动调整相当的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "在强化学习中，设计有效的奖励函数仍然是一个核心挑战，尤其是在多目标环境中。本文提出了一种基于探索的多目标奖励塑造（MORSE）方法，这是一个通用框架，可自动将多个人工设计的启发式奖励集成到一个统一的奖励函数中。MORSE将塑造过程形式化为一个双层优化问题：内循环训练策略，以最大化当前的塑造奖励；外循环更新奖励函数，以优化任务性能。为了鼓励在奖励空间中进行探索并避免次优局部最小值，MORSE在塑造过程中引入了随机性，注入由任务性能和固定、随机初始化的神经网络的预测误差所引导的噪声。在MuJoCo和Isaac Sim环境中的实验结果表明，MORSE能够有效地平衡各种机器人任务中的多个目标，实现与手动调整奖励函数所获得的任务性能相当的性能。"
    },
    {
        "title": "LADY: Linear Attention for Autonomous Driving Efficiency without Transformers",
        "summary": "End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.",
        "url": "http://arxiv.org/abs/2512.15038v2",
        "published_date": "2025-12-17T03:03:40+00:00",
        "updated_date": "2025-12-18T04:52:38+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Jihao Huang",
            "Xi Xia",
            "Zhiyuan Li",
            "Tianle Liu",
            "Jingke Wang",
            "Junbo Chen",
            "Tengju Ye"
        ],
        "tldr": "The paper introduces LADY, a fully linear attention-based generative model for end-to-end autonomous driving that achieves state-of-the-art performance with constant-time and memory complexity, validated on edge devices.",
        "tldr_zh": "该论文介绍了一种名为LADY的完全线性注意力生成模型，用于端到端自动驾驶，在边缘设备上验证后，以恒定的时间和内存复杂度实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "端到端范式已展现出自动驾驶的巨大潜力。此外，目前大多数方法都构建于Transformer架构之上。然而，Transformer会产生二次方级别的注意力计算成本，限制了其对长时空序列进行建模的能力——尤其是在资源受限的边缘平台上。由于自动驾驶本质上需要高效的时序建模，这一挑战严重限制了它们的部署和实时性能。近年来，线性注意力机制因其优越的时空复杂度而受到越来越多的关注。然而，现有的线性注意力架构仅限于自注意力，缺乏对跨模态和跨时序交互的支持——而这两者对于自动驾驶至关重要。在这项工作中，我们提出了LADY，这是首个基于全线性注意力的端到端自动驾驶生成模型。LADY能够在推理时融合长程时序上下文，且计算和内存成本恒定，与相机和激光雷达特征的历史长度无关。此外，我们还引入了一种轻量级的线性交叉注意力机制，能够实现有效的跨模态信息交换。在NAVSIM和Bench2Drive基准测试上的实验表明，LADY以恒定的时间和内存复杂度实现了最先进的性能，提供了改进的规划性能并显著降低了计算成本。此外，该模型已在边缘设备上部署和验证，证明了其在资源有限场景中的实用性。"
    },
    {
        "title": "Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections",
        "summary": "A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.",
        "url": "http://arxiv.org/abs/2512.14895v1",
        "published_date": "2025-12-16T20:19:07+00:00",
        "updated_date": "2025-12-16T20:19:07+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Niklas Lauffer",
            "Xiang Deng",
            "Srivatsa Kundurthy",
            "Brad Kenstler",
            "Jeff Da"
        ],
        "tldr": "This paper introduces On-policy Expert Corrections (OECs), a novel data generation method inspired by DAgger, to mitigate covariate shift in imitation learning for multi-turn LM agents, demonstrating improvements in software engineering tasks.",
        "tldr_zh": "本文提出了一种名为On-policy Expert Corrections (OECs) 的新数据生成方法，灵感来自DAgger，旨在缓解多轮LM智能体模仿学习中的协变量偏移，并在软件工程任务中展示了改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "训练语言模型代理的一种流行范式依赖于模仿学习，即基于专家轨迹进行微调。然而，我们表明，多轮语言模型代理的模仿学习的离策略特性受到称为协变量偏移的基本限制：随着学生策略的行为偏离专家策略，它会遇到训练数据中不存在的状态，从而降低微调的有效性。受到经典DAgger算法的启发，我们提出了一种新的数据生成方法，用于解决多轮大型语言模型训练中的协变量偏移。我们引入了在线策略专家修正（OEC），这是一种部分在线策略数据，它通过使用学生模型开始rollout，然后在轨迹中途切换到专家模型来生成。我们在软件工程（SWE）任务领域探索了我们数据生成技术的有效性，这是一个多轮设置，其中大型语言模型代理必须与开发环境交互以修复软件错误。我们的实验将OEC数据与SWE代理问题上的各种其他在线策略和模仿学习方法进行了比较，并使用一种常见的拒绝采样（即，使用环境奖励）与监督微调技术相结合来训练模型。实验发现，在SWE-bench verified上，在7b和32b设置中，OEC轨迹相对于传统模仿学习分别表现出14％和13％的相对改进。我们的结果表明，为了有效训练多轮语言模型代理，需要将专家演示与在线策略数据相结合。"
    },
    {
        "title": "Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse",
        "summary": "Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.",
        "url": "http://arxiv.org/abs/2512.14879v1",
        "published_date": "2025-12-16T19:50:03+00:00",
        "updated_date": "2025-12-16T19:50:03+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Jingwei Chen"
        ],
        "tldr": "This paper introduces Entropy-Reservoir Bregman Projection (ERBP), a theoretical framework that unifies various stabilization techniques for self-referential learning by modeling it as a Bregman projection sequence with a controllable entropy flux, addressing model collapse issues in LLMs, RL, and GANs.",
        "tldr_zh": "本文提出了熵-储水池布雷格曼投影(ERBP)，一个理论框架，通过将自引用学习建模为具有可控熵通量的布雷格曼投影序列，统一了各种稳定技术，解决了LLM、RL和GAN中的模型崩溃问题。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自引用学习——利用模型自身生成的数据训练模型——具有无限的可扩展性，但长期以来饱受模型崩塌之苦：语言模型退化为重复文本，GAN丢弃模式，强化学习策略过度利用。尽管从业者采用了诸如真实数据混合、熵奖励、知识蒸馏或检索增强生成等临时性修复方法，但解释这种失效模式和这些修复措施成功原因的单一原则仍然难以捉摸。我们提出了熵储备 Bregman 投影 (ERBP)，这是一个信息几何框架，可以统一这些现象。我们将闭环建模为分布空间中的随机 Bregman 投影序列。在没有外部耦合的情况下，有限样本噪声迫使系统投影到不断缩小的经验支持上，导致指数级的熵衰减和最终的崩塌。引入熵储备——混合到每次投影中的高熵分布——注入可控的熵通量，从而可证明地稳定动力学。我们的理论产生了 (i) 崩塌的必要条件，(ii) 保证非平凡熵底的充分条件，以及 (iii) 仅取决于样本大小和 Bregman 生成器的强凸/Lipschitz 常数的闭式速率。在大型语言模型自训练、强化学习中的 Soft Actor-Critic 和 GAN 优化上的实验验证了我们的预测，并表明不同的稳定化启发式方法对应于特定的储备选择和耦合系数。因此，ERBP 将一系列民间疗法转化为一个单一的、定量的设计规则：监控并预算你的熵通量。"
    },
    {
        "title": "OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence",
        "summary": "Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: \"what would happen given a specific future action\". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.",
        "url": "http://arxiv.org/abs/2512.15621v1",
        "published_date": "2025-12-17T17:29:20+00:00",
        "updated_date": "2025-12-17T17:29:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Zheng",
            "Jie Hu",
            "Kailun Yang",
            "Jiaming Zhang"
        ],
        "tldr": "The paper introduces OccSTeP, a new benchmark and world model (OccSTeP-WM) for 4D occupancy spatio-temporal persistence in autonomous driving, addressing reactive and proactive forecasting with robust performance against noisy sensor data.",
        "tldr_zh": "本文介绍了一个新的基准测试 OccSTeP 和一个世界模型 (OccSTeP-WM)，用于自动驾驶中的 4D occupancy spatio-temporal persistence，解决了反应式和主动式预测问题，并且对噪声传感器数据具有鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "自动驾驶需要对3D场景进行持久理解，这种理解能够应对时间扰动并考虑到潜在的未来动作。我们引入了一种新的概念，即4D占据时空持久性(OccSTeP)，旨在解决两个任务：（1）反应式预测：“接下来会发生什么”，以及（2）主动式预测：“如果采取特定未来动作会发生什么”。我们首次创建了一个新的OccSTeP基准，其中包含具有挑战性的场景（例如，有错误的语义标签和丢帧）。为了解决这个任务，我们提出了OccSTeP-WM，一种无分词器的世界模型，其维护一个密集的基于体素的场景状态，并随着时间推移增量式地融合时空上下文。OccSTeP-WM利用线性复杂度的注意力骨干网络和一个循环状态空间模块来捕获长程空间依赖关系，同时通过自运动补偿持续更新场景记忆。这种设计实现了在线推理和鲁棒的性能，即使在丢失或嘈杂的历史传感器输入的情况下。大量的实验证明了OccSTeP概念和我们的OccSTeP-WM的有效性，产生了平均语义mIoU为 23.70%（+6.56%的提升）和占据IoU为 35.89%（+9.26%的提升）。数据和代码将在https://github.com/FaterYU/OccSTeP开源。"
    },
    {
        "title": "MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors",
        "summary": "In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.",
        "url": "http://arxiv.org/abs/2512.15577v1",
        "published_date": "2025-12-17T16:36:16+00:00",
        "updated_date": "2025-12-17T16:36:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhipeng Du",
            "Duolikun Danier",
            "Jan Eric Lenssen",
            "Hakan Bilen"
        ],
        "tldr": "The paper introduces MoonSeg3R, a novel method for online zero-shot monocular 3D instance segmentation using a reconstructive foundation model to provide geometric priors from a single RGB stream, achieving competitive results with RGB-D based systems.",
        "tldr_zh": "该论文介绍了MoonSeg3R，一种新颖的在线零样本单目3D实例分割方法，它利用重建基础模型从单个RGB流中提供几何先验，实现了与基于RGB-D系统的竞争力。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文重点研究在线零样本单目3D实例分割，这是一个新颖的实际场景，现有方法由于依赖姿态RGB-D序列而无法执行。为了克服这一局限，我们利用了CUT3R，一种最新的重建基础模型（RFM），以从单个RGB流中提供可靠的几何先验。我们提出了MoonSeg3R，它引入了三个关键组件：（1）一个具有空间语义蒸馏的自监督查询细化模块，该模块将来自2D视觉基础模型（VFM）的分割掩码转换为判别性3D查询；（2）一个3D查询索引记忆，通过检索上下文查询来提供时间一致性；以及（3）一个来自CUT3R的状态分布令牌，作为掩码身份描述符，以加强跨帧融合。在ScanNet200和SceneNN上的实验表明，MoonSeg3R是第一个实现在线单目3D分割的方法，并取得了与最先进的基于RGB-D的系统相媲美的性能。代码和模型将会发布。"
    },
    {
        "title": "EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence",
        "summary": "Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for \"thinking with images\" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.",
        "url": "http://arxiv.org/abs/2512.15160v1",
        "published_date": "2025-12-17T07:51:36+00:00",
        "updated_date": "2025-12-17T07:51:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxu Wan",
            "Xu Wang",
            "Mengwei Xie",
            "Hang Zhang",
            "Mu Xu",
            "Yang Han",
            "Hong Zhang",
            "Ding Yuan",
            "Yifan Yang"
        ],
        "tldr": "EagleVision is a dual-stage vision-language framework using BEV-grounded Chain-of-Thought for spatial reasoning, achieving SOTA on VSI-Bench via a novel semantics-perspective-fusion keyframe selection and reinforcement learning approach.",
        "tldr_zh": "EagleVision是一个双阶段视觉语言框架，采用基于BEV的思维链进行空间推理。它通过一种新颖的语义-视角融合关键帧选择和强化学习方法，在VSI-Bench上实现了SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "近期空间智能方法通常将 3D 线索附加到 2D 推理流程中，或将 MLLM 与黑盒重建模块耦合，导致空间一致性较弱、视点多样性有限，以及证据链无法追溯到支持视图。诸如 \"通过图像思考\" (例如，ChatGPT-o3 和 DeepEyes) 的框架表明，通过将假设形成与主动获取视觉证据相结合，可以涌现出逐步多模态推理，但它们没有解决空间思维链 (CoT) 中的三个关键挑战：在严格的 token 预算下构建全局空间感知，将 3D 假设与视频帧显式关联以进行验证，以及为强化学习设计空间定位奖励。 为了解决这些问题，我们提出了 EagleVision，一个通过宏观感知和微观验证来实现渐进式空间认知的双阶段框架。 在宏观感知阶段，EagleVision 采用语义-透视-融合行列点过程 (SPF-DPP) 在固定 token 预算下，从长视频中选择一组紧凑的、具有几何和语义意识的关键帧集合。 在微观验证阶段，我们将空间 CoT 形式化为基于 BEV 的姿态查询：智能体在 BEV 平面上迭代预测姿态，检索最近的真实帧，并通过仅使用强化学习进行训练，其中强化学习采用空间定位奖励来评估预测姿态与观察视图之间的一致性。 在 VSI-Bench 上，EagleVision 在开源视觉-语言模型中实现了最先进的性能，展示了强大且可泛化的空间理解能力。"
    },
    {
        "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning",
        "summary": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.",
        "url": "http://arxiv.org/abs/2512.14944v1",
        "published_date": "2025-12-16T22:17:25+00:00",
        "updated_date": "2025-12-16T22:17:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ahmadreza Jeddi",
            "Hakki Can Karaimer",
            "Hue Nguyen",
            "Zhongling Wang",
            "Ke Zhao",
            "Javad Rajabi",
            "Ran Zhang",
            "Raghav Goyal",
            "Babak Taati",
            "Radek Grzeszczuk"
        ],
        "tldr": "The paper introduces Puzzle Curriculum GRPO (PC-GRPO), a supervision-free reinforcement learning method for improving visual reasoning in VLMs using self-supervised puzzle environments and a difficulty-aware curriculum, enhancing training stability and downstream accuracy.",
        "tldr_zh": "该论文介绍了Puzzle Curriculum GRPO (PC-GRPO)，一种无需监督的强化学习方法，通过自监督的拼图环境和难度感知的课程来提升视觉语言模型中的视觉推理能力，从而增强训练稳定性和下游任务准确率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "最近的强化学习(RL)方法，如结果监督 GRPO，推动了视觉语言模型(VLM)中链式思维推理的发展，但仍存在关键问题：(i) 依赖于昂贵且嘈杂的手工管理标注或外部验证器；(ii) GRPO 中扁平且稀疏的奖励机制；以及 (iii) 链式推理与其最终答案之间的逻辑不一致。我们提出了拼图课程 GRPO (PC-GRPO)，一种使用可验证奖励的强化学习(RLVR)的无监督方法，无需标注或外部验证器即可增强 VLM 中的视觉推理能力。PC-GRPO 将标签替换为三个自监督拼图环境：PatchFit、旋转（具有二元奖励）和拼图（具有分级部分奖励，减轻奖励稀疏性）。为了对抗扁平奖励和递减的组相对优势，我们引入了一种难度感知课程，动态地对样本进行加权，并在中等难度处达到峰值。我们进一步在后训练期间监控推理-答案一致性(RAC)：与 LLM 中 vanilla GRPO 的报告相似，RAC 通常早期上升然后下降；我们的课程延迟了这种下降，并且强制一致性的奖励机制进一步提高了 RAC。RAC 与下游准确率相关。在各种基准测试以及 Qwen-7B 和 Qwen-3B 主干网络上，PC-GRPO 提高了推理质量、训练稳定性和最终任务准确率，为 VLM 的可扩展、可验证和可解释的 RL 后训练提供了一条实用途径。"
    },
    {
        "title": "EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning",
        "summary": "At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.",
        "url": "http://arxiv.org/abs/2512.15405v1",
        "published_date": "2025-12-17T12:55:05+00:00",
        "updated_date": "2025-12-17T12:55:05+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Jianfei Ma",
            "Wee Sun Lee"
        ],
        "tldr": "The paper introduces EUBRL, a Bayesian reinforcement learning algorithm that uses epistemic uncertainty to guide exploration and achieves near minimax-optimal regret in infinite-horizon discounted MDPs, demonstrating improved sample efficiency in complex RL tasks.",
        "tldr_zh": "该论文介绍了一种名为EUBRL的贝叶斯强化学习算法，该算法利用认知不确定性来指导探索，并在无限水平贴现MDP中实现了接近 minimax 最佳遗憾，并在复杂的强化学习任务中展示了更高的样本效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "在已知与未知的边界，智能体不可避免地会面临探索或利用的困境。认知不确定性反映了这样的边界，代表了由于知识有限而产生的系统性不确定性。在本文中，我们提出了一种贝叶斯强化学习（RL）算法$\\texttt{EUBRL}$，它利用认知指导来实现有原则的探索。这种指导能够自适应地减少由估计误差引起的每步遗憾。我们针对无限视界折扣马尔可夫决策过程（MDPs）中一类充分表达的先验分布，建立了近于极小极大最优的遗憾和样本复杂度保证。在实验上，我们在具有稀疏奖励、长时域和随机性的任务中评估了$\\texttt{EUBRL}$。结果表明，$\\texttt{EUBRL}$实现了卓越的样本效率、可扩展性和一致性。"
    },
    {
        "title": "Remotely Detectable Robot Policy Watermarking",
        "summary": "The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \\textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.",
        "url": "http://arxiv.org/abs/2512.15379v1",
        "published_date": "2025-12-17T12:28:03+00:00",
        "updated_date": "2025-12-17T12:28:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CR",
            "cs.LG",
            "eess.SY"
        ],
        "authors": [
            "Michael Amir",
            "Manon Flageat",
            "Amanda Prorok"
        ],
        "tldr": "This paper introduces Colored Noise Coherency (CoNoCo), a novel robot policy watermarking strategy designed for remote detection via external observations, addressing the challenge of verifying ownership and detecting misuse of trained policies without access to the robot's internal state.",
        "tldr_zh": "该论文介绍了彩色噪声相干性（CoNoCo），一种新型的机器人策略水印策略，旨在通过外部观察进行远程检测。它解决了在无法访问机器人内部状态的情况下，验证所有权并检测训练策略误用的难题。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "机器学习在实际机器人系统中的成功催生了一种新型知识产权：训练后的策略。这引发了对验证所有权和检测未经授权的、可能不安全的滥用的新型方法的迫切需求。虽然水印技术已在其他领域得到应用，但物理策略提出了一个独特的挑战：远程检测。现有方法假定可以访问机器人的内部状态，但审计人员通常只能获得外部观察结果（例如，视频片段）。这种“物理观察差距”意味着水印必须能够从受未知系统动力学过滤的、噪声大的、异步信号中检测出来。我们使用 \\textit{窥视序列} 的概念来形式化这一挑战，并引入了彩色噪声相干性 (CoNoCo)，这是第一个专为远程检测而设计的水印策略。CoNoCo 通过利用策略固有的随机性，将频谱信号嵌入到机器人的运动中。为了证明它不会降低性能，我们证明 CoNoCo 保留了边际动作分布。我们的实验表明，在各种远程模式（包括运动捕捉以及侧方/俯视视频片段）下，在模拟和真实的机器人实验中，都具有强大而稳健的检测效果。这项工作为保护机器人领域的知识产权提供了必要的一步，提供了第一种使用纯粹的远程观察结果，以非侵入方式验证物理策略来源的方法。"
    },
    {
        "title": "A Network-Based Framework for Modeling and Analyzing Human-Robot Coordination Strategies",
        "summary": "Studies of human-robot interaction in dynamic and unstructured environments show that as more advanced robotic capabilities are deployed, the need for cooperative competencies to support collaboration with human problem-holders increases. Designing human-robot systems to meet these demands requires an explicit understanding of the work functions and constraints that shape the feasibility of alternative joint work strategies. Yet existing human-robot interaction frameworks either emphasize computational support for real-time execution or rely on static representations for design, offering limited support for reasoning about coordination dynamics during early-stage conceptual design. To address this gap, this article presents a novel computational framework for analyzing joint work strategies in human-robot systems by integrating techniques from functional modeling with graph-theoretic representations. The framework characterizes collective work in terms of the relationships among system functions and the physical and informational structure of the work environment, while explicitly capturing how coordination demands evolve over time. Its use during conceptual design is demonstrated through a case study in disaster robotics, which shows how the framework can be used to support early trade-space exploration of human-robot coordination strategies and to identify cooperative competencies that support flexible management of coordination overhead. These results show how the framework makes coordination demands and their temporal evolution explicit, supporting design-time reasoning about cooperative competency requirements and work demands prior to implementation.",
        "url": "http://arxiv.org/abs/2512.15282v1",
        "published_date": "2025-12-17T10:37:34+00:00",
        "updated_date": "2025-12-17T10:37:34+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Martijn IJtsma",
            "Salvatore Hargis"
        ],
        "tldr": "The paper introduces a network-based framework integrating functional modeling and graph theory to analyze human-robot coordination strategies in dynamic environments during the conceptual design phase, demonstrating its use in a disaster robotics case study.",
        "tldr_zh": "该论文介绍了一个基于网络的框架，该框架整合了功能建模和图论，用于在概念设计阶段分析动态环境中的人机协作策略，并通过灾难机器人案例研究展示了其应用。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在动态和非结构化环境中人机交互的研究表明，随着更先进的机器人能力的部署，对支持与问题持有者进行协作的协同能力的需求也在增加。要设计满足这些需求的人机系统，需要明确理解影响替代联合工作策略可行性的工作职能和约束条件。然而，现有的人机交互框架要么强调对实时执行的计算支持，要么依赖于设计的静态表示，对早期概念设计阶段的协调动态的推理提供的支持有限。为了弥补这一差距，本文提出了一种新颖的计算框架，通过将功能建模技术与图论表示相结合，来分析人机系统中的联合工作策略。该框架根据系统功能之间的关系以及工作环境的物理和信息结构来描述集体工作，同时明确捕捉协调需求如何随时间演变。通过一个灾难机器人领域的案例研究，展示了该框架在概念设计中的应用，该案例研究表明该框架如何用于支持人机协调策略的早期权衡分析，并识别支持灵活管理协调开销的协同能力。这些结果表明该框架使协调需求及其时间演变变得明确，支持在实现之前对协同能力需求和工作需求进行设计时推理。"
    },
    {
        "title": "BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization",
        "summary": "We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.",
        "url": "http://arxiv.org/abs/2512.15111v1",
        "published_date": "2025-12-17T06:03:36+00:00",
        "updated_date": "2025-12-17T06:03:36+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Dongmyeong Lee",
            "Jesse Quattrociocchi",
            "Christian Ellis",
            "Rwik Rana",
            "Amanda Adkins",
            "Adam Uccello",
            "Garrett Warnell",
            "Joydeep Biswas"
        ],
        "tldr": "The paper presents BEV-Patch-PF, a real-time, GPS-free geo-localization system for off-road environments that uses a particle filter with learned BEV and aerial features, achieving significantly lower trajectory error compared to a baseline.",
        "tldr_zh": "该论文提出了BEV-Patch-PF，一种用于越野环境的实时、无GPS的地理定位系统，该系统使用粒子滤波器以及学习到的鸟瞰图（BEV）和航拍图特征，与基线相比，显著降低了轨迹误差。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们提出了一种名为BEV-Patch-PF的无需GPS的序列地理定位系统，该系统将粒子滤波器与学习到的鸟瞰图（BEV）和航拍特征图相结合。 我们从车载RGB和深度图像构建BEV特征图。 对于每个3自由度粒子姿态假设，我们从围绕近似位置查询的局部航拍图像计算出的航拍特征图中，裁剪对应的图像块。 BEV-Patch-PF通过将BEV特征与航拍图块特征进行匹配，计算每个粒子的对数似然。 在两个真实世界的越野数据集上，我们的方法在已见过的路径上实现了比基于检索方法的基线低7.5倍的绝对轨迹误差（ATE），在未见过的路径上实现了低7.0倍的ATE，同时在茂密的植被和阴影下仍保持了准确性。 该系统可在NVIDIA Tesla T4上以10 Hz的频率实时运行，从而实现了实际的机器人部署。"
    },
    {
        "title": "CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation",
        "summary": "Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.",
        "url": "http://arxiv.org/abs/2512.14689v1",
        "published_date": "2025-12-16T18:56:04+00:00",
        "updated_date": "2025-12-16T18:56:04+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Sirui Chen",
            "Zi-ang Cao",
            "Zhengyi Luo",
            "Fernando Castañeda",
            "Chenran Li",
            "Tingwu Wang",
            "Ye Yuan",
            "Linxi \"Jim\" Fan",
            "C. Karen Liu",
            "Yuke Zhu"
        ],
        "tldr": "The paper introduces CHIP, a plug-and-play module for humanoid robots that enables controllable end-effector stiffness during forceful manipulation tasks without additional data or reward tuning, allowing a generalist controller to perform various manipulation tasks.",
        "tldr_zh": "该论文介绍了CHIP，一个即插即用的模块，用于人形机器人，可在强力操作任务期间实现可控的末端执行器刚度，无需额外的数据或奖励调整，从而使通用控制器能够执行各种操作任务。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "人形机器人的最新进展已经解锁了敏捷的运动技能，包括后空翻、跑步和爬行。然而，人形机器人执行需要力量的操控任务仍然具有挑战性，比如移动物体、擦拭和推动小车。我们提出自适应柔顺人形控制，通过视觉扰动 (CHIP)，这是一种即插即用模块，能够在保持对动态参考运动进行敏捷跟踪的同时，实现可控的末端执行器刚度。CHIP易于实现，不需要数据增强或额外的奖励调整。我们证明了，使用CHIP训练的通用运动跟踪控制器可以执行各种需要不同末端执行器柔顺性的力量操控任务，例如多机器人协作、擦拭、箱子递送和开门。"
    },
    {
        "title": "CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory",
        "summary": "Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.",
        "url": "http://arxiv.org/abs/2512.15813v1",
        "published_date": "2025-12-17T11:28:25+00:00",
        "updated_date": "2025-12-17T11:28:25+00:00",
        "categories": [
            "cs.SE",
            "cs.AI"
        ],
        "authors": [
            "Nishant Gaurav",
            "Adit Akarsh",
            "Tejas Ravishankar",
            "Manoj Bajaj"
        ],
        "tldr": "CodeMem proposes an architecture for building reproducible tool-using AI agents by incorporating procedural memory implemented via code, addressing the probabilistic instability issues of existing LLM-based agents.",
        "tldr_zh": "CodeMem 提出了一种架构，通过结合代码实现的过程记忆来构建可复现的工具使用 AI 代理，解决了现有基于 LLM 的代理的概率不稳定性问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "当前使用工具的AI智能体存在行动空间有限、上下文效率低下以及概率不稳定性等问题，使其不适合处理重复性任务，而这些任务通常可以由构建在 n8n 和 Zapier 等平台上的智能体工作流可靠高效地完成。诸如 CodeAct、DynaSaur 和 Code Mode 等早期工作试图通过使用完整的 Python 语言作为其行动空间来解决前两个问题：智能体可以调用的工具数量变得无限。Python 代码块可以将复杂的操作执行到一个步骤中，并且仅打印相关的结果，这有助于保持上下文简洁。然而，概率不稳定性问题仍然存在，因为在相同的环境中的相同任务下，由于 LLM 的概率性，智能体可能会遵循不同的轨迹。因此，我们需要程序性记忆以确保一致性和可靠性。本文提出 CodeMem，一种通过代码实现程序性记忆的架构，可用于构建和运行具有确定性可靠性的可重用智能体工作流。"
    },
    {
        "title": "Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis",
        "summary": "Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.",
        "url": "http://arxiv.org/abs/2512.15295v1",
        "published_date": "2025-12-17T10:45:27+00:00",
        "updated_date": "2025-12-17T10:45:27+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Toshihide Ubukata",
            "Enhong Mu",
            "Takuto Yamauchi",
            "Mingyue Zhang",
            "Jialong Li",
            "Kenji Tei"
        ],
        "tldr": "This paper presents GCRL, a Graph Neural Network-enhanced Reinforcement Learning method for controller synthesis, which learns more efficiently and generalizes better than state-of-the-art methods by encoding LTS exploration history into a graph.",
        "tldr_zh": "该论文提出了GCRL，一种基于图神经网络增强的强化学习方法，用于控制器综合。通过将LTS探索历史编码为图结构，GCRL比现有方法学习效率更高且泛化能力更强。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 7,
        "summary_zh": "控制器综合是一种形式化方法，用于自动生成满足特定属性的标记迁移系统（LTS）控制器。然而，综合过程的效率严重依赖于探索策略。这些策略通常依赖于固定的规则或通过强化学习（RL）学习的策略，而这些策略仅考虑了有限的当前特征集合。为了解决这一局限性，本文提出了一种名为GCRL的方法，该方法通过集成图神经网络（GNN）来增强基于RL的方法。GCRL将LTS探索的历史编码成图结构，使其能够捕获更广泛的、非基于当前状态的上下文信息。在与最先进方法的对比实验中，GCRL在五个基准测试领域中的四个领域表现出卓越的学习效率和泛化能力，只有一个以高对称性和严格局部交互为特征的特定领域除外。"
    },
    {
        "title": "Offline Multi-Task Multi-Objective Data-Driven Evolutionary Algorithm with Language Surrogate Model and Implicit Q-Learning",
        "summary": "Data-driven evolutionary algorithms has shown surprising results in addressing expensive optimization problems through robust surrogate modeling. Though promising, existing surrogate modeling schemes may encounter limitations in complex optimization problems with many sub-objectives, which rely on repeated and tedious approximation. To address such technical gap, we propose Q-MetaSur as a plug-and-play surrogate modeling scheme capable of providing unified and generalized surrogate learning. Specifically, we consider multi-task-multi-objective optimization~(MTMOO) in offline setting. Several key designs are proposed: 1) we transform objective approximation into sequence-to-sequence modeling where MTMOO problem can be represented by tenxual tokenization. To operate under such auto-regressive modeling, we introduce a Large Language Model-based surrogate model that first encodes a MTMOO instance and then decodes objective values of unseen decision variables. To ensure stability in training the proposed model, we propose a two-stage offline training strategy that operates as a synergy of supervised tuning and RL fine-tuning, which first exploits offline dataset to fit existing knowledge and then leverages RL to enhance model's generalization performance. Extensive empirical results on the CEC2019 benchmark demonstrate that Q-MetaSur not only outperforms representative surrogate baselines in objective approximation accuracy, but also helps underlying evolutionary algorithms achieve both desired optimization convergence and improved pareto optimality.",
        "url": "http://arxiv.org/abs/2512.15149v1",
        "published_date": "2025-12-17T07:30:11+00:00",
        "updated_date": "2025-12-17T07:30:11+00:00",
        "categories": [
            "cs.NE",
            "cs.AI"
        ],
        "authors": [
            "Xian-Rong Zhang",
            "Yue-Jiao Gong",
            "Zeyuan Ma",
            "Jun Zhang"
        ],
        "tldr": "This paper introduces Q-MetaSur, a language model-based surrogate modeling scheme for offline multi-task multi-objective optimization, trained with a two-stage supervised and RL fine-tuning approach, demonstrating improved performance on the CEC2019 benchmark.",
        "tldr_zh": "本文介绍了一种名为Q-MetaSur的基于语言模型的代理建模方案，用于离线多任务多目标优化。该方案采用两阶段监督和强化学习微调方法进行训练，并在CEC2019基准测试中表现出改进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "数据驱动的进化算法在通过稳健的代理建模解决昂贵优化问题方面表现出令人惊讶的结果。尽管充满希望，但现有的代理建模方案在具有许多子目标的复杂优化问题中可能会遇到局限性，这些问题依赖于重复且繁琐的近似。为了解决这种技术差距，我们提出 Q-MetaSur 作为一种即插即用的代理建模方案，能够提供统一和通用的代理学习。具体来说，我们考虑离线设置中的多任务多目标优化（MTMOO）。提出了几个关键设计：1）我们将目标近似转换为序列到序列建模，其中 MTMOO 问题可以通过文本标记化来表示。为了在此自回归建模下运行，我们引入了一个基于大型语言模型的代理模型，该模型首先编码一个 MTMOO 实例，然后解码未见决策变量的目标值。为了确保所提出模型在训练中的稳定性，我们提出了一种两阶段离线训练策略，该策略作为监督调优和强化学习微调的协同作用，首先利用离线数据集来拟合现有知识，然后利用强化学习来增强模型的泛化性能。在 CEC2019 基准上的大量实验结果表明，Q-MetaSur 不仅在目标近似精度方面优于代表性的代理基线，还有助于底层进化算法实现所需的优化收敛和改进的帕累托最优性。"
    },
    {
        "title": "QoS-Aware Hierarchical Reinforcement Learning for Joint Link Selection and Trajectory Optimization in SAGIN-Supported UAV Mobility Management",
        "summary": "Due to the significant variations in unmanned aerial vehicle (UAV) altitude and horizontal mobility, it becomes difficult for any single network to ensure continuous and reliable threedimensional coverage. Towards that end, the space-air-ground integrated network (SAGIN) has emerged as an essential architecture for enabling ubiquitous UAV connectivity. To address the pronounced disparities in coverage and signal characteristics across heterogeneous networks, this paper formulates UAV mobility management in SAGIN as a constrained multi-objective joint optimization problem. The formulation couples discrete link selection with continuous trajectory optimization. Building on this, we propose a two-level multi-agent hierarchical deep reinforcement learning (HDRL) framework that decomposes the problem into two alternately solvable subproblems. To map complex link selection decisions into a compact discrete action space, we conceive a double deep Q-network (DDQN) algorithm in the top-level, which achieves stable and high-quality policy learning through double Q-value estimation. To handle the continuous trajectory action space while satisfying quality of service (QoS) constraints, we integrate the maximum-entropy mechanism of the soft actor-critic (SAC) and employ a Lagrangian-based constrained SAC (CSAC) algorithm in the lower-level that dynamically adjusts the Lagrange multipliers to balance constraint satisfaction and policy optimization. Moreover, the proposed algorithm can be extended to multi-UAV scenarios under the centralized training and decentralized execution (CTDE) paradigm, which enables more generalizable policies. Simulation results demonstrate that the proposed scheme substantially outperforms existing benchmarks in throughput, link switching frequency and QoS satisfaction.",
        "url": "http://arxiv.org/abs/2512.15119v1",
        "published_date": "2025-12-17T06:22:46+00:00",
        "updated_date": "2025-12-17T06:22:46+00:00",
        "categories": [
            "eess.SP",
            "cs.AI"
        ],
        "authors": [
            "Jiayang Wan",
            "Ke He",
            "Yafei Wang",
            "Fan Liu",
            "Wenjin Wang",
            "Shi Jin"
        ],
        "tldr": "The paper proposes a two-level hierarchical deep reinforcement learning framework for UAV mobility management in SAGIN, addressing joint link selection and trajectory optimization to enhance QoS and throughput.",
        "tldr_zh": "本文提出了一种两层分层深度强化学习框架，用于空间空地一体化网络（SAGIN）支持下的无人机移动性管理，解决了联合链路选择和轨迹优化问题，以提高服务质量和吞吐量。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "由于无人机（UAV）在高度和水平机动性方面存在显著差异，任何单一网络都难以确保连续可靠的三维覆盖。为此，空天地一体化网络（SAGIN）已成为实现普遍无人机连接的关键架构。为了解决异构网络中覆盖范围和信号特征的显著差异，本文将SAGIN中的无人机移动性管理建模为一个带约束的多目标联合优化问题。该公式将离散链路选择与连续轨迹优化相结合。在此基础上，我们提出了一种两级多智能体分层深度强化学习（HDRL）框架，将问题分解为两个交替可解的子问题。为了将复杂的链路选择决策映射到紧凑的离散动作空间，我们在顶层设计了一种双深度Q网络（DDQN）算法，通过双Q值估计实现稳定且高质量的策略学习。为了处理连续轨迹动作空间，同时满足服务质量（QoS）约束，我们集成了soft actor-critic（SAC）的最大熵机制，并在底层采用了一种基于拉格朗日的约束SAC（CSAC）算法，该算法动态调整拉格朗日乘子以平衡约束满足和策略优化。此外，所提出的算法可以在集中训练分散执行（CTDE）范式下扩展到多无人机场景，从而实现更具泛化性的策略。仿真结果表明，所提出的方案在吞吐量、链路切换频率和QoS满足度方面均显著优于现有基准。"
    },
    {
        "title": "Large Model Enabled Embodied Intelligence for 6G Integrated Perception, Communication, and Computation Network",
        "summary": "The advent of sixth-generation (6G) places intelligence at the core of wireless architecture, fusing perception, communication, and computation into a single closed-loop. This paper argues that large artificial intelligence models (LAMs) can endow base stations with perception, reasoning, and acting capabilities, thus transforming them into intelligent base station agents (IBSAs). We first review the historical evolution of BSs from single-functional analog infrastructure to distributed, software-defined, and finally LAM-empowered IBSA, highlighting the accompanying changes in architecture, hardware platforms, and deployment. We then present an IBSA architecture that couples a perception-cognition-execution pipeline with cloud-edge-end collaboration and parameter-efficient adaptation. Subsequently,we study two representative scenarios: (i) cooperative vehicle-road perception for autonomous driving, and (ii) ubiquitous base station support for low-altitude uncrewed aerial vehicle safety monitoring and response against unauthorized drones. On this basis, we analyze key enabling technologies spanning LAM design and training, efficient edge-cloud inference, multi-modal perception and actuation, as well as trustworthy security and governance. We further propose a holistic evaluation framework and benchmark considerations that jointly cover communication performance, perception accuracy, decision-making reliability, safety, and energy efficiency. Finally, we distill open challenges on benchmarks, continual adaptation, trustworthy decision-making, and standardization. Together, this work positions LAM-enabled IBSAs as a practical path toward integrated perception, communication, and computation native, safety-critical 6G systems.",
        "url": "http://arxiv.org/abs/2512.15109v1",
        "published_date": "2025-12-17T06:01:16+00:00",
        "updated_date": "2025-12-17T06:01:16+00:00",
        "categories": [
            "eess.SP",
            "cs.AI",
            "cs.IT"
        ],
        "authors": [
            "Zhuoran Li",
            "Zhen Gao",
            "Xinhua Liu",
            "Zheng Wang",
            "Xiaotian Zhou",
            "Lei Liu",
            "Yongpeng Wu",
            "Wei Feng",
            "Yongming Huang"
        ],
        "tldr": "This paper proposes using Large AI Models to transform base stations into intelligent agents capable of perception, reasoning, and acting, integrating them into future 6G networks for applications like autonomous driving and drone monitoring. It outlines architecture, key technologies, evaluation frameworks, and open challenges.",
        "tldr_zh": "本文提出使用大型人工智能模型将基站转变为能够感知、推理和行动的智能代理，将其集成到未来的6G网络中，用于自动驾驶和无人机监控等应用。文章概述了架构、关键技术、评估框架和开放性挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "第六代(6G)的出现将智能置于无线架构的核心，将感知、通信和计算融合到一个闭环中。本文认为，大型人工智能模型（LAM）可以赋予基站感知、推理和行动能力，从而将它们转变为智能基站代理（IBSA）。我们首先回顾了基站从单功能模拟基础设施到分布式、软件定义，最终到LAM赋能的IBSA的历史演变，重点介绍了架构、硬件平台和部署方面的伴随变化。然后，我们提出了一种IBSA架构，该架构将感知-认知-执行管道与云-边-端协同和参数高效自适应相结合。随后，我们研究了两个有代表性的场景：（i）用于自动驾驶的协同车路感知，以及（ii）用于低空无人飞行器安全监控和应对未经授权无人机的普适基站支持。在此基础上，我们分析了关键的使能技术，涵盖LAM设计和训练、高效的边缘-云推理、多模态感知和执行，以及可信的安全和治理。我们进一步提出了一个整体评估框架和基准测试考虑因素，这些因素共同涵盖通信性能、感知准确性、决策可靠性、安全性以及能源效率。最后，我们提炼了关于基准测试、持续自适应、可信决策制定和标准化的开放性挑战。总而言之，这项工作将LAM赋能的IBSA定位为实现集成感知、通信和计算原生、安全关键型6G系统的可行途径。"
    },
    {
        "title": "Spectral Representation-based Reinforcement Learning",
        "summary": "In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.",
        "url": "http://arxiv.org/abs/2512.15036v1",
        "published_date": "2025-12-17T02:54:42+00:00",
        "updated_date": "2025-12-17T02:54:42+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Chenxiao Gao",
            "Haotian Sun",
            "Na Li",
            "Dale Schuurmans",
            "Bo Dai"
        ],
        "tldr": "This paper introduces a spectral representation-based framework for reinforcement learning to address issues with function approximation in large state/action spaces, achieving competitive performance on the DeepMind Control Suite.",
        "tldr_zh": "本文提出了一种基于谱表示的强化学习框架，旨在解决大型状态/动作空间中函数逼近的问题，并在DeepMind控制套件上取得了具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在具有大型状态和动作空间的实际应用中，强化学习（RL）通常采用函数逼近来表示如策略、价值函数和动力学模型等核心组成部分。虽然诸如神经网络等强大的逼近方法提供了极佳的表达能力，但它们往往存在理论上的模糊性、优化不稳定性和探索困难，并且在实践中产生巨大的计算成本。在本文中，我们引入了谱表示的视角，以解决RL中的这些难题。该框架源于转移算子的谱分解，可为后续的策略优化产生有效的系统动态抽象，同时还能提供清晰的理论表征。我们揭示了如何为具有潜在变量结构或基于能量的结构的转移算子构建谱表示，这暗示了从数据中提取谱表示的不同学习方法。值得注意的是，这些学习方法中的每一种都在该框架下实现了一种有效的RL算法。我们还从理论上将这种谱视角扩展到部分可观测马尔可夫决策过程（POMDP）。最后，我们在DeepMind Control Suite中的20多个具有挑战性的任务上验证了这些算法，它们的性能与当前最先进的无模型和基于模型的基线算法相当或更优。"
    },
    {
        "title": "A Preprocessing Framework for Video Machine Vision under Compression",
        "summary": "There has been a growing trend in compressing and transmitting videos from terminals for machine vision tasks. Nevertheless, most video coding optimization method focus on minimizing distortion according to human perceptual metrics, overlooking the heightened demands posed by machine vision systems. In this paper, we propose a video preprocessing framework tailored for machine vision tasks to address this challenge. The proposed method incorporates a neural preprocessor which retaining crucial information for subsequent tasks, resulting in the boosting of rate-accuracy performance. We further introduce a differentiable virtual codec to provide constraints on rate and distortion during the training stage. We directly apply widely used standard codecs for testing. Therefore, our solution can be easily applied to real-world scenarios. We conducted extensive experiments evaluating our compression method on two typical downstream tasks with various backbone networks. The experimental results indicate that our approach can save over 15% of bitrate compared to using only the standard codec anchor version.",
        "url": "http://arxiv.org/abs/2512.15331v1",
        "published_date": "2025-12-17T11:26:19+00:00",
        "updated_date": "2025-12-17T11:26:19+00:00",
        "categories": [
            "cs.MM",
            "cs.CV"
        ],
        "authors": [
            "Fei Zhao",
            "Mengxi Guo",
            "Shijie Zhao",
            "Junlin Li",
            "Li Zhang",
            "Xiaodong Xie"
        ],
        "tldr": "This paper proposes a video preprocessing framework for machine vision tasks that optimizes rate-accuracy performance by incorporating a neural preprocessor and a differentiable virtual codec, achieving bitrate savings compared to standard codecs.",
        "tldr_zh": "本文提出了一种针对机器视觉任务的视频预处理框架，通过结合神经预处理器和可微虚拟编解码器来优化速率-准确率性能，与标准编解码器相比，实现了码率的节省。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "为了机器视觉任务，从终端压缩和传输视频的趋势日益增长。然而，大多数视频编码优化方法侧重于根据人类感知指标来最小化失真，忽略了机器视觉系统提出的更高要求。 针对这一挑战，本文提出了一种为机器视觉任务定制的视频预处理框架。该方法包含一个神经预处理器，其保留了后续任务的关键信息，从而提高了率-精度性能。 我们进一步引入了一个可微的虚拟编解码器，以便在训练阶段对码率和失真进行约束。 我们直接应用广泛使用的标准编解码器进行测试。 因此，我们的解决方案可以很容易地应用于实际场景。 我们进行了大量的实验，用各种骨干网络在两个典型的下游任务上评估了我们的压缩方法。 实验结果表明，与仅使用标准编解码器相比，我们的方法可以节省超过15%的码率。"
    },
    {
        "title": "KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation",
        "summary": "We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.",
        "url": "http://arxiv.org/abs/2512.15311v1",
        "published_date": "2025-12-17T11:00:00+00:00",
        "updated_date": "2025-12-17T11:00:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenke E",
            "Yixin Sun",
            "Jiaxu Liu",
            "Hubert P. H. Shum",
            "Amir Atapour-Abarghouei",
            "Toby P. Breckon"
        ],
        "tldr": "The paper introduces a cross-modality knowledge distillation framework for single-panoramic-camera BEV segmentation, using LiDAR as a teacher signal to train a lightweight camera-only student network, achieving state-of-the-art performance and speed.",
        "tldr_zh": "该论文提出了一种用于单全景相机BEV分割的跨模态知识蒸馏框架，利用激光雷达作为教师信号来训练一个轻量级的纯相机学生网络，实现了最先进的性能和速度。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们提出了首个专门为单全景相机鸟瞰图（BEV）分割定制的跨模态蒸馏框架。我们的方法利用一种新颖的LiDAR图像表示，该图像表示由距离、强度和环境通道融合而成，以及一个体素对齐的视角转换器，它在保持空间保真度的同时实现高效的BEV处理。在训练过程中，一个高容量的LiDAR和相机融合教师网络提取丰富的空间和语义特征，用于跨模态知识蒸馏到仅依赖于单个360度全景相机图像的轻量级学生网络中。在Dur360BEV数据集上的大量实验表明，我们的教师模型显著优于现有的基于相机的BEV分割方法，实现了25.6%的IoU提升。与此同时，蒸馏的学生网络取得了具有竞争力的性能，IoU增益为8.5%，并且实现了31.2 FPS的最先进推理速度。此外，在KITTI-360（两个鱼眼相机）上的评估证实了我们的蒸馏框架可以推广到不同的相机设置，突出了其可行性和稳健性。该方法降低了传感器复杂性和部署成本，同时为实际自动驾驶中高效、低成本的BEV分割提供了实用的解决方案。"
    },
    {
        "title": "3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding",
        "summary": "3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.\n  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.",
        "url": "http://arxiv.org/abs/2512.15126v1",
        "published_date": "2025-12-17T06:38:07+00:00",
        "updated_date": "2025-12-17T06:38:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yupeng Zhu",
            "Xiongzhen Zhang",
            "Ye Chen",
            "Bingbing Ni"
        ],
        "tldr": "The paper introduces a novel framework called 3DProxyImg for efficient and controllable 3D animation synthesis from a single image, using a 2D-3D aligned proxy representation to decouple geometric control from appearance synthesis.",
        "tldr_zh": "该论文介绍了一种名为 3DProxyImg 的新框架，用于从单张图像中进行高效且可控的 3D 动画合成，它使用 2D-3D 对齐的代理表示来将几何控制与外观合成分离。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "3D动画是现代视觉媒体的核心，但传统的制作流程仍然是劳动密集型、专业知识需求高且计算成本昂贵的。最近基于AIGC的方法在一定程度上自动化了资产创建和绑定，但它们要么继承了完整3D流程的沉重成本，要么依赖于牺牲3D可控性和交互性的视频合成范式。我们关注单图像3D动画生成，并认为渲染质量和3D控制之间的权衡从根本上限制了进展。\n\n为了解决这一局限性，我们提出了一种轻量级的3D动画框架，该框架将几何控制与外观合成解耦。核心思想是使用粗略的3D估计作为结构载体的2D-3D对齐代理表示，同时将高保真外观和视图合成委托给学习到的图像空间生成先验。这种代理公式实现了可与经典流程相媲美的3D感知运动控制和交互，而无需精确的几何形状或昂贵的优化，并自然地扩展到连贯的背景动画。大量实验表明，我们的方法能够在低功耗平台上实现高效的动画生成，并且在身份保持、几何和纹理一致性以及为用户提供的精确、交互式控制水平方面优于基于视频的3D动画生成。"
    },
    {
        "title": "ART: Articulated Reconstruction Transformer",
        "summary": "We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.",
        "url": "http://arxiv.org/abs/2512.14671v1",
        "published_date": "2025-12-16T18:35:23+00:00",
        "updated_date": "2025-12-16T18:35:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zizhang Li",
            "Cheng Zhang",
            "Zhengqin Li",
            "Henry Howard-Jenkins",
            "Zhaoyang Lv",
            "Chen Geng",
            "Jiajun Wu",
            "Richard Newcombe",
            "Jakob Engel",
            "Zhao Dong"
        ],
        "tldr": "ART is a novel category-agnostic transformer model for reconstructing complete 3D articulated objects from sparse images, achieving state-of-the-art performance by predicting part-based representations with explicit articulation parameters, enabling physically interpretable and simulation-ready reconstructions.",
        "tldr_zh": "ART 是一种新型的、类别无关的 Transformer 模型，用于从稀疏图像中重建完整的 3D 铰接物体。它通过预测基于零件的表示形式，并包含明确的铰接参数，实现了最先进的性能，从而能够进行物理上可解释的和可用于仿真的重建。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "我们提出了ART，即铰接重建Transformer——一种与类别无关的前馈模型，它仅从稀疏的多状态RGB图像中重建完整的3D铰接物体。以往的铰接物体重建方法要么依赖于缓慢的优化过程，且具有脆弱的跨状态对应关系，要么使用局限于特定物体类别的前馈模型。相比之下，ART将铰接物体视为刚性部件的组合，并将重建问题转化为基于部件的预测。我们新设计的Transformer架构将稀疏图像输入映射到一组可学习的部件槽位，ART从中联合解码各个部件的统一表示，包括它们的3D几何形状、纹理和显式铰接参数。由此产生的重建结果具有物理可解释性，并且可以轻松导出用于模拟。ART在一个大规模、多样化的数据集上进行训练，并采用了逐部件监督，并在多个基准测试中进行了评估，与现有基线相比，ART取得了显著的改进，并为从图像输入进行铰接物体重建建立了一个新的技术水平。"
    },
    {
        "title": "Autonomous Pressure Control in MuVacAS via Deep Reinforcement Learning and Deep Learning Surrogate Models",
        "summary": "The development of nuclear fusion requires materials that can withstand extreme conditions. The IFMIF-DONES facility, a high-power particle accelerator, is being designed to qualify these materials. A critical testbed for its development is the MuVacAS prototype, which replicates the final segment of the accelerator beamline. Precise regulation of argon gas pressure within its ultra-high vacuum chamber is vital for this task. This work presents a fully data-driven approach for autonomous pressure control. A Deep Learning Surrogate Model, trained on real operational data, emulates the dynamics of the argon injection system. This high-fidelity digital twin then serves as a fast-simulation environment to train a Deep Reinforcement Learning agent. The results demonstrate that the agent successfully learns a control policy that maintains gas pressure within strict operational limits despite dynamic disturbances. This approach marks a significant step toward the intelligent, autonomous control systems required for the demanding next-generation particle accelerator facilities.",
        "url": "http://arxiv.org/abs/2512.15521v1",
        "published_date": "2025-12-17T15:19:55+00:00",
        "updated_date": "2025-12-17T15:19:55+00:00",
        "categories": [
            "physics.acc-ph",
            "cs.LG"
        ],
        "authors": [
            "Guillermo Rodriguez-Llorente",
            "Galo Gallardo",
            "Rodrigo Morant Navascués",
            "Nikita Khvatkin Petrovsky",
            "Anderson Sabogal",
            "Roberto Gómez-Espinosa Martín"
        ],
        "tldr": "This paper presents a data-driven approach using Deep Reinforcement Learning and a Deep Learning Surrogate Model for autonomous pressure control in the MuVacAS prototype, a crucial component for next-generation particle accelerator facilities.",
        "tldr_zh": "本文提出了一种数据驱动的方法，使用深度强化学习和深度学习替代模型，用于 MuVacAS 原型中的自主压力控制，MuVacAS 原型是下一代粒子加速器设施的关键组成部分。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "核聚变的发展需要能够承受极端条件的材料。高功率粒子加速器IFMIF-DONES装置正被设计用于验证这些材料。MuVacAS原型是其开发的关键试验台，它复制了加速器束流线的末端部分。精确调节其超高真空室内的氩气压力对于这项任务至关重要。本工作提出了一种完全数据驱动的自主压力控制方法。一个基于深度学习的代理模型，通过真实运行数据训练，模拟了氩气注入系统的动态特性。然后，这个高保真数字孪生充当快速仿真环境，用于训练一个深度强化学习代理。结果表明，该代理成功地学习了一种控制策略，能够在动态扰动下将气体压力维持在严格的运行限值内。这种方法标志着朝着下一代高要求粒子加速器设施所需的智能、自主控制系统迈出了重要一步。"
    },
    {
        "title": "Neural Modular Physics for Elastic Simulation",
        "summary": "Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.",
        "url": "http://arxiv.org/abs/2512.15083v1",
        "published_date": "2025-12-17T05:02:03+00:00",
        "updated_date": "2025-12-17T05:02:03+00:00",
        "categories": [
            "cs.LG",
            "cs.CE"
        ],
        "authors": [
            "Yifei Li",
            "Haixu Wu",
            "Zeyi Xu",
            "Tuur Stuyck",
            "Wojciech Matusik"
        ],
        "tldr": "The paper introduces Neural Modular Physics (NMP), a modular neural network approach for elastic simulation, improving physical consistency and generalization compared to monolithic neural simulators and traditional simulators.",
        "tldr_zh": "该论文介绍了神经模块化物理（NMP），一种用于弹性模拟的模块化神经网络方法，与单片神经模拟器和传统模拟器相比，提高了物理一致性和泛化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于学习的方法在物理模拟领域取得了显著进展，通常使用单块端到端优化的神经网络来近似动力学。尽管这些模型提供了一种有效的模拟方式，但与传统的数值模拟器相比，它们可能会丢失一些关键特性，例如物理可解释性和可靠性。本文从以模块化方式运行的经典模拟器中汲取灵感，提出了一种用于弹性模拟的神经模块化物理（NMP）方法，该方法结合了神经网络的近似能力和传统模拟器的物理可靠性。与以前的单块学习范式不同，NMP通过将弹性动力学分解为通过中间物理量连接的具有物理意义的神经模块，实现了对中间量和物理约束的直接监督。通过专门的架构和训练策略，我们的方法将数值计算流程转换为模块化的神经模拟器，从而实现改进的物理一致性和泛化性。实验表明，与其他神经模拟器相比，NMP能够更好地泛化到未知的初始条件和分辨率，实现稳定的长程模拟，更好地保持物理特性，并且在具有未知底层动力学的情景中，比传统模拟器更具可行性。"
    },
    {
        "title": "QuantGraph: A Receding-Horizon Quantum Graph Solver",
        "summary": "Dynamic programming is a cornerstone of graph-based optimization. While effective, it scales unfavorably with problem size. In this work, we present QuantGraph, a two-stage quantum-enhanced framework that casts local and global graph-optimization problems as quantum searches over discrete trajectory spaces. The solver is designed to operate efficiently by first finding a sequence of locally optimal transitions in the graph (local stage), without considering full trajectories. The accumulated cost of these transitions acts as a threshold that prunes the search space (up to 60% reduction for certain examples). The subsequent global stage, based on this threshold, refines the solution. Both stages utilize variants of the Grover-adaptive-search algorithm. To achieve scalability and robustness, we draw on principles from control theory and embed QuantGraph's global stage within a receding-horizon model-predictive-control scheme. This classical layer stabilizes and guides the quantum search, improving precision and reducing computational burden. In practice, the resulting closed-loop system exhibits robust behavior and lower overall complexity. Notably, for a fixed query budget, QuantGraph attains a 2x increase in control-discretization precision while still benefiting from Grover-search's inherent quadratic speedup compared to classical methods.",
        "url": "http://arxiv.org/abs/2512.15476v1",
        "published_date": "2025-12-17T14:22:08+00:00",
        "updated_date": "2025-12-17T14:22:08+00:00",
        "categories": [
            "quant-ph",
            "cs.RO",
            "eess.SY",
            "physics.comp-ph"
        ],
        "authors": [
            "Pranav Vaidhyanathan",
            "Aristotelis Papatheodorou",
            "David R. M. Arvidsson-Shukur",
            "Mark T. Mitchison",
            "Natalia Ares",
            "Ioannis Havoutis"
        ],
        "tldr": "The paper introduces QuantGraph, a quantum-enhanced framework for graph optimization that uses a receding-horizon control scheme to improve scalability and robustness, achieving a 2x increase in control-discretization precision compared to classical methods given a fixed query budget.",
        "tldr_zh": "该论文介绍了一个名为QuantGraph的量子增强图优化框架，该框架使用后退水平控制方案来提高可扩展性和鲁棒性。在固定查询预算下，该框架与经典方法相比，控制离散化精度提高了2倍。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 7,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "动态规划是基于图的优化方法的基石。虽然有效，但其规模随问题大小而呈不利增长。在这项工作中，我们提出了 QuantGraph，一个两阶段的量子增强框架，它将局部和全局图优化问题转化为离散轨迹空间上的量子搜索。该求解器旨在通过首先找到图中一系列局部最优的转移（局部阶段），而不考虑完整轨迹，来高效运行。这些转移的累积成本充当阈值，可以剪枝搜索空间（对于某些示例最多减少60%）。随后的全局阶段，基于此阈值，细化解决方案。两个阶段都采用了 Grover 自适应搜索算法的变体。为了实现可扩展性和鲁棒性，我们借鉴了控制理论的原理，并将 QuantGraph 的全局阶段嵌入到后退视界模型预测控制方案中。这个经典层稳定并引导量子搜索，提高精度并减少计算负担。在实践中，由此产生的闭环系统表现出稳健的行为和较低的总体复杂度。值得注意的是，对于固定的查询预算，QuantGraph 实现了控制离散化精度2倍的提升，同时仍然受益于 Grover 搜索与经典方法相比固有的二次加速。"
    },
    {
        "title": "Infrastructure-based Autonomous Mobile Robots for Internal Logistics -- Challenges and Future Perspectives",
        "summary": "The adoption of Autonomous Mobile Robots (AMRs) for internal logistics is accelerating, with most solutions emphasizing decentralized, onboard intelligence. While AMRs in indoor environments like factories can be supported by infrastructure, involving external sensors and computational resources, such systems remain underexplored in the literature. This paper presents a comprehensive overview of infrastructure-based AMR systems, outlining key opportunities and challenges. To support this, we introduce a reference architecture combining infrastructure-based sensing, on-premise cloud computing, and onboard autonomy. Based on the architecture, we review core technologies for localization, perception, and planning. We demonstrate the approach in a real-world deployment in a heavy-vehicle manufacturing environment and summarize findings from a user experience (UX) evaluation. Our aim is to provide a holistic foundation for future development of scalable, robust, and human-compatible AMR systems in complex industrial environments.",
        "url": "http://arxiv.org/abs/2512.15215v1",
        "published_date": "2025-12-17T09:10:43+00:00",
        "updated_date": "2025-12-17T09:10:43+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Erik Brorsson",
            "Kristian Ceder",
            "Ze Zhang",
            "Sabino Francesco Roselli",
            "Endre Erős",
            "Martin Dahl",
            "Beatrice Alenljung",
            "Jessica Lindblom",
            "Thanh Bui",
            "Emmanuel Dean",
            "Lennart Svensson",
            "Kristofer Bengtsson",
            "Per-Lage Götvall",
            "Knut Åkesson"
        ],
        "tldr": "This paper explores the under-researched area of infrastructure-based Autonomous Mobile Robot (AMR) systems for internal logistics, presenting a reference architecture and real-world deployment in a heavy-vehicle manufacturing environment.",
        "tldr_zh": "本文探讨了内部物流中基于基础设施的自主移动机器人（AMR）系统这一研究不足的领域，提出了一个参考架构，并在重型车辆制造环境中进行了实际部署。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "自主移动机器人（AMR）在内部物流中的应用正在加速，大多数解决方案都强调去中心化、车载智能。 虽然工厂等室内环境中的AMR可以得到基础设施的支持，包括外部传感器和计算资源，但此类系统在文献中仍未得到充分探索。 本文对基于基础设施的AMR系统进行了全面的概述，概括了其关键机遇和挑战。 为了支持这一点，我们介绍了一种参考架构，该架构结合了基于基础设施的感知、本地云运算和车载自主性。 基于该架构，我们回顾了用于定位、感知和规划的核心技术。 我们在重型车辆制造环境中的实际部署中验证了该方法，并总结了用户体验（UX）评估的结果。 我们的目标是为未来在复杂工业环境中开发可扩展、鲁棒且与人类兼容的AMR系统提供一个整体性基础。"
    },
    {
        "title": "EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving",
        "summary": "Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.",
        "url": "http://arxiv.org/abs/2512.15195v1",
        "published_date": "2025-12-17T08:46:49+00:00",
        "updated_date": "2025-12-17T08:46:49+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jörg Gamerdinger",
            "Sven Teufel",
            "Stephan Amann",
            "Lukas Marc Listl",
            "Oliver Bringmann"
        ],
        "tldr": "This paper introduces a new metric, EPSM, for evaluating the safety of environmental perception in autonomous driving, focusing on the risk associated with object and lane detection errors which are not captured by conventional metrics.",
        "tldr_zh": "本文介绍了一种新的指标EPSM，用于评估自动驾驶中环境感知的安全性，重点关注物体和车道检测错误的风险，而这些风险无法通过传统的指标来捕获。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "对感知系统进行广泛评估对于确保智能车辆在复杂驾驶场景中的安全性至关重要。传统的性能指标，如精确率、召回率和 F1 值，评估了整体检测精度，但它们没有考虑感知的安全相关方面。因此，在这些指标中获得高分的感知系统仍然可能导致可能引发严重事故的误检。因此，重要的是不仅要评估感知系统的整体性能，还要评估其安全性。因此，我们引入了一种新的安全指标，用于联合评估最关键的感知任务：目标和车道检测。我们提出的框架集成了一种新的、轻量级的目标安全指标，该指标量化了与目标检测错误相关的潜在风险，以及一种车道安全指标，该指标涵盖了安全评估中可能发生的两个任务之间的相互依赖性。由此产生的组合安全分数提供了一种统一的、可解释的感知安全性能衡量标准。利用 DeepAccident 数据集，我们证明了我们的方法能够识别出传统性能指标无法捕捉到的安全关键型感知错误。我们的研究结果强调了以安全为中心的评估方法对于自动驾驶感知系统的重要性。"
    },
    {
        "title": "Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving",
        "summary": "Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles.",
        "url": "http://arxiv.org/abs/2512.15181v1",
        "published_date": "2025-12-17T08:28:53+00:00",
        "updated_date": "2025-12-17T08:28:53+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jörg Gamerdinger",
            "Sven Teufel",
            "Stephan Amann",
            "Oliver Bringmann"
        ],
        "tldr": "This paper introduces criticality metrics and application strategies for improved safety evaluation of object detection in automated driving systems, validated on the DeepAccident dataset with significant accuracy improvements.",
        "tldr_zh": "本文介绍了关键性指标和应用策略，以改进自动驾驶系统中目标检测的安全评估。该方法在DeepAccident数据集上进行了验证，并显著提高了准确性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 6,
        "summary_zh": "确保安全是自动驾驶的首要目标，这需要对环境进行全面而准确的感知。虽然存在大量用于评估感知性能的指标，但纳入安全特定的指标对于可靠地评估目标检测系统至关重要。安全评估的关键组成部分是区分相关和非相关对象的能力——这是一个由重要性或相关性指标解决的挑战。本文首次对用于目标检测系统安全评估的重要性指标进行了深入分析。通过对现有文献的全面回顾，我们识别并评估了一系列适用的指标。通过使用包含各种安全关键场景的DeepAccident数据集，我们对这些指标的有效性进行了实证验证。为了提高评估准确性，我们提出了两种新的应用策略：双向重要性评分和多指标聚合。我们的方法在重要性分类准确性方面表现出高达100%的改进，突显了其在显著提升自动驾驶车辆中目标检测系统安全评估方面的潜力。"
    },
    {
        "title": "Human-like Working Memory from Artificial Intrinsic Plasticity Neurons",
        "summary": "Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.",
        "url": "http://arxiv.org/abs/2512.15829v1",
        "published_date": "2025-12-17T17:24:37+00:00",
        "updated_date": "2025-12-17T17:24:37+00:00",
        "categories": [
            "cs.ET",
            "cs.AI",
            "cs.CV",
            "cs.NE"
        ],
        "authors": [
            "Jingli Liu",
            "Huannan Zheng",
            "Bohao Zou",
            "Kezhou Yang"
        ],
        "tldr": "The paper presents IPNet, a neuromorphic architecture leveraging Magnetic Tunnel Junctions to emulate human-like working memory with significantly reduced power consumption and improved performance on dynamic vision tasks compared to traditional deep learning models.",
        "tldr_zh": "该论文提出了IPNet，一种利用磁性隧道结模拟人脑工作记忆的神经形态架构，与传统深度学习模型相比，显著降低了功耗并在动态视觉任务上实现了更优的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 6,
        "summary_zh": "工作记忆使大脑能够整合瞬时信息以进行快速决策。人工神经网络通常通过循环或并行架构复制这一过程，但会产生高能耗和噪声敏感性。在此，我们报告IPNet，一种硬件-软件协同设计的神经形态架构，通过神经元内在可塑性实现类人工作记忆。利用磁隧道结 (MTJ) 的焦耳热动力学，IPNet 在物理上模拟生物记忆的易失性。所提出的架构的记忆行为在 n-back、自由回忆和记忆干扰任务中表现出与报告的人类受试者相似的趋势。通过完全由 MTJ 神经元实现，具有类人工作记忆的架构在 11 类 DVS 手势数据集上实现了 99.65% 的准确率，并在一个新的 22 类时间反转基准测试中保持 99.48% 的准确率，优于共享相同骨干网络的 RNN、LSTM 和 2+1D CNN 基线。对于自动驾驶 (DDD-20)，与 ResNet-LSTM 相比，IPNet 将转向预测误差降低了 14.4%。在架构上，我们发现了一种“前沿记忆”效应，即在传感界面处性能最大化，验证了一种生物合理的近传感器处理范式。至关重要的是，所有结果都依赖于来自已制造设备的原始参数，无需优化。硬件在环验证证实了系统的物理可实现性。另一方面，能量分析显示，与 LSTM 相比，内存功耗降低了 2,874 倍，与并行 3D-CNN 相比降低了 90,920 倍。这种无电容设计实现了紧凑的 ~1.5um2 占位面积（28 nm CMOS）：比标准 LIF 神经元减少了 20 倍以上。最终，我们证明了通过神经元内在可塑性实例化类人工作记忆使神经网络具备动态视觉处理的卓越性和代谢成本的最小化这两种生物优势。"
    },
    {
        "title": "Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations",
        "summary": "This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.",
        "url": "http://arxiv.org/abs/2512.15388v1",
        "published_date": "2025-12-17T12:40:01+00:00",
        "updated_date": "2025-12-17T12:40:01+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Reinhard Moratz",
            "Niklas Daute",
            "James Ondieki",
            "Markus Kattenbeck",
            "Mario Krajina",
            "Ioannis Giannopoulos"
        ],
        "tldr": "The paper explores improving LLM-generated pedestrian route instructions using qualitative spatial relations and graph-based retrieval augmented generation (RAG) on street networks.",
        "tldr_zh": "本文探讨了如何通过使用定性空间关系和基于图的检索增强生成 (RAG)，来改进 LLM 生成的行人路线指引。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "本文探讨了通过定性空间关系来提升大型语言模型(LLM)为行人寻路者提供路线指引的能力。"
    },
    {
        "title": "Vision-based module for accurately reading linear scales in a laboratory",
        "summary": "Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.",
        "url": "http://arxiv.org/abs/2512.15327v1",
        "published_date": "2025-12-17T11:24:22+00:00",
        "updated_date": "2025-12-17T11:24:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Parvesh Saini",
            "Soumyadipta Maiti",
            "Beena Rai"
        ],
        "tldr": "This paper presents a vision-based system for accurately reading linear scales, demonstrated on syringes and measuring cylinders, achieving human-level accuracy.",
        "tldr_zh": "本文提出了一种基于视觉的系统，用于精确读取线性刻度，并在注射器和量筒上进行了演示，实现了人类水平的精度。",
        "relevance_score": 5,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "基于视觉的模型的能力和数量正在迅速增加。这些视觉模型现在能够以极高的精度完成诸如目标检测、图像分类、实例分割等更多任务。然而，能够像人类一样仅通过观察图像就能进行准确的定量测量的模型仍然很少见。为使机器人在实验室环境中完全自主地工作，它需要具备一些基本技能，如导航、处理物体、准备样品等，以在非结构化环境中达到类似人类的能力。另一个重要的能力是从仪器和设备上读取测量值。这里，我们尝试模仿一种受人类启发的方法来读取线性标尺上的测量值。作为一个测试用例，我们选择了读取注射器和量筒的液位。对于随机定向的注射器，我们进行变换以校正方向。为了使系统高效且稳健，感兴趣区域被缩小至仅包含图像中线性刻度的部分。之后，提取一系列特征，如主刻度线、相应的数字和液位指示器的位置，并由此计算出最终读数。使用该系统获得的读数也与同一实例的人工读取值进行了比较，并观察到准确的对应关系。"
    },
    {
        "title": "Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study",
        "summary": "Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.",
        "url": "http://arxiv.org/abs/2512.15044v1",
        "published_date": "2025-12-17T03:16:06+00:00",
        "updated_date": "2025-12-17T03:16:06+00:00",
        "categories": [
            "cs.AI",
            "cs.NI"
        ],
        "authors": [
            "Wenwen Xie",
            "Geng Sun",
            "Ruichen Zhang",
            "Xuejie Liu",
            "Yinqiu Liu",
            "Jiacheng Wang",
            "Dusit Niyato",
            "Ping Zhang"
        ],
        "tldr": "This paper explores the application of agentic AI, especially GenAI, to integrated sensing and communication (ISAC) systems, proposing a novel framework and case study to demonstrate its performance optimization capabilities.",
        "tldr_zh": "该论文探讨了类智能体人工智能（尤其是生成式人工智能）在集成感知与通信（ISAC）系统中的应用，提出了一个新颖的框架，并通过案例研究证明了其性能优化能力。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "集成感知与通信（ISAC）已成为第六代（6G）时代的关键发展方向，它为未来智能网络的协同感知和通信提供了重要支持。然而，随着无线环境变得日益动态和复杂，ISAC系统需要更智能的处理和更自主的运行，以保持效率和适应性。同时，代理人工智能（AI）通过在动态环境中实现连续的感知-推理-行动循环，为解决这些挑战提供了一种可行的方案，从而支持ISAC系统的智能、自主和高效运行。因此，在这项工作中，我们将深入研究代理AI在ISAC系统中的应用价值和前景。首先，我们对代理AI和ISAC系统进行了全面的回顾，以展示它们的关键特性。其次，我们展示了几种常见的ISAC系统优化方法，并强调了基于生成式人工智能（GenAI）的代理AI的显著优势。第三，我们提出了一个新颖的代理ISAC框架，并提出了一个案例研究来验证其在优化ISAC性能方面的优越性。最后，我们阐明了基于代理AI的ISAC系统未来的研究方向。"
    },
    {
        "title": "Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies",
        "summary": "High-content screening microscopy generates large amounts of live-cell imaging data, yet its potential remains constrained by the inability to determine when and where to image most effectively. Optimally balancing acquisition time, computational capacity, and photobleaching budgets across thousands of dynamically evolving regions of interest remains an open challenge, further complicated by limited field-of-view adjustments and sensor sensitivity. Existing approaches either rely on static sampling or heuristics that neglect the dynamic evolution of biological processes, leading to inefficiencies and missed events. Here, we introduce the restless multi-process multi-armed bandit (RMPMAB), a new decision-theoretic framework in which each experimental region is modeled not as a single process but as an ensemble of Markov chains, thereby capturing the inherent heterogeneity of biological systems such as asynchronous cell cycles and heterogeneous drug responses. Building upon this foundation, we derive closed-form expressions for transient and asymptotic behaviors of aggregated processes, and design scalable Whittle index policies with sub-linear complexity in the number of imaging regions. Through both simulations and a real biological live-cell imaging dataset, we show that our approach achieves substantial improvements in throughput under resource constraints. Notably, our algorithm outperforms Thomson Sampling, Bayesian UCB, epsilon-Greedy, and Round Robin by reducing cumulative regret by more than 37% in simulations and capturing 93% more biologically relevant events in live imaging experiments, underscoring its potential for transformative smart microscopy. Beyond improving experimental efficiency, the RMPMAB framework unifies stochastic decision theory with optimal autonomous microscopy control, offering a principled approach to accelerate discovery across multidisciplinary sciences.",
        "url": "http://arxiv.org/abs/2512.14930v1",
        "published_date": "2025-12-16T21:42:46+00:00",
        "updated_date": "2025-12-16T21:42:46+00:00",
        "categories": [
            "stat.AP",
            "cs.AI",
            "eess.SY"
        ],
        "authors": [
            "Jaume Anguera Peris",
            "Songtao Cheng",
            "Hanzhao Zhang",
            "Wei Ouyang",
            "Joakim Jaldén"
        ],
        "tldr": "The paper introduces a restless multi-process multi-armed bandit (RMPMAB) framework for optimizing live-cell imaging microscopy, demonstrating significant improvements in throughput and event capture compared to existing methods. This leads to more efficient experiments and discoveries.",
        "tldr_zh": "该论文介绍了一种用于优化活细胞成像显微镜的restless multi-process multi-armed bandit (RMPMAB) 框架，与现有方法相比，在吞吐量和事件捕获方面表现出显著的改进。这有助于实现更高效的实验和发现。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "高内涵筛选显微镜技术产生大量的活细胞成像数据，但其潜力仍受限于无法确定最佳成像时间和位置。如何在成千上万个动态演化的感兴趣区域内，优化平衡采集时间、计算能力和光漂白预算，仍然是一个开放的挑战，且因有限的视野调整和传感器灵敏度而进一步复杂化。现有方法要么依赖于静态采样，要么依赖于忽略生物过程动态演化的启发式方法，导致效率低下和遗漏事件。在此，我们引入了“不安分多进程多臂老虎机”（RMPMAB），这是一个新的决策理论框架，其中每个实验区域不是被建模为单个进程，而是被建模为马尔可夫链的集合，从而捕捉了生物系统的内在异质性，例如异步细胞周期和异质性药物反应。在此基础上，我们推导出聚合过程的瞬态和渐近行为的闭式表达式，并设计了具有子线性复杂度的可扩展Whittle指数策略，算法复杂度与成像区域的数量成正比。通过模拟和真实的生物活细胞成像数据集，我们表明我们的方法在资源约束下实现了吞吐量的显着提升。值得注意的是，我们的算法在模拟中将累积遗憾降低37%以上，并在活体成像实验中捕获到多93%的生物学相关事件，优于汤姆森采样、贝叶esian UCB、epsilon-Greedy和轮询算法，突显了其变革智能显微镜技术的潜力。除了提高实验效率外，RMPMAB框架统一了随机决策理论与最优自主显微镜控制，为加速跨多学科科学的发现提供了一种有原则的方法。"
    },
    {
        "title": "RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting",
        "summary": "Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL",
        "url": "http://arxiv.org/abs/2512.15488v1",
        "published_date": "2025-12-17T14:37:27+00:00",
        "updated_date": "2025-12-17T14:37:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seyed Abolfazl Ghasemzadeh",
            "Alexandre Alahi",
            "Christophe De Vleeschouwer"
        ],
        "tldr": "This paper introduces RUMPL, a transformer-based 3D pose lifter that uses a 3D ray-based representation of 2D keypoints to achieve camera calibration and view-number independence for multi-view 3D human pose estimation, demonstrating significant improvement over existing methods on various benchmarks.",
        "tldr_zh": "本文介绍了一种基于 Transformer 的3D姿态提升器RUMPL，它采用基于3D射线的2D关键点表示方法，实现了相机校准和视角数量的独立性，用于多视角3D人体姿态估计，并在各种基准测试中相比现有方法实现了显著提升。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "从2D图像估计3D人体姿态仍然具有挑战性，这归因于遮挡和透视歧义。基于多视角学习的方法可以缓解这些问题，但由于缺乏带有3D真实标注的大规模多视角数据集，且这些数据集通常在受限条件下捕获，因此常常无法推广到真实场景。为了克服这一局限性，最近的方法依赖于2D姿态估计，并结合在合成数据上训练的2D到3D姿态提升。在之前的MPL框架的基础上，我们提出了RUMPL，一个基于Transformer的3D姿态提升器，它引入了2D关键点的3D射线表示。这种形式化使得模型独立于相机标定和视角数量，从而能够在任意多视角配置中进行通用部署，而无需重新训练或微调。一种新的视角融合Transformer利用学习到的融合射线令牌来聚合沿射线的信息，从而进一步提高多视角一致性。大量的实验表明，与三角测量相比，RUMPL将MPJPE降低了高达53%，与基于图像表示的Transformer基线相比，则降低了超过60%。在新基准测试上的结果，包括野外多视角和多人数据集，证实了其鲁棒性和可扩展性。该框架的源代码可在https://github.com/aghasemzadeh/OpenRUMPL获取。"
    },
    {
        "title": "Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models",
        "summary": "Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an \"Expand-and-Prune\" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.",
        "url": "http://arxiv.org/abs/2512.15347v1",
        "published_date": "2025-12-17T11:44:34+00:00",
        "updated_date": "2025-12-17T11:44:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shiran Ge",
            "Chenyi Huang",
            "Yuang Ai",
            "Qihang Fan",
            "Huaibo Huang",
            "Ran He"
        ],
        "tldr": "The paper introduces Pro-GRPO, a novel dynamic framework for aligning generative models that addresses the computational bottleneck of Group Relative Policy Optimization (GRPO) by proactively pruning reward-clustered trajectories during the sampling process, improving efficiency and allowing for increased trajectory diversity.",
        "tldr_zh": "该论文介绍了一种名为Pro-GRPO的新型动态框架，用于对齐生成模型。它通过在采样过程中主动修剪奖励聚集的轨迹来解决Group Relative Policy Optimization (GRPO)的计算瓶颈，从而提高效率并允许更大的轨迹多样性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "群组相对策略优化（GRPO）是一种用于对齐生成模型的强大技术，但其有效性受限于大组规模和高昂计算成本之间的冲突。本文通过实证研究对此权衡进行考察，得出了两个关键观察结果。首先，我们发现了奖励聚类现象，其中许多轨迹坍缩到组平均奖励附近，优化价值有限。其次，我们设计了一种名为最优方差滤波（OVF）的启发式策略，并验证了OVF选择的高方差轨迹子集可以优于更大的、未过滤的组。然而，这种静态的、后采样的OVF方法仍然需要显著的计算开销，因为它对最终被丢弃的轨迹执行了不必要的采样。为了解决这个问题，我们提出了Pro-GRPO（主动GRPO），这是一种新颖的动态框架，集成了基于潜在特征的轨迹剪枝到采样过程中。通过提前终止奖励聚类的轨迹，Pro-GRPO降低了计算开销。凭借其效率，Pro-GRPO采用了一种“扩展与剪枝”策略。该策略首先扩展初始采样组的规模，以最大化轨迹多样性，然后将多步OVF应用于潜在表示，从而避免了高昂的计算成本。在基于扩散和基于流的模型上的大量实验表明了我们Pro-GRPO框架的通用性和有效性。"
    },
    {
        "title": "From Camera to World: A Plug-and-Play Module for Human Mesh Transformation",
        "summary": "Reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images remains challenging due to the lack of camera rotation information. While existing methods achieve promising results in the camera coordinate system by assuming zero camera rotation, this simplification leads to significant errors when transforming the reconstructed mesh to the world coordinate system. To address this challenge, we propose Mesh-Plug, a plug-and-play module that accurately transforms human meshes from camera coordinates to world coordinates. Our key innovation lies in a human-centered approach that leverages both RGB images and depth maps rendered from the initial mesh to estimate camera rotation parameters, eliminating the dependency on environmental cues. Specifically, we first train a camera rotation prediction module that focuses on the human body's spatial configuration to estimate camera pitch angle. Then, by integrating the predicted camera parameters with the initial mesh, we design a mesh adjustment module that simultaneously refines the root joint orientation and body pose. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on the benchmark datasets SPEC-SYN and SPEC-MTP.",
        "url": "http://arxiv.org/abs/2512.15212v1",
        "published_date": "2025-12-17T09:05:46+00:00",
        "updated_date": "2025-12-17T09:05:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changhai Ma",
            "Ziyu Wu",
            "Yunkang Zhang",
            "Qijun Ying",
            "Boyan Liu",
            "Xiaohui Cai"
        ],
        "tldr": "This paper introduces Mesh-Plug, a plug-and-play module for transforming 3D human meshes from camera to world coordinates by estimating camera rotation parameters using a human-centered approach, achieving state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "本文介绍了一种名为 Mesh-Plug 的即插即用模块，通过使用以人为中心的方法估计相机旋转参数，将 3D 人体网格从相机坐标转换为世界坐标，并在基准数据集上实现了最先进的性能。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "从真实场景图像中重建世界坐标系下精确的3D人体网格模型仍然具有挑战性，其主要原因是缺乏相机旋转信息。现有方法通过假设零相机旋转在相机坐标系中取得了较好的结果，但这种简化在将重建的网格转换到世界坐标系时会导致显著的误差。为了解决这一挑战，我们提出了Mesh-Plug，一个即插即用的模块，能够准确地将人体网格从相机坐标系转换到世界坐标系。我们的主要创新在于一种以人为中心的方法，该方法利用RGB图像和从初始网格渲染的深度图来估计相机旋转参数，从而消除了对环境线索的依赖。具体而言，我们首先训练一个相机旋转预测模块，该模块专注于人体空间构型以估计相机俯仰角。然后，通过将预测的相机参数与初始网格集成，我们设计了一个网格调整模块，该模块可以同时细化根关节方向和身体姿势。大量实验表明，我们的框架在基准数据集SPEC-SYN和SPEC-MTP上优于最先进的方法。"
    },
    {
        "title": "Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning",
        "summary": "Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.",
        "url": "http://arxiv.org/abs/2512.15153v1",
        "published_date": "2025-12-17T07:35:03+00:00",
        "updated_date": "2025-12-17T07:35:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengshi Qi",
            "Yeteng Wu",
            "Xianlin Zhang",
            "Huadong Ma"
        ],
        "tldr": "This paper introduces a new task of Human Action Form Assessment (AFA), a corresponding dataset CoT-AFA with Chain-of-Thought explanations, and an Explainable Fitness Assessor framework that judges, explains, and provides solutions for action assessment.",
        "tldr_zh": "本文介绍了一个新的人类动作形态评估（AFA）任务，一个相应的包含思维链解释的CoT-AFA数据集，以及一个可解释的健身评估器框架, 可以判断、解释和提供动作评估的解决方案。",
        "relevance_score": 5,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "评估人类行为是否规范并提供合理的反馈以提高行为标准化程度在实际场景中至关重要，但也极具挑战性。然而，当前的视频理解方法主要关注行为是什么以及在哪里发生，无法满足上述需求。同时，现有的大多数数据集缺乏指示行为标准化程度的标签，并且行为质量评估数据集缺乏可解释性和详细的反馈。因此，我们定义了一个新的“人体动作形态评估（AFA）”任务，并引入了一个新的多样化数据集CoT-AFA，其中包含大量健身和武术视频，并带有用于全面视频分析的多层次标注。我们利用一种新颖的“思维链”解释范式来丰富CoT-AFA数据集。我们的解释不是提供孤立的反馈，而是提供了一个完整的推理过程——从识别一个动作步骤到分析其结果并提出具体的解决方案。此外，我们提出了一个名为“可解释健身评估器”的框架，该框架不仅可以判断一个动作，还可以解释原因并提供解决方案。该框架采用两个并行处理流和一个动态门控机制来融合视觉和语义信息，从而提高其分析能力。实验结果表明，我们的方法在解释生成（例如，CIDEr指标提升16.0%）、动作分类（准确率提升2.7%）和质量评估（准确率提升2.1%）方面都取得了改进，揭示了CoT-AFA在未来研究中的巨大潜力。我们的数据集和源代码可在https://github.com/MICLAB-BUPT/EFA获取。"
    },
    {
        "title": "Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs",
        "summary": "Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.",
        "url": "http://arxiv.org/abs/2512.15483v1",
        "published_date": "2025-12-17T14:35:31+00:00",
        "updated_date": "2025-12-17T14:35:31+00:00",
        "categories": [
            "cs.LG"
        ],
        "authors": [
            "Luca Torresi",
            "Pascal Friederich"
        ],
        "tldr": "This paper introduces a multi-stage Bayesian optimization method that incorporates intermediate measurements (proxy measurements) for dynamic decision-making in self-driving labs, demonstrating improved performance over conventional Bayesian optimization.",
        "tldr_zh": "本文介绍了一种多阶段贝叶斯优化方法，该方法结合了中间测量（代理测量）来实现自动驾驶实验室中的动态决策，并证明其性能优于传统的贝叶斯优化。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "自动驾驶实验室（SDL）正结合机器人、自动化以及基于机器学习的数据分析和决策领域的最新技术进展，在无需任何直接人工干预的情况下，自主地进行实验，以实现人为设定的目标。SDL已成功应用于材料科学、化学及其他领域，以系统且数据高效的方式优化流程、材料和设备。目前，最广泛使用的识别最具信息量下一步实验的算法是贝叶斯优化。虽然贝叶斯优化相对简单且适用于各种优化问题，但标准的贝叶斯优化依赖于固定的实验流程，具有清晰的优化参数集以及一个或多个可测量的目标函数。这排除了对计划操作顺序进行即时变更，以及将中间测量纳入决策过程的可能性。因此，许多现实世界的实验需要进行调整和简化，才能转换为自动驾驶实验室中的常见设置。在本文中，我们介绍了一种贝叶斯优化的扩展，该扩展允许灵活采样多阶段工作流程，并基于中间可观测值（我们称之为代理测量）做出最优决策。我们系统地比较了考虑代理测量与传统贝叶斯优化（仅观察最终测量）的优势。我们发现，在各种场景下，代理测量在寻找良好解决方案的时间和所找到解决方案的整体最优性方面均产生了显著的改进。这不仅为在自动实验室中使用更复杂、从而更真实的实验工作流程铺平了道路，也为在下一代SDL中顺利结合模拟和实验提供了可能。"
    },
    {
        "title": "Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes",
        "summary": "We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.",
        "url": "http://arxiv.org/abs/2512.14991v1",
        "published_date": "2025-12-17T00:52:19+00:00",
        "updated_date": "2025-12-17T00:52:19+00:00",
        "categories": [
            "cs.LG",
            "math.OC",
            "q-fin.PM"
        ],
        "authors": [
            "Hanqing Jin",
            "Renyuan Xu",
            "Yanzhao Yang"
        ],
        "tldr": "This paper introduces a model-based reinforcement learning algorithm for controlled diffusion processes with unbounded continuous state spaces, using adaptive partitioning to balance exploration and approximation, and provides regret bounds with applications to high-dimensional problems like portfolio selection.",
        "tldr_zh": "该论文提出了一种基于模型的强化学习算法，用于具有无界连续状态空间的可控扩散过程，通过自适应分区来平衡探索和近似，并提供了后悔界限，应用于高维问题，如投资组合选择。",
        "relevance_score": 5,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "我们研究了针对受控扩散过程的强化学习，该过程具有无界的连续状态空间、有界的连续动作以及多项式增长的奖励：这些设置自然出现在金融、经济学和运筹学中。为了克服连续和高维域的挑战，我们引入了一种基于模型的算法，该算法自适应地划分联合状态-动作空间。该算法在每个分区内维护漂移、波动率和奖励的估计器，并在估计偏差超过统计置信度时细化离散化。这种自适应方案平衡了探索和近似，从而能够在无界域中进行高效学习。我们的分析建立了后悔界限，这些界限取决于问题时域、状态维度、奖励增长阶数，以及为无界扩散过程量身定制的新定义的缩放维度概念。这些界限在特定情况下恢复了有界设置的现有结果，同时将理论保证扩展到更广泛的扩散类型问题。最后，我们通过数值实验验证了我们方法的有效性，包括应用于高维问题（如多资产均值-方差投资组合选择）的应用。"
    },
    {
        "title": "An Open Toolkit for Underwater Field Robotics",
        "summary": "Underwater robotics is becoming increasingly important for marine science, environmental monitoring, and subsea industrial operations, yet the development of underwater manipulation and actuation systems remains restricted by high costs, proprietary designs, and limited access to modular, research-oriented hardware. While open-source initiatives have democratized vehicle construction and control software, a substantial gap persists for joint-actuated systems-particularly those requiring waterproof, feedback-enabled actuation suitable for manipulators, grippers, and bioinspired devices. As a result, many research groups face lengthy development cycles, limited reproducibility, and difficulty transitioning laboratory prototypes to field-ready platforms.\n  To address this gap, we introduce an open, cost-effective hardware and software toolkit for underwater manipulation research. The toolkit includes a depth-rated Underwater Robotic Joint (URJ) with early leakage detection, compact control and power management electronics, and a ROS2-based software stack for sensing and multi-mode actuation. All CAD models, fabrication files, PCB sources, firmware, and ROS2 packages are openly released, enabling local manufacturing, modification, and community-driven improvement.\n  The toolkit has undergone extensive laboratory testing and multiple field deployments, demonstrating reliable operation up to 40 m depth across diverse applications, including a 3-DoF underwater manipulator, a tendon-driven soft gripper, and an underactuated sediment sampler. These results validate the robustness, versatility, and reusability of the toolkit for real marine environments.\n  By providing a fully open, field-tested platform, this work aims to lower the barrier to entry for underwater manipulation research, improve reproducibility, and accelerate innovation in underwater field robotics.",
        "url": "http://arxiv.org/abs/2512.15597v1",
        "published_date": "2025-12-17T17:06:35+00:00",
        "updated_date": "2025-12-17T17:06:35+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Giacomo Picardi",
            "Saverio Iacoponi",
            "Matias Carandell",
            "Jorge Aguirregomezcorta",
            "Mrudul Chellapurath",
            "Joaquin del Rio",
            "Marcello Calisti",
            "Iacopo Aguzzi"
        ],
        "tldr": "This paper introduces an open-source hardware and software toolkit for underwater robotics manipulation, designed to lower the barrier to entry for research and improve reproducibility.",
        "tldr_zh": "本文介绍了一个用于水下机器人操作的开源硬件和软件工具包，旨在降低研究门槛并提高可重复性。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "水下机器人技术在海洋科学、环境监测和海底工业作业中变得日益重要，然而，水下操作和驱动系统的开发仍然受到高成本、专利设计以及模块化、研究导向型硬件获取受限的制约。虽然开源计划已经普及了车辆的制造和控制软件，但对于联合驱动系统（特别是那些需要防水、具有反馈功能的驱动系统，适用于机械臂、抓取器和仿生设备）仍然存在显著差距。因此，许多研究团队面临着漫长的开发周期、有限的可重复性和将实验室原型过渡到现场就绪平台的困难。\n\n为了解决这一差距，我们推出了一套开放、经济高效的硬件和软件工具包，用于水下操作研究。该工具包包括一个具有早期泄漏检测功能的深度额定水下机器人关节（URJ）、紧凑型控制和电源管理电子设备以及基于ROS2的用于传感和多模式驱动的软件堆栈。所有的CAD模型、制造文件、PCB源文件、固件和ROS2软件包都已开放发布，从而能够进行本地制造、修改和社区驱动的改进。\n\n该工具包已经过广泛的实验室测试和多次现场部署，证明了其在 40 米深度内各种应用中的可靠运行，包括一个 3 自由度水下机械臂、一个肌腱驱动的软抓取器和一个欠驱动沉积物采样器。这些结果验证了该工具包在真实海洋环境中具有鲁棒性、通用性和可重用性。\n\n通过提供一个完全开放、经过现场测试的平台，这项工作旨在降低水下操作研究的入门门槛，提高可重复性，并加速水下现场机器人技术的创新。"
    },
    {
        "title": "Load-Based Variable Transmission Mechanism for Robotic Applications",
        "summary": "This paper presents a Load-Based Variable Transmission (LBVT) mechanism designed to enhance robotic actuation by dynamically adjusting the transmission ratio in response to external torque demands. Unlike existing variable transmission systems that require additional actuators for active control, the proposed LBVT mechanism leverages a pre-tensioned spring and a four-bar linkage to passively modify the transmission ratio, thereby reducing the complexity of robot joint actuation systems. The effectiveness of the LBVT mechanism is evaluated through simulation-based analyses. The results confirm that the system achieves up to a 40 percent increase in transmission ratio upon reaching a predefined torque threshold, effectively amplifying joint torque when required without additional actuation. Furthermore, the simulations demonstrate a torque amplification effect triggered when the applied force exceeds 18 N, highlighting the system ability to autonomously respond to varying load conditions. This research contributes to the development of lightweight, efficient, and adaptive transmission systems for robotic applications, particularly in legged robots where dynamic torque adaptation is critical.",
        "url": "http://arxiv.org/abs/2512.15448v1",
        "published_date": "2025-12-17T13:45:22+00:00",
        "updated_date": "2025-12-17T13:45:22+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Sinan Emre",
            "Victor Barasuol",
            "Matteo Villa",
            "Claudio Semini"
        ],
        "tldr": "This paper introduces a passive Load-Based Variable Transmission (LBVT) mechanism for robotic joints that dynamically adjusts the transmission ratio based on external torque, offering improved efficiency and adaptability without active control.",
        "tldr_zh": "本文介绍了一种用于机器人关节的被动式负载可变传动（LBVT）机制，该机制可以根据外部扭矩动态调整传动比，无需主动控制，从而提高效率和适应性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文提出了一种基于负载的可变传动（LBVT）机制，旨在通过动态调整传动比以响应外部扭矩需求，从而增强机器人驱动性能。与现有需要额外执行器进行主动控制的可变传动系统不同，所提出的LBVT机制利用预紧弹簧和四杆机构被动地改变传动比，从而降低了机器人关节驱动系统的复杂性。通过仿真分析对LBVT机制的有效性进行了评估。结果证实，该系统在达到预定义的扭矩阈值时，传动比可提高高达40%，从而在不需要额外驱动的情况下有效放大关节扭矩。此外，仿真结果表明，当施加的力超过18 N时，会触发扭矩放大效应，突出了系统自主响应变化负载情况的能力。这项研究有助于为机器人应用，特别是动态扭矩适应至关重要的腿式机器人，开发轻量化、高效且自适应的传动系统。"
    },
    {
        "title": "GuangMing-Explorer: A Four-Legged Robot Platform for Autonomous Exploration in General Environments",
        "summary": "Autonomous exploration is a fundamental capability that tightly integrates perception, planning, control, and motion execution. It plays a critical role in a wide range of applications, including indoor target search, mapping of extreme environments, resource exploration, etc. Despite significant progress in individual components, a holistic and practical description of a completely autonomous exploration system, encompassing both hardware and software, remains scarce. In this paper, we present GuangMing-Explorer, a fully integrated autonomous exploration platform designed for robust operation across diverse environments. We provide a comprehensive overview of the system architecture, including hardware design, software stack, algorithm deployment, and experimental configuration. Extensive real-world experiments demonstrate the platform's effectiveness and efficiency in executing autonomous exploration tasks, highlighting its potential for practical deployment in complex and unstructured environments.",
        "url": "http://arxiv.org/abs/2512.15309v1",
        "published_date": "2025-12-17T10:53:32+00:00",
        "updated_date": "2025-12-17T10:53:32+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Kai Zhang",
            "Shoubin Chen",
            "Dong Li",
            "Baiyang Zhang",
            "Tao Huang",
            "Zehao Wu",
            "Jiasheng Chen",
            "Bo Zhang"
        ],
        "tldr": "The paper introduces GuangMing-Explorer, a complete four-legged robot platform for autonomous exploration, showcasing its hardware, software, and experimental results in real-world environments, but with no apparent RL or LLM integration.",
        "tldr_zh": "本文介绍了一种完整的四足机器人自主探索平台GuangMing-Explorer，展示了其硬件、软件以及在真实环境中的实验结果，但没有明显的强化学习或大型语言模型集成。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "自主探索是一项将感知、规划、控制和运动执行紧密结合的关键能力。它在广泛的应用中扮演着至关重要的角色，包括室内目标搜索、极端环境地图构建、资源勘探等。尽管各个组成部分都取得了显著进展，但对于一个完整的自主探索系统（包括硬件和软件）的整体和实用性描述仍然不足。在本文中，我们提出了 GuangMing-Explorer，一个为在不同环境中稳健运行而设计的完全集成的自主探索平台。我们提供了系统架构的全面概述，包括硬件设计、软件栈、算法部署和实验配置。大量的真实世界实验证明了该平台在执行自主探索任务中的有效性和效率，突显了其在复杂和非结构化环境中实际部署的潜力。"
    },
    {
        "title": "NAP3D: NeRF Assisted 3D-3D Pose Alignment for Autonomous Vehicles",
        "summary": "Accurate localization is essential for autonomous vehicles, yet sensor noise and drift over time can lead to significant pose estimation errors, particularly in long-horizon environments. A common strategy for correcting accumulated error is visual loop closure in SLAM, which adjusts the pose graph when the agent revisits previously mapped locations. These techniques typically rely on identifying visual mappings between the current view and previously observed scenes and often require fusing data from multiple sensors.\n  In contrast, this work introduces NeRF-Assisted 3D-3D Pose Alignment (NAP3D), a complementary approach that leverages 3D-3D correspondences between the agent's current depth image and a pre-trained Neural Radiance Field (NeRF). By directly aligning 3D points from the observed scene with synthesized points from the NeRF, NAP3D refines the estimated pose even from novel viewpoints, without relying on revisiting previously observed locations.\n  This robust 3D-3D formulation provides advantages over conventional 2D-3D localization methods while remaining comparable in accuracy and applicability. Experiments demonstrate that NAP3D achieves camera pose correction within 5 cm on a custom dataset, robustly outperforming a 2D-3D Perspective-N-Point baseline. On TUM RGB-D, NAP3D consistently improves 3D alignment RMSE by approximately 6 cm compared to this baseline given varying noise, despite PnP achieving lower raw rotation and translation parameter error in some regimes, highlighting NAP3D's improved geometric consistency in 3D space. By providing a lightweight, dataset-agnostic tool, NAP3D complements existing SLAM and localization pipelines when traditional loop closure is unavailable.",
        "url": "http://arxiv.org/abs/2512.15080v1",
        "published_date": "2025-12-17T04:56:38+00:00",
        "updated_date": "2025-12-17T04:56:38+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Gaurav Bansal"
        ],
        "tldr": "The paper introduces NAP3D, a NeRF-assisted approach for refining camera pose in autonomous vehicles by aligning 3D points from the current view with a pre-trained NeRF, improving pose estimation accuracy, especially when traditional loop closure is unavailable.",
        "tldr_zh": "该论文介绍了一种名为 NAP3D 的 NeRF 辅助方法，通过将当前视角的 3D 点与预训练的 NeRF 对齐来改进自动驾驶汽车中的相机姿态，从而提高姿态估计精度，尤其是在传统回环检测不可用时。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "精确的定位对于自动驾驶车辆至关重要，但传感器噪声和随时间推移的漂移会导致显著的姿态估计误差，尤其是在长时程环境中。纠正累积误差的常见策略是SLAM中的视觉闭环，它在智能体重新访问先前映射的位置时调整姿态图。这些技术通常依赖于识别当前视图与先前观察到的场景之间的视觉映射，并且通常需要融合来自多个传感器的数据。\n\n与此相反，这项工作介绍了一种补充方法，即NeRF辅助的3D-3D姿态对齐 (NAP3D)，它利用智能体当前深度图像与预训练神经辐射场(NeRF)之间的3D-3D对应关系。通过直接将来自观测场景的3D点与来自NeRF合成的3D点对齐，NAP3D即使从新的视角也能优化估计的姿态，而无需依赖于重新访问先前观察到的位置。\n\n这种稳健的3D-3D公式在保持与传统2D-3D定位方法相当的精度和适用性的同时，提供了优于传统2D-3D定位方法的优势。实验表明，NAP3D在自定义数据集上实现了5厘米以内的相机姿态校正，稳健地优于2D-3D透视-N点 (Perspective-N-Point) 基线。在TUM RGB-D数据集上，NAP3D始终将3D对齐RMSE改进约6厘米，与该基线相比，尽管 PnP 在某些情况下实现了较低的原始旋转和平移参数误差，但这突出了 NAP3D 在 3D 空间中改进的几何一致性。通过提供一种轻量级、数据集无关的工具，NAP3D在传统闭环不可用时，补充了现有的SLAM和定位流程。"
    },
    {
        "title": "Breathe with Me: Synchronizing Biosignals for User Embodiment in Robots",
        "summary": "Embodiment of users within robotic systems has been explored in human-robot interaction, most often in telepresence and teleoperation. In these applications, synchronized visuomotor feedback can evoke a sense of body ownership and agency, contributing to the experience of embodiment. We extend this work by employing embreathment, the representation of the user's own breath in real time, as a means for enhancing user embodiment experience in robots. In a within-subjects experiment, participants controlled a robotic arm, while its movements were either synchronized or non-synchronized with their own breath. Synchrony was shown to significantly increase body ownership, and was preferred by most participants. We propose the representation of physiological signals as a novel interoceptive pathway for human-robot interaction, and discuss implications for telepresence, prosthetics, collaboration with robots, and shared autonomy.",
        "url": "http://arxiv.org/abs/2512.14952v1",
        "published_date": "2025-12-16T22:38:57+00:00",
        "updated_date": "2025-12-16T22:38:57+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Iddo Yehoshua Wald",
            "Amber Maimon",
            "Shiyao Zhang",
            "Dennis Küster",
            "Robert Porzel",
            "Tanja Schultz",
            "Rainer Malaka"
        ],
        "tldr": "This paper explores enhancing user embodiment in robots by synchronizing a robotic arm's movements with the user's breath, demonstrating increased body ownership through this \"embreathment\" technique.",
        "tldr_zh": "本文探讨通过将机器人手臂的运动与用户的呼吸同步来增强用户在机器人中的具身感，并通过这种“呼吸同步”技术展示了身体所有权的增加。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "在人机交互领域，用户在机器人系统中的具身性已得到探索，主要集中在远程呈现和遥操作方面。在这些应用中，同步的视动反馈可以唤起身体所有权和行动控制感，从而增强具身性体验。我们扩展了这项研究，采用心息同步（embreathment），即实时呈现用户自身的呼吸，作为增强用户在机器人系统中具身性体验的手段。在一个被试内实验中，参与者控制一个机械臂，其运动与他们自身的呼吸同步或非同步。结果表明，同步显著提高了身体所有权，并且受到大多数参与者的偏好。我们提出将生理信号的呈现作为一种新型的内感受通路应用于人机交互，并讨论其在远程呈现、假肢、与机器人协作以及共享自主方面的潜在影响。"
    },
    {
        "title": "State-Augmented Graphs for Circular Economy Triage",
        "summary": "Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.",
        "url": "http://arxiv.org/abs/2512.15824v1",
        "published_date": "2025-12-17T16:23:47+00:00",
        "updated_date": "2025-12-17T16:23:47+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Richard Fox",
            "Rui Li",
            "Gustav Jonsson",
            "Farzaneh Goli",
            "Miying Yang",
            "Emel Aktas",
            "Yongjing Wang"
        ],
        "tldr": "The paper introduces a decision-making framework for circular economy triage using state-augmented disassembly graphs, enabling optimal recursive evaluation of product disassembly and recycling pathways, demonstrated on EV batteries.",
        "tldr_zh": "该论文介绍了一个用于循环经济分类的决策框架，该框架使用状态增强的拆卸图，从而可以对产品拆卸和回收路径进行最佳的递归评估，并通过电动汽车电池进行了演示。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "循环经济（CE）分选是指对产品进行评估，以确定它们在达到目前使用状态的终点后可以遵循的可持续路径。有效的循环经济分选需要适应性决策，从而在保留价值与处理和劳动力成本及约束之间取得平衡。本文提出了一种新的决策框架，它是一个基于状态增强型拆卸序列规划（DSP）图的简单确定性求解器。通过将拆卸历史编码到状态中，我们的框架强制执行马尔可夫性质，从而通过确保每个决策仅依赖于前一个状态来实现最优的递归评估。分选决策涉及继续拆卸或选择循环经济方案之间的选择。该模型集成了基于诊断健康评分的状态感知效用以及复杂的运营约束。我们通过一个实际案例展示了该框架的灵活性：电动汽车（EV）电池的分层分选，其决策由组件的递归估值驱动。该示例说明了统一的形式如何能够适应不同的机械复杂性、安全要求和经济驱动因素。因此，这种统一的形式为优化不同产品和运营环境下的循环经济分选决策提供了易处理且可推广的基础。"
    },
    {
        "title": "IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion",
        "summary": "High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.",
        "url": "http://arxiv.org/abs/2512.15581v1",
        "published_date": "2025-12-17T16:40:52+00:00",
        "updated_date": "2025-12-17T16:40:52+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shashank Mishra",
            "Karan Patil",
            "Didier Stricker",
            "Jason Rambach"
        ],
        "tldr": "The paper introduces IMKD, a knowledge distillation framework for camera-radar fusion that uses intensity-aware multi-level distillation to improve 3D object detection performance, outperforming prior distillation methods on nuScenes.",
        "tldr_zh": "该论文介绍了IMKD，一个用于相机-雷达融合的知识蒸馏框架，它使用强度感知的多层蒸馏来提高3D目标检测性能，并在nuScenes上优于之前的蒸馏方法。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "通过在推理时不使用激光雷达，利用知识蒸馏可以实现高性能的雷达-相机3D目标检测。然而，现有的蒸馏方法通常将模态特定的特征直接传递给每个传感器，这可能会扭曲其独特的特性并降低其各自的优势。为了解决这个问题，我们提出了IMKD，一种基于多层知识蒸馏的雷达-相机融合框架，它在增强它们互补优势的同时，保留了每个传感器的内在特性。IMKD应用了一种三阶段、强度感知的蒸馏策略来丰富整个架构中的融合表示：（1）激光雷达到雷达的强度感知特征蒸馏，以使用精细的结构线索增强雷达表示；（2）激光雷达到融合特征的强度引导蒸馏，以选择性地突出融合级别的有用几何和深度信息，促进模态之间的互补性，而不是强制它们对齐；（3）相机-雷达的强度引导融合机制，以促进有效的特征对齐和校准。在nuScenes基准上的大量实验表明，IMKD达到67.0%的NDS和61.0%的mAP，优于所有先前的基于蒸馏的雷达-相机融合方法。我们的代码和模型可在https://github.com/dfki-av/IMKD/ 上找到。"
    },
    {
        "title": "Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics",
        "summary": "Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.",
        "url": "http://arxiv.org/abs/2512.15340v1",
        "published_date": "2025-12-17T11:37:35+00:00",
        "updated_date": "2025-12-17T11:37:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junjie Chen",
            "Fei Wang",
            "Zhihao Huang",
            "Qing Zhou",
            "Kun Li",
            "Dan Guo",
            "Linfeng Zhang",
            "Xun Yang"
        ],
        "tldr": "The paper introduces TIMAR, a causal framework for generating realistic 3D conversational head dynamics from audio-visual dialogue context, demonstrating improved performance on the DualTalk benchmark.",
        "tldr_zh": "该论文介绍了TIMAR，一个从视听对话语境中生成逼真3D对话头部动态的因果框架，并在 DualTalk 基准测试中展示了改进的性能。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "人类对话涉及语音和非言语线索的持续交换，例如点头、视线转移和面部表情，这些线索传达着注意力和情感。对这些3D双向动态进行建模对于构建富有表现力的虚拟化身和交互式机器人至关重要。然而，现有的框架通常将说话和倾听视为独立的过程，或者依赖于非因果的全序列建模，从而阻碍了跨回合的时间连贯性。我们提出了TIMAR（Turn-level Interleaved Masked AutoRegression，回合级交错掩码自回归），这是一个用于3D对话头部生成的因果框架，它将对话建模为交错的音频-视觉上下文。它融合了每个回合内的多模态信息，并应用回合级因果注意力来累积对话历史，同时一个轻量级的扩散头预测连续的3D头部动态，捕捉协调性和表达性变异。在 DualTalk 基准测试上的实验表明，TIMAR 在测试集上将 Fréchet 距离和均方误差降低了 15-30%，并且在分布外数据上获得了相似的收益。源代码将在 GitHub 仓库 https://github.com/CoderChen01/towards-seamleass-interaction 中发布。"
    },
    {
        "title": "MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance",
        "summary": "Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.",
        "url": "http://arxiv.org/abs/2512.15048v1",
        "published_date": "2025-12-17T03:23:12+00:00",
        "updated_date": "2025-12-17T03:23:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaizhe Zhang",
            "Shinan Chen",
            "Qian Zhao",
            "Weizhan Zhang",
            "Caixia Yan",
            "Yudeng Xin"
        ],
        "tldr": "The paper introduces MVGSR, a multi-view consistent 3D Gaussian Splatting super-resolution framework that uses epipolar-constrained multi-view attention to improve consistency and detail fidelity in 3DGS representations from low-resolution images, achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了MVGSR，一个多视角一致的3D高斯溅射超分辨率框架，利用极线约束的多视角注意力机制来提高从低分辨率图像重建的3DGS表示的一致性和细节保真度，并实现了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "基于低分辨率(LR)图像训练的3D高斯溅射(3DGS)重建的场景不适用于高分辨率(HR)渲染。因此，需要一种3DGS超分辨率(SR)方法来桥接低分辨率输入和高分辨率渲染。早期的3DGS超分辨率方法依赖于单图像超分辨率网络，缺乏跨视角一致性，并且无法融合跨视角的互补信息。最近的基于视频的超分辨率方法试图解决这一局限性，但需要严格的顺序帧，限制了它们在非结构化多视角数据集中的适用性。在这项工作中，我们引入了多视角一致性3D高斯溅射超分辨率(MVGSR)框架，该框架专注于整合多视角信息，以进行具有高频细节和增强一致性的3DGS渲染。我们首先提出了一种基于相机姿态的辅助视角选择方法，使我们的方法能够适应任意组织的多视角数据集，而无需时间连续性或数据重排序。此外，我们首次将极线约束的多视角注意力机制引入到3DGS超分辨率中，作为我们提出的多视角超分辨率网络的核心。这种设计使得模型能够选择性地聚合来自辅助视角的一致信息，从而增强3DGS表示的几何一致性和细节保真度。大量的实验表明，我们的方法在以物体为中心和场景级别的3DGS超分辨率基准测试中都取得了最先进的性能。"
    },
    {
        "title": "Isolated Sign Language Recognition with Segmentation and Pose Estimation",
        "summary": "The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.",
        "url": "http://arxiv.org/abs/2512.14876v1",
        "published_date": "2025-12-16T19:44:12+00:00",
        "updated_date": "2025-12-16T19:44:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Perkins",
            "Davis Hunter",
            "Dhrumil Patel",
            "Galen Flanagan"
        ],
        "tldr": "This paper presents a model for isolated sign language recognition (ISLR) that uses pose estimation, segmentation, and a ResNet-Transformer to improve robustness to signer variation and reduce computational costs.",
        "tldr_zh": "本文提出了一种用于孤立手语识别（ISLR）的模型，该模型使用姿势估计、分割和ResNet-Transformer来提高对手语者差异的鲁棒性并降低计算成本。",
        "relevance_score": 3,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "近年来，大型语言模型的涌现已经实现了口语和书面语言的自动化翻译。然而，这些进展对于美国手语（ASL）使用者来说仍然在很大程度上是无法访问的，他们的语言依赖于复杂的视觉线索。孤立手语识别（ISLR）——对单个手语视频进行分类的任务——可以帮助弥合这一差距，但目前受到每个手语数据稀缺、手语者差异性大以及计算成本高等因素的限制。我们提出了一种用于ISLR的模型，该模型在降低计算需求的同时，保持了对手语者差异的鲁棒性。我们的方法整合了（i）一个姿态估计流水线，用于提取手部和面部关节坐标，（ii）一个分割模块，用于隔离相关信息，以及（iii）一个ResNet-Transformer骨干网络，用于联合建模空间和时间依赖关系。"
    },
    {
        "title": "Attention in Motion: Secure Platooning via Transformer-based Misbehavior Detection",
        "summary": "Vehicular platooning promises transformative improvements in transportation efficiency and safety through the coordination of multi-vehicle formations enabled by Vehicle-to-Everything (V2X) communication. However, the distributed nature of platoon coordination creates security vulnerabilities, allowing authenticated vehicles to inject falsified kinematic data, compromise operational stability, and pose a threat to passenger safety. Traditional misbehaviour detection approaches, which rely on plausibility checks and statistical methods, suffer from high False Positive (FP) rates and cannot capture the complex temporal dependencies inherent in multi-vehicle coordination dynamics. We present Attention In Motion (AIMformer), a transformer-based framework specifically tailored for real-time misbehaviour detection in vehicular platoons with edge deployment capabilities. AIMformer leverages multi-head self-attention mechanisms to simultaneously capture intra-vehicle temporal dynamics and inter-vehicle spatial correlations. It incorporates global positional encoding with vehicle-specific temporal offsets to handle join/exit maneuvers. We propose a Precision-Focused (BCE) loss function that penalizes FPs to meet the requirements of safety-critical vehicular systems. Extensive evaluation across 4 platoon controllers, multiple attack vectors, and diverse mobility scenarios demonstrates superior performance ($\\geq$ 0.93) compared to state-of-the-art baseline architectures. A comprehensive deployment analysis utilizing TensorFlow Lite (TFLite), Open Neural Network Exchange (ONNX), and TensorRT achieves sub-millisecond inference latency, making it suitable for real-time operation on resource-constrained edge platforms. Hence, validating AIMformer is viable for both in-vehicle and roadside infrastructure deployment.",
        "url": "http://arxiv.org/abs/2512.15503v1",
        "published_date": "2025-12-17T14:45:33+00:00",
        "updated_date": "2025-12-17T14:45:33+00:00",
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "authors": [
            "Konstantinos Kalogiannis",
            "Ahmed Mohamed Hussain",
            "Hexu Li",
            "Panos Papadimitratos"
        ],
        "tldr": "The paper introduces AIMformer, a transformer-based framework for real-time misbehavior detection in vehicular platoons, achieving high accuracy and low latency suitable for edge deployment.",
        "tldr_zh": "该论文介绍了AIMformer，一个基于Transformer的用于车辆队列中实时异常行为检测的框架，具有高精度和低延迟，适合边缘部署。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "车辆编队通过车联网(V2X)通信实现多车协同编队，有望在交通效率和安全性方面带来变革性提升。然而，编队协调的分布式本质导致安全漏洞，允许经过身份验证的车辆注入伪造的运动学数据，从而损害运行稳定性并对乘客安全构成威胁。依赖于合理性检查和统计方法的传统恶意行为检测方法，存在较高的误报率(FP)，并且无法捕捉多车协同动态中固有的复杂时间依赖性。我们提出了一种基于Transformer (AIMformer) 的框架，该框架专门为车辆编队中的实时恶意行为检测而设计，并具备边缘部署能力。AIMformer利用多头自注意力机制来同时捕捉车辆内部时间动态和车辆间空间相关性。它结合了全局位置编码和车辆特定的时间偏移量，以处理加入/退出操作。我们提出了一种以精度为中心的二元交叉熵(BCE)损失函数，以惩罚误报，从而满足安全关键型车辆系统的需求。对多个编队控制器、多种攻击向量和不同的移动场景进行的广泛评估表明，与最先进的基线架构相比，AIMformer具有卓越的性能（$\\geq$ 0.93）。利用TensorFlow Lite (TFLite)、开放神经网络交换(ONNX)和TensorRT进行的全面部署分析实现了亚毫秒级的推理延迟，使其适用于资源受限的边缘平台上的实时操作。因此，验证了AIMformer适用于车载和路边基础设施部署。"
    },
    {
        "title": "Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery",
        "summary": "Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\\% accuracy (+5.4\\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.",
        "url": "http://arxiv.org/abs/2512.15344v1",
        "published_date": "2025-12-17T11:41:42+00:00",
        "updated_date": "2025-12-17T11:41:42+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "eess.SP"
        ],
        "authors": [
            "Hiroyoshi Nagahama",
            "Katsufumi Inoue",
            "Masayoshi Todorokihara",
            "Michifumi Yoshioka"
        ],
        "tldr": "The paper introduces two phase-aware preprocessing methods for vibration signals in rotating machinery fault diagnosis, demonstrating improved accuracy using deep learning models compared to methods that discard phase information.",
        "tldr_zh": "该论文提出了两种相位感知的预处理方法，用于旋转机械故障诊断中的振动信号，并表明与丢弃相位信息的方法相比，使用深度学习模型可以提高准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "旋转机械的预测性维护越来越依赖于振动信号，然而大多数基于学习的方法要么在频谱特征提取过程中丢弃相位信息，要么使用原始时间波形，而没有明确利用相位信息。本文介绍了两种相位感知预处理策略，以解决多轴振动数据中的随机相位变化：（1）三轴独立相位调整，将每个轴独立对齐到零相位；（2）单轴参考相位调整，通过应用一致的时间偏移来保持轴间关系。使用一个新构建的、通过同步的三轴传感器采集的转子数据集，我们在一个两阶段学习框架下评估了六种深度学习架构。结果表明，存在与架构无关的改进：三轴独立方法实现了持续的增益（Transformer提升 +2.7%），而单轴参考方法通过保持空间相位关系，实现了高达 96.2% 的准确率（+5.4%）的卓越性能。这些发现确立了这两种相位对齐策略作为预测性维护系统中实用且可扩展的增强手段。"
    },
    {
        "title": "Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle",
        "summary": "Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.",
        "url": "http://arxiv.org/abs/2512.14998v1",
        "published_date": "2025-12-17T01:01:51+00:00",
        "updated_date": "2025-12-17T01:01:51+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sibi Parivendan",
            "Kashfia Sailunaz",
            "Suresh Neethirajan"
        ],
        "tldr": "This paper presents a pose-based computer vision framework using keypoint trajectories to classify affiliative and agonistic social interactions in dairy cattle, achieving improved accuracy compared to proximity-based methods.",
        "tldr_zh": "本文提出了一个基于姿势的计算机视觉框架，通过关键点轨迹对奶牛的亲和与对抗性社会互动进行分类，与基于邻近度的方法相比，提高了准确性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "精准畜牧业需要对社会行为进行客观评估以支持畜群福利监测，然而现有的大多数方法使用静态的邻近阈值来推断互动，这种方法无法在复杂的畜舍环境中区分亲和行为和攻击行为。这种局限性限制了商业环境中自动化社交网络分析的可解释性。我们提出了一种基于姿态的计算框架，用于互动分类，该框架通过对解剖关键点的时空几何建模，超越了邻近启发式方法。该方法不依赖于像素级别的外观或简单的距离度量，而是从关键点轨迹中编码特定于互动的运动特征，从而能够区分社会互动的效价。该框架实现为一个端到端的计算机视觉流程，集成了用于目标检测的YOLOv11（mAP@0.50: 96.24%）、受监督的个体识别（98.24%准确率）、用于多目标跟踪的ByteTrack（81.96%准确率）、用于27点解剖关键点估计的ZebraPose，以及一个基于姿态导出的距离动态训练的支持向量机分类器。在从商业奶牛场收集的带注释的互动片段上，仅使用姿态信息，该分类器在区分亲和行为和攻击行为方面实现了77.51%的准确率。与仅基于邻近性的基线进行的比较评估显示，在行为区分方面有显著提高，尤其是对于亲和互动。结果为基于视觉的自动化社会互动推断建立了一个概念验证，该方法适用于构建互动感知的社交网络，并且可以在普通硬件上实现接近实时的性能。"
    },
    {
        "title": "ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence",
        "summary": "Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.",
        "url": "http://arxiv.org/abs/2512.15445v1",
        "published_date": "2025-12-17T13:42:34+00:00",
        "updated_date": "2025-12-17T13:42:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yueqianji Chen",
            "Kevin Williams",
            "John H. Doonan",
            "Paolo Remagnino",
            "Jo Hepworth"
        ],
        "tldr": "The paper introduces ST-DETrack, a novel spatiotemporal-fusion dual-decoder network for tracking individual plant branches in complex canopies, achieving significantly improved branch matching accuracy.",
        "tldr_zh": "该论文介绍了ST-DETrack，一种新颖的时空融合双解码器网络，用于在复杂的植物冠层中跟踪单个植物分支，显著提高了分支匹配的准确性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "从时序图像中自动提取单株植物枝条对于高通量表型分析至关重要，但由于非刚性生长动态和缠绕冠层中严重的身份碎片化，这仍然面临着计算上的挑战。为了克服这些与生长阶段相关的模糊性，我们提出了一种时空融合双解码器网络ST-DETrack，旨在保持枝条从萌发到开花阶段的身份。我们的架构集成了一个空间解码器（利用位置和角度等几何先验进行早期阶段的追踪）和一个时间解码器（利用运动一致性来解决晚期阶段的遮挡）。关键在于，一种自适应门控机制动态地改变对这些空间和时间线索的依赖性，同时基于负向地性的生物约束减轻了垂直生长方向的歧义。在甘蓝型油菜数据集上的验证表明，ST-DETrack实现了93.6%的枝条匹配准确率（BMA），显著优于空间和时间基线，分别提高了28.9和3.3个百分点。这些结果表明该方法在复杂的动态植物结构中保持长期身份一致性的鲁棒性。"
    },
    {
        "title": "Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement",
        "summary": "Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.",
        "url": "http://arxiv.org/abs/2512.15055v1",
        "published_date": "2025-12-17T03:38:12+00:00",
        "updated_date": "2025-12-17T03:38:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifei Bian",
            "Banglei Guan",
            "Zibin Liu",
            "Ang Su",
            "Shiyao Zhu",
            "Yang Shang",
            "Qifeng Yu"
        ],
        "tldr": "This paper presents a method leveraging event cameras and LED markers to measure high-frequency deformations of large-scale structures under challenging lighting conditions, filtering noise from the event stream to accurately track LED movement.",
        "tldr_zh": "本文提出了一种利用事件相机和LED标记测量大型结构在高频形变的方法。该方法在复杂光照条件下通过过滤事件流中的噪声，精确跟踪LED标记的运动。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "大型结构在复杂载荷作用下会产生高频形变。然而，恶劣的光照条件和高昂的设备成本限制了基于传统高速摄像机的测量方法。本文提出了一种利用事件相机和LED标记来测量高频形变的方法。首先，基于LED标记闪烁产生的事件流特性和时空相关性对观测噪声进行滤波。然后，通过区分运动引起的事件和LED闪烁引起的事件，从事件流中提取LED标记，从而能够提取高速移动的LED标记。最终，通过单目事件相机测量高频平面形变。实验结果验证了本文方法在测量高频平面形变方面的准确性。"
    },
    {
        "title": "Intent-Driven UAM Rescheduling",
        "summary": "Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.",
        "url": "http://arxiv.org/abs/2512.15462v1",
        "published_date": "2025-12-17T14:04:14+00:00",
        "updated_date": "2025-12-17T14:04:14+00:00",
        "categories": [
            "cs.AI",
            "cs.HC",
            "cs.SC"
        ],
        "authors": [
            "Jeongseok Kim",
            "Kangjin Kim"
        ],
        "tldr": "This paper presents a hybrid ASP-MILP framework for UAM scheduling, incorporating user intent interpretation and providing explainable, adaptive rescheduling capabilities.",
        "tldr_zh": "本文提出了一种混合ASP-MILP框架，用于城市空中交通（UAM）调度，结合了用户意图解释并提供可解释的、自适应的重新调度能力。",
        "relevance_score": 2,
        "novelty_claim_score": 6,
        "clarity_score": 7,
        "potential_impact_score": 5,
        "overall_priority_score": 3,
        "summary_zh": "由于资源的限制，垂直起降场的高效调度在城市空中交通（UAM）领域受到了越来越多的关注。对于调度问题，我们利用了混合整数线性规划（MILP），它通常用于资源约束项目调度问题（RCPSP）。本文展示了我们处理动态运营需求和来自人类的模糊重调度请求的方法。特别地，我们利用三值逻辑来解释含糊不清的用户意图以及使用决策树，提出了一个结合了应答集程序设计（ASP）和MILP的全新集成系统。这个集成框架优化了调度方案并透明地支持人工输入。借助该系统，我们为可解释的、自适应的UAM调度提供了一个稳健的结构。"
    },
    {
        "title": "Intrusion Detection in Internet of Vehicles Using Machine Learning",
        "summary": "The Internet of Vehicles (IoV) has evolved modern transportation through enhanced connectivity and intelligent systems. However, this increased connectivity introduces critical vulnerabilities, making vehicles susceptible to cyber-attacks such Denial-ofService (DoS) and message spoofing. This project aims to develop a machine learning-based intrusion detection system to classify malicious Controller Area network (CAN) bus traffic using the CiCIoV2024 benchmark dataset. We analyzed various attack patterns including DoS and spoofing attacks targeting critical vehicle parameters such as Spoofing-GAS - gas pedal position, Spoofing-RPM, Spoofing-Speed, and Spoofing-Steering\\_Wheel. Our initial findings confirm a multi-class classification problem with a clear structural difference between attack types and benign data, providing a strong foundation for machine learning models.",
        "url": "http://arxiv.org/abs/2512.14958v1",
        "published_date": "2025-12-16T22:54:39+00:00",
        "updated_date": "2025-12-16T22:54:39+00:00",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Hop Le",
            "Izzat Alsmadi"
        ],
        "tldr": "This paper explores using machine learning for intrusion detection in Internet of Vehicles (IoV) using the CICIoV2024 dataset, focusing on identifying DoS and spoofing attacks on CAN bus traffic.",
        "tldr_zh": "本文探讨了使用机器学习进行车联网（IoV）中的入侵检测，使用CICIoV2024数据集，重点是识别CAN总线流量上的DoS和欺骗攻击。",
        "relevance_score": 2,
        "novelty_claim_score": 3,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 3,
        "summary_zh": "车辆网（IoV）通过增强的连接性和智能系统革新了现代交通运输。然而，这种连接性的增加带来了关键的漏洞，使得车辆容易受到诸如拒绝服务（DoS）和消息欺骗等网络攻击。本项目旨在开发一种基于机器学习的入侵检测系统，使用CiCIoV2024基准数据集对恶意控制器局域网（CAN）总线流量进行分类。我们分析了包括针对关键车辆参数（如Spoofing-GAS - 油门踏板位置、Spoofing-RPM、Spoofing-Speed和Spoofing-Steering_Wheel）的DoS和欺骗攻击在内的各种攻击模式。我们的初步研究结果证实了一个多类分类问题，其中攻击类型和良性数据之间存在明显的结构差异，为机器学习模型提供了坚实的基础。"
    }
]