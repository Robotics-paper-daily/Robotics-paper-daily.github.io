[
    {
        "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
        "summary": "Embodied agents powered by vision-language models (VLMs) are increasingly capable of executing complex real-world tasks, yet they remain vulnerable to hazardous instructions that may trigger unsafe behaviors. Runtime safety guardrails, which intercept hazardous actions during task execution, offer a promising solution due to their flexibility. However, existing defenses often rely on static rule filters or prompt-level control, which struggle to address implicit risks arising in dynamic, temporally dependent, and context-rich environments. To address this, we propose RoboSafe, a hybrid reasoning runtime safeguard for embodied agents through executable predicate-based safety logic. RoboSafe integrates two complementary reasoning processes on a Hybrid Long-Short Safety Memory. We first propose a Backward Reflective Reasoning module that continuously revisits recent trajectories in short-term memory to infer temporal safety predicates and proactively triggers replanning when violations are detected. We then propose a Forward Predictive Reasoning module that anticipates upcoming risks by generating context-aware safety predicates from the long-term safety memory and the agent's multimodal observations. Together, these components form an adaptive, verifiable safety logic that is both interpretable and executable as code. Extensive experiments across multiple agents demonstrate that RoboSafe substantially reduces hazardous actions (-36.8% risk occurrence) compared with leading baselines, while maintaining near-original task performance. Real-world evaluations on physical robotic arms further confirm its practicality. Code will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2512.21220v1",
        "published_date": "2025-12-24T15:01:26+00:00",
        "updated_date": "2025-12-24T15:01:26+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Le Wang",
            "Zonghao Ying",
            "Xiao Yang",
            "Quanchen Zou",
            "Zhenfei Yin",
            "Tianlin Li",
            "Jian Yang",
            "Yaodong Yang",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "tldr": "The paper introduces RoboSafe, a runtime safety guardrail for embodied agents using vision-language models, which employs hybrid reasoning and executable safety logic for proactive risk mitigation, significantly reducing hazardous actions in simulations and real-world robotic arm experiments.",
        "tldr_zh": "该论文介绍了RoboSafe，一种用于具身智能体（使用视觉语言模型）的运行时安全护栏，它采用混合推理和可执行安全逻辑来主动降低风险，在模拟和现实世界机器人手臂实验中显著减少了危险行为。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "由视觉-语言模型（VLM）驱动的具身智能体越来越能够执行复杂的现实世界任务，但同时它们也容易受到可能触发不安全行为的危险指令的影响。运行时安全保障措施，即在任务执行期间拦截危险动作，由于其灵活性，提供了一种有前景的解决方案。然而，现有的防御措施通常依赖于静态规则过滤或提示级别的控制，难以解决在动态、时间依赖和上下文丰富的环境中出现的隐性风险。为了解决这个问题，我们提出RoboSafe，一种通过基于可执行谓词的安全逻辑为具身智能体提供的混合推理运行时安全防护。RoboSafe在混合长短期安全记忆上集成了两个互补的推理过程。我们首先提出一个向后反射推理模块，该模块持续回顾短期记忆中最近的轨迹，以推断时间安全谓词，并在检测到违规时主动触发重新规划。然后，我们提出一个向前预测推理模块，通过从长期安全记忆和智能体的多模态观察中生成上下文感知的安全谓词来预测即将到来的风险。这些组件共同构成了一种自适应、可验证的安全逻辑，既可解释又可作为代码执行。跨多个智能体的大量实验表明，与领先的基线相比，RoboSafe显著减少了危险行为（风险发生率降低-36.8%），同时保持了接近原始的任务性能。在物理机器人手臂上的真实世界评估进一步证实了其实用性。代码将在接收后发布。"
    },
    {
        "title": "ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments",
        "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires an embodied agent to navigate towards target in continuous environments, following natural language instructions. While current graph-based methods offer an efficient, structured approach by abstracting the environment into a topological map and simplifying the action space to waypoint selection, they lag behind methods based on Large Vision-Language Models (LVLMs) in leveraging large-scale data and advanced training paradigms. In this paper, we try to bridge this gap by introducing ETP-R1, a framework that applies the paradigm of scaling up data and Reinforcement Fine-Tuning (RFT) to a graph-based VLN-CE model. To build a strong foundation, we first construct a high-quality, large-scale pretraining dataset using the Gemini API. This dataset consists of diverse, low-hallucination instructions for topological trajectories, providing rich supervision for our graph-based policy to map language to topological paths. This foundation is further strengthened by unifying data from both R2R and RxR tasks for joint pretraining. Building on this, we introduce a three-stage training paradigm, which culminates in the first application of closed-loop, online RFT to a graph-based VLN-CE model, powered by the Group Relative Policy Optimization (GRPO) algorithm. Extensive experiments demonstrate that our approach is highly effective, establishing new state-of-the-art performance across all major metrics on both the R2R-CE and RxR-CE benchmarks. Our code is available at https://github.com/Cepillar/ETP-R1.",
        "url": "http://arxiv.org/abs/2512.20940v1",
        "published_date": "2025-12-24T04:53:03+00:00",
        "updated_date": "2025-12-24T04:53:03+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Shuhao Ye",
            "Sitong Mao",
            "Yuxiang Cui",
            "Xuan Yu",
            "Shichao Zhai",
            "Wen Chen",
            "Shunbo Zhou",
            "Rong Xiong",
            "Yue Wang"
        ],
        "tldr": "The paper introduces ETP-R1, a graph-based VLN-CE framework that leverages large-scale pretraining with Gemini API-generated data and reinforcement fine-tuning (RFT) using GRPO, achieving state-of-the-art results on R2R-CE and RxR-CE benchmarks.",
        "tldr_zh": "本文介绍了ETP-R1，一个基于图的VLN-CE框架，它利用Gemini API生成的大规模预训练数据和使用GRPO的强化微调（RFT），在R2R-CE和RxR-CE基准测试中取得了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "在连续环境中进行的视觉-语言导航 (VLN-CE) 要求具身智能体根据自然语言指令在连续环境中导航至目标。虽然当前基于图的方法通过将环境抽象成拓扑地图并将动作空间简化为航路点选择，提供了一种高效、结构化的方案，但在利用大规模数据和先进训练范式方面，它们落后于基于大型视觉-语言模型 (LVLMs) 的方法。在本文中，我们尝试通过引入 ETP-R1 来弥合这一差距，该框架将扩大数据规模和强化精调 (RFT) 的范式应用于基于图的 VLN-CE 模型。为了构建一个坚实的基础，我们首先使用 Gemini API 构建了一个高质量的大规模预训练数据集。该数据集包含多样化的、低幻觉的拓扑轨迹指令，为我们的基于图的策略提供了丰富的监督，以便将语言映射到拓扑路径。通过统一来自 R2R 和 RxR 任务的数据进行联合预训练，进一步加强了这一基础。在此基础上，我们引入了一个三阶段的训练范式，最终在基于图的 VLN-CE 模型上首次应用了闭环、在线 RFT，并由组相对策略优化 (GRPO) 算法提供支持。广泛的实验表明，我们的方法非常有效，在 R2R-CE 和 RxR-CE 基准测试的所有主要指标上都建立了新的最先进性能。我们的代码可在 https://github.com/Cepillar/ETP-R1 获取。"
    },
    {
        "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents",
        "summary": "As autonomous AI agents are increasingly deployed in high-stakes environments, ensuring their safety and alignment with human values has become a paramount concern. Current safety benchmarks often focusing only on single-step decision-making, simulated environments for tasks with malicious intent, or evaluating adherence to explicit negative constraints. There is a lack of benchmarks that are designed to capture emergent forms of outcome-driven constraint violations, which arise when agents pursue goal optimization under strong performance incentives while deprioritizing ethical, legal, or safety constraints over multiple steps in realistic production settings. To address this gap, we introduce a new benchmark comprising 40 distinct scenarios. Each scenario presents a task that requires multi-step actions, and the agent's performance is tied to a specific Key Performance Indicator (KPI). Each scenario features Mandated (instruction-commanded) and Incentivized (KPI-pressure-driven) variations to distinguish between obedience and emergent misalignment. Across 12 state-of-the-art large language models, we observe outcome-driven constraint violations ranging from 1.3% to 71.4%, with 9 of the 12 evaluated models exhibiting misalignment rates between 30% and 50%. Strikingly, we find that superior reasoning capability does not inherently ensure safety; for instance, Gemini-3-Pro-Preview, one of the most capable models evaluated, exhibits the highest violation rate at over 60%, frequently escalating to severe misconduct to satisfy KPIs. Furthermore, we observe significant \"deliberative misalignment\", where the models that power the agents recognize their actions as unethical during separate evaluation. These results emphasize the critical need for more realistic agentic-safety training before deployment to mitigate their risks in the real world.",
        "url": "http://arxiv.org/abs/2512.20798v1",
        "published_date": "2025-12-23T21:52:53+00:00",
        "updated_date": "2025-12-23T21:52:53+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Miles Q. Li",
            "Benjamin C. M. Fung",
            "Martin Weiss",
            "Pulei Xiong",
            "Khalil Al-Hussaeni",
            "Claude Fachkha"
        ],
        "tldr": "This paper introduces a benchmark to evaluate outcome-driven constraint violations in autonomous AI agents, finding that even state-of-the-art LLMs exhibit significant misalignment and that reasoning ability doesn't guarantee safety.",
        "tldr_zh": "本文介绍了一个评估自主AI Agent中结果驱动的约束违规的基准，发现即使是最先进的LLM也表现出显著的偏差，并且推理能力并不能保证安全性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "随着自主人工智能体越来越多地部署在高风险环境中，确保其安全性和与人类价值观的一致性已成为首要关注的问题。目前的安全性基准通常只关注单步决策、模拟环境中针对恶意意图的任务，或者评估对明确负面约束的遵守情况。缺乏能够捕捉到涌现式的、由结果驱动的约束违背的基准，这些违背发生在智能体在强烈的性能激励下追求目标优化，同时在现实生产环境中，跨多个步骤降低对伦理、法律或安全约束的优先级时。为了弥补这一差距，我们引入了一个包含 40 个不同场景的新基准。每个场景都提供了一个需要多步操作的任务，并且智能体的性能与特定的关键绩效指标 (KPI) 相关联。每个场景都包含强制（指令驱动）和激励（KPI压力驱动）的变体，以区分服从和涌现式的不对齐。在 12 个最先进的大型语言模型中，我们观察到由结果驱动的约束违背率从 1.3% 到 71.4% 不等，其中 12 个评估模型中的 9 个表现出 30% 到 50% 的不对齐率。引人注目的是，我们发现卓越的推理能力并不一定确保安全性；例如，Gemini-3-Pro-Preview，作为评估的最有能力的模型之一，表现出最高的违规率，超过 60%，并且经常升级到严重的违规行为以满足 KPI。此外，我们观察到显著的“审慎式不对齐”，即驱动智能体的模型在单独评估期间承认其行为是不道德的。这些结果强调了在部署之前对智能体进行更真实的安全性训练的迫切需求，以减轻它们在现实世界中的风险。"
    },
    {
        "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
        "summary": "Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.\n  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.\n  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.",
        "url": "http://arxiv.org/abs/2512.20561v1",
        "published_date": "2025-12-23T18:05:43+00:00",
        "updated_date": "2025-12-23T18:05:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaitong Cai",
            "Jusheng Zhang",
            "Jing Yang",
            "Yijia Fan",
            "Pengtao Xie",
            "Jian Wang",
            "Keze Wang"
        ],
        "tldr": "FlashVLM dynamically selects relevant visual tokens for VLMs using text-guided cross-modal similarity, achieving lossless or even surpassing performance with significant compression rates.",
        "tldr_zh": "FlashVLM利用文本引导的跨模态相似性动态选择VLMs的相关视觉tokens，在显著压缩率下实现了无损甚至超越的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "大型视觉语言模型(VLMs)通常需要处理每张图像或视频帧中成百上千个视觉标记，导致二次方级别的注意力计算代价和显著的冗余。现有的标记缩减方法通常忽略文本查询或依赖深度注意力图，而在激进剪枝下，其不稳定性会导致语义对齐的退化。\n\n我们提出了 FlashVLM，一个文本引导的视觉标记选择框架，可以动态地使视觉输入适应查询。FlashVLM 不依赖于噪声大的注意力权重，而是计算投影后的图像标记和语言模型空间中归一化的文本嵌入之间的显式跨模态相似度。这种外在相关性与内在视觉显著性通过对数域加权和温度控制锐化进行融合。此外，一个保留多样性的分割保留了一组最小但具有代表性的背景标记，以维持全局上下文。\n\n在相同的标记预算和评估协议下，FlashVLM实现了超越无损的压缩效果，在 LLaVA 1.5 上裁剪高达 77.8% 的视觉标记时，略微超过了未剪枝的基线，并在裁剪 94.4% 的情况下，保持 92.8% 的准确率。在 14 个图像和视频基准上的大量实验表明，FlashVLM 提供了最先进的效率-性能权衡，同时在主流 VLM 中保持了强大的鲁棒性和泛化能力。"
    },
    {
        "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
        "summary": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
        "url": "http://arxiv.org/abs/2512.20957v1",
        "published_date": "2025-12-24T05:27:53+00:00",
        "updated_date": "2025-12-24T05:27:53+00:00",
        "categories": [
            "cs.SE",
            "cs.AI"
        ],
        "authors": [
            "Zhaoxi Zhang",
            "Yitong Duan",
            "Yanzhi Zhang",
            "Yiming Xu",
            "Jiyan He",
            "Yunfang Wu"
        ],
        "tldr": "The paper introduces RepoNavigator, an LLM agent trained via reinforcement learning with a single 'jump-to-definition' tool, achieving state-of-the-art performance in repository-level issue localization by simplifying tool manipulation and reflecting code execution flow.",
        "tldr_zh": "本文介绍了RepoNavigator，一种通过强化学习训练的LLM Agent，它使用单个“跳转到定义”工具，通过简化工具操作和反映代码执行流程，在仓库级别的问题定位方面实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8.2,
        "summary_zh": "在大型开源软件仓库 (OSS) 中，定位需要修改的文件和函数极具挑战，这源于它们的规模和结构复杂度。现有的基于大型语言模型 (LLM) 的方法通常将其视为仓库级别的检索任务，并依赖于多种辅助工具，这忽略了代码执行逻辑并使模型控制复杂化。我们提出了 RepoNavigator，一个配备了单一执行感知型工具（跳转到被调用符号的定义）的 LLM 代理。这种统一的设计反映了代码执行的实际流程，同时简化了工具操作。RepoNavigator 通过强化学习 (RL) 从预训练模型端到端地进行训练，无需任何闭源提炼。实验表明，经过 RL 训练的 RepoNavigator 取得了最先进的性能，其中 7B 模型胜过了 14B 基线模型，14B 模型超越了 32B 竞争对手，甚至 32B 模型也超过了诸如 Claude-3.7 等闭源模型。这些结果证实，将单一的、结构化基础工具与 RL 训练相结合，为仓库级别的 Issue 定位提供了一种高效且可扩展的解决方案。"
    },
    {
        "title": "Quadrupped-Legged Robot Movement Plan Generation using Large Language Model",
        "summary": "Traditional control interfaces for quadruped robots often impose a high barrier to entry, requiring specialized technical knowledge for effective operation. To address this, this paper presents a novel control framework that integrates Large Language Models (LLMs) to enable intuitive, natural language-based navigation. We propose a distributed architecture where high-level instruction processing is offloaded to an external server to overcome the onboard computational constraints of the DeepRobotics Jueying Lite 3 platform. The system grounds LLM-generated plans into executable ROS navigation commands using real-time sensor fusion (LiDAR, IMU, and Odometry). Experimental validation was conducted in a structured indoor environment across four distinct scenarios, ranging from single-room tasks to complex cross-zone navigation. The results demonstrate the system's robustness, achieving an aggregate success rate of over 90\\% across all scenarios, validating the feasibility of offloaded LLM-based planning for autonomous quadruped deployment in real-world settings.",
        "url": "http://arxiv.org/abs/2512.21293v1",
        "published_date": "2025-12-24T17:22:00+00:00",
        "updated_date": "2025-12-24T17:22:00+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Muhtadin",
            "Vincentius Gusti Putu A. B. M.",
            "Ahmad Zaini",
            "Mauridhi Hery Purnomo",
            "I Ketut Eddy Purnama",
            "Chastine Fatichah"
        ],
        "tldr": "This paper presents a system that uses a Large Language Model for natural language-based navigation of a quadruped robot. High-level planning is offloaded to a server and grounded into ROS commands with real-time sensor fusion, achieving high success rates in indoor navigation scenarios.",
        "tldr_zh": "本文提出了一种使用大型语言模型对四足机器人进行自然语言导航的系统。 高级规划被卸载到服务器，并通过实时传感器融合以ROS命令来执行，在室内导航场景中取得了很高的成功率。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文提出了一种新颖的控制框架，该框架集成了大型语言模型（LLMs），以实现基于直观、自然语言的导航，从而解决四足机器人传统控制界面通常存在的高入门门槛，即需要专门的技术知识才能有效操作的问题。 我们提出了一种分布式架构，将高级指令处理卸载到外部服务器，以克服DeepRobotics Jueying Lite 3平台的板载计算约束。系统利用实时传感器融合（激光雷达、IMU和里程计），将LLM生成的规划落地为可执行的ROS导航命令。 在结构化的室内环境中，针对从单房间任务到复杂跨区域导航的四个不同场景进行了实验验证。 结果表明，该系统具有鲁棒性，在所有场景中的总成功率超过90%，验证了基于LLM的卸载规划在真实世界环境中自主四足机器人部署的可行性。"
    },
    {
        "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
        "summary": "Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",
        "url": "http://arxiv.org/abs/2512.21243v1",
        "published_date": "2025-12-24T15:36:21+00:00",
        "updated_date": "2025-12-24T15:36:21+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Anatoly O. Onishchenko",
            "Alexey K. Kovalev",
            "Aleksandr I. Panov"
        ],
        "tldr": "The paper introduces LookPlanGraph, a method that enhances LLM-based embodied instruction following by dynamically updating scene graphs using a Vision Language Model (VLM) to account for environmental changes, demonstrating improved performance in simulated and real-world settings and introducing a new dataset, GraSIF.",
        "tldr_zh": "该论文介绍了LookPlanGraph，一种通过使用视觉语言模型（VLM）动态更新场景图来增强基于LLM的具身指令跟随的方法，以应对环境变化，在模拟和真实场景中展示了改进的性能，并引入了一个新的数据集GraSIF。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "以下是将大型语言模型（LLM）作为具身指令跟随任务规划器的方法已变得十分普遍。为了成功完成任务，LLM必须扎根于机器人运作的环境中。一种解决方案是使用包含所有必要信息的场景图。现代方法依赖于预先构建的场景图，并假设所有任务相关的信息在规划开始时都是可用的。然而，这些方法没有考虑到环境在图构建和任务执行之间可能发生的改变。我们提出了LookPlanGraph——一种利用由静态资产和对象先验组成的场景图的方法。在计划执行期间，LookPlanGraph通过验证现有的先验或发现新的实体，持续更新场景图，从而处理了智能体以自我为中心的相机视图，并利用了视觉语言模型。我们在VirtualHome和OmniGibson模拟环境中进行了对象位置改变的实验，结果表明LookPlanGraph优于基于预定义静态场景图的方法。为了展示我们方法的实际适用性，我们还在真实世界的环境中进行了实验。此外，我们引入了GraSIF（用于指令跟随的图场景）数据集，包含来自SayPlan Office、BEHAVIOR-1K和VirtualHome RobotHow的514个任务以及自动化的验证框架。项目页面可在https://lookplangraph.github.io 访问。"
    },
    {
        "title": "RoboCade: Gamifying Robot Data Collection",
        "summary": "Imitation learning from human demonstrations has become a dominant approach for training autonomous robot policies. However, collecting demonstration datasets is costly: it often requires access to robots and needs sustained effort in a tedious, long process. These factors limit the scale of data available for training policies. We aim to address this scalability challenge by involving a broader audience in a gamified data collection experience that is both accessible and motivating. Specifically, we develop a gamified remote teleoperation platform, RoboCade, to engage general users in collecting data that is beneficial for downstream policy training. To do this, we embed gamification strategies into the design of the system interface and data collection tasks. In the system interface, we include components such as visual feedback, sound effects, goal visualizations, progress bars, leaderboards, and badges. We additionally propose principles for constructing gamified tasks that have overlapping structure with useful downstream target tasks. We instantiate RoboCade on three manipulation tasks -- including spatial arrangement, scanning, and insertion. To illustrate the viability of gamified robot data collection, we collect a demonstration dataset through our platform, and show that co-training robot policies with this data can improve success rate on non-gamified target tasks (+16-56%). Further, we conduct a user study to validate that novice users find the gamified platform significantly more enjoyable than a standard non-gamified platform (+24%). These results highlight the promise of gamified data collection as a scalable, accessible, and engaging method for collecting demonstration data.",
        "url": "http://arxiv.org/abs/2512.21235v1",
        "published_date": "2025-12-24T15:20:54+00:00",
        "updated_date": "2025-12-24T15:20:54+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Suvir Mirchandani",
            "Mia Tang",
            "Jiafei Duan",
            "Jubayer Ibn Hamid",
            "Michael Cho",
            "Dorsa Sadigh"
        ],
        "tldr": "The paper introduces RoboCade, a gamified remote teleoperation platform for collecting robot demonstration data, demonstrating improved policy training and user engagement compared to non-gamified methods.",
        "tldr_zh": "该论文介绍了一个游戏化的远程遥操作平台RoboCade，用于收集机器人演示数据，与非游戏化方法相比，该平台在改进策略训练和用户参与度方面表现更佳。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "从人类演示中学习的模仿学习已经成为训练自主机器人策略的主流方法。然而，收集演示数据集成本高昂：通常需要使用机器人，并且需要在乏味漫长的过程中持续投入精力。这些因素限制了可用于训练策略的数据规模。我们旨在通过让更广泛的受众参与到游戏化的数据收集体验中，来解决这种可扩展性挑战，使数据收集既可及又具有激励性。具体而言，我们开发了一个游戏化的远程遥操作平台 RoboCade，以鼓励普通用户收集对下游策略训练有益的数据。为此，我们将游戏化策略嵌入到系统界面和数据收集任务的设计中。在系统界面中，我们包含诸如视觉反馈、音效、目标可视化、进度条、排行榜和徽章等组件。此外，我们还提出了构建游戏化任务的原则，这些任务与有用的下游目标任务具有重叠结构。我们在三个操作任务上实例化了 RoboCade，包括空间排列、扫描和插入。为了说明游戏化机器人数据收集的可行性，我们通过我们的平台收集了一个演示数据集，并表明使用此数据共同训练机器人策略可以提高非游戏化目标任务的成功率（+16-56%）。此外，我们进行了一项用户研究，以验证新手用户发现游戏化平台比标准非游戏化平台更令人愉快（+24%）。这些结果突出了游戏化数据收集作为一种可扩展、可访问且引人入胜的演示数据收集方法的潜力。"
    },
    {
        "title": "UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer",
        "summary": "Tactile sensing is crucial for robotic hands to achieve human-level dexterous manipulation, especially in scenarios with visual occlusion. However, its application is often hindered by the difficulty of collecting large-scale real-world robotic tactile data. In this study, we propose to collect low-cost human manipulation data using haptic gloves for tactile-based robotic policy learning. The misalignment between human and robotic tactile data makes it challenging to transfer policies learned from human data to robots. To bridge this gap, we propose UniTacHand, a unified representation to align robotic tactile information captured by dexterous hands with human hand touch obtained from gloves. First, we project tactile signals from both human hands and robotic hands onto a morphologically consistent 2D surface space of the MANO hand model. This unification standardizes the heterogeneous data structures and inherently embeds the tactile signals with spatial context. Then, we introduce a contrastive learning method to align them into a unified latent space, trained on only 10 minutes of paired data from our data collection system. Our approach enables zero-shot tactile-based policy transfer from humans to a real robot, generalizing to objects unseen in the pre-training data. We also demonstrate that co-training on mixed data, including both human and robotic demonstrations via UniTacHand, yields better performance and data efficiency compared with using only robotic data. UniTacHand paves a path toward general, scalable, and data-efficient learning for tactile-based dexterous hands.",
        "url": "http://arxiv.org/abs/2512.21233v1",
        "published_date": "2025-12-24T15:18:54+00:00",
        "updated_date": "2025-12-24T15:18:54+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Chi Zhang",
            "Penglin Cai",
            "Haoqi Yuan",
            "Chaoyi Xu",
            "Zongqing Lu"
        ],
        "tldr": "The paper introduces UniTacHand, a unified spatio-tactile representation for transferring tactile-based manipulation policies from humans to robots by aligning human and robotic tactile data in a common latent space using contrastive learning and a morphologically consistent 2D surface.",
        "tldr_zh": "该论文介绍了UniTacHand，一种统一的时空触觉表示，通过使用对比学习和形态一致的2D表面将人类和机器人触觉数据对齐在公共的潜在空间中，从而实现将基于触觉的操作策略从人类转移到机器人。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "触觉感知对于机器人手实现人类水平的灵巧操作至关重要，尤其是在视觉遮挡场景中。然而，其应用常常受到大规模真实机器人触觉数据采集困难的阻碍。在本研究中，我们提出使用触觉手套收集低成本的人类操作数据，用于基于触觉的机器人策略学习。人类和机器人触觉数据之间的不对齐使得将从人类数据中学习到的策略迁移到机器人变得具有挑战性。为了弥合这一差距，我们提出了UniTacHand，一种统一的表示方法，用于对齐灵巧手捕获的机器人触觉信息和从手套获得的人手触摸信息。首先，我们将来自人手和机器人手的触觉信号投射到MANO手模型的形态一致的二维表面空间上。这种统一标准化了异构数据结构，并内在性地将触觉信号嵌入空间上下文。然后，我们引入了一种对比学习方法，将它们对齐到一个统一的潜在空间中，该空间仅在由我们的数据收集系统产生的10分钟配对数据上进行训练。我们的方法实现了从人类到真实机器人的零样本基于触觉的策略迁移，并泛化到预训练数据中未见过的物体。我们还证明了，通过UniTacHand对包括人类和机器人演示的混合数据进行联合训练，与仅使用机器人数据相比，可以产生更好的性能和数据效率。UniTacHand为基于触觉的灵巧手的一般、可扩展的和数据高效的学习铺平了道路。"
    },
    {
        "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
        "summary": "Zero-shot object navigation (ZSON) requires a robot to locate a target object in a previously unseen environment without relying on pre-built maps or task-specific training. However, existing ZSON methods often struggle in realistic and cluttered environments, particularly when the scene contains heavy occlusions, unknown risks, or dynamically moving target objects. To address these challenges, we propose \\textbf{Schrödinger's Navigator}, a navigation framework inspired by Schrödinger's thought experiment on uncertainty. The framework treats unobserved space as a set of plausible future worlds and reasons over them before acting. Conditioned on egocentric visual inputs and three candidate trajectories, a trajectory-conditioned 3D world model imagines future observations along each path. This enables the agent to see beyond occlusions and anticipate risks in unseen regions without requiring extra detours or dense global mapping. The imagined 3D observations are fused into the navigation map and used to update a value map. These updates guide the policy toward trajectories that avoid occlusions, reduce exposure to uncertain space, and better track moving targets. Experiments on a Go2 quadruped robot across three challenging scenarios, including severe static occlusions, unknown risks, and dynamically moving targets, show that Schrödinger's Navigator consistently outperforms strong ZSON baselines in self-localization, object localization, and overall Success Rate in occlusion-heavy environments. These results demonstrate the effectiveness of trajectory-conditioned 3D imagination in enabling robust zero-shot object navigation.",
        "url": "http://arxiv.org/abs/2512.21201v1",
        "published_date": "2025-12-24T14:28:17+00:00",
        "updated_date": "2025-12-24T14:28:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yu He",
            "Da Huang",
            "Zhenyang Liu",
            "Zixiao Gu",
            "Qiang Sun",
            "Guangnan Ye",
            "Yanwei Fu"
        ],
        "tldr": "This paper introduces Schrödinger's Navigator, a novel zero-shot object navigation framework that uses trajectory-conditioned 3D world models to imagine future observations, enabling robots to navigate in challenging environments with occlusions, risks, and moving targets.",
        "tldr_zh": "本文介绍了一种新颖的零样本物体导航框架Schrödinger's Navigator，它使用轨迹条件3D世界模型来想象未来的观察结果，从而使机器人能够在具有遮挡、风险和移动目标的具有挑战性的环境中导航。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "零样本物体导航 (ZSON) 要求机器人在先前未见过的环境中定位目标物体，且不依赖于预先构建的地图或特定于任务的训练。然而，现有的 ZSON 方法在现实且杂乱的环境中经常表现不佳，尤其是在场景包含大量遮挡、未知风险或动态移动目标物体时。为了应对这些挑战，我们提出 \\textbf{薛定谔导航器}，这是一个受到薛定谔关于不确定性思想实验启发的导航框架。该框架将未观察到的空间视为一组合理的未来世界，并在采取行动之前对它们进行推理。在以自我中心视觉输入和三个候选轨迹为条件的情况下，一个轨迹条件 3D 世界模型会想象沿每条路径的未来观测。这使得智能体能够看到遮挡之外的区域，并预测未见区域中的风险，而无需额外的绕路或密集的全局地图绘制。想象的 3D 观测被融合到导航地图中，并用于更新价值地图。这些更新引导策略朝着避开遮挡、减少暴露于不确定空间以及更好地跟踪移动目标的轨迹前进。在一台 Go2 四足机器人上，针对三种具有挑战性的场景，包括严重的静态遮挡、未知风险和动态移动目标，进行的实验表明，薛定谔导航器在自我定位、物体定位和在遮挡严重环境中的总体成功率方面始终优于强大的 ZSON 基线。这些结果证明了轨迹条件 3D 想象在实现鲁棒的零样本物体导航中的有效性。"
    },
    {
        "title": "Global End-Effector Pose Control of an Underactuated Aerial Manipulator via Reinforcement Learning",
        "summary": "Aerial manipulators, which combine robotic arms with multi-rotor drones, face strict constraints on arm weight and mechanical complexity. In this work, we study a lightweight 2-degree-of-freedom (DoF) arm mounted on a quadrotor via a differential mechanism, capable of full six-DoF end-effector pose control. While the minimal design enables simplicity and reduced payload, it also introduces challenges such as underactuation and sensitivity to external disturbances, including manipulation of heavy loads and pushing tasks. To address these, we employ reinforcement learning, training a Proximal Policy Optimization (PPO) agent in simulation to generate feedforward commands for quadrotor acceleration and body rates, along with joint angle targets. These commands are tracked by an incremental nonlinear dynamic inversion (INDI) attitude controller and a PID joint controller, respectively. Flight experiments demonstrate centimeter-level position accuracy and degree-level orientation precision, with robust performance under external force disturbances. The results highlight the potential of learning-based control strategies for enabling contact-rich aerial manipulation using simple, lightweight platforms.",
        "url": "http://arxiv.org/abs/2512.21085v1",
        "published_date": "2025-12-24T10:00:01+00:00",
        "updated_date": "2025-12-24T10:00:01+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Shlok Deshmukh",
            "Javier Alonso-Mora",
            "Sihao Sun"
        ],
        "tldr": "This paper uses reinforcement learning (PPO) to control a lightweight, underactuated aerial manipulator with a 2-DoF arm for precise pose control and robust performance under disturbances, demonstrating its effectiveness in flight experiments.",
        "tldr_zh": "本文利用强化学习（PPO）控制一个轻量级的欠驱动空中机械臂，该机械臂具有2自由度的手臂，用于精确的姿态控制和在干扰下的鲁棒性能，并在飞行实验中验证了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "空中机械臂是将机械臂与多旋翼无人机相结合的系统，面临着机械臂重量和机械复杂性方面的严格约束。 本文研究了一种通过差动机构安装在四旋翼飞行器上的轻量级 2 自由度 (DoF) 机械臂，该机械臂能够实现全六自由度末端执行器姿态控制。 这种极简设计虽实现了简易性和有效载荷的降低，但也引入了欠驱动和对外部扰动敏感等挑战，包括重物操作和推力任务。 为了解决这些问题，我们采用了强化学习，在仿真环境中训练了一个近端策略优化 (PPO) 智能体，以生成四旋翼飞行器的加速度和机体角速率的前馈指令，以及关节角度目标。 这些指令分别由增量非线性动态逆 (INDI) 姿态控制器和 PID 关节控制器跟踪。 飞行实验表明，该方法能够实现厘米级的定位精度和角度级的定向精度，并在外部力扰动下表现出稳健的性能。 研究结果凸显了基于学习的控制策略在利用简单、轻量级平台实现富接触空中操控方面的潜力。"
    },
    {
        "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
        "summary": "Grasping is one of the most fundamental challenging capabilities in robotic manipulation, especially in unstructured, cluttered, and semantically diverse environments. Recent researches have increasingly explored language-guided manipulation, where robots not only perceive the scene but also interpret task-relevant natural language instructions. However, existing language-conditioned grasping methods typically rely on shallow fusion strategies, leading to limited semantic grounding and weak alignment between linguistic intent and visual grasp reasoning.In this work, we propose Language-Guided Grasp Detection (LGGD) with a coarse-to-fine learning paradigm for robotic manipulation. LGGD leverages CLIP-based visual and textual embeddings within a hierarchical cross-modal fusion pipeline, progressively injecting linguistic cues into the visual feature reconstruction process. This design enables fine-grained visual-semantic alignment and improves the feasibility of the predicted grasps with respect to task instructions. In addition, we introduce a language-conditioned dynamic convolution head (LDCH) that mixes multiple convolution experts based on sentence-level features, enabling instruction-adaptive coarse mask and grasp predictions. A final refinement module further enhances grasp consistency and robustness in complex scenes.Experiments on the OCID-VLG and Grasp-Anything++ datasets show that LGGD surpasses existing language-guided grasping methods, exhibiting strong generalization to unseen objects and diverse language queries. Moreover, deployment on a real robotic platform demonstrates the practical effectiveness of our approach in executing accurate, instruction-conditioned grasp actions. The code will be released publicly upon acceptance.",
        "url": "http://arxiv.org/abs/2512.21065v1",
        "published_date": "2025-12-24T09:16:42+00:00",
        "updated_date": "2025-12-24T09:16:42+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zebin Jiang",
            "Tianle Jin",
            "Xiangtong Yao",
            "Alois Knoll",
            "Hu Cao"
        ],
        "tldr": "This paper introduces a Language-Guided Grasp Detection (LGGD) method using a coarse-to-fine learning paradigm with CLIP embeddings and a language-conditioned dynamic convolution head (LDCH) to improve grasping accuracy based on language instructions. Experiments demonstrate strong performance on benchmark datasets and a real robot.",
        "tldr_zh": "本文提出了一种语言引导的抓取检测（LGGD）方法，该方法采用粗到精的学习范式，结合CLIP嵌入和语言条件动态卷积头（LDCH），以提高基于语言指令的抓取准确性。实验表明，该方法在基准数据集和真实机器人上表现出色。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "抓取是机器人操作中最基本且具有挑战性的能力之一，尤其是在非结构化、杂乱和语义多样化的环境中。 近期研究越来越多地探索语言引导的操作，其中机器人不仅感知场景，还解读任务相关的自然语言指令。 然而，现有的语言条件抓取方法通常依赖于浅层融合策略，导致语义 grounding 有限，以及语言意图与视觉抓取推理之间的对齐较弱。\n\n在这项工作中，我们提出了一种基于粗到细学习范式的语言引导抓取检测 (LGGD) 用于机器人操作。 LGGD 在分层的跨模态融合流程中利用基于 CLIP 的视觉和文本嵌入，逐步将语言线索注入到视觉特征重建过程中。 这种设计实现了细粒度的视觉-语义对齐，并提高了预测抓取相对于任务指令的可行性。 此外，我们引入了一种语言条件动态卷积头 (LDCH)，它基于句子级特征混合多个卷积专家，从而实现指令自适应的粗糙掩码和抓取预测。 最终的精细化模块进一步增强了复杂场景中抓取的一致性和鲁棒性。\n\n在 OCID-VLG 和 Grasp-Anything++ 数据集上的实验表明，LGGD 优于现有的语言引导抓取方法，对未见过的物体和多样化的语言查询表现出很强的泛化能力。 此外，在真实机器人平台上的部署证明了我们的方法在执行准确的、指令条件抓取动作方面的实际有效性。 代码将在接收后公开。"
    },
    {
        "title": "Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task",
        "summary": "From the perspective of future developments in robotics, it is crucial to verify whether foundation models trained exclusively on offline data, such as images and language, can understand the robot motion. In particular, since Vision Language Models (VLMs) do not include low-level motion information from robots in their training datasets, video understanding including trajectory information remains a significant challenge. In this study, we assess two capabilities of VLMs through a video captioning task with low-level robot motion information: (1) automatic captioning of robot tasks and (2) segmentation of a series of tasks. Both capabilities are expected to enhance the efficiency of robot imitation learning by linking language and motion and serve as a measure of the foundation model's performance. The proposed method generates multiple \"scene\" captions using image captions and trajectory data from robot tasks. The full task caption is then generated by summarizing these individual captions. Additionally, the method performs subtask segmentation by comparing the similarity between text embeddings of image captions. In both captioning tasks, the proposed method aims to improve performance by providing the robot's motion data - joint and end-effector states - as input to the VLM. Simulator experiments were conducted to validate the effectiveness of the proposed method.",
        "url": "http://arxiv.org/abs/2512.20876v1",
        "published_date": "2025-12-24T01:36:12+00:00",
        "updated_date": "2025-12-24T01:36:12+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Kanata Suzuki",
            "Shota Shimizu",
            "Tetsuya Ogata"
        ],
        "tldr": "This paper explores how incorporating robot proprioception data (joint and end-effector states) into Vision Language Models (VLMs) enhances their ability to generate captions and segment subtasks for robot tasks, aiming to improve robot imitation learning.",
        "tldr_zh": "本文探讨了将机器人本体感受数据（关节和末端执行器状态）融入视觉语言模型（VLMs），如何增强其生成机器人任务的字幕和分割子任务的能力，旨在提高机器人模仿学习。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "从机器人未来的发展角度来看，验证仅在离线数据（如图像和语言）上训练的基础模型是否能够理解机器人运动至关重要。特别地，由于视觉语言模型（VLMs）的训练数据集中不包含来自机器人的底层运动信息，因此包含轨迹信息的视频理解仍然是一个重大挑战。本研究通过带有底层机器人运动信息的视频字幕任务评估VLMs的两个能力：（1）机器人任务的自动字幕生成和（2）一系列任务的分割。这两种能力有望通过连接语言和运动来提高机器人模仿学习的效率，并作为衡量基础模型性能的指标。所提出的方法使用图像字幕和来自机器人任务的轨迹数据生成多个“场景”字幕。然后，通过总结这些单独的字幕生成完整的任务字幕。此外，该方法通过比较图像字幕文本嵌入之间的相似性来执行子任务分割。在这两个字幕任务中，所提出的方法旨在通过向VLM提供机器人的运动数据（关节和末端执行器状态）作为输入来提高性能。通过模拟器实验来验证所提出方法的有效性。"
    },
    {
        "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing",
        "summary": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.",
        "url": "http://arxiv.org/abs/2512.20591v1",
        "published_date": "2025-12-23T18:38:25+00:00",
        "updated_date": "2025-12-23T18:38:25+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Changyi Lin",
            "Boda Huo",
            "Mingyang Yu",
            "Emily Ruppel",
            "Bingqing Chen",
            "Jonathan Francis",
            "Ding Zhao"
        ],
        "tldr": "The paper introduces LightTact, a visual-tactile fingertip sensor that uses an optics-based approach to detect light contacts independent of surface deformation, enabling robotic manipulation of liquids and interaction with vision-language models.",
        "tldr_zh": "该论文介绍了一种名为LightTact的视觉触觉指尖传感器，它采用基于光学的方法来检测独立于表面形变的轻微接触，从而能够进行液体的机器人操作并与视觉-语言模型进行交互。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "接触经常发生在没有宏观表面形变的情况下，例如与液体、半液体或超软材料的交互。现有的大多数触觉传感器依赖于形变来推断接触，这使得此类轻接触交互难以稳健地感知。为了解决这个问题，我们提出 LightTact，一种视觉-触觉指尖传感器，它通过一种与形变无关的、基于光学原理的方式，使接触直接可见。LightTact 使用一种环境光阻挡的光学配置，抑制非接触区域的外部光和内部照明，同时仅传输在真实接触处产生的漫射光。因此，LightTact 产生高对比度的原始图像，其中非接触像素保持接近黑色（平均灰度值 < 3），而接触像素保留接触表面的自然外观。在此基础上，LightTact 实现了精确的像素级接触分割，并且对材料属性、接触力、表面外观和环境光照具有鲁棒性。我们进一步将 LightTact 集成到机械臂上，并展示了由极轻接触驱动的操作行为，包括水面扩散、面霜蘸取和薄膜交互。最后，我们表明 LightTact 空间对齐的视觉-触觉图像可以直接被现有的视觉-语言模型解释，从而实现电阻值推理以用于机器人分拣。"
    },
    {
        "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
        "summary": "Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.",
        "url": "http://arxiv.org/abs/2512.20563v1",
        "published_date": "2025-12-23T18:07:43+00:00",
        "updated_date": "2025-12-23T18:07:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Long Nguyen",
            "Micha Fauth",
            "Bernhard Jaeger",
            "Daniel Dauner",
            "Maximilian Igl",
            "Andreas Geiger",
            "Kashyap Chitta"
        ],
        "tldr": "This paper addresses the performance gap in imitation learning for end-to-end driving by minimizing the asymmetry between expert demonstrations and student observations, achieving state-of-the-art results on CARLA benchmarks and showing consistent sim-to-real gains.",
        "tldr_zh": "本文通过最小化专家演示和学生观察之间的不对称性，解决了端到端驾驶中模仿学习的性能差距，在CARLA基准测试中取得了最先进的结果，并展示了一致的模拟到真实世界的收益。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "模拟器可以生成几乎无限的驾驶数据，然而模拟环境中的模仿学习策略仍然难以实现稳健的闭环性能。受到这种差距的驱动，我们通过实验研究了特权专家演示与基于传感器的学生观测之间的不一致如何限制模仿学习的有效性。更具体地说，专家具有明显更高的可见性（例如，忽略遮挡）和远低于学生的不确定性（例如，知道其他车辆的动作），这使得他们难以可靠地模仿。此外，在测试时，学生模型中通过仅单个目标点来低估导航意图（即，遵循的路线）。我们证明了这些不对称性可以显著限制CARLA中的驾驶性能，并提供了解决这些问题的实用干预措施。经过仔细修改以缩小专家和学生之间的差距后，我们的TransFuser v6 (TFv6) 学生策略在所有主要的公开CARLA闭环基准测试中都达到了新的最先进水平，在Bench2Drive上达到了 95 DS，在Longest6~v2和Town13上的性能比之前提高了一倍以上。此外，通过将我们数据集的感知监督集成到共享的 sim-to-real 管道中，我们展示了在NAVSIM和Waymo Vision-Based End-to-End 驾驶基准测试中持续的收益。我们的代码、数据和模型可在 https://github.com/autonomousvision/lead 上公开获取。"
    },
    {
        "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
        "summary": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",
        "url": "http://arxiv.org/abs/2512.21024v1",
        "published_date": "2025-12-24T07:42:10+00:00",
        "updated_date": "2025-12-24T07:42:10+00:00",
        "categories": [
            "cs.GT",
            "cs.AI"
        ],
        "authors": [
            "Yue Lin",
            "Shuhui Zhu",
            "Wenhao Li",
            "Ang Li",
            "Dan Qiao",
            "Pascal Poupart",
            "Hongyuan Zha",
            "Baoxiang Wang"
        ],
        "tldr": "This paper introduces Programmatic Iterated Best Response (PIBR), a novel multi-agent reinforcement learning algorithm that uses LLMs to optimize human-interpretable policy code by utilizing game utility and runtime unit tests as feedback. The method successfully solves coordination matrix games and a cooperative foraging environment.",
        "tldr_zh": "该论文介绍了一种新颖的多智能体强化学习算法，名为程序化迭代最佳响应（PIBR），它利用大型语言模型优化人类可解释的策略代码，并使用博弈效用和运行时单元测试作为反馈。该方法成功解决了协调矩阵博弈和一个合作觅食环境。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "在多智能体任务中，核心挑战在于策略的动态适应。然而，在流行的深度强化学习范式中，直接以对手的策略为条件进行优化是难以处理的，因为存在一个根本的“表征瓶颈”：神经策略是不透明的、高维的参数向量，其他智能体难以理解。在这项工作中，我们提出一种范式转变，通过将策略表示为人类可读的源代码，并利用大型语言模型（LLM）作为近似解释器，来弥合这一差距。这种程序化的表示使我们能够实施博弈论中的*\\textit{程序均衡}*概念。我们通过利用LLM直接在程序化策略空间中进行优化，来重新构建学习问题。 LLM充当逐点最佳响应算子，迭代地合成和改进自我智能体的策略代码，以响应对手的策略。我们将这个过程形式化为*\\textit{程序化迭代最佳响应（PIBR）}*，这是一种通过文本梯度优化策略代码的算法，使用从博弈效用和运行时单元测试中导出的结构化反馈。我们证明，这种方法有效地解决了几个标准协调矩阵博弈和一个合作性的基于等级的觅食环境。"
    },
    {
        "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
        "summary": "Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.",
        "url": "http://arxiv.org/abs/2512.20934v1",
        "published_date": "2025-12-24T04:30:21+00:00",
        "updated_date": "2025-12-24T04:30:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.MA"
        ],
        "authors": [
            "Shengguang Wu",
            "Xiaohan Wang",
            "Yuhui Zhang",
            "Hao Zhu",
            "Serena Yeung-Levy"
        ],
        "tldr": "This paper introduces Transductive Visual Programming (TVP), a novel framework that learns and evolves tool libraries from experience to improve spatial reasoning in 3D scenes, achieving state-of-the-art results on Omni3D-Bench.",
        "tldr_zh": "本文介绍了Transductive Visual Programming (TVP)，一种新型框架，通过从经验中学习和发展工具库来提升3D场景中的空间推理能力，并在Omni3D-Bench上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在3D场景中的空间推理需要精确的几何计算，这对视觉-语言模型提出了挑战。视觉编程通过将问题分解为调用专门工具的步骤来解决这个问题，但现有方法依赖于固定的工具集或在解决问题之前进行推测性的工具归纳，导致次优程序和对归纳工具的低效利用。我们提出了转导视觉编程（TVP），这是一个新颖的框架，它从自身的经验而非推测中构建新的工具。TVP首先使用基本工具解决问题，同时将经验性的解决方案积累到示例库中，然后从这些程序中抽象出可重用的高层模式到不断演进的工具库中。这使得TVP能够使用从经验中学习到的日益强大的工具来解决新问题。在Omni3D-Bench上，TVP取得了最先进的性能，超越GPT-4o 22%，并超越之前的最佳视觉编程系统11%。我们通过转导学习到的工具作为核心程序依赖的使用频率是归纳创建的工具的5倍，表明更有效的工具发现和重用。演进后的工具还表现出对未见过的空间任务的强大泛化能力，在SpatialScore-Hard系列基准测试中取得了优异的性能，而无需进行任何测试集特定的修改。我们的工作确立了经验驱动的转导工具创建作为一种强大的范例，用于构建能够有效解决具有挑战性的空间推理任务的自演进视觉编程智能体。我们在https://transductive-visualprogram.github.io/发布了我们的代码。"
    },
    {
        "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
        "summary": "Autonomous agents powered by LLMs and Retrieval-Augmented Generation (RAG) are proficient consumers of digital content but remain unidirectional, a limitation we term epistemic asymmetry. This isolation leads to redundant reasoning and stagnates collective intelligence. Current self-reflection frameworks remain largely heuristic and private, lacking a probabilistic foundation to quantify certainty or justify external interaction.To bridge this gap, we propose a formal probabilistic framework that provides agents with a non-altruistic motive for bidirectional knowledge exchange. We model an agent's belief in a proposition using a Beta-Bernoulli distribution with a forgetting factor ($γ$). This allows us to isolate epistemic uncertainty as the variance of belief, establishing a dual drive for interaction: A homeostatic motive: The need to maintain certainty against the temporal decay introduced by $γ$. An optimal learning strategy: Targeting points of maximum ambiguity ($\\mathbb{E}[θ]=0.5$) to maximize information gain. Under this framework, public contribution is reframed as optimal active learning: sharing solutions to elicit feedback is the most efficient method for an agent to reduce its own uncertainty. To ensure scalability, we introduce epistemic caching, which leverages the forgetting factor to dynamically prioritize resources for the active head of non-stationary knowledge distributions. Finally, we demonstrate how these accumulated belief states serve as verifiable reward signals for Reinforcement Learning from Human Feedback (RLHF) and high-quality data filters for Supervised Fine-Tuning (SFT). Simulation results validate that this uncertainty-driven strategy significantly outperforms random baselines in heterogeneous (Zipfian) environments, maintaining high adaptability to concept drift.",
        "url": "http://arxiv.org/abs/2512.20884v1",
        "published_date": "2025-12-24T02:02:25+00:00",
        "updated_date": "2025-12-24T02:02:25+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Zan-Kai Chong",
            "Hiroyuki Ohsaki",
            "Bryan Ng"
        ],
        "tldr": "The paper introduces a probabilistic framework using Beta-Bernoulli distributions to model LLM agent beliefs and uncertainty, driving knowledge exchange and improving learning through active learning strategies like sharing solutions for feedback. This ultimately leads to better RLHF and SFT.",
        "tldr_zh": "该论文提出了一个概率框架，使用 Beta-Bernoulli 分布来建模 LLM 代理的信念和不确定性，从而驱动知识交换，并通过主动学习策略（如分享解决方案以获取反馈）来改进学习。 最终这会导致更好的 RLHF 和 SFT。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "由LLM和检索增强生成(RAG)驱动的自主智能体是数字内容的高效消费者，但仍然是单向的，我们称之为认知不对称的局限性。这种孤立导致冗余推理并阻碍集体智能的进步。当前的自我反思框架在很大程度上仍然是启发式的和私有的，缺乏量化确定性或为外部交互辩护的概率基础。为了弥合这一差距，我们提出了一个正式的概率框架，为智能体提供知识双向交换的非利他动机。我们使用带有遗忘因子($γ$)的Beta-Bernoulli分布来建模智能体对命题的信念。这使我们能够将认知不确定性隔离为信念的方差，从而建立交互的双重驱动力：稳态动机：维持确定性以对抗由$γ$引入的时间衰减的需求。最优学习策略：瞄准最大模糊点($\\mathbb{E}[θ]=0.5$)以最大化信息增益。在此框架下，公共贡献被重新构造为最优主动学习：分享解决方案以引出反馈是智能体减少自身不确定性的最有效方法。为了确保可扩展性，我们引入了认知缓存，它利用遗忘因子来动态地优先处理非平稳知识分布的活跃头部资源。最后，我们展示了这些累积的信念状态如何作为可验证的奖励信号，用于来自人类反馈的强化学习(RLHF)，以及用于监督微调(SFT)的高质量数据过滤器。仿真结果验证了这种不确定性驱动的策略在异构（Zipfian）环境中显著优于随机基线，并保持了对概念漂移的高度适应性。"
    },
    {
        "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
        "summary": "We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.",
        "url": "http://arxiv.org/abs/2512.20848v1",
        "published_date": "2025-12-23T23:54:32+00:00",
        "updated_date": "2025-12-23T23:54:32+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "NVIDIA",
            ":",
            "Aaron Blakeman",
            "Aaron Grattafiori",
            "Aarti Basant",
            "Abhibha Gupta",
            "Abhinav Khattar",
            "Adi Renduchintala",
            "Aditya Vavre",
            "Akanksha Shukla",
            "Akhiad Bercovich",
            "Aleksander Ficek",
            "Aleksandr Shaposhnikov",
            "Alex Kondratenko",
            "Alexander Bukharin",
            "Alexandre Milesi",
            "Ali Taghibakhshi",
            "Alisa Liu",
            "Amelia Barton",
            "Ameya Sunil Mahabaleshwarkar",
            "Amir Klein",
            "Amit Zuker",
            "Amnon Geifman",
            "Amy Shen",
            "Anahita Bhiwandiwalla",
            "Andrew Tao",
            "Ann Guan",
            "Anubhav Mandarwal",
            "Arham Mehta",
            "Ashwath Aithal",
            "Ashwin Poojary",
            "Asif Ahamed",
            "Asma Kuriparambil Thekkumpate",
            "Ayush Dattagupta",
            "Banghua Zhu",
            "Bardiya Sadeghi",
            "Barnaby Simkin",
            "Ben Lanir",
            "Benedikt Schifferer",
            "Besmira Nushi",
            "Bilal Kartal",
            "Bita Darvish Rouhani",
            "Boris Ginsburg",
            "Brandon Norick",
            "Brandon Soubasis",
            "Branislav Kisacanin",
            "Brian Yu",
            "Bryan Catanzaro",
            "Carlo del Mundo",
            "Chantal Hwang",
            "Charles Wang",
            "Cheng-Ping Hsieh",
            "Chenghao Zhang",
            "Chenhan Yu",
            "Chetan Mungekar",
            "Chintan Patel",
            "Chris Alexiuk",
            "Christopher Parisien",
            "Collin Neale",
            "Damon Mosk-Aoyama",
            "Dan Su",
            "Dane Corneil",
            "Daniel Afrimi",
            "Daniel Rohrer",
            "Daniel Serebrenik",
            "Daria Gitman",
            "Daria Levy",
            "Darko Stosic",
            "David Mosallanezhad",
            "Deepak Narayanan",
            "Dhruv Nathawani",
            "Dima Rekesh",
            "Dina Yared",
            "Divyanshu Kakwani",
            "Dong Ahn",
            "Duncan Riach",
            "Dusan Stosic",
            "Edgar Minasyan",
            "Edward Lin",
            "Eileen Long",
            "Eileen Peters Long",
            "Elena Lantz",
            "Ellie Evans",
            "Elliott Ning",
            "Eric Chung",
            "Eric Harper",
            "Eric Tramel",
            "Erick Galinkin",
            "Erik Pounds",
            "Evan Briones",
            "Evelina Bakhturina",
            "Faisal Ladhak",
            "Fay Wang",
            "Fei Jia",
            "Felipe Soares",
            "Feng Chen",
            "Ferenc Galko",
            "Frankie Siino",
            "Gal Hubara Agam",
            "Ganesh Ajjanagadde",
            "Gantavya Bhatt",
            "Gargi Prasad",
            "George Armstrong",
            "Gerald Shen",
            "Gorkem Batmaz",
            "Grigor Nalbandyan",
            "Haifeng Qian",
            "Harsh Sharma",
            "Hayley Ross",
            "Helen Ngo",
            "Herman Sahota",
            "Hexin Wang",
            "Himanshu Soni",
            "Hiren Upadhyay",
            "Huizi Mao",
            "Huy C Nguyen",
            "Huy Q Nguyen",
            "Iain Cunningham",
            "Ido Shahaf",
            "Igor Gitman",
            "Ilya Loshchilov",
            "Ivan Moshkov",
            "Izzy Putterman",
            "Jan Kautz",
            "Jane Polak Scowcroft",
            "Jared Casper",
            "Jatin Mitra",
            "Jeffrey Glick",
            "Jenny Chen",
            "Jesse Oliver",
            "Jian Zhang",
            "Jiaqi Zeng",
            "Jie Lou",
            "Jimmy Zhang",
            "Jining Huang",
            "Joey Conway",
            "Joey Guman",
            "John Kamalu",
            "Johnny Greco",
            "Jonathan Cohen",
            "Joseph Jennings",
            "Joyjit Daw",
            "Julien Veron Vialard",
            "Junkeun Yi",
            "Jupinder Parmar",
            "Kai Xu",
            "Kan Zhu",
            "Kari Briski",
            "Katherine Cheung",
            "Katherine Luna",
            "Keshav Santhanam",
            "Kevin Shih",
            "Kezhi Kong",
            "Khushi Bhardwaj",
            "Krishna C. Puvvada",
            "Krzysztof Pawelec",
            "Kumar Anik",
            "Lawrence McAfee",
            "Laya Sleiman",
            "Leon Derczynski",
            "Li Ding",
            "Lucas Liebenwein",
            "Luis Vega",
            "Maanu Grover",
            "Maarten Van Segbroeck",
            "Maer Rodrigues de Melo",
            "Makesh Narsimhan Sreedhar",
            "Manoj Kilaru",
            "Maor Ashkenazi",
            "Marc Romeijn",
            "Mark Cai",
            "Markus Kliegl",
            "Maryam Moosaei",
            "Matvei Novikov",
            "Mehrzad Samadi",
            "Melissa Corpuz",
            "Mengru Wang",
            "Meredith Price",
            "Michael Boone",
            "Michael Evans",
            "Miguel Martinez",
            "Mike Chrzanowski",
            "Mohammad Shoeybi",
            "Mostofa Patwary",
            "Nabin Mulepati",
            "Natalie Hereth",
            "Nave Assaf",
            "Negar Habibi",
            "Neta Zmora",
            "Netanel Haber",
            "Nicola Sessions",
            "Nidhi Bhatia",
            "Nikhil Jukar",
            "Nikki Pope",
            "Nikolai Ludwig",
            "Nima Tajbakhsh",
            "Nirmal Juluru",
            "Oleksii Hrinchuk",
            "Oleksii Kuchaiev",
            "Olivier Delalleau",
            "Oluwatobi Olabiyi",
            "Omer Ullman Argov",
            "Ouye Xie",
            "Parth Chadha",
            "Pasha Shamis",
            "Pavlo Molchanov",
            "Pawel Morkisz",
            "Peter Dykas",
            "Peter Jin",
            "Pinky Xu",
            "Piotr Januszewski",
            "Pranav Prashant Thombre",
            "Prasoon Varshney",
            "Pritam Gundecha",
            "Qing Miao",
            "Rabeeh Karimi Mahabadi",
            "Ran El-Yaniv",
            "Ran Zilberstein",
            "Rasoul Shafipour",
            "Rich Harang",
            "Rick Izzo",
            "Rima Shahbazyan",
            "Rishabh Garg",
            "Ritika Borkar",
            "Ritu Gala",
            "Riyad Islam",
            "Roger Waleffe",
            "Rohit Watve",
            "Roi Koren",
            "Ruoxi Zhang",
            "Russell J. Hewett",
            "Ryan Prenger",
            "Ryan Timbrook",
            "Sadegh Mahdavi",
            "Sahil Modi",
            "Samuel Kriman",
            "Sanjay Kariyappa",
            "Sanjeev Satheesh",
            "Saori Kaji",
            "Satish Pasumarthi",
            "Sean Narentharen",
            "Sean Narenthiran",
            "Seonmyeong Bak",
            "Sergey Kashirsky",
            "Seth Poulos",
            "Shahar Mor",
            "Shanmugam Ramasamy",
            "Shantanu Acharya",
            "Shaona Ghosh",
            "Sharath Turuvekere Sreenivas",
            "Shelby Thomas",
            "Shiqing Fan",
            "Shreya Gopal",
            "Shrimai Prabhumoye",
            "Shubham Pachori",
            "Shubham Toshniwal",
            "Shuoyang Ding",
            "Siddharth Singh",
            "Simeng Sun",
            "Smita Ithape",
            "Somshubra Majumdar",
            "Soumye Singhal",
            "Stefania Alborghetti",
            "Stephen Ge",
            "Sugam Dipak Devare",
            "Sumeet Kumar Barua",
            "Suseella Panguluri",
            "Suyog Gupta",
            "Sweta Priyadarshi",
            "Syeda Nahida Akter",
            "Tan Bui",
            "Teodor-Dumitru Ene",
            "Terry Kong",
            "Thanh Do",
            "Tijmen Blankevoort",
            "Tom Balough",
            "Tomer Asida",
            "Tomer Bar Natan",
            "Tugrul Konuk",
            "Twinkle Vashishth",
            "Udi Karpas",
            "Ushnish De",
            "Vahid Noorozi",
            "Vahid Noroozi",
            "Venkat Srinivasan",
            "Venmugil Elango",
            "Vijay Korthikanti",
            "Vitaly Kurin",
            "Vitaly Lavrukhin",
            "Wanli Jiang",
            "Wasi Uddin Ahmad",
            "Wei Du",
            "Wei Ping",
            "Wenfei Zhou",
            "Will Jennings",
            "William Zhang",
            "Wojciech Prazuch",
            "Xiaowei Ren",
            "Yashaswi Karnati",
            "Yejin Choi",
            "Yev Meyer",
            "Yi-Fu Wu",
            "Yian Zhang",
            "Ying Lin",
            "Yonatan Geifman",
            "Yonggan Fu",
            "Yoshi Subara",
            "Yoshi Suhara",
            "Yubo Gao",
            "Zach Moshe",
            "Zhen Dong",
            "Zihan Liu",
            "Zijia Chen",
            "Zijie Yan"
        ],
        "tldr": "Nemotron 3 Nano, a 30B parameter Mixture-of-Experts model with Mamba and Transformer architectures, achieves higher throughput and accuracy compared to similar open-source models while demonstrating enhanced agentic reasoning and a 1M token context length. The base and fine-tuned models are released on Hugging Face.",
        "tldr_zh": "Nemotron 3 Nano是一个30B参数的混合专家模型，结合了Mamba和Transformer架构，与类似的开源模型相比，实现了更高的吞吐量和准确性，同时展示了增强的agentic推理能力和1M token的上下文长度。基础模型和微调后的模型已在Hugging Face上发布。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了 Nemotron 3 Nano 30B-A3B，一种混合专家模型的混合Mamba-Transformer语言模型。Nemotron 3 Nano 经过了25万亿文本token的预训练，其中包括比 Nemotron 2 多3万亿的全新独特token，随后进行了监督微调和在多样化环境中的大规模强化学习。Nemotron 3 Nano 比我们前一代 Nemotron 2 Nano 实现了更高的准确率，同时每次前向传递激活的参数不到其一半。 与 GPT-OSS-20B 和 Qwen3-30B-A3B-Thinking-2507 等类似规模的开放模型相比，它实现了高达 3.3 倍的推理吞吐量，同时在流行的基准测试中也更准确。 Nemotron 3 Nano 展示了增强的代理、推理和聊天能力，并支持高达 100 万 token 的上下文长度。 我们在 Hugging Face 上发布了经过预训练的 Nemotron 3 Nano 30B-A3B Base 和经过后训练的 Nemotron 3 Nano 30B-A3B checkpoint。"
    },
    {
        "title": "Safety Alignment of LMs via Non-cooperative Games",
        "summary": "Ensuring the safety of language models (LMs) while maintaining their usefulness remains a critical challenge in AI alignment. Current approaches rely on sequential adversarial training: generating adversarial prompts and fine-tuning LMs to defend against them. We introduce a different paradigm: framing safety alignment as a non-zero-sum game between an Attacker LM and a Defender LM trained jointly via online reinforcement learning. Each LM continuously adapts to the other's evolving strategies, driving iterative improvement. Our method uses a preference-based reward signal derived from pairwise comparisons instead of point-wise scores, providing more robust supervision and potentially reducing reward hacking. Our RL recipe, AdvGame, shifts the Pareto frontier of safety and utility, yielding a Defender LM that is simultaneously more helpful and more resilient to adversarial attacks. In addition, the resulting Attacker LM converges into a strong, general-purpose red-teaming agent that can be directly deployed to probe arbitrary target models.",
        "url": "http://arxiv.org/abs/2512.20806v1",
        "published_date": "2025-12-23T22:13:14+00:00",
        "updated_date": "2025-12-23T22:13:14+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Anselm Paulus",
            "Ilia Kulikov",
            "Brandon Amos",
            "Rémi Munos",
            "Ivan Evtimov",
            "Kamalika Chaudhuri",
            "Arman Zharmagambetov"
        ],
        "tldr": "This paper introduces a novel approach to language model safety alignment using a non-cooperative game between an Attacker and a Defender LM, trained jointly via reinforcement learning and preference-based rewards, resulting in improved safety and utility.",
        "tldr_zh": "该论文提出了一种新颖的语言模型安全对齐方法，使用攻击者和防御者 LM 之间的非合作博弈，通过强化学习和基于偏好的奖励进行联合训练，从而提高安全性和效用。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在人工智能校准中，确保语言模型（LM）安全性的同时维持其效用，仍然是一项关键挑战。现有方法依赖于序列对抗训练：生成对抗提示并对语言模型进行微调，以防御这些提示。我们引入了一种不同的范式：将安全对齐构建为攻击者LM和防御者LM之间的非零和博弈，二者通过在线强化学习联合训练。每个LM持续适应对方不断演变的策略，从而驱动迭代改进。我们的方法使用基于成对比较得出的偏好奖励信号，而不是逐点分数的奖励信号，提供更鲁棒的监督，并可能减少奖励篡改。我们的强化学习方案AdvGame，移动了安全性和效用的帕累托前沿，产生了一个既更有帮助又更能抵抗对抗性攻击的防御者LM。此外，由此产生的攻击者LM会收敛为一个强大的、通用的红队代理，可以直接部署来探测任意目标模型。"
    },
    {
        "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
        "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
        "url": "http://arxiv.org/abs/2512.20618v1",
        "published_date": "2025-12-23T18:59:49+00:00",
        "updated_date": "2025-12-23T18:59:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.MA"
        ],
        "authors": [
            "Runtao Liu",
            "Ziyi Liu",
            "Jiaqi Tang",
            "Yue Ma",
            "Renjie Pi",
            "Jipeng Zhang",
            "Qifeng Chen"
        ],
        "tldr": "The paper introduces a multi-agent framework (LongVideoAgent) with reinforcement learning for long-video QA, improving performance by focusing on relevant clips via a grounding mechanism and complementing subtitles with visual details. It outperforms existing baselines on a new long-video QA dataset.",
        "tldr_zh": "该论文提出了一个基于强化学习的多智能体框架 (LongVideoAgent) 用于长视频问答，通过使用 grounding 机制聚焦于相关片段并使用视觉细节补充字幕来提高性能。 实验表明，该框架在新的长视频问答数据集上优于现有基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "多模态大型语言模型和使用工具进行长视频问答的系统的最新进展，预示着对长达数小时剧集进行推理的可能性。然而，许多方法仍然将内容压缩成有损摘要，或依赖于有限的工具集，从而削弱了时间定位并遗漏了细粒度的线索。我们提出了一个多智能体框架，其中主大型语言模型（Master LLM）协调一个定位智能体来定位与问题相关的片段，以及一个视觉智能体来提取有针对性的文本观测。Master智能体在步数限制下进行规划，并使用强化学习进行训练，以鼓励简洁、正确和高效的多智能体合作。这种设计有助于Master智能体通过定位来关注相关片段，利用视觉细节补充字幕，并产生可解释的轨迹。在我们提出的LongTVQA和LongTVQA+（从TVQA/TVQA+聚合而成的剧集级别数据集）上，我们的多智能体系统显著优于强大的非智能体基线。实验还表明，强化学习进一步加强了已训练智能体的推理和规划能力。代码和数据将在https://longvideoagent.github.io/共享。"
    },
    {
        "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
        "summary": "Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term \"internal RL\", enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.",
        "url": "http://arxiv.org/abs/2512.20605v2",
        "published_date": "2025-12-23T18:51:50+00:00",
        "updated_date": "2025-12-24T08:32:45+00:00",
        "categories": [
            "cs.LG",
            "cs.AI"
        ],
        "authors": [
            "Seijin Kobayashi",
            "Yanick Schimpf",
            "Maximilian Schlegel",
            "Angelika Steger",
            "Maciej Wolczyk",
            "Johannes von Oswald",
            "Nino Scherrer",
            "Kaitlin Maile",
            "Guillaume Lajoie",
            "Blake A. Richards",
            "Rif A. Saurous",
            "James Manyika",
            "Blaise Agüera y Arcas",
            "Alexander Meulemans",
            "João Sacramento"
        ],
        "tldr": "This paper introduces \"internal RL,\" a method for hierarchical reinforcement learning within autoregressive models by learning temporally abstract actions via a higher-order sequence model controlling the base model's residual stream, enabling efficient exploration in sparse reward settings.",
        "tldr_zh": "本文介绍了一种名为“内部RL”的分层强化学习方法，该方法通过一个高阶序列模型控制基础模型的残差流，从而在自回归模型中学习时间抽象的动作，从而在稀疏奖励环境中实现高效探索。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基于下一词预测进行预训练并使用强化学习（RL）进行微调的大规模自回归模型，已在许多问题领域取得了前所未有的成功。在强化学习过程中，这些模型通过逐个生成新的token输出来进行探索。然而，逐个token采样动作可能导致学习效率低下，尤其是在奖励稀疏的情况下。在此，我们表明，可以通过在自回归模型的内部表示中进行动作和探索来克服这个问题。具体来说，为了发现时间抽象动作，我们引入了一个高阶的、非因果的序列模型，该模型的输出控制着基本自回归模型的残差流激活。在具有层级结构的网格世界和基于MuJoCo的任务中，我们发现高阶模型学会将长激活序列块压缩到内部控制器上。至关重要的是，每个控制器执行一系列在长时间尺度上展开的行为上有意义的动作，并伴随着一个学习到的终止条件，从而使得随着时间的推移组合多个控制器可以有效地探索新的任务。我们表明，直接的内部控制器强化，我们称之为“内部RL”的过程，使得在标准RL微调失败的情况下能够从稀疏奖励中学习。我们的结果证明了在自回归模型中潜在动作生成和强化的优势，表明内部RL是实现基础模型中层级RL的一种有希望的途径。"
    },
    {
        "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
        "summary": "We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.",
        "url": "http://arxiv.org/abs/2512.20595v1",
        "published_date": "2025-12-23T18:43:05+00:00",
        "updated_date": "2025-12-23T18:43:05+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Dhruv Anand",
            "Ehsan Shareghi"
        ],
        "tldr": "The paper introduces Cube Bench, a benchmark using Rubik's Cubes to evaluate spatial and sequential reasoning capabilities in MLLMs, revealing performance bottlenecks and a gap between closed-source and open-source models.",
        "tldr_zh": "该论文介绍了Cube Bench，一个使用魔方的基准测试，用于评估多模态大型语言模型（MLLMs）的空间和序列推理能力，揭示了性能瓶颈以及闭源和开源模型之间的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们引入了Cube Bench，一个魔方基准测试，用于评估多模态大型语言模型（MLLM）中的空间和序列推理能力。该基准测试将性能分解为五个技能：（i）从图像和文本中重建魔方的各个面，（ii）选择最佳的下一步操作，（iii）在不执行候选操作的情况下预测其结果，（iv）执行多步计划并在错误中恢复，以及（v）检测和修订自身错误。我们使用一组共享的打乱魔方状态、相同的提示和解析器以及单一的距离已解状态度量，对最新的MLLM进行并排比较，并将其作为打乱深度的函数。在七个MLLM中，准确率随深度急剧下降；一旦轨迹停滞或发散，模型几乎无法恢复，并且高的面部重建准确率不能保证胜任的操作选择或多步执行。出现了明显的闭源模型与开源模型之间的差距：最强的闭源模型在单步感知任务和多步控制任务上均处于领先地位，而开源权重模型在最困难的设置中接近于随机水平；然而，即使是最好的MLLM也会在高魔方复杂度下性能下降。通过反思性思维进行的简单自我纠正会产生适度的提升，但也可能引入过度思考。Cube Bench为MLLM中的序列空间推理提供了一个紧凑、可复现的探针。"
    },
    {
        "title": "AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents",
        "summary": "Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.",
        "url": "http://arxiv.org/abs/2512.21302v1",
        "published_date": "2025-12-24T17:40:42+00:00",
        "updated_date": "2025-12-24T17:40:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Cao",
            "Yingyao Wang",
            "Pi Bu",
            "Jingxuan Xing",
            "Wei Jiang",
            "Zekun Zhu",
            "Junpeng Ma",
            "Sashuai Zhou",
            "Tong Lu",
            "Jun Song",
            "Yu Cheng",
            "Yuning Jiang",
            "Bo Zheng"
        ],
        "tldr": "The paper introduces AndroidLens, a new benchmark for evaluating GUI agents on long-latency mobile tasks, highlighting the challenges of real-world Android automation with current models.",
        "tldr_zh": "该论文介绍了AndroidLens，一个新的基准测试，用于评估GUI代理在长延迟移动任务上的表现，强调了当前模型在现实世界的Android自动化中面临的挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "图形用户界面 (GUI) 代理可以通过自动化移动设备上频繁执行的长延迟任务来大幅提高生产力。然而，现有的评估基准仍然局限于有限的应用、简单的任务和粗粒度的指标。为了解决这个问题，我们推出了AndroidLens，这是一个具有挑战性的移动GUI代理评估框架，包含中文和英文环境下的571个长延迟任务，每个任务平均需要完成超过26个步骤。该框架的特点包括：（1）任务来源于38个领域中的真实用户场景，涵盖了多约束、多目标和特定领域任务等复杂类型；（2）静态评估，保留了真实世界的异常情况，并允许多条有效路径以减少偏差；（3）动态评估，采用基于里程碑的方案，通过平均任务进度 (ATP) 进行细粒度的进度测量。我们的评估表明，即使是最好的模型也仅达到12.7%的任务成功率和50.47%的ATP。我们还强调了真实世界环境中的关键挑战，包括环境异常、自适应探索和长期记忆保持。"
    },
    {
        "title": "PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding",
        "summary": "3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.",
        "url": "http://arxiv.org/abs/2512.20907v1",
        "published_date": "2025-12-24T03:18:51+00:00",
        "updated_date": "2025-12-24T03:18:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seongmin Jung",
            "Seongho Choi",
            "Gunwoo Jeon",
            "Minsu Cho",
            "Jongwoo Lim"
        ],
        "tldr": "The paper introduces PanoGrounder, a novel framework for 3D Visual Grounding that leverages panoramic scene representations and pre-trained 2D VLMs to achieve state-of-the-art results and generalization on 3D datasets.",
        "tldr_zh": "该论文介绍了PanoGrounder，一种新颖的3D视觉定位框架，它利用全景场景表示和预训练的2D VLM，在3D数据集上实现了最先进的结果和泛化。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "三维视觉定位 (3DVG) 是视觉-语言感知到机器人学的关键桥梁，需要语言理解和三维场景推理。传统的监督模型利用显式的三维几何信息，但由于三维视觉-语言数据集的稀缺以及与现代视觉-语言模型 (VLMs) 相比有限的推理能力，泛化能力受到限制。我们提出了一种可泛化的3DVG框架 PanoGrounder，它将多模态全景表示与预训练的二维 VLMs 结合，以实现强大的视觉-语言推理能力。全景渲染图，通过三维语义和几何特征增强，充当二维和三维之间的中间表示，并提供两个主要优势：(i) 它们可以通过最小的调整直接输入到 VLMs 中，以及 (ii) 它们能够保留长程的对象-对象关系，这得益于其 360 度的视野。我们设计了一个三阶段流程，该流程考虑到场景布局和几何放置了一组紧凑的全景视点，使用 VLM 在每个全景渲染图上定位文本查询，并通过提升操作将每个视点的预测融合为单个三维包围盒。我们的方法在 ScanRefer 和 Nr3D 数据集上取得了最先进的结果，并展示了对未见过的三维数据集和文本释义的卓越泛化能力。"
    },
    {
        "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following",
        "summary": "Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.",
        "url": "http://arxiv.org/abs/2512.20735v1",
        "published_date": "2025-12-23T19:47:11+00:00",
        "updated_date": "2025-12-23T19:47:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shijing Wang",
            "Chaoqun Cui",
            "Yaping Huang",
            "Hyung Jin Chang",
            "Yihua Cheng"
        ],
        "tldr": "The paper introduces VL4Gaze, a large-scale benchmark for evaluating and training Vision-Language Models (VLMs) for gaze understanding, showing that VLMs struggle with gaze interpretation without task-specific training but can be significantly improved with VL4Gaze.",
        "tldr_zh": "该论文介绍了VL4Gaze，一个大规模的基准，用于评估和训练视觉语言模型（VLMs）以理解视线，表明VLMs在没有特定任务训练的情况下难以进行视线解读，但通过VL4Gaze可以显著提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人类注视提供了在视觉场景中理解注意力、意图和社交互动的重要线索，然而，在当前的视觉-语言模型（VLMs）中，对注视的理解仍然很大程度上未被探索。尽管最近的VLMs在一系列视觉任务中实现了强大的场景级推理能力，但目前还没有系统性地评估或训练它们进行注视解读的基准，这使得注视理解是否可以从通用视觉-语言预训练中涌现出来这个问题仍然悬而未决。为了解决这一差距，我们引入了VL4Gaze，这是首个旨在研究、评估和释放VLMs在注视理解方面潜力的超大规模基准。VL4Gaze包含12.4万张图像中的48.9万个自动生成的问答对，并通过四个互补的任务将注视理解形式化为一个统一的视觉问答（VQA）问题：(1) 注视对象描述，(2) 注视方向描述，(3) 注视点定位，以及 (4) 歧义问题识别。我们全面评估了在上下文学习和微调设置下的商业和开源VLMs。结果表明，即使是超大规模的VLMs，在没有特定任务监督的情况下，也很难可靠地推断注视语义和空间定位。相比之下，在VL4Gaze上进行训练可以在所有任务中带来实质性和一致的改进，突显了针对性的多任务监督对于开发VLMs的注视理解能力的重要性。我们将发布数据集和代码，以支持该方向的进一步研究和开发。"
    },
    {
        "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
        "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
        "url": "http://arxiv.org/abs/2512.20617v1",
        "published_date": "2025-12-23T18:59:46+00:00",
        "updated_date": "2025-12-23T18:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxi Xiao",
            "Longfei Li",
            "Shen Yan",
            "Xinhang Liu",
            "Sida Peng",
            "Yunchao Wei",
            "Xiaowei Zhou",
            "Bingyi Kang"
        ],
        "tldr": "The paper introduces SpatialTree, a hierarchical benchmark for evaluating spatial abilities in MLLMs, revealing transfer dynamics and proposing an 'auto-think' strategy to improve performance across all levels.",
        "tldr_zh": "该论文介绍了SpatialTree，一个用于评估多模态大语言模型中空间能力的层级基准。研究揭示了迁移动态性，并提出了一种“自动思考”策略，以提高所有级别的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "认知科学表明空间能力是渐进发展的，从感知到推理再到交互。然而，在多模态大型语言模型 (MLLM) 中，这种层级结构仍然认识不足，因为大多数研究都集中于狭窄的任务集。我们引入SpatialTree，这是一个受认知科学启发的层级结构，将空间能力组织成四个层次：低级感知 (L1)、心智绘图 (L2)、模拟 (L3) 和能动性能力 (L4)。基于此分类法，我们构建了第一个能力中心的分层基准，全面评估主流 MLLM 在 27 个子能力上的表现。评估结果揭示了清晰的结构：L1 技能在很大程度上是正交的，而更高层次的技能则强烈相关，表明相互依赖性日益增强。通过有针对性的监督式微调，我们发现了一个令人惊讶的迁移动态：L1 内的负迁移，但从低级到高级能力的强跨层级迁移，并具有显著的协同效应。最后，我们探讨了如何改进整个层级结构。我们发现，鼓励广泛“思考”的朴素强化学习是不可靠的：它有助于复杂推理，但损害直觉感知。我们提出了一种简单的自发思考策略，抑制不必要的深思熟虑，使强化学习能够持续提高所有级别的性能。通过构建 SpatialTree，我们为理解和系统地扩展 MLLM 中的空间能力提供了一个概念验证框架。"
    },
    {
        "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
        "summary": "Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.",
        "url": "http://arxiv.org/abs/2512.20615v1",
        "published_date": "2025-12-23T18:59:16+00:00",
        "updated_date": "2025-12-23T18:59:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuanhua He",
            "Tianyu Yang",
            "Ke Cao",
            "Ruiqi Wu",
            "Cheng Meng",
            "Yong Zhang",
            "Zhuoliang Kang",
            "Xiaoming Wei",
            "Qifeng Chen"
        ],
        "tldr": "The paper introduces L-IVA, a benchmark and ORCA, a framework for enabling active intelligence in video avatars via a closed-loop architecture with internal world modeling, achieving autonomous goal-directed behavior in open-domain scenarios.",
        "tldr_zh": "该论文介绍了L-IVA，一个用于评估视频化身主动智能的基准，以及ORCA，一个通过闭环架构和内部世界建模实现视频化身主动智能的框架，从而在开放领域场景中实现自主的、以目标为导向的行为。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "当前视频化身生成方法擅长于身份保持和动作对齐，但缺乏真正的自主性，无法通过自适应的环境交互来自主追求长期目标。为了解决这个问题，我们引入了 L-IVA（长时程交互视觉化身），一个用于评估随机生成环境中目标导向规划的任务和基准，以及 ORCA（在线推理和认知架构），第一个使视频化身具备主动智能的框架。ORCA 通过两项关键创新体现了内部世界模型 (IWM) 的能力：(1) 一个闭环 OTAR 循环（观察-思考-行动-反思），通过持续验证预测结果与实际生成结果，在生成不确定性下保持稳健的状态追踪；(2) 一个分层双系统架构，其中系统 2 利用状态预测执行战略推理，而系统 1 将抽象计划转化为精确的、模型特定的动作描述。通过将化身控制表述为 POMDP 并利用结果验证实施连续信念更新，ORCA 能够在开放域场景中实现自主的多步骤任务完成。大量实验表明，ORCA 在任务成功率和行为连贯性方面显著优于开环和非反思基线，验证了我们受 IWM 启发的视频化身智能设计，使其从被动动画转变为主动的、目标导向的行为。"
    },
    {
        "title": "Dyna-Style Reinforcement Learning Modeling and Control of Non-linear Dynamics",
        "summary": "Controlling systems with complex, nonlinear dynamics poses a significant challenge, particularly in achieving efficient and robust control. In this paper, we propose a Dyna-Style Reinforcement Learning control framework that integrates Sparse Identification of Nonlinear Dynamics (SINDy) with Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning. SINDy is used to identify a data-driven model of the system, capturing its key dynamics without requiring an explicit physical model. This identified model is used to generate synthetic rollouts that are periodically injected into the reinforcement learning replay buffer during training on the real environment, enabling efficient policy learning with limited data available. By leveraging this hybrid approach, we mitigate the sample inefficiency of traditional model-free reinforcement learning methods while ensuring accurate control of nonlinear systems. To demonstrate the effectiveness of this framework, we apply it to a bi-rotor system as a case study, evaluating its performance in stabilization and trajectory tracking. The results show that our SINDy-TD3 approach achieves superior accuracy and robustness compared to direct reinforcement learning techniques, highlighting the potential of combining data-driven modeling with reinforcement learning for complex dynamical systems.",
        "url": "http://arxiv.org/abs/2512.21081v1",
        "published_date": "2025-12-24T09:56:28+00:00",
        "updated_date": "2025-12-24T09:56:28+00:00",
        "categories": [
            "eess.SY",
            "cs.LG"
        ],
        "authors": [
            "Karim Abdelsalam",
            "Zeyad Gamal",
            "Ayman El-Badawy"
        ],
        "tldr": "This paper proposes a Dyna-style Reinforcement Learning framework that combines SINDy for system identification with TD3 for control, improving sample efficiency for controlling nonlinear systems. It demonstrates its effectiveness on a bi-rotor system.",
        "tldr_zh": "本文提出了一种Dyna风格的强化学习框架，它结合了SINDy用于系统辨识和TD3用于控制，从而提高了控制非线性系统的样本效率。它在一个双旋翼系统中证明了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "控制具有复杂非线性动力学的系统构成了一项重大挑战，尤其是在实现高效且鲁棒的控制方面。在本文中，我们提出了一种Dyna风格的强化学习控制框架，该框架将非线性动力学的稀疏辨识（SINDy）与双延迟深度确定性策略梯度（TD3）强化学习相结合。SINDy用于辨识系统的数据驱动模型，无需显式物理模型即可捕获其关键动力学。该辨识出的模型用于生成合成轨迹，这些轨迹在真实环境中的训练期间被定期注入到强化学习的回放缓冲器中，从而能够在有限的可用数据下实现高效的策略学习。通过利用这种混合方法，我们缓解了传统无模型强化学习方法的样本低效问题，同时确保对非线性系统的精确控制。为了证明该框架的有效性，我们将其应用于一个双旋翼系统作为案例研究，评估其在稳定和轨迹跟踪方面的性能。结果表明，与直接强化学习技术相比，我们的SINDy-TD3方法实现了更高的精度和鲁棒性，突出了将数据驱动建模与强化学习相结合解决复杂动力学系统的潜力。"
    },
    {
        "title": "Robust and Efficient MuJoCo-based Model Predictive Control via Web of Affine Spaces Derivatives",
        "summary": "MuJoCo is a powerful and efficient physics simulator widely used in robotics. One common way it is applied in practice is through Model Predictive Control (MPC), which uses repeated rollouts of the simulator to optimize future actions and generate responsive control policies in real time. To make this process more accessible, the open source library MuJoCo MPC (MJPC) provides ready-to-use MPC algorithms and implementations built directly on top of the MuJoCo simulator. However, MJPC relies on finite differencing (FD) to compute derivatives through the underlying MuJoCo simulator, which is often a key bottleneck that can make it prohibitively costly for time-sensitive tasks, especially in high-DOF systems or complex scenes. In this paper, we introduce the use of Web of Affine Spaces (WASP) derivatives within MJPC as a drop-in replacement for FD. WASP is a recently developed approach for efficiently computing sequences of accurate derivative approximations. By reusing information from prior, related derivative calculations, WASP accelerates and stabilizes the computation of new derivatives, making it especially well suited for MPC's iterative, fine-grained updates over time. We evaluate WASP across a diverse suite of MJPC tasks spanning multiple robot embodiments. Our results suggest that WASP derivatives are particularly effective in MJPC: it integrates seamlessly across tasks, delivers consistently robust performance, and achieves up to a 2$\\mathsf{x}$ speedup compared to an FD backend when used with derivative-based planners, such as iLQG. In addition, WASP-based MPC outperforms MJPC's stochastic sampling-based planners on our evaluation tasks, offering both greater efficiency and reliability. To support adoption and future research, we release an open-source implementation of MJPC with WASP derivatives fully integrated.",
        "url": "http://arxiv.org/abs/2512.21109v1",
        "published_date": "2025-12-24T11:13:41+00:00",
        "updated_date": "2025-12-24T11:13:41+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Chen Liang",
            "Daniel Rakita"
        ],
        "tldr": "This paper introduces Web of Affine Spaces (WASP) derivatives as a drop-in replacement for finite differencing in the MuJoCo MPC (MJPC) library, demonstrating significant speedups and improved performance in various robotic tasks. They release an open-source implementation.",
        "tldr_zh": "该论文介绍了 Web of Affine Spaces (WASP) 导数，作为 MuJoCo MPC (MJPC) 库中有限差分的直接替代品，并在各种机器人任务中展示了显著的加速和性能提升。 他们发布了一个开源的实现。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "MuJoCo是一款强大且高效的物理引擎模拟器，广泛应用于机器人领域。一种常见的实践方法是通过模型预测控制（MPC）应用它，MPC通过重复模拟器的rollout来优化未来的动作，并实时生成反应灵敏的控制策略。为了使这一过程更易于使用，开源库MuJoCo MPC (MJPC) 提供了现成的MPC算法和直接构建在MuJoCo模拟器之上的实现。然而，MJPC依赖于有限差分 (FD) 来计算通过底层MuJoCo模拟器的导数，这通常是一个关键的瓶颈，可能使其对于时间敏感的任务来说成本过高，特别是在高自由度系统或复杂场景中。在本文中，我们引入了在MJPC中使用仿射空间网络（WASP）导数来作为FD的即插即用替代方案。WASP是一种最近开发的用于有效计算精确导数近似序列的方法。通过重用先前相关的导数计算中的信息，WASP加速并稳定了新导数的计算，使其特别适合MPC随时间进行的迭代、细粒度更新。我们在涵盖多种机器人实体的各种MJPC任务中评估了WASP。我们的结果表明，WASP导数在MJPC中特别有效：它可以无缝集成到各种任务中，提供始终如一的强大性能，并且在使用基于导数的规划器（例如iLQG）时，与FD后端相比，速度最高可提高2倍。此外，基于WASP的MPC在我们的评估任务中优于MJPC的基于随机采样的规划器，从而提供了更高的效率和可靠性。为了支持采用和未来研究，我们发布了完全集成了WASP导数的MJPC开源实现。"
    },
    {
        "title": "Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction",
        "summary": "Regulating grasping force to reduce slippage during dynamic object interaction remains a fundamental challenge in robotic manipulation, especially when objects are manipulated by multiple rolling contacts, have unknown properties (such as mass or surface conditions), and when external sensing is unreliable. In contrast, humans can quickly regulate grasping force by touch, even without visual cues. Inspired by this ability, we aim to enable robotic hands to rapidly explore objects and learn tactile-driven grasping force control under motion and limited sensing. We propose a physics-informed energy abstraction that models the object as a virtual energy container. The inconsistency between the fingers' applied power and the object's retained energy provides a physically grounded signal for inferring slip-aware stability. Building on this abstraction, we employ model-based learning and planning to efficiently model energy dynamics from tactile sensing and perform real-time grasping force optimization. Experiments in both simulation and hardware demonstrate that our method can learn grasping force control from scratch within minutes, effectively reduce slippage, and extend grasp duration across diverse motion-object pairs, all without relying on external sensing or prior object knowledge.",
        "url": "http://arxiv.org/abs/2512.21043v1",
        "published_date": "2025-12-24T08:19:25+00:00",
        "updated_date": "2025-12-24T08:19:25+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Cheng-Yu Kuo",
            "Hirofumi Shin",
            "Takamitsu Matsubara"
        ],
        "tldr": "This paper presents a novel tactile-based grasping force control method that uses a physics-informed energy abstraction to minimize slippage during dynamic object interaction, achieving rapid learning and robust performance even with limited sensing and unknown object properties.",
        "tldr_zh": "本文提出了一种新型的基于触觉的抓取力控制方法，该方法使用基于物理信息的能量抽象来最小化动态物体交互过程中的滑动。即使在有限的传感和未知的物体属性下，它也能实现快速学习和鲁棒的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在动态物体交互过程中调节抓取力以减少滑移仍然是机器人操作的一个根本挑战，尤其是在物体由多个滚动接触操控，具有未知属性（如质量或表面条件），以及外部传感不可靠的情况下。相比之下，即使没有视觉线索，人类也可以通过触觉快速调节抓取力。受到这种能力的启发，我们旨在使机器人手能够在运动和有限传感条件下快速探索物体并学习触觉驱动的抓取力控制。我们提出了一种物理信息能量抽象，将物体建模为虚拟能量容器。手指施加的功率与物体保持的能量之间的不一致性，为推断具有滑移意识的稳定性提供了一个基于物理的信号。在此抽象的基础上，我们采用基于模型的学习和规划，以有效建模来自触觉传感的能量动力学，并执行实时抓取力优化。在仿真和硬件中的实验表明，我们的方法可以在几分钟内从零开始学习抓取力控制，有效减少滑移，并在各种运动-物体对中延长抓取持续时间，所有这些都不依赖于外部传感或先验物体知识。"
    },
    {
        "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
        "summary": "Bayesian Reinforcement Learning (BRL) provides a framework for generalisation of Reinforcement Learning (RL) problems from its use of Bayesian task parameters in the transition and reward models. However, classical BRL methods assume known forms of transition and reward models, reducing their applicability in real-world problems. As a result, recent deep BRL methods have started to incorporate model learning, though the use of neural networks directly on the joint data and task parameters requires optimising the Evidence Lower Bound (ELBO). ELBOs are difficult to optimise and may result in indistinctive task parameters, hence compromised BRL policies. To this end, we introduce a novel deep BRL method, Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions (GLiBRL), that enables efficient and accurate learning of transition and reward models, with fully tractable marginal likelihood and Bayesian inference on task parameters and model noises. On challenging MetaWorld ML10/45 benchmarks, GLiBRL improves the success rate of one of the state-of-the-art deep BRL methods, VariBAD, by up to 2.7x. Comparing against representative or recent deep BRL / Meta-RL methods, such as MAML, RL2, SDVT, TrMRL and ECET, GLiBRL also demonstrates its low-variance and decent performance consistently.",
        "url": "http://arxiv.org/abs/2512.20974v1",
        "published_date": "2025-12-24T06:00:51+00:00",
        "updated_date": "2025-12-24T06:00:51+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Jingyang You",
            "Hanna Kurniawati"
        ],
        "tldr": "The paper introduces GLiBRL, a novel deep Bayesian RL method using Generalized Linear Models with learnable basis functions, achieving improved performance and stability in MetaWorld benchmarks compared to existing deep BRL methods by efficiently learning transition and reward models.",
        "tldr_zh": "本文介绍了一种名为GLiBRL的新型深度贝叶斯强化学习方法，它使用具有可学习基函数的广义线性模型，通过有效学习转移和奖励模型，在MetaWorld基准测试中实现了比现有深度BRL方法更高的性能和稳定性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "贝叶斯强化学习（BRL）通过在转移和奖励模型中使用贝叶斯任务参数，为强化学习（RL）问题的泛化提供了一个框架。 然而，传统的BRL方法假设已知转移和奖励模型的形式，降低了其在实际问题中的适用性。 因此，最近的深度BRL方法开始结合模型学习，但直接在联合数据和任务参数上使用神经网络需要优化证据下界（ELBO）。 ELBO很难优化，并且可能导致不清晰的任务参数，从而损害BRL策略。 为此，我们提出了一种新颖的深度BRL方法，即基于可学习基函数的深度贝叶斯RL中的广义线性模型（GLiBRL），该方法能够高效且准确地学习转移和奖励模型，并对任务参数和模型噪声进行完全可处理的边际似然和贝叶斯推断。 在具有挑战性的MetaWorld ML10/45基准测试中，GLiBRL将最先进的深度BRL方法之一VariBAD的成功率提高了高达2.7倍。 与具有代表性或最近的深度BRL / Meta-RL方法（如 MAML、RL2、SDVT、TrMRL 和 ECET）相比，GLiBRL也表现出其低方差和始终如一的良好性能。"
    },
    {
        "title": "YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion",
        "summary": "This paper introduces the YCB-Handovers dataset, capturing motion data of 2771 human-human handovers with varying object weights. The dataset aims to bridge a gap in human-robot collaboration research, providing insights into the impact of object weight in human handovers and readiness cues for intuitive robotic motion planning. The underlying dataset for object recognition and tracking is the YCB (Yale-CMU-Berkeley) dataset, which is an established standard dataset used in algorithms for robotic manipulation, including grasping and carrying objects. The YCB-Handovers dataset incorporates human motion patterns in handovers, making it applicable for data-driven, human-inspired models aimed at weight-sensitive motion planning and adaptive robotic behaviors. This dataset covers an extensive range of weights, allowing for a more robust study of handover behavior and weight variation. Some objects also require careful handovers, highlighting contrasts with standard handovers. We also provide a detailed analysis of the object's weight impact on the human reaching motion in these handovers.",
        "url": "http://arxiv.org/abs/2512.20847v1",
        "published_date": "2025-12-23T23:50:55+00:00",
        "updated_date": "2025-12-23T23:50:55+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Parag Khanna",
            "Karen Jane Dsouza",
            "Chunyu Wang",
            "Mårten Björkman",
            "Christian Smith"
        ],
        "tldr": "The paper introduces YCB-Handovers, a new dataset of human-human handovers annotated with object weights, designed to inform weight-sensitive robotic motion planning and improve human-robot collaboration.",
        "tldr_zh": "该论文介绍了一个名为 YCB-Handovers 的新数据集，该数据集包含带有对象重量标注的人与人之间的物体交接数据，旨在为重量敏感的机器人运动规划提供信息并改善人机协作。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文介绍YCB-Handovers数据集，该数据集捕获了2771次人与人之间交接物体的运动数据，并涵盖了不同的物体重量。该数据集旨在弥合人机协作研究中的差距，提供关于物体重量对人类物体交接的影响以及直观机器人运动规划的准备提示的见解。用于物体识别和跟踪的底层数据集是YCB (耶鲁-卡内基梅隆-伯克利)数据集，它是在机器人操作算法（包括抓取和搬运物体）中使用的既定标准数据集。YCB-Handovers数据集融合了物体交接过程中的人类运动模式，使其适用于以数据驱动、人类启发的模型，旨在实现重量敏感的运动规划和自适应的机器人行为。该数据集涵盖了广泛的重量范围，从而可以更稳健地研究交接行为和重量变化。一些物体也需要谨慎的交接，突出了与标准交接的对比。我们还提供了关于物体重量对这些交接过程中人类伸手运动影响的详细分析。"
    },
    {
        "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication",
        "summary": "Multi-agent decision-making under uncertainty is fundamental for effective and safe autonomous operation. In many real-world scenarios, each agent maintains its own belief over the environment and must plan actions accordingly. However, most existing approaches assume that all agents have identical beliefs at planning time, implying these beliefs are conditioned on the same data. Such an assumption is often impractical due to limited communication. In reality, agents frequently operate with inconsistent beliefs, which can lead to poor coordination and suboptimal, potentially unsafe, performance. In this paper, we address this critical challenge by introducing a novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies. Our approach provides probabilistic guarantees for both action consistency and performance with respect to open-loop multi-agent POMDP (which assumes all data is always communicated), and selectively triggers communication only when needed. Furthermore, we address another key aspect of whether, given a chosen joint action, the agents should share data to improve expected performance in inference. Simulation results show our approach outperforms state-of-the-art algorithms.",
        "url": "http://arxiv.org/abs/2512.20778v1",
        "published_date": "2025-12-23T21:25:53+00:00",
        "updated_date": "2025-12-23T21:25:53+00:00",
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Moshe Rafaeli Shimron",
            "Vadim Indelman"
        ],
        "tldr": "This paper introduces a new framework for decentralized partially observable Markov decision processes (Dec-POMDPs) that tackles the problem of inconsistent beliefs among agents with limited communication, providing performance and action consistency guarantees.",
        "tldr_zh": "本文提出了一种新的去中心化部分可观察马尔可夫决策过程（Dec-POMDP）框架，旨在解决智能体间因通信受限而导致的不一致信念问题，并提供性能和动作一致性保证。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "在不确定性下的多智能体决策是实现有效和安全自主运行的基础。在许多现实场景中，每个智能体都维护着自己对环境的信念，并且必须据此规划行动。然而，大多数现有方法假设所有智能体在规划时拥有相同的信念，这意味着这些信念是以相同的数据为条件的。由于通信的限制，这种假设通常是不切实际的。在现实中，智能体经常在信念不一致的情况下运行，这可能导致不良的协调以及次优的、潜在不安全的性能。在本文中，我们通过引入一种新颖的去中心化框架来解决这一 critical challenge，该框架用于在信念不一致的情况下进行最优的联合行动选择。我们的方法为行动一致性和相对于开放循环多智能体 POMDP（假设所有数据始终被通信）的性能提供了概率保证，并且仅在需要时选择性地触发通信。此外，我们还解决了另一个关键方面，即在给定选择的联合行动后，智能体是否应该共享数据以提高推理中的预期性能。仿真结果表明，我们的方法优于目前最先进的算法。"
    },
    {
        "title": "A General Purpose Method for Robotic Interception of Non-Cooperative Dynamic Targets",
        "summary": "This paper presents a general purpose framework for autonomous, vision-based interception of dynamic, non-cooperative targets, validated across three distinct mobility platforms: an unmanned aerial vehicle (UAV), a four-wheeled ground rover, and an air-thruster spacecraft testbed. The approach relies solely on a monocular camera with fiducials for target tracking and operates entirely in the local observer frame without the need for global information. The core contribution of this work is a streamlined and general approach to autonomous interception that can be adapted across robots with varying dynamics, as well as our comprehensive study of the robot interception problem across heterogenous mobility systems under limited observability and no global localization. Our method integrates (1) an Extended Kalman Filter for relative pose estimation amid intermittent measurements, (2) a history-conditioned motion predictor for dynamic target trajectory propagation, and (3) a receding-horizon planner solving a constrained convex program in real time to ensure time-efficient and kinematically feasible interception paths. Our operating regime assumes that observability is restricted by partial fields of view, sensor dropouts, and target occlusions. Experiments are performed in these conditions and include autonomous UAV landing on dynamic targets, rover rendezvous and leader-follower tasks, and spacecraft proximity operations. Results from simulated and physical experiments demonstrate robust performance with low interception errors (both during station-keeping and upon scenario completion), high success rates under deterministic and stochastic target motion profiles, and real-time execution on embedded processors such as the Jetson Orin, VOXL2, and Raspberry Pi 5. These results highlight the framework's generalizability, robustness, and computational efficiency.",
        "url": "http://arxiv.org/abs/2512.20769v1",
        "published_date": "2025-12-23T21:14:03+00:00",
        "updated_date": "2025-12-23T21:14:03+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Tanmay P. Patel",
            "Erica L. Tevere",
            "Erik H. Kramer",
            "Rudranarayan M. Mukherjee"
        ],
        "tldr": "This paper presents a general-purpose, vision-based framework for autonomous interception of non-cooperative dynamic targets, validated on diverse robotic platforms using a monocular camera and a combination of EKF, motion prediction, and receding-horizon planning.",
        "tldr_zh": "本文提出了一种通用的、基于视觉的自主拦截非合作动态目标框架，该框架通过单目摄像头，结合扩展卡尔曼滤波、运动预测和后退视野规划，在多种机器人平台上进行了验证。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文提出了一种通用的自主、基于视觉的动态非合作目标拦截框架，并在三种不同的移动平台上进行了验证：无人机(UAV)、四轮地面漫游车和气动推进航天器测试平台。该方法仅依赖于带标志物的单目相机进行目标跟踪，并在局部观察者坐标系中完全运行，无需全局信息。这项工作的核心贡献是一种精简且通用的自主拦截方法，该方法可以适用于具有不同动力学的机器人，以及我们在有限可观测性和无全局定位条件下，对异构移动系统中的机器人拦截问题进行的全面研究。我们的方法集成了 (1) 扩展卡尔曼滤波器，用于在间歇性测量中进行相对位姿估计，(2) 历史条件运动预测器，用于动态目标轨迹推算，以及 (3) 滚动时域规划器，实时求解约束凸规划，以确保高效且运动学上可行的拦截路径。我们的运行状态假设可观测性受到部分视场、传感器丢失和目标遮挡的限制。在这些条件下进行了实验，包括无人机自主降落在动态目标上、漫游车会合和领导者-跟随者任务，以及航天器近距离操作。模拟和物理实验的结果表明，该方法具有鲁棒的性能，拦截误差较低（无论是在定点保持期间还是在场景完成时），在确定性和随机目标运动曲线下的成功率较高，并且可以在Jetson Orin、VOXL2和Raspberry Pi 5等嵌入式处理器上实时执行。这些结果突出了该框架的通用性、鲁棒性和计算效率。"
    },
    {
        "title": "Anytime Metaheuristic Framework for Global Route Optimization in Expected-Time Mobile Search",
        "summary": "Expected-time mobile search (ETS) is a fundamental robotics task where a mobile sensor navigates an environment to minimize the expected time required to locate a hidden object. Global route optimization for ETS in static 2D continuous environments remains largely underexplored due to the intractability of objective evaluation, stemming from the continuous nature of the environment and the interplay of motion and visibility constraints. Prior work has addressed this through partial discretization, leading to discrete-sensing formulations tackled via utility-greedy heuristics. Others have taken an indirect approach by heuristically approximating the objective using minimum latency problems on fixed graphs, enabling global route optimization via efficient metaheuristics. This paper builds on and significantly extends the latter by introducing Milaps (Minimum latency problems), a model-based solution framework for ETS. Milaps integrates novel auxiliary objectives and adapts a recent anytime metaheuristic for the traveling deliveryman problem, chosen for its strong performance under tight runtime constraints. Evaluations on a novel large-scale dataset demonstrate superior trade-offs between solution quality and runtime compared to state-of-the-art baselines. The best-performing strategy rapidly generates a preliminary solution, assigns static weights to sensing configurations, and optimizes global costs metaheuristically. Additionally, a qualitative study highlights the framework's flexibility across diverse scenarios.",
        "url": "http://arxiv.org/abs/2512.20711v1",
        "published_date": "2025-12-23T19:19:27+00:00",
        "updated_date": "2025-12-23T19:19:27+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Jan Mikula",
            "Miroslav Kulich"
        ],
        "tldr": "This paper introduces Milaps, a novel anytime metaheuristic framework for global route optimization in expected-time mobile search (ETS), demonstrating superior performance in trading off solution quality and runtime compared to state-of-the-art methods on a new large-scale dataset.",
        "tldr_zh": "该论文提出了Milaps，一种用于预期时间移动搜索（ETS）中全局路径优化的新型随时元启发式框架。实验表明，与最先进的方法相比，Milaps在解决方案质量和运行时间之间取得了更好的平衡，并在新的大规模数据集上进行了验证。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "预期时间移动搜索 (ETS) 是一项基本的机器人任务，其中移动传感器在环境中导航，以最小化定位隐藏物体所需的预期时间。由于目标评估的棘手性，静态二维连续环境中 ETS 的全局路径优化在很大程度上仍未被探索，这种棘手性源于环境的连续性以及运动和可见性约束的相互作用。先前的工作通过部分离散化来解决这个问题，从而产生了通过效用贪婪启发式方法解决的离散感知公式。其他方法则采取间接途径，通过使用固定图上的最小延迟问题来启发式地逼近目标，从而可以通过有效的元启发式方法实现全局路径优化。本文在前者的基础上进行了扩展，并引入了 Milaps（最小延迟问题），这是一种基于模型的 ETS 解决方案框架。 Milaps 集成了新颖的辅助目标，并针对旅行投递员问题改进了一种最新的随时可用的元启发式算法，选择该算法是因为其在严格的运行时约束下具有强大的性能。在一个新的大规模数据集上的评估表明，与最先进的基线相比，Milaps 在解决方案质量和运行时之间取得了更好的权衡。性能最佳的策略能够快速生成初步解决方案，为感知配置分配静态权重，并通过元启发式方法优化全局成本。此外，一项定性研究突出了该框架在各种场景中的灵活性。"
    },
    {
        "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control",
        "summary": "Traffic simulation is important for transportation optimization and policy making. While existing simulators such as SUMO and MATSim offer fully-featured platforms and utilities, users without too much knowledge about these platforms often face significant challenges when conducting experiments from scratch and applying them to their daily work. To solve this challenge, we propose TrafficSimAgent, an LLM-based agent framework that serves as an expert in experiment design and decision optimization for general-purpose traffic simulation tasks. The framework facilitates execution through cross-level collaboration among expert agents: high-level expert agents comprehend natural language instructions with high flexibility, plan the overall experiment workflow, and invoke corresponding MCP-compatible tools on demand; meanwhile, low-level expert agents select optimal action plans for fundamental elements based on real-time traffic conditions. Extensive experiments across multiple scenarios show that TrafficSimAgent effectively executes simulations under various conditions and consistently produces reasonable outcomes even when user instructions are ambiguous. Besides, the carefully designed expert-level autonomous decision-driven optimization in TrafficSimAgent yields superior performance when compared with other systems and SOTA LLM based methods.",
        "url": "http://arxiv.org/abs/2512.20996v1",
        "published_date": "2025-12-24T06:48:04+00:00",
        "updated_date": "2025-12-24T06:48:04+00:00",
        "categories": [
            "cs.AI"
        ],
        "authors": [
            "Yuwei Du",
            "Jun Zhang",
            "Jie Feng",
            "Zhicheng Liu",
            "Jian Yuan",
            "Yong Li"
        ],
        "tldr": "The paper introduces TrafficSimAgent, an LLM-based agent framework that simplifies traffic simulation experiment design and optimization by using hierarchical expert agents to interpret instructions, plan workflows, and optimize actions based on real-time traffic conditions, achieving superior performance compared to other systems.",
        "tldr_zh": "该论文介绍了TrafficSimAgent，一个基于LLM的智能体框架，通过使用分层专家智能体解释指令、规划工作流程并根据实时交通状况优化行动，简化了交通仿真实验设计和优化，与其它系统相比，实现了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "交通仿真对于交通优化和政策制定至关重要。虽然现有的仿真器，如SUMO和MATSim，提供了功能全面的平台和工具，但缺乏对这些平台深入了解的用户在从头开始进行实验并将其应用于日常工作中时，常常面临严峻的挑战。为了解决这一挑战，我们提出了TrafficSimAgent，一个基于LLM的智能体框架，作为实验设计和通用交通仿真任务决策优化的专家。该框架通过专家智能体之间的跨层协作来促进执行：高层专家智能体具备高度的灵活性，能够理解自然语言指令，规划整体实验流程，并根据需求调用兼容MCP（模型控制协议）的工具；同时，低层专家智能体基于实时交通状况，为基本元素选择最优行动方案。在多个场景下进行的大量实验表明，TrafficSimAgent能够有效地执行各种条件下的仿真，并且即使在用户指令模糊不清的情况下，也能始终产生合理的结果。此外， TrafficSimAgent中精心设计的专家级自主决策驱动优化，与其它系统和最先进的基于LLM的方法相比，产生了优异的性能。"
    },
    {
        "title": "Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction",
        "summary": "Due to their inherent flexibility and autonomous operation, unmanned aerial vehicles (UAVs) have been widely used in Internet of Medical Things (IoMT) to provide real-time biomedical edge computing service for wireless body area network (WBAN) users. In this paper, considering the time-varying task criticality characteristics of diverse WBAN users and the dual mobility between WBAN users and UAV, we investigate the dynamic task offloading and UAV flight trajectory optimization problem to minimize the weighted average task completion time of all the WBAN users, under the constraint of UAV energy consumption. To tackle the problem, an embodied AI-enhanced IoMT edge computing framework is established. Specifically, we propose a novel hierarchical multi-scale Transformer-based user trajectory prediction model based on the users' historical trajectory traces captured by the embodied AI agent (i.e., UAV). Afterwards, a prediction-enhanced deep reinforcement learning (DRL) algorithm that integrates predicted users' mobility information is designed for intelligently optimizing UAV flight trajectory and task offloading decisions. Real-word movement traces and simulation results demonstrate the superiority of the proposed methods in comparison with the existing benchmarks.",
        "url": "http://arxiv.org/abs/2512.20902v1",
        "published_date": "2025-12-24T03:06:37+00:00",
        "updated_date": "2025-12-24T03:06:37+00:00",
        "categories": [
            "cs.NI",
            "cs.AI"
        ],
        "authors": [
            "Siqi Mu",
            "Shuo Wen",
            "Yang Lu",
            "Ruihong Jiang",
            "Bo Ai"
        ],
        "tldr": "This paper proposes an embodied AI-enhanced IoMT edge computing framework using UAVs for task offloading and trajectory optimization, incorporating a novel Transformer-based user trajectory prediction model and prediction-enhanced DRL algorithm to minimize task completion time for WBAN users.",
        "tldr_zh": "本文提出了一种基于具身人工智能增强的IoMT边缘计算框架，利用无人机进行任务卸载和轨迹优化。该框架结合了一种新颖的基于Transformer的用户轨迹预测模型和预测增强型DRL算法，旨在最小化WBAN用户的任务完成时间。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "鉴于其固有的灵活性和自主运行能力，无人机（UAV）已被广泛应用于医疗物联网（IoMT），为无线体域网（WBAN）用户提供实时的生物医学边缘计算服务。本文针对不同WBAN用户随时间变化的任務关键性特征以及WBAN用户和UAV之间的双重移动性，研究了动态任务卸载和UAV飞行轨迹优化问题，旨在最小化所有WBAN用户的加权平均任务完成时间，同时满足UAV的能量消耗约束。为了解决该问题，我们建立了一个具身人工智能增强的IoMT边缘计算框架。具体而言，我们提出了一种基于用户历史轨迹数据的分层多尺度Transformer用户轨迹预测模型，该数据由具身人工智能代理（即UAV）捕获。然后，设计了一种预测增强型深度强化学习（DRL）算法，该算法集成了预测到的用户移动性信息，用于智能地优化UAV飞行轨迹和任务卸载决策。真实世界的运动轨迹和仿真结果表明，与现有基准相比，所提出的方法具有优越性。"
    },
    {
        "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions",
        "summary": "Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2512.20831v1",
        "published_date": "2025-12-23T23:12:53+00:00",
        "updated_date": "2025-12-23T23:12:53+00:00",
        "categories": [
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Rashmeet Kaur Nayyar",
            "Naman Shah",
            "Siddharth Srivastava"
        ],
        "tldr": "This paper introduces a novel reinforcement learning approach for parameterized action spaces by autonomously learning and refining state and action abstractions online, achieving higher sample efficiency in long-horizon, sparse-reward settings compared to state-of-the-art baselines.",
        "tldr_zh": "本文提出了一种新的强化学习方法，用于参数化动作空间，通过在线自主学习和细化状态和动作抽象，与最先进的基线相比，在长时程、稀疏奖励环境中实现了更高的样本效率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "真实世界的序贯决策通常涉及参数化动作空间，这需要既选择离散动作，又决定控制动作执行方式的连续动作参数。现有的方法在这种情景下表现出严重的局限性——规划方法需要手工设计的动作模型，而标准的强化学习(RL)算法要么为离散动作设计，要么为连续动作设计，但不能同时处理两者，而且少数能够处理参数化动作的RL方法通常依赖于特定领域的工程设计，并且无法利用这些空间的潜在结构。本文通过使智能体能够在线自主学习状态和动作抽象，将RL算法的范围扩展到具有参数化动作的、长程、稀疏奖励的环境。我们引入了在学习过程中逐步细化这些抽象的算法，在状态-动作空间的关键区域增加细粒度细节，因为更高的分辨率可以提高性能。在几个连续状态、参数化动作的领域中，我们基于抽象驱动的方法使得TD($λ$)能够比最先进的基线方法实现显著更高的样本效率。"
    },
    {
        "title": "X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis",
        "summary": "The growing complexity of power system operations has created an urgent need for intelligent, automated tools to support reliable and efficient grid management. Conventional analysis tools often require significant domain expertise and manual effort, which limits their accessibility and adaptability. To address these challenges, this paper presents X-GridAgent, a novel large language model (LLM)-powered agentic AI system designed to automate complex power system analysis through natural language queries. The system integrates domain-specific tools and specialized databases under a three-layer hierarchical architecture comprising planning, coordination, and action layers. This architecture offers high flexibility and adaptability to previously unseen tasks, while providing a modular and extensible framework that can be readily expanded to incorporate new tools, data sources, or analytical capabilities. To further enhance performance, we introduce two novel algorithms: (1) LLM-driven prompt refinement with human feedback, and (2) schema-adaptive hybrid retrieval-augmented generation (RAG) for accurate information retrieval from large-scale structured grid datasets. Experimental evaluations across a variety of user queries and power grid cases demonstrate the effectiveness and reliability of X-GridAgent in automating interpretable and rigorous power system analysis.",
        "url": "http://arxiv.org/abs/2512.20789v1",
        "published_date": "2025-12-23T21:36:20+00:00",
        "updated_date": "2025-12-23T21:36:20+00:00",
        "categories": [
            "eess.SY",
            "cs.AI"
        ],
        "authors": [
            "Yihan",
            "Wen",
            "Xin Chen"
        ],
        "tldr": "The paper introduces X-GridAgent, an LLM-powered agentic AI system for automating power system analysis using natural language queries, and incorporating novel algorithms for prompt refinement and schema-adaptive RAG.",
        "tldr_zh": "该论文介绍了X-GridAgent，一个基于LLM的智能AI系统，通过自然语言查询自动进行电力系统分析，并融合了用于提示改进和模式自适应RAG的新算法。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "电力系统运行日益复杂，迫切需要智能化的自动化工具来支持可靠高效的电网管理。传统的分析工具通常需要大量的领域专业知识和人工干预，这限制了它们的可访问性和适应性。为了应对这些挑战，本文提出了一种新型的基于大型语言模型（LLM）的智能体AI系统X-GridAgent，旨在通过自然语言查询来自动化复杂的电力系统分析。该系统在规划层、协调层和执行层所构成的三层分级架构下，集成了领域特定的工具和专业数据库。这种架构为先前未见过的任务提供了高度的灵活性和适应性，同时提供了一个模块化和可扩展的框架，可以方便地扩展以集成新的工具、数据源或分析功能。为了进一步提高性能，我们引入了两种新颖的算法：（1）基于LLM并结合人工反馈的提示精炼；（2）用于从大规模结构化电网数据集中进行精确信息检索的模式自适应混合检索增强生成（RAG）。在各种用户查询和电力网案例中的实验评估表明，X-GridAgent在自动化可解释且严谨的电力系统分析方面是有效且可靠的。"
    },
    {
        "title": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
        "summary": "As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.",
        "url": "http://arxiv.org/abs/2512.20589v1",
        "published_date": "2025-12-23T18:36:07+00:00",
        "updated_date": "2025-12-23T18:36:07+00:00",
        "categories": [
            "cs.CY",
            "cs.AI",
            "eess.SY",
            "math.OC"
        ],
        "authors": [
            "İbrahim Oğuz Çetinkaya",
            "Sajad Khodadadian",
            "Taylan G. Topçu"
        ],
        "tldr": "This paper explores using Reinforcement Learning (RL) within a Digital Engineering (DE) framework for intelligent mission coordination, specifically for adaptive task allocation in aerial firefighting, demonstrating improved and more consistent mission performance compared to baselines.",
        "tldr_zh": "本文探讨了在数字工程（DE）框架内使用强化学习（RL）进行智能任务协调，特别是用于空中消防中的自适应任务分配，结果表明与基线相比，任务性能得到改善且更加稳定。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "随着系统工程(SE)的目标从单一系统的设计和运行演变为复杂的系统之系统(SoS)，任务工程(ME)学科应运而生，并日益被系统工程界接受为一种新的思维方式。此外，任务环境是不确定的、动态的，任务结果直接取决于任务资产如何与该环境交互。这证明静态架构的脆弱性并要求采用分析上严谨的任务工程方法。为此，本文提出一种智能任务协调方法，该方法将数字化任务模型与强化学习(RL)相结合，专门解决自适应任务分配和重配置的需求。更具体地说，我们利用一个基于数字工程(DE)的基础设施，该设施由高保真度的数字化任务模型和基于Agent的仿真组成；然后，我们将任务战术管理问题公式化为一个马尔科夫决策过程(MDP)，并采用通过近端策略优化训练的强化学习Agent。通过将仿真作为沙盒，我们将系统状态映射到动作，并根据已实现的任务结果改进策略。通过一项空中消防案例研究，我们展示了基于强化学习的智能任务协调器的效用。我们的研究结果表明，基于强化学习的智能任务协调器不仅超越了基准性能，而且显著降低了任务性能的可变性。因此，这项研究作为一个概念验证，表明基于数字工程的任务仿真与先进的分析工具相结合，为改进任务工程实践提供了一个与任务无关的框架；从任务优先的角度来看，未来可以将其扩展到更复杂的机队设计和选择问题中。"
    },
    {
        "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
        "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",
        "url": "http://arxiv.org/abs/2512.20586v1",
        "published_date": "2025-12-23T18:32:17+00:00",
        "updated_date": "2025-12-23T18:32:17+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.HC"
        ],
        "authors": [
            "Humza Nusrat",
            "Luke Francisco",
            "Bing Luo",
            "Hassan Bagher-Ebadian",
            "Joshua Kim",
            "Karen Chin-Snyder",
            "Salim Siddiqui",
            "Mira Shah",
            "Eric Mellon",
            "Mohammad Ghassemi",
            "Anthony Doemer",
            "Benjamin Movsas",
            "Kundan Thind"
        ],
        "tldr": "This paper introduces SAGE, an LLM-based agent for automated SRS treatment planning, demonstrating comparable or improved dosimetry compared to human planners and showcasing reasoning-based auditability.",
        "tldr_zh": "该论文介绍了一种基于LLM的自动化SRS治疗计划智能体SAGE，它展示了与人类计划者相比相当或改进的剂量学，并展示了基于推理的可审计性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "立体定向放射外科（SRS）要求在关键结构周围进行精确的剂量塑形，然而，由于缺乏透明度，黑盒AI系统在临床应用中受到限制。我们回顾性地测试了思维链推理是否能够改善对接受18 Gy单次分割SRS治疗的41例脑转移患者的智能体式规划。我们开发了SAGE（用于生成剂量专业知识的安全智能体），这是一个基于LLM的规划智能体，用于自动SRS治疗计划。两种变体为每个病例生成计划：一种使用非推理模型，一种使用推理模型。在主要终点（PTV覆盖率、最大剂量、适形指数、梯度指数；所有p > 0.21）上，推理变体显示出与人类计划员相当的计划剂量学，同时降低了耳蜗剂量，使其低于人类基线（p = 0.022）。当提示改进适形性时，推理模型表现出系统的规划行为，包括前瞻性约束验证（457个实例）和权衡考虑（609个实例），而标准模型则没有表现出这些审议过程（分别为0个和7个实例）。内容分析显示，约束验证和因果解释集中在推理智能体中。优化轨迹可作为可审计的日志，为透明的自动化规划提供了一条途径。"
    },
    {
        "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
        "summary": "Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.",
        "url": "http://arxiv.org/abs/2512.20576v1",
        "published_date": "2025-12-23T18:20:06+00:00",
        "updated_date": "2025-12-23T18:20:06+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "math.OC"
        ],
        "authors": [
            "Debabrota Basu",
            "Udvas Das",
            "Brahim Driss",
            "Uddalak Mukherjee"
        ],
        "tldr": "This paper introduces Performative Policy Gradient (PePG), a novel policy gradient algorithm designed for performative reinforcement learning, proving its convergence to performatively optimal policies and demonstrating its superiority over existing methods in performative RL environments.",
        "tldr_zh": "本文介绍了Performative Policy Gradient (PePG)，一种为表现性强化学习设计的新型策略梯度算法，证明了其收敛到表现性最优策略，并证明了其在表现性强化学习环境中优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "机器学习算法在部署后往往会影响它们所作用的环境，从而改变标准强化学习(RL)方法所忽略的底层动态。虽然在可执行性设置中设计最优算法最近已在监督学习中得到研究，但RL对应部分仍未得到充分探索。在本文中，我们证明了RL中性能差异引理和策略梯度定理的可执行性对应版本，并进一步引入了可执行性策略梯度算法(PePG)。PePG是第一个旨在解决RL可执行性的策略梯度算法。在softmax参数化下，无论是否使用熵正则化，我们证明PePG收敛到可执行性最优策略，即在自身引起的分布漂移下仍保持最优的策略。因此，PePG显著扩展了旨在实现可执行性稳定而非最优性的先前可执行性RL工作。此外，我们在标准可执行性RL环境上的经验分析验证了PePG优于标准策略梯度算法和现有旨在实现稳定性的可执行性RL算法。"
    },
    {
        "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
        "summary": "Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.",
        "url": "http://arxiv.org/abs/2512.21268v1",
        "published_date": "2025-12-24T16:24:18+00:00",
        "updated_date": "2025-12-24T16:24:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiqi Li",
            "Zehao Zhang",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "tldr": "This paper introduces Attention-Conditional Diffusion (ACD), a novel approach for directly controlling video diffusion models by aligning attention maps with external control signals, specifically sparse 3D-aware object layouts, achieving superior controllability in video generation.",
        "tldr_zh": "该论文介绍了注意力条件扩散（ACD），一种通过将注意力图与外部控制信号（特别是稀疏的3D感知对象布局）对齐来直接控制视频扩散模型的新方法，从而在视频生成中实现卓越的可控性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "可控性是视频合成的一项基本要求，其中与条件信号的精确对齐至关重要。现有的无分类器引导方法通常通过建模数据和条件的联合分布间接地实现调节（conditioning），这往往导致对指定条件下的可控性有限。基于分类器的引导通过外部分类器来强制施加条件，但模型可能会利用这种机制来提高分类器得分，而没有真正满足预期的条件，从而导致对抗性伪影和有限的有效可控性。在本文中，我们提出了一种名为注意力条件扩散（ACD）的新型框架，用于通过注意力监督在视频扩散模型中进行直接的条件控制。通过使模型的注意力图与外部控制信号对齐，ACD实现了更好的可控性。为了支持这一点，我们引入了一种稀疏的3D感知对象布局作为一种高效的调节信号，以及一个专用的布局控制网络（Layout ControlNet）和一个用于可扩展布局集成的自动化标注流程。在基准视频生成数据集上进行的大量实验表明，ACD在保持时间连贯性和视觉保真度的同时，实现了与调节输入更好的对齐，从而建立了一种有效的条件视频合成范式。"
    },
    {
        "title": "Towards Arbitrary Motion Completing via Hierarchical Continuous Representation",
        "summary": "Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.",
        "url": "http://arxiv.org/abs/2512.21183v1",
        "published_date": "2025-12-24T14:07:04+00:00",
        "updated_date": "2025-12-24T14:07:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenghao Xu",
            "Guangtao Lyu",
            "Qi Liu",
            "Jiexi Yan",
            "Muli Yang",
            "Cheng Deng"
        ],
        "tldr": "This paper introduces a novel hierarchical implicit representation framework (NAME) for human motion sequences, enabling interpolation, inbetweening, and extrapolation at arbitrary frame rates using a parametric activation function within an INR-based architecture.",
        "tldr_zh": "本文介绍了一种新颖的用于人体运动序列的分层隐式表示框架 (NAME)，它能够使用 INR 架构中的参数激活函数，以任意帧速率进行插值、补间和外推。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "物理运动本质上是连续的，更高的相机帧率通常有助于提高流畅性和时间一致性。我们首次探索了人体运动序列的连续表示，该表示具有以任意帧率插值、帧间生成甚至外推任何输入运动序列的能力。为了实现这一点，我们提出了一种新颖的参数激活诱导的分层隐式表示框架，称为NAME，它基于隐式神经表示（INRs）。我们的方法引入了一种分层时间编码机制，可以从多个时间尺度上提取运动序列的特征，从而能够有效地捕捉复杂的时序模式。此外，我们将由傅里叶变换驱动的自定义参数激活函数集成到基于MLP的解码器中，以增强连续表示的表达能力。这种参数化形式显著增强了模型以高精度表示复杂运动行为的能力。在多个基准数据集上的广泛评估证明了我们提出的方法的有效性和鲁棒性。"
    },
    {
        "title": "XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping",
        "summary": "Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.",
        "url": "http://arxiv.org/abs/2512.20976v1",
        "published_date": "2025-12-24T06:08:50+00:00",
        "updated_date": "2025-12-24T06:08:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeqing Song",
            "Zhongmiao Yan",
            "Junyuan Deng",
            "Songpengcheng Xia",
            "Xiang Mu",
            "Jingyi Xu",
            "Qi Wu",
            "Ling Pei"
        ],
        "tldr": "The paper introduces XGrid-Mapping, a hybrid explicit-implicit grid submap framework for efficient and accurate incremental neural LiDAR mapping, using distillation-based overlap alignment and a dynamic removal module for enhanced robustness.",
        "tldr_zh": "本文介绍了一种用于高效准确的增量神经激光雷达测绘的混合显式-隐式网格子图框架XGrid-Mapping，使用基于蒸馏的重叠对齐和动态移除模块来增强鲁棒性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "大规模增量式建图对于开发稳健可靠的自主系统至关重要，因为它支持利用顺序输入进行环境的增量式理解，进而服务于导航和决策。激光雷达因其精度和鲁棒性而被广泛用于此目的。近年来，神经激光雷达建图展现出令人印象深刻的性能；然而，大多数方法依赖于密集的隐式表示，未能充分利用几何结构，而现有的体素引导方法难以实现实时性能。为了解决这些挑战，我们提出了XGrid-Mapping，一种混合网格框架，它联合利用显式和隐式表示来实现高效的神经激光雷达建图。具体来说，该策略将提供几何先验和结构引导的稀疏网格与丰富场景表示的隐式密集网格相结合。通过将VDB结构与基于子图的组织方式相结合，该框架降低了计算负担，并实现了大规模的有效增量式建图。为了减轻子图之间的不连续性，我们引入了一种基于知识蒸馏的重叠区域对齐策略，其中先前的子图监督后续的子图，以确保重叠区域的一致性。为了进一步提高鲁棒性和采样效率，我们集成了一个动态移除模块。大量实验表明，我们的方法提供了卓越的建图质量，同时克服了体素引导方法的效率限制，从而优于现有的最先进的建图方法。"
    },
    {
        "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
        "summary": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.",
        "url": "http://arxiv.org/abs/2512.20975v1",
        "published_date": "2025-12-24T06:04:58+00:00",
        "updated_date": "2025-12-24T06:04:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujin Noh",
            "Inho Jake Park",
            "Chigon Hwang"
        ],
        "tldr": "The paper proposes SPOT, a map-guided LLM agent for unsupervised, continuous vehicle tracking across multi-CCTV environments, even in blind spots, by leveraging spatial information and beam search in the CARLA simulator.",
        "tldr_zh": "该论文提出了SPOT，一种地图引导的 LLM 代理，用于在多摄像头环境中进行无监督的连续车辆跟踪，即使在盲区也能通过利用空间信息和在CARLA模拟器中进行波束搜索来实现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "基于闭路电视的车辆跟踪系统在跨多个摄像头环境中连续连接同一车辆的轨迹时面临结构性限制。 特别是，由于闭路电视之间的间隔和有限的视野（FOV）而出现的盲点，会导致对象ID切换和轨迹丢失，从而降低实时路径预测的可靠性。本文提出了一种SPOT（基于轨迹的空间预测）方法，该方法是一个地图引导的LLM代理，即使在多闭路电视环境的盲点中也能跟踪车辆，而无需事先训练。所提出的方法将道路结构（航路点）和闭路电视放置信息表示为基于二维空间坐标的文档，并通过分块技术对其进行组织，以实现实时查询和推理。 此外，它利用在闭路电视图像中观察到的物体的相对位置和FOV信息，将车辆的位置转换为实际的世界坐标系。通过将地图空间信息与车辆的行驶方向、速度和驾驶模式相结合，在交叉路口级别执行波束搜索，以推导车辆最有可能在盲点后进入的候选闭路电视位置。基于CARLA模拟器在虚拟城市环境中的实验结果证实，即使在盲点区域，所提出的方法也能准确预测下一个出现的闭路电视，与现有技术相比，更有效地保持了连续的车辆轨迹。"
    },
    {
        "title": "Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation",
        "summary": "Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.",
        "url": "http://arxiv.org/abs/2512.20936v1",
        "published_date": "2025-12-24T04:39:45+00:00",
        "updated_date": "2025-12-24T04:39:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongxing Fan",
            "Shuyu Zhao",
            "Jiayang Ao",
            "Lu Sheng"
        ],
        "tldr": "This paper introduces a collaborative multi-agent reasoning framework for amodal completion, focusing on semantic consistency and structural integrity. It also presents a new evaluation metric (MAC-Score) aligned with human perception.",
        "tldr_zh": "该论文介绍了一个用于非模态补全的协作式多智能体推理框架，侧重于语义一致性和结构完整性。它还提出了一种与人类感知对齐的新型评估指标（MAC-Score）。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "非模态补全，即推断不可见物体部分的任务，在保持语义一致性和结构完整性方面面临着重大挑战。 先前的渐进式方法本质上受到推理不稳定性和误差累积的限制。 为了解决这些局限性，我们提出了一种协同多智能体推理框架，该框架明确地将语义规划与视觉合成解耦。 通过采用专门的智能体进行前期推理，我们的方法在像素生成之前生成一个结构化的、显式的计划，从而实现视觉和语义连贯的单次合成。 我们将此框架与两个关键机制集成：（1）一个自纠正的验证智能体，它采用思维链推理来纠正可见区域分割并严格在语义规划阶段识别残留遮挡物，以及（2）一个多样性假设生成器，它通过提供多样、合理的语义解释来解决不可见区域的模糊性，超越了标准随机种子采样的有限像素级变化。 此外，为了解决传统指标在评估推断出的不可见内容方面的局限性，我们引入了 MAC-Score（MLLM非模态补全分数），这是一种新颖的、与人类对齐的评估指标。 经过与人类判断和真实值的验证，这些指标建立了一个用于评估结构完整性和与可见上下文的语义一致性的强大标准。 大量实验表明，我们的框架在多个数据集上显著优于最先进的方法。 我们的项目可在以下网址获取：https://fanhongxing.github.io/remac-page。"
    },
    {
        "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction",
        "summary": "Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.\n  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.\n  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.\n  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.",
        "url": "http://arxiv.org/abs/2512.20858v1",
        "published_date": "2025-12-24T00:33:59+00:00",
        "updated_date": "2025-12-24T00:33:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Zabirul Islam",
            "Md Motaleb Hossen Manik",
            "Ge Wang"
        ],
        "tldr": "The paper introduces ALIVE, a local, real-time interactive video engine that uses neural avatars, content-aware retrieval, and multimodal interaction to enhance lecture viewing. It enables students to ask questions and receive explanations from an avatar integrated with lecture content.",
        "tldr_zh": "该论文介绍了ALIVE，一个本地的实时互动视频引擎，它使用神经人像、内容感知检索和多模态交互来增强讲座观看体验。它允许学生提问并从与讲座内容集成的人像处获得答案。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "传统讲座视频虽然具有灵活性，但缺乏实时澄清机制，导致学习者在产生困惑时不得不进行外部搜索。大型语言模型和神经化身技术的最新进展为交互式学习提供了新的机遇，但现有系统通常缺乏讲座感知能力、依赖基于云的服务，或者无法在一个统一且保护隐私的流程中整合检索和化身提供的解释。\n\n我们提出了ALIVE，即化身-讲座交互视频引擎，它将被动的讲座观看转变为动态的实时学习体验。ALIVE完全在本地硬件上运行，并集成了：（1）通过ASR转录、LLM精炼和神经口型合成生成的化身讲座；（2）一种内容感知检索机制，它将语义相似性与时间戳对齐相结合，以呈现上下文相关的讲座片段；以及（3）实时多模态交互，使学生能够暂停讲座，通过文本或语音提问，并以文本或化身提供的回复形式接收有依据的解释。\n\n为保持响应性，ALIVE采用轻量级嵌入模型、基于FAISS的检索以及具有渐进式预加载的分段化身合成。我们采用一套完整的医学影像课程来演示该系统，评估其检索准确性、延迟特性和用户体验，并表明ALIVE提供了准确、内容感知且引人入胜的实时支持。\n\nALIVE说明了多模态AI（当与内容感知检索和本地部署相结合时）如何显著提高录制讲座的教学价值，为下一代交互式学习环境提供了一条可扩展的途径。"
    },
    {
        "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective",
        "summary": "Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.",
        "url": "http://arxiv.org/abs/2512.20770v1",
        "published_date": "2025-12-23T21:14:55+00:00",
        "updated_date": "2025-12-23T21:14:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Markus Gross",
            "Sai B. Matha",
            "Aya Fahmy",
            "Rui Song",
            "Daniel Cremers",
            "Henri Meess"
        ],
        "tldr": "The paper introduces OccuFly, a novel camera-based aerial Semantic Scene Completion benchmark with a LiDAR-free data generation framework, addressing a gap in aerial robotics research and facilitating holistic 3D scene understanding from elevated viewpoints.",
        "tldr_zh": "该论文介绍了OccuFly，一个新型的基于摄像头的空中语义场景补全基准，具有无激光雷达的数据生成框架，解决了空中机器人研究中的一个缺口，并促进了从高处视角对整体3D场景的理解。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "语义场景补全(SSC)对于移动机器人中的3D感知至关重要，因为它通过联合估计密集体素占用和体素语义来实现整体场景理解。尽管SSC已在自动驾驶等地面领域得到广泛研究，但自主飞行等空中场景仍未得到充分探索，从而限制了下游应用的进展。此外，激光雷达传感器是SSC数据生成的主要方式，由于飞行规定、质量和能量限制，以及高架视角下基于激光雷达的点云稀疏性，这给大多数无人机(UAV)带来了挑战。为了解决这些限制，我们推出了OccuFly，这是首个基于真实世界相机的空中SSC基准，在春、夏、秋、冬四个季节，分别于50米、40米和30米高度采集。OccuFly覆盖城市、工业和农村场景，提供22个语义类别，并且数据格式符合既定规范，以便与现有研究无缝集成。至关重要的是，我们提出了一种基于相机模态的无激光雷达数据生成框架，该模态在现代无人机上无处不在。通过利用传统的3D重建技术，我们的框架通过将带注释的2D掩码子集提升到重建的点云中来自动进行标签迁移，从而大大减少了手动3D标注工作。最后，我们在OccuFly上对最先进的技术进行了基准测试，并强调了高架视角特有的挑战，从而为整体空中3D场景理解提供了一个全面的视觉基准。"
    },
    {
        "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
        "summary": "Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.",
        "url": "http://arxiv.org/abs/2512.20606v1",
        "published_date": "2025-12-23T18:54:10+00:00",
        "updated_date": "2025-12-23T18:54:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Soowon Son",
            "Honggyu An",
            "Chaehyun Kim",
            "Hyunah Ko",
            "Jisu Nam",
            "Dahyun Chung",
            "Siyoon Jin",
            "Jung Yi",
            "Jaewon Min",
            "Junhwa Hur",
            "Seungryong Kim"
        ],
        "tldr": "This paper introduces DiTracker, which repurposes video Diffusion Transformers (DiTs) for robust point tracking in videos, achieving state-of-the-art performance by leveraging spatio-temporal attention and lightweight tuning.",
        "tldr_zh": "本文介绍了DiTracker，它将视频扩散Transformer（DiT）重新用于视频中鲁棒的点跟踪，通过利用时空注意力和轻量级调优，实现了最先进的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "点跟踪旨在定位视频帧之间的对应点，是4D重建、机器人技术和视频编辑的一项基础任务。现有方法通常依赖诸如ResNet等浅层卷积骨干网络，独立处理帧，缺乏时间一致性，并且在具有挑战性的条件下产生不可靠的匹配代价。通过系统分析，我们发现预训练于大规模真实世界视频并具有时空注意力的视频扩散 Transformer (DiT) 本质上表现出强大的点跟踪能力，并且能够稳健地处理动态运动和频繁遮挡。我们提出 DiTracker，通过以下方法调整视频 DiT：（1）查询-键注意力匹配，（2）轻量级 LoRA 微调，以及（3）使用 ResNet 骨干网络进行代价融合。尽管使用小 8 倍的批次大小进行训练，DiTracker 在具有挑战性的 ITTO 基准测试中实现了最先进的性能，并且在 TAP-Vid 基准测试中达到或超过了最先进的模型。我们的工作验证了视频 DiT 特征作为点跟踪的一种有效且高效的基础。"
    },
    {
        "title": "Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3",
        "summary": "Maintaining stability during the single-support phase is a fundamental challenge in humanoid robotics, particularly in dance robots that require complex maneuvers and high mechanical freedom. Traditional tethered sensor configurations often restrict joint movement and introduce mechanical noises. This study proposes a wireless embedded balance system designed to maintain stability on uneven surfaces. The system utilizes a custom-designed foot unit integrated with four load cells and an ESP32-C3 microcontroller to estimate the Center of Pressure (CoP) in real time. The CoP data were transmitted wirelessly to the main controller to minimize the wiring complexity of the 29-DoF VI-ROSE humanoid robot. A PID control strategy is implemented to adjust the torso, hip, and ankle roll joints based on CoP feedback. Experimental characterization demonstrated high sensor precision with an average measurement error of 14.8 g. Furthermore, the proposed control system achieved a 100% success rate in maintaining balance during single-leg lifting tasks at a 3-degree inclination with optimized PID parameters (Kp=0.10, Kd=0.005). These results validate the efficacy of wireless CoP feedback in enhancing the postural stability of humanoid robots, without compromising their mechanical flexibility.",
        "url": "http://arxiv.org/abs/2512.21219v1",
        "published_date": "2025-12-24T15:00:23+00:00",
        "updated_date": "2025-12-24T15:00:23+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Muhtadin",
            "Faris Rafi Pramana",
            "Dion Hayu Fandiantoro",
            "Moh Ismarintan Zazuli",
            "Atar Fuady Babgei"
        ],
        "tldr": "This paper presents a wireless Center of Pressure (CoP) feedback system for improving the balance control of humanoid robots, particularly in single-leg stance on uneven surfaces, achieving a 100% success rate in experiments.",
        "tldr_zh": "本文提出了一种无线质心压力 (CoP) 反馈系统，用于提高人形机器人的平衡控制，尤其是在不平坦表面上的单腿站立姿势，实验成功率达到 100%。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "在单腿支撑阶段维持平衡是人形机器人领域的一项根本性挑战，尤其是在需要复杂动作和高机械自由度的跳舞机器人中。传统的有线传感器配置常常会限制关节的运动并引入机械噪声。本研究提出了一种无线嵌入式平衡系统，旨在维持在不平表面上的平衡。该系统利用定制设计的足部单元，该单元集成了四个力传感器和一个ESP32-C3微控制器，以实时估计压力中心（CoP）。CoP数据被无线传输到主控制器，从而最大限度地减少了29自由度 VI-ROSE 人形机器人的布线复杂性。实施了一种PID控制策略，基于CoP反馈来调整躯干、髋部以及踝部的横滚关节。实验表征结果表明传感器具有较高的精度，平均测量误差为14.8 g。此外，所提出的控制系统在优化PID参数（Kp=0.10, Kd=0.005）的情况下，在3度倾斜的单腿抬起任务中，实现了100%的平衡保持成功率。这些结果验证了无线CoP反馈在增强人形机器人姿势稳定性方面的有效性，同时又不损害其机械灵活性。"
    },
    {
        "title": "SparScene: Efficient Traffic Scene Representation via Sparse Graph Learning for Large-Scale Trajectory Generation",
        "summary": "Multi-agent trajectory generation is a core problem for autonomous driving and intelligent transportation systems. However, efficiently modeling the dynamic interactions between numerous road users and infrastructures in complex scenes remains an open problem. Existing methods typically employ distance-based or fully connected dense graph structures to capture interaction information, which not only introduces a large number of redundant edges but also requires complex and heavily parameterized networks for encoding, thereby resulting in low training and inference efficiency, limiting scalability to large and complex traffic scenes. To overcome the limitations of existing methods, we propose SparScene, a sparse graph learning framework designed for efficient and scalable traffic scene representation. Instead of relying on distance thresholds, SparScene leverages the lane graph topology to construct structure-aware sparse connections between agents and lanes, enabling efficient yet informative scene graph representation. SparScene adopts a lightweight graph encoder that efficiently aggregates agent-map and agent-agent interactions, yielding compact scene representations with substantially improved efficiency and scalability. On the motion prediction benchmark of the Waymo Open Motion Dataset (WOMD), SparScene achieves competitive performance with remarkable efficiency. It generates trajectories for more than 200 agents in a scene within 5 ms and scales to more than 5,000 agents and 17,000 lanes with merely 54 ms of inference time with a GPU memory of 2.9 GB, highlighting its superior scalability for large-scale traffic scenes.",
        "url": "http://arxiv.org/abs/2512.21133v1",
        "published_date": "2025-12-24T12:02:35+00:00",
        "updated_date": "2025-12-24T12:02:35+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Xiaoyu Mo",
            "Jintian Ge",
            "Zifan Wang",
            "Chen Lv",
            "Karl Henrik Johansson"
        ],
        "tldr": "SparScene introduces a sparse graph learning framework that uses lane graph topology to efficiently represent traffic scenes for multi-agent trajectory generation, achieving competitive performance on WOMD with significantly improved scalability and efficiency.",
        "tldr_zh": "SparScene 提出了一种稀疏图学习框架，该框架利用车道图拓扑来有效表示交通场景，用于多智能体轨迹生成，在 WOMD 上取得了具有竞争力的性能，并且显著提高了可扩展性和效率。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "多智能体轨迹生成是自动驾驶和智能交通系统的核心问题。然而，如何在复杂场景中高效地建模众多道路使用者和基础设施之间的动态交互仍然是一个开放性问题。现有方法通常采用基于距离或全连接密集图结构来捕获交互信息，这不仅引入了大量冗余边，还需要复杂且参数量大的网络进行编码，从而导致训练和推理效率低下，限制了其向大型复杂交通场景的可扩展性。为了克服现有方法的局限性，我们提出SparScene，一个为高效且可扩展的交通场景表示而设计的稀疏图学习框架。SparScene没有依赖距离阈值，而是利用车道图拓扑来构建智能体和车道之间具有结构感知的稀疏连接，从而实现高效且信息丰富的场景图表示。SparScene采用轻量级的图编码器，高效地聚合智能体-地图和智能体-智能体交互，产生紧凑的场景表示，从而显著提高效率和可扩展性。在Waymo开放运动数据集(WOMD)的运动预测基准测试中，SparScene以卓越的效率实现了具有竞争力的性能。它可以在5毫秒内为一个场景中超过200个智能体生成轨迹，并且仅需54毫秒的推理时间和2.9 GB的GPU内存，即可扩展到超过5000个智能体和17000条车道，突显了其针对大规模交通场景的卓越可扩展性。"
    },
    {
        "title": "Stretchable and High-Precision Optical Tactile Sensor for Trajectory Tracking of Parallel Mechanisms",
        "summary": "Stretchable sensors indicate promising prospects for soft robotics, medical devices, and human-machine interactions due to the high compliance of soft materials. Discrete sensing strategies, including sensor arrays and distributed sensors, are broadly involved in tactile sensors across versatile applications. However, it remains a challenge to achieve high spatial resolution with self-decoupled capacity and insensitivity to other off-axis stimuli for stretchable tactile sensors. Herein, we develop a stretchable tactile sensor based on the proposed continuous spectral-filtering principle, allowing superhigh resolution for applied stimuli. This proposed sensor enables a high-linear spatial response (0.996) even during stretching and bending, and high continuous spatial (7 μm) and force (5 mN) resolutions with design scalability and interaction robustness to survive piercing and cutting. We further demonstrate the sensors' performance by integrating them into a planar parallel mechanism for precise trajectory tracking (rotational resolution: 0.02°) in real time.",
        "url": "http://arxiv.org/abs/2512.20888v1",
        "published_date": "2025-12-24T02:13:16+00:00",
        "updated_date": "2025-12-24T02:13:16+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Yiding Nie",
            "Dongliang Fan",
            "Jiatai Huang",
            "Chunyu Liu",
            "Jian S. Dai"
        ],
        "tldr": "This paper introduces a high-resolution, stretchable optical tactile sensor for precise trajectory tracking in parallel mechanisms, boasting robustness to stretching and damage.",
        "tldr_zh": "本文介绍了一种高分辨率、可拉伸的光学触觉传感器，用于并联机构的精确轨迹跟踪，并且具有拉伸和损伤的鲁棒性。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "基于柔性材料高顺应性的特点，可拉伸传感器在软体机器人、医疗设备和人机交互等领域展现出广阔的应用前景。包含传感器阵列和分布式传感器在内的离散传感策略被广泛应用于各种触觉传感器中。然而，对于可拉伸触觉传感器而言，在实现高空间分辨率的同时，保持自解耦能力并对其他非轴向刺激不敏感仍然是一个挑战。本文中，我们开发了一种基于连续光谱滤波原理的可拉伸触觉传感器，从而实现了对施加刺激的超高分辨率。所提出的传感器即使在拉伸和弯曲时也能实现高线性空间响应 (0.996)，并具有高连续空间 (7 μm) 和力 (5 mN) 分辨率，同时具有设计上的可扩展性和交互鲁棒性，能够承受刺穿和切割。我们进一步通过将传感器集成到平面平行机构中，实时进行精确的轨迹跟踪（旋转分辨率：0.02°）来验证传感器的性能。"
    },
    {
        "title": "Early warning signals for loss of control",
        "summary": "Maintaining stability in feedback systems, from aircraft and autonomous robots to biological and physiological systems, relies on monitoring their behavior and continuously adjusting their inputs. Incremental damage can make such control fragile. This tends to go unnoticed until a small perturbation induces instability (i.e. loss of control). Traditional methods in the field of engineering rely on accurate system models to compute a safe set of operating instructions, which become invalid when the, possibly damaged, system diverges from its model. Here we demonstrate that the approach of such a feedback system towards instability can nonetheless be monitored through dynamical indicators of resilience. This holistic system safety monitor does not rely on a system model and is based on the generic phenomenon of critical slowing down, shown to occur in the climate, biology and other complex nonlinear systems approaching criticality. Our findings for engineered devices opens up a wide range of applications involving real-time early warning systems as well as an empirical guidance of resilient system design exploration, or \"tinkering\". While we demonstrate the validity using drones, the generic nature of the underlying principles suggest that these indicators could apply across a wider class of controlled systems including reactors, aircraft, and self-driving cars.",
        "url": "http://arxiv.org/abs/2512.20868v1",
        "published_date": "2025-12-24T00:59:46+00:00",
        "updated_date": "2025-12-24T00:59:46+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Jasper J. van Beers",
            "Marten Scheffer",
            "Prashant Solanki",
            "Ingrid A. van de Leemput",
            "Egbert H. van Nes",
            "Coen C. de Visser"
        ],
        "tldr": "This paper proposes a model-free method for detecting impending instability in feedback control systems by monitoring dynamical indicators of resilience, potentially applicable to various engineered systems like drones, reactors, and self-driving cars.",
        "tldr_zh": "本文提出了一种无需系统模型的方法，通过监测恢复力的动态指标来检测反馈控制系统中即将发生的不稳定性，可能适用于无人机、反应堆和自动驾驶汽车等各种工程系统。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 6,
        "summary_zh": "从飞机、自主机器人到生物和生理系统，维持反馈系统的稳定性依赖于监测其行为并持续调整其输入。累积性损伤会使这种控制变得脆弱。这种情况往往不会被注意到，直到一个小的扰动诱发了不稳定性（即失去控制）。工程领域的传统方法依赖于精确的系统模型来计算一组安全的操作指令，但一旦（可能已损坏的）系统偏离了其模型，这些指令就会失效。在此，我们证明了，即便如此，仍可以通过韧性的动态指标来监测此类反馈系统接近不稳定状态的过程。这种整体的系统安全监测器不依赖于系统模型，而是基于临界慢化的普遍现象，该现象已被证明发生在气候、生物学和其他接近临界点的复杂非线性系统中。我们针对工程设备的发现开启了广泛的应用，包括实时预警系统，以及对弹性系统设计的经验指导探索或“修补”。虽然我们使用无人机验证了其有效性，但潜在原理的通用性表明，这些指标可以应用于更广泛的受控系统，包括反应堆、飞机和自动驾驶汽车。"
    },
    {
        "title": "SegMo: Segment-aligned Text to 3D Human Motion Generation",
        "summary": "Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.",
        "url": "http://arxiv.org/abs/2512.21237v1",
        "published_date": "2025-12-24T15:26:11+00:00",
        "updated_date": "2025-12-24T15:26:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bowen Dang",
            "Lin Wu",
            "Xiaohang Yang",
            "Zheng Yuan",
            "Zhixiang Chen"
        ],
        "tldr": "This paper introduces SegMo, a framework for generating 3D human motion from text descriptions by aligning text and motion segments using contrastive learning, achieving improved performance on standard datasets.",
        "tldr_zh": "这篇论文介绍了 SegMo，一个通过使用对比学习对齐文本和运动片段，从而从文本描述生成 3D 人体运动的框架，并在标准数据集上取得了更好的性能。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6,
        "summary_zh": "从文本描述生成3D人体运动是一个重要的研究问题，在视频游戏、虚拟现实和增强现实领域具有广泛的应用。 近期的方法在序列层面对齐文本描述和人体运动，忽略了模态内部的语义结构。然而，运动描述和运动序列都可以自然地分解为更小且语义连贯的片段，这些片段可以作为原子对齐单元来实现更细粒度的对应关系。受此启发，我们提出了SegMo，一种新颖的段对齐文本条件人体运动生成框架，以实现细粒度的文本-运动对齐。我们的框架包含三个模块：（1）文本段提取，将复杂的文本描述分解为时间顺序排列的短语，每个短语代表一个简单的原子动作；（2）运动段提取，将完整的运动序列划分为相应的运动段；（3）细粒度文本-运动对齐，通过对比学习对齐文本段和运动段。大量实验表明，SegMo改进了在两个广泛使用的数据集上的强基线，在HumanML3D测试集上达到了0.553的TOP 1得分。此外，由于学习了文本和运动段的共享嵌入空间，SegMo还可以应用于检索式任务，例如运动定位和运动到文本的检索。"
    },
    {
        "title": "Human Motion Estimation with Everyday Wearables",
        "summary": "While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.",
        "url": "http://arxiv.org/abs/2512.21209v1",
        "published_date": "2025-12-24T14:44:51+00:00",
        "updated_date": "2025-12-24T14:44:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siqi Zhu",
            "Yixuan Li",
            "Junfu Li",
            "Qi Wu",
            "Zan Wang",
            "Haozhe Ma",
            "Wei Liang"
        ],
        "tldr": "The paper introduces EveryWear, a calibration-free human motion capture approach using common wearables (smartphone, smartwatch, earbuds, smart glasses) and a multimodal teacher-student framework trained on a new real-world dataset, Ego-Elec, achieving better performance than baselines.",
        "tldr_zh": "该论文介绍了EveryWear，一种基于常见可穿戴设备（智能手机、智能手表、耳机、智能眼镜）的免校准人体动作捕捉方法，采用多模态师生框架，并在新的真实世界数据集Ego-Elec上进行训练，性能优于基线。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "基于可穿戴设备的人体运动估计对于如XR互动等应用至关重要，然而，现有方法通常面临着佩戴体验差、硬件成本高昂和校准繁琐等问题，阻碍了其在日常生活中的应用。为了解决这些挑战，我们提出了EveryWear，一种轻量级且实用的人体运动捕捉方案，完全基于日常可穿戴设备：智能手机、智能手表、耳塞和智能眼镜，后者配备了一个前置摄像头和两个下视摄像头，使用前无需显式校准。我们引入了Ego-Elec，一个9小时的真实世界数据集，涵盖17个不同的室内和室外环境下的56种日常活动，并由运动捕捉系统(MoCap)提供真值3D标注，以促进该方向的稳健研究和基准测试。我们的方法采用了一种多模态师生框架，该框架将来自自我中心相机的视觉线索与来自消费级设备的惯性信号相结合。通过直接在真实世界数据上进行训练，而不是合成数据，我们的模型有效地消除了限制先前工作的sim-to-real差距。实验表明，我们的方法优于基线模型，验证了其在实际全身运动估计中的有效性。"
    },
    {
        "title": "UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer",
        "summary": "Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.",
        "url": "http://arxiv.org/abs/2512.21078v1",
        "published_date": "2025-12-24T09:55:16+00:00",
        "updated_date": "2025-12-24T09:55:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianchen Deng",
            "Xun Chen",
            "Ziming Li",
            "Hongming Shen",
            "Danwei Wang",
            "Javier Civera",
            "Hesheng Wang"
        ],
        "tldr": "The paper introduces UniPR-3D, a novel Visual Place Recognition (VPR) architecture that leverages a Visual Geometry Grounded Transformer (VGGT) to effectively integrate multi-view information, achieving state-of-the-art performance and improved generalization across diverse environments.",
        "tldr_zh": "该论文介绍了 UniPR-3D，一种新的视觉场景识别 (VPR) 架构，它利用视觉几何基础的 Transformer (VGGT) 有效地整合多视图信息，实现了最先进的性能，并提高了在不同环境中的泛化能力。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "视觉场景识别（VPR）传统上被表述为单图检索任务。使用多个视角有着明显的优势，但这种设置仍然相对未被充分探索，并且现有方法通常难以泛化到不同的环境中。在这项工作中，我们引入了UniPR-3D，这是第一个有效整合来自多个视角的VPR架构。UniPR-3D构建在VGGT骨干网络之上，该网络能够编码多视角的3D表示，我们通过设计特征聚合器并针对场景识别任务进行微调来进行适配。为了构建我们的描述符，我们共同利用VGGT产生的3D令牌和中间2D令牌。基于它们的不同特性，我们为2D和3D特征设计了专门的聚合模块，从而使我们的描述符能够在跨视点推理的同时捕获细粒度的纹理线索。为了进一步增强泛化能力，我们结合了单帧和多帧聚合方案，以及可变长度序列检索策略。我们的实验表明，UniPR-3D建立了新的技术水平，超越了单视角和多视角基线，并突出了基于几何信息的令牌在VPR中的有效性。我们的代码和模型将在Github上公开提供：https://github.com/dtc111111/UniPR-3D。"
    },
    {
        "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation",
        "summary": "Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.",
        "url": "http://arxiv.org/abs/2512.20815v1",
        "published_date": "2025-12-23T22:28:30+00:00",
        "updated_date": "2025-12-23T22:28:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Reeshad Khan amd John Gauch"
        ],
        "tldr": "This paper presents a co-design framework for autonomous driving that jointly optimizes optics, sensor modeling, and semantic segmentation networks, demonstrating improved mIoU and robustness, especially in challenging conditions, with a compact model deployable on edge devices.",
        "tldr_zh": "本文提出了一种自动驾驶的协同设计框架，该框架联合优化光学器件、传感器建模和语义分割网络，展示了改进的 mIoU 和鲁棒性，尤其是在具有挑战性的条件下，并且模型紧凑，可部署在边缘设备上。",
        "relevance_score": 4,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 6,
        "summary_zh": "传统的自动驾驶流程将相机设计与下游感知分离，依赖于固定的光学器件和手工制作的图像信号处理器(ISP)，这些处理器优先处理人眼可观看的图像而非机器语义。这种分离在去马赛克、去噪或量化过程中会丢弃信息，同时迫使模型适应传感器伪影。我们提出了一种任务驱动的协同设计框架，将光学器件、传感器建模和轻量级语义分割网络统一到一个端到端的原始数据(RAW)到任务的流程中。基于DeepLens[19]，我们的系统集成了真实的手机级镜头模型、可学习的彩色滤光片阵列、泊松-高斯噪声过程和量化，所有这些都直接针对分割目标进行优化。在KITTI-360上的评估显示，相对于固定流程，其平均交并比(mIoU)得到了持续的提升，其中光学建模和彩色滤光片阵列学习提供了最大的增益，尤其是对于细小或低光照敏感的类别。重要的是，这些鲁棒性增益是通过一个紧凑的约100万参数的模型实现的，该模型以约28帧/秒的速度运行，证明了其边缘部署能力。视觉和定量分析进一步突出了协同设计的传感器如何使采集适应语义结构，锐化边界并在模糊、噪声和低位深度下保持准确性。总之，这些发现确立了光学器件、传感器和网络的完整堆栈协同优化，是实现自动系统中高效、可靠和可部署感知的原则性途径。"
    },
    {
        "title": "Flocking phase transition and threat responses in bio-inspired autonomous drone swarms",
        "summary": "Collective motion inspired by animal groups offers powerful design principles for autonomous aerial swarms. We present a bio-inspired 3D flocking algorithm in which each drone interacts only with a minimal set of influential neighbors, relying solely on local alignment and attraction cues. By systematically tuning these two interaction gains, we map a phase diagram revealing sharp transitions between swarming and schooling, as well as a critical region where susceptibility, polarization fluctuations, and reorganization capacity peak. Outdoor experiments with a swarm of ten drones, combined with simulations using a calibrated flight-dynamics model, show that operating near this transition enhances responsiveness to external disturbances. When confronted with an intruder, the swarm performs rapid collective turns, transient expansions, and reliably recovers high alignment within seconds. These results demonstrate that minimal local-interaction rules are sufficient to generate multiple collective phases and that simple gain modulation offers an efficient mechanism to adjust stability, flexibility, and resilience in drone swarms.",
        "url": "http://arxiv.org/abs/2512.21196v1",
        "published_date": "2025-12-24T14:20:19+00:00",
        "updated_date": "2025-12-24T14:20:19+00:00",
        "categories": [
            "cs.RO",
            "eess.SY",
            "nlin.AO"
        ],
        "authors": [
            "Matthieu Verdoucq",
            "Dari Trendafilov",
            "Clément Sire",
            "Ramón Escobedo",
            "Guy Theraulaz",
            "Gautier Hattenberger"
        ],
        "tldr": "This paper presents a bio-inspired drone flocking algorithm with minimal local interactions, demonstrating phase transitions between swarming and schooling and enhanced responsiveness to disturbances near a critical point through outdoor experiments.",
        "tldr_zh": "本文提出了一种受生物启发无人机集群算法，通过最少的局部交互实现了蜂拥和群游之间的相变，并通过户外实验证明了在临界点附近对干扰的响应能力增强。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 5,
        "overall_priority_score": 5,
        "summary_zh": "受动物群体启发而来的集体运动为自主飞行无人机蜂群提供了强大的设计原则。我们提出了一种受生物启发的3D集群算法，其中每架无人机仅与其最少数量的有影响力的邻居进行交互，完全依赖于局部对齐和吸引提示。通过系统地调整这两个交互增益，我们绘制了一个相图，揭示了集群和鱼群行为之间的急剧转变，以及一个临界区域，其中敏感性、极化波动和重组能力达到峰值。包含十架无人机的室外实验，结合使用校准后的飞行动力学模型的仿真，表明在此转变附近运行可增强对外部干扰的响应性。当面对入侵者时，蜂群会执行快速集体转弯、瞬时扩张，并在几秒钟内可靠地恢复高度对齐。这些结果表明，最少的局部交互规则足以产生多种集体相位，并且简单的增益调制提供了一种有效的机制来调整无人机蜂群的稳定性、灵活性和复原力。"
    },
    {
        "title": "Multimodal Sensing for Robot-Assisted Sub-Tissue Feature Detection in Physiotherapy Palpation",
        "summary": "Robotic palpation relies on force sensing, but force signals in soft-tissue environments are variable and cannot reliably reveal subtle subsurface features. We present a compact multimodal sensor that integrates high-resolution vision-based tactile imaging with a 6-axis force-torque sensor. In experiments on silicone phantoms with diverse subsurface tendon geometries, force signals alone frequently produce ambiguous responses, while tactile images reveal clear structural differences in presence, diameter, depth, crossings, and multiplicity. Yet accurate force tracking remains essential for maintaining safe, consistent contact during physiotherapeutic interaction. Preliminary results show that combining tactile and force modalities enables robust subsurface feature detection and controlled robotic palpation.",
        "url": "http://arxiv.org/abs/2512.20992v1",
        "published_date": "2025-12-24T06:35:48+00:00",
        "updated_date": "2025-12-24T06:35:48+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Tian-Ao Ren",
            "Jorge Garcia",
            "Seongheon Hong",
            "Jared Grinberg",
            "Hojung Choi",
            "Julia Di",
            "Hao Li",
            "Dmitry Grinberg",
            "Mark R. Cutkosky"
        ],
        "tldr": "This paper introduces a multimodal tactile and force sensor for robot-assisted physiotherapy palpation, demonstrating improved subsurface feature detection compared to force sensing alone. The integration enhances robotic palpation accuracy and safety.",
        "tldr_zh": "本文介绍了一种用于机器人辅助物理治疗触诊的多模态触觉和力传感器，证明与单独使用力觉传感相比，该传感器能够更好地检测次表面特征。 这种集成提高了机器人触诊的准确性和安全性。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "机器人触诊依赖于力传感，但软组织环境中的力信号具有可变性，无法可靠地揭示细微的次表面特征。我们提出了一种紧凑的多模态传感器，该传感器集成了基于视觉的高分辨率触觉成像与六轴力矩传感器。在具有不同次表面肌腱几何形状的硅胶体模上的实验表明，单独的力信号经常产生模糊的响应，但触觉图像揭示了存在、直径、深度、交叉和多重性方面的清晰结构差异。然而，精确的力跟踪对于在理疗互动期间保持安全、一致的接触仍然至关重要。初步结果表明，结合触觉和力模态能够实现稳健的次表面特征检测和受控的机器人触诊。"
    },
    {
        "title": "Fixed-time control with prescribed performance for path following of underwater gliders",
        "summary": "Underwater gliders are increasingly deployed in challenging missions - such as hurricane-season observations and long-endurance environmental monitoring - where strong currents and turbulence pose significant risks to navigation safety. To address these practical challenges, this paper presents a fixed-time prescribed performance control scheme for the 3D path following of underwater gliders subject to model uncertainties and environmental disturbances. The primary contribution is the integration of a finite-time performance function within a fixed-time control framework. This synthesis ensures that the tracking errors are constrained within prescribed performance bounds and converge to a compact set within a fixed time, independent of initial conditions. A second key contribution is the development of a fixed-time sliding mode disturbance observer that provides accurate finite-time estimation of lumped disturbances, enhancing the system's robustness. Integrated with an iLOS guidance law, the proposed controller enables precise and safe waypoint following. Numerical simulations demonstrate that the proposed method outperforms conventional sliding mode and prescribed performance controllers in tracking accuracy, convergence speed, and control effort smoothness, validating its efficacy for robust underwater navigation.",
        "url": "http://arxiv.org/abs/2512.20748v1",
        "published_date": "2025-12-23T20:11:25+00:00",
        "updated_date": "2025-12-23T20:11:25+00:00",
        "categories": [
            "eess.SY",
            "cs.RO",
            "math.OC"
        ],
        "authors": [
            "Hanzhi Yang",
            "Nina Mahmoudian"
        ],
        "tldr": "This paper presents a fixed-time prescribed performance control scheme for underwater gliders, integrating a finite-time performance function and a fixed-time sliding mode disturbance observer to improve tracking accuracy and robustness in challenging navigation environments.",
        "tldr_zh": "本文提出了一种用于水下航行器的固定时间规定性能控制方案，集成了有限时间性能函数和固定时间滑模扰动观测器，以提高在具有挑战性的导航环境中的跟踪精度和鲁棒性。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "水下滑翔机正日益被部署于具有挑战性的任务中，例如飓风季观测和长航时环境监测，而强劲的水流和湍流给导航安全带来重大风险。为了应对这些实际挑战，本文提出了一种固定时间预定性能控制方案，用于水下滑翔机在模型不确定性和环境扰动下的三维路径跟踪。主要贡献是将有限时间性能函数集成到固定时间控制框架中。这种综合确保了跟踪误差被限制在预定的性能边界内，并在固定时间内收敛到一个紧致集，且与初始条件无关。第二个关键贡献是开发了一种固定时间滑模扰动观测器，该观测器提供对集总扰动的精确有限时间估计，增强了系统的鲁棒性。结合改进的视线(iLOS)制导律，所提出的控制器可实现精确和安全的航路点跟踪。数值仿真表明，所提出的方法在跟踪精度、收敛速度和控制力平滑性方面优于传统的滑模和预定性能控制器，验证了其在鲁棒性水下导航中的有效性。"
    },
    {
        "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
        "summary": "The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.",
        "url": "http://arxiv.org/abs/2512.21054v1",
        "published_date": "2025-12-24T08:44:58+00:00",
        "updated_date": "2025-12-24T08:44:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Kaustubh Kundu",
            "Hrishav Bakul Barua",
            "Lucy Robertson-Bell",
            "Zhixi Cai",
            "Kalin Stefanov"
        ],
        "tldr": "The paper introduces DexAvatar, a novel framework for reconstructing accurate 3D hand and body poses from monocular sign language videos, achieving significant improvements compared to existing methods on the SGNify dataset.",
        "tldr_zh": "该论文介绍了 DexAvatar，一种从单目手语视频重建精确三维手部和身体姿势的新框架，并在 SGNify 数据集上实现了相对于现有方法的显著改进。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "手语生成的发展趋势集中于数据驱动的生成方法，这些方法需要海量的精确2D和3D人体姿态数据才能达到可接受的生成质量。然而，目前大多数手语数据集是基于视频的，并且仅限于自动重建的2D人体姿态（即关键点），缺乏精确的3D信息。此外，现有的从手语视频中自动进行3D人体姿态估计的最先进技术容易受到自遮挡、噪声和运动模糊效应的影响，导致较差的重建质量。为了解决这个问题，我们引入了DexAvatar，一个新型框架，用于从野外单目手语视频中重建生物力学上精确的精细手部关节运动和身体动作，并由学习到的3D手部和身体先验引导。DexAvatar在SGNify运动捕捉数据集上取得了强大的性能，这是唯一可用于该任务的基准，与最先进的技术相比，在身体和手部姿态的估计方面取得了35.11%的改进。该项目的官方网站是：https://github.com/kaustesseract/DexAvatar。"
    },
    {
        "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
        "summary": "The application of agentic AI systems in autonomous decision-making is growing in the areas of healthcare, smart cities, digital forensics, and supply chain management. Even though these systems are flexible and offer real-time reasoning, they also raise concerns of trust and oversight, and integrity of the information and activities upon which they are founded. The paper suggests a single architecture model comprising of LangChain-based multi-agent system with a permissioned blockchain to guarantee constant monitoring, policy enforcement, and immutable auditability of agentic action. The framework relates the perception conceptualization-action cycle to a blockchain layer of governance that verifies the inputs, evaluates recommended actions, and documents the outcomes of the execution. A Hyperledger Fabric-based system, action executors MCP-integrated, and LangChain agent are introduced and experiments of smart inventory management, traffic-signal control, and healthcare monitoring are done. The results suggest that blockchain-security verification is efficient in preventing unauthorized practices, offers traceability throughout the whole decision-making process, and maintains operational latency within reasonable ranges. The suggested framework provides a universal system of implementing high-impact agentic AI applications that are autonomous yet responsible.",
        "url": "http://arxiv.org/abs/2512.20985v1",
        "published_date": "2025-12-24T06:20:28+00:00",
        "updated_date": "2025-12-24T06:20:28+00:00",
        "categories": [
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Salman Jan",
            "Hassan Ali Razzaqi",
            "Ali Akarma",
            "Mohammad Riyaz Belgaum"
        ],
        "tldr": "This paper proposes a blockchain-monitored LangChain-based multi-agent system for trusted perception-reasoning-action pipelines, ensuring constant monitoring, policy enforcement, and auditability, demonstrated in smart inventory, traffic control, and healthcare. It uses Hyperledger Fabric.",
        "tldr_zh": "本文提出了一种基于区块链监控的LangChain多智能体系统，用于可信的感知-推理-行动管道，确保持续监控、策略执行和可审计性，并在智能库存、交通控制和医疗保健中进行了演示。它使用Hyperledger Fabric。",
        "relevance_score": 4,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "代理式人工智能系统在自主决策领域的应用日益增长，包括医疗保健、智慧城市、数字取证和供应链管理。尽管这些系统具有灵活性并提供实时推理能力，但它们也引发了对信任、监督以及其所依据的信息和活动的完整性的担忧。本文提出了一种单一架构模型，该模型由基于LangChain的多智能体系统和许可链组成，以保证对代理行为的持续监控、策略执行和不可篡改的审计。该框架将感知-概念化-行动周期关联到一个治理区块链层，该区块链层验证输入、评估建议的行动并记录执行结果。本文介绍了一个基于Hyperledger Fabric的系统、集成了MCP的行动执行器和LangChain代理，并进行了智能库存管理、交通信号控制和医疗保健监控的实验。结果表明，区块链安全验证能够有效防止未经授权的行为，提供整个决策过程的可追溯性，并将运营延迟保持在合理的范围内。所提出的框架提供了一种通用系统，用于实施具有自主性但又负责任的高影响力代理式人工智能应用。"
    },
    {
        "title": "AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication",
        "summary": "The 6G wireless aims at the Tb/s peak data rates are expected, a sub-millisecond latency, massive Internet of Things/vehicle connectivity, which requires sustainable access to audio over the air and energy-saving functionality. Cognitive Radio Networks CCNs help in alleviating the problem of spectrum scarcity, but classical sensing and allocation are still energy-consumption intensive, and sensitive to rapid spectrum variations. Our framework which centers on AI driven green CRN aims at integrating deep reinforcement learning (DRL) with transfer learning, energy harvesting (EH), reconfigurable intelligent surfaces (RIS) with other light-weight genetic refinement operations that optimally combine sensing timelines, transmit power, bandwidth distribution and RIS phase selection. Compared to two baselines, the utilization of MATLAB + NS-3 under dense loads, a traditional CRN with energy sensing under fixed policies, and a hybrid CRN with cooperative sensing under heuristic distribution of resource, there are (25-30%) fewer energy reserves used, sensing AUC greater than 0.90 and +6-13 p.p. higher PDR. The integrated framework is easily scalable to large IoT and vehicular applications, and it provides a feasible and sustainable roadmap to 6G CRNs.\n  Index Terms--Cognitive Radio Networks (CRNs), 6G, Green Communication, Energy Efficiency, Deep Reinforcement Learning (DRL), Spectrum Sensing, RIS, Energy Harvesting, QoS, IoT.",
        "url": "http://arxiv.org/abs/2512.20739v1",
        "published_date": "2025-12-23T19:50:13+00:00",
        "updated_date": "2025-12-23T19:50:13+00:00",
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Anshul Sharma",
            "Shujaatali Badami",
            "Biky Chouhan",
            "Pushpanjali Pandey",
            "Brijeena Rana",
            "Navneet Kaur"
        ],
        "tldr": "This paper proposes an AI-driven green cognitive radio network for 6G communication, integrating DRL, transfer learning, energy harvesting, and RIS to improve energy efficiency and spectrum sensing performance. Results show significant improvements in energy reserves and PDR compared to baseline methods.",
        "tldr_zh": "本文提出了一种用于6G通信的AI驱动的绿色认知无线电网络，集成了DRL、迁移学习、能量收集和RIS，以提高能源效率和频谱感知性能。结果表明，与基线方法相比，能量储备和PDR有显着改善。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "摘　要：\n6G无线通信旨在实现Tb/s峰值数据速率、亚毫秒级延迟，以及大规模物联网/车辆连接，这需要可持续的空中音频访问和节能功能。认知无线电网络(CRNs) 有助于缓解频谱稀缺问题，但传统的频谱感知和分配仍然是能量消耗密集型的，并且对快速频谱变化敏感。我们以人工智能驱动的绿色CRN为中心的框架，旨在将深度强化学习(DRL) 与迁移学习、能量收集(EH)、可重构智能表面(RIS) 以及其他轻量级遗传优化操作相集成，以最佳地组合感知时间线、发射功率、带宽分配和RIS相位选择。与两个基线相比，在密集负载下使用MATLAB + NS-3进行的仿真结果表明，即采用固定策略的传统CRN与能量感知，以及采用启发式资源分配的协作感知的混合CRN相比，所提出的方法使用的能量储备减少了(25-30%)，感知AUC大于0.90，且 PDR提高了+6-13个百分点。该集成框架易于扩展到大型物联网和车辆应用，并为6G CRN提供了一条可行且可持续的路线图。\n\n索引词——认知无线电网络(CRNs)，6G，绿色通信，能源效率，深度强化学习(DRL)，频谱感知，RIS，能量收集，服务质量(QoS)，物联网。"
    },
    {
        "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential",
        "summary": "Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \\textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\\times$. Notably, it delivers over $20\\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.",
        "url": "http://arxiv.org/abs/2512.21284v1",
        "published_date": "2025-12-24T17:05:09+00:00",
        "updated_date": "2025-12-24T17:05:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shihao Zou",
            "Jingjing Li",
            "Wei Ji",
            "Jincai Huang",
            "Kai Wang",
            "Guo Dan",
            "Weixin Si",
            "Yi Pan"
        ],
        "tldr": "The paper introduces SpikeSurgSeg, a spike-driven video Transformer for real-time surgical scene segmentation, achieving comparable accuracy to ANN-based models with significantly reduced latency, especially on non-GPU platforms.",
        "tldr_zh": "该论文介绍了SpikeSurgSeg，一种用于实时手术场景分割的脉冲驱动视频Transformer，在非GPU平台上实现了与基于ANN的模型相当的精度，但显著降低了延迟。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "现代外科手术系统越来越依赖于智能场景理解，以便及时提供态势感知，从而提高术中安全性。在该流程中，手术场景分割在准确感知手术事件方面起着核心作用。尽管最近的深度学习模型，特别是大规模的基础模型，取得了显著的分割精度，但它们巨大的计算需求和功耗阻碍了在资源受限的手术环境中进行实时部署。为了解决这一限制，我们探索了新兴的脉冲神经网络(SNN) 作为一种具有前景的高效手术智能范式。然而，它们的性能仍然受到标记手术数据稀缺性和手术视频表示的固有稀疏性的限制。为此，我们提出了\\textit{SpikeSurgSeg}，这是第一个脉冲驱动的视频Transformer框架，专为手术场景分割定制，具有在非GPU平台上实现实时性的潜力。为了解决手术注释的有限可用性，我们引入了一种用于SNN的手术场景掩码自编码预训练策略，该策略通过逐层管道掩码实现稳健的时空表示学习。在此预训练骨干网络的基础上，我们进一步采用了一个轻量级的脉冲驱动分割头，该分割头产生时间上一致的预测，同时保持SNN的低延迟特性。在EndoVis18和我们内部的SurgBleed数据集上的大量实验表明，SpikeSurgSeg实现了与SOTA基于人工神经网络(ANN)的模型相当的mIoU，同时将推理延迟至少降低了$8\\times$。值得注意的是，相对于大多数基础模型基线，它实现了超过$20\\times$的加速，突显了其在时间紧迫的手术场景分割中的潜力。"
    },
    {
        "title": "Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera",
        "summary": "Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.",
        "url": "http://arxiv.org/abs/2512.21053v1",
        "published_date": "2025-12-24T08:40:57+00:00",
        "updated_date": "2025-12-24T08:40:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zibin Liu",
            "Banglei Guan",
            "Yang Shang",
            "Shunkun Liang",
            "Zhenbao Yu",
            "Qifeng Yu"
        ],
        "tldr": "This paper presents a 6DoF object pose tracking method using an event camera, guided by optical flow, demonstrating improved accuracy and robustness compared to existing event-based methods.",
        "tldr_zh": "本文提出了一种基于事件相机的光流引导的6自由度物体姿态跟踪方法，与现有的基于事件的方法相比，展示了更高的准确性和鲁棒性。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "目标姿态跟踪是多媒体领域中的一项关键技术，近年来受到越来越多的关注。现有的采用传统相机的方法面临诸多挑战，例如运动模糊、传感器噪声、部分遮挡和变化的光照条件。新兴的仿生传感器，特别是事件相机，具有高动态范围和低延迟等优点，有望解决上述挑战。在这项工作中，我们提出了一种基于光流引导的事件相机6自由度物体姿态跟踪方法。首先，我们采用了一种2D-3D混合特征提取策略，从事件和物体模型中检测角点和边缘，从而精确地表征物体运动。然后，我们通过最大化时空窗口内的事件相关概率来搜索角点的光流，并在光流的引导下建立角点和边缘之间的关联。此外，通过最小化角点和边缘之间的距离，迭代优化6自由度物体姿态，实现连续的姿态跟踪。模拟和真实事件的实验结果表明，我们的方法在准确性和鲁棒性方面均优于基于事件相机的最先进方法。"
    },
    {
        "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection",
        "summary": "This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.",
        "url": "http://arxiv.org/abs/2512.20746v1",
        "published_date": "2025-12-23T20:00:34+00:00",
        "updated_date": "2025-12-23T20:00:34+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Tony Tran",
            "Bin Hu"
        ],
        "tldr": "This paper presents TrashDet, an efficient waste detection system for TinyML devices, achieved through iterative neural architecture search, resulting in significant improvements in accuracy, energy consumption, and latency compared to existing methods.",
        "tldr_zh": "本文提出了TrashDet，一个用于TinyML设备的高效垃圾检测系统，通过迭代神经架构搜索实现，与现有方法相比，在精度、能耗和延迟方面都有显著提高。",
        "relevance_score": 3,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文针对TACO数据集，在严格的TinyML约束下，采用一种面向边缘和物联网设备的迭代式硬件感知神经架构搜索框架进行垃圾检测。该方法构建了一个Once-for-All风格的ResDets超网络，并执行迭代进化搜索，交替优化主干网络和颈部/头部，通过种群传递机制和精度预测器来降低搜索成本并提高稳定性。该框架产生了一系列可直接部署的检测器，称为TrashDets。在包含五个类别的TACO子集（纸张、塑料、瓶子、罐和香烟）上，最强变体TrashDet-l实现了19.5 mAP50，拥有30.5M参数，在参数量大幅减少的同时，精度比之前的检测器提高了高达3.6 mAP50。TrashDet家族的参数量范围为1.2M到30.5M，mAP50值在11.4到19.5之间，为资源受限硬件上不同的TinyML部署预算提供可扩展的检测器选项。在MAX78002微控制器上，使用TrashNet数据集，两个专门的变体TrashDet-ResNet和TrashDet-MBNet共同优于ai87-fpndetector基线，其中TrashDet-ResNet在26.7 ms延迟和37.45 FPS下，每次推理的能耗为7525~$μ$J，而TrashDet-MBNet的mAP50提高了10.2%；与现有的TinyML检测器相比，它们共同降低了高达88%的能耗，高达78%的延迟，以及高达53%的平均功耗。"
    },
    {
        "title": "Real-World Adversarial Attacks on RF-Based Drone Detectors",
        "summary": "Radio frequency (RF) based systems are increasingly used to detect drones by analyzing their RF signal patterns, converting them into spectrogram images which are processed by object detection models. Existing RF attacks against image based models alter digital features, making over-the-air (OTA) implementation difficult due to the challenge of converting digital perturbations to transmittable waveforms that may introduce synchronization errors and interference, and encounter hardware limitations. We present the first physical attack on RF image based drone detectors, optimizing class-specific universal complex baseband (I/Q) perturbation waveforms that are transmitted alongside legitimate communications. We evaluated the attack using RF recordings and OTA experiments with four types of drones. Our results show that modest, structured I/Q perturbations are compatible with standard RF chains and reliably reduce target drone detection while preserving detection of legitimate drones.",
        "url": "http://arxiv.org/abs/2512.20712v1",
        "published_date": "2025-12-23T19:19:45+00:00",
        "updated_date": "2025-12-23T19:19:45+00:00",
        "categories": [
            "cs.CR",
            "cs.LG"
        ],
        "authors": [
            "Omer Gazit",
            "Yael Itzhakev",
            "Yuval Elovici",
            "Asaf Shabtai"
        ],
        "tldr": "This paper presents a novel physical adversarial attack on RF-based drone detection systems by transmitting optimized I/Q perturbation waveforms, demonstrating successful evasion of target drone detection without affecting legitimate drone detection via RF recordings and over-the-air experiments.",
        "tldr_zh": "本文提出了一种针对基于射频的无人机检测系统的新型物理对抗攻击方法，通过传输优化的I/Q扰动波形，成功规避了目标无人机的检测，而不会影响合法无人机的检测。该方法通过射频记录和无线实验进行了验证。",
        "relevance_score": 3,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5,
        "summary_zh": "基于射频(RF)的系统正日益普及，通过分析无人机的射频信号模式，将其转化为频谱图图像，并由目标检测模型进行处理，从而实现无人机检测。现有的针对基于图像模型的射频攻击会改变数字特征，由于难以将数字扰动转换为可传输的波形，可能引入同步误差和干扰，并面临硬件限制，因此难以进行空中(OTA)实现。我们提出了首个针对基于射频图像的无人机检测器的物理攻击，优化了特定类别的通用复基带(I/Q)扰动波形，并将其与合法通信一同传输。我们使用射频录音和包括四种类型无人机的OTA实验评估了该攻击。我们的结果表明，适度的、结构化的I/Q扰动与标准射频链路兼容，并能可靠地降低目标无人机的检测率，同时保持对合法无人机的检测。"
    },
    {
        "title": "Relative Localization System Design for SnailBot: A Modular Self-reconfigurable Robot",
        "summary": "This paper presents the design and implementation of a relative localization system for SnailBot, a modular self reconfigurable robot. The system integrates ArUco marker recognition, optical flow analysis, and IMU data processing into a unified fusion framework, enabling robust and accurate relative positioning for collaborative robotic tasks. Experimental validation demonstrates the effectiveness of the system in realtime operation, with a rule based fusion strategy ensuring reliability across dynamic scenarios. The results highlight the potential for scalable deployment in modular robotic systems.",
        "url": "http://arxiv.org/abs/2512.21226v1",
        "published_date": "2025-12-24T15:07:09+00:00",
        "updated_date": "2025-12-24T15:07:09+00:00",
        "categories": [
            "cs.RO",
            "eess.SY"
        ],
        "authors": [
            "Shuhan Zhang",
            "Tin Lun Lam"
        ],
        "tldr": "This paper introduces a relative localization system for the SnailBot, a modular self-reconfigurable robot, utilizing ArUco markers, optical flow, and IMU data with a fusion framework to enable robust and accurate relative positioning.",
        "tldr_zh": "本文介绍了一种用于模块化自重构机器人SnailBot的相对定位系统，该系统利用ArUco标记、光流和IMU数据，通过融合框架实现稳健而精确的相对定位。",
        "relevance_score": 3,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 4,
        "overall_priority_score": 4,
        "summary_zh": "本文介绍了用于模块化自重构机器人SnailBot的相对定位系统的设计与实现。该系统将 ArUco 标志识别、光流分析和IMU数据处理集成到一个统一的融合框架中，从而为协作机器人任务提供稳健而精确的相对定位。实验验证表明了该系统在实时运行中的有效性，基于规则的融合策略确保了在动态场景中的可靠性。结果突出了该系统在模块化机器人系统中进行可扩展部署的潜力。"
    },
    {
        "title": "From Human Bias to Robot Choice: How Occupational Contexts and Racial Priming Shape Robot Selection",
        "summary": "As artificial agents increasingly integrate into professional environments, fundamental questions have emerged about how societal biases influence human-robot selection decisions. We conducted two comprehensive experiments (N = 1,038) examining how occupational contexts and stereotype activation shape robotic agent choices across construction, healthcare, educational, and athletic domains. Participants made selections from artificial agents that varied systematically in skin tone and anthropomorphic characteristics. Our study revealed distinct context-dependent patterns. Healthcare and educational scenarios demonstrated strong favoritism toward lighter-skinned artificial agents, while construction and athletic contexts showed greater acceptance of darker-toned alternatives. Participant race was associated with systematic differences in selection patterns across professional domains. The second experiment demonstrated that exposure to human professionals from specific racial backgrounds systematically shifted later robotic agent preferences in stereotype-consistent directions. These findings show that occupational biases and color-based discrimination transfer directly from human-human to human-robot evaluation contexts. The results highlight mechanisms through which robotic deployment may unintentionally perpetuate existing social inequalities.",
        "url": "http://arxiv.org/abs/2512.20951v1",
        "published_date": "2025-12-24T05:15:26+00:00",
        "updated_date": "2025-12-24T05:15:26+00:00",
        "categories": [
            "cs.RO",
            "cs.HC"
        ],
        "authors": [
            "Jiangen He",
            "Wanqi Zhang",
            "Jessica Barfield"
        ],
        "tldr": "This paper investigates how occupational contexts and racial priming influence human selection of robots with different skin tones in various professional settings, revealing biases that mirror human-human interactions and potentially perpetuate social inequalities.",
        "tldr_zh": "本文研究了职业背景和种族启动如何影响人类在不同职业环境中选择具有不同肤色的机器人，揭示了反映人际互动的偏见，并可能延续社会不平等。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 4,
        "summary_zh": "摘要：\n\n随着人工智能体日益融入专业环境，关于社会偏见如何影响人-机器人选择决策的基本问题开始涌现。我们进行了两个全面的实验（N = 1,038），研究了职业背景和刻板印象激活如何在建筑、医疗保健、教育和体育领域塑造对机器人智能体的选择。参与者从肤色和拟人化特征系统性变化的智能体中进行选择。我们的研究揭示了明显的背景依赖性模式。医疗保健和教育场景表现出对肤色较浅的智能体的强烈偏爱，而建筑和体育背景则表现出对肤色较深者的更大接受度。参与者的种族与跨专业领域的选择模式的系统性差异相关。第二个实验表明，接触来自特定种族背景的人类专业人士会系统性地将后续的机器人智能体偏好转移到符合刻板印象的方向。这些发现表明，职业偏见和基于肤色的歧视直接从人-人评估背景转移到人-机器人评估背景。结果强调了机器人部署可能无意中延续现有社会不平等现象的机制。"
    },
    {
        "title": "Certifiable Alignment of GNSS and Local Frames via Lagrangian Duality",
        "summary": "Estimating the absolute orientation of a local system relative to a global navigation satellite system (GNSS) reference often suffers from local minima and high dependency on satellite availability. Existing methods for this alignment task rely on abundant satellites unavailable in GNSS-degraded environments, or use local optimization methods which cannot guarantee the optimality of a solution. This work introduces a globally optimal solver that transforms raw pseudo-range or Doppler measurements into a convexly relaxed problem. The proposed method is certifiable, meaning it can numerically verify the correctness of the result, filling a gap where existing local optimizers fail. We first formulate the original frame alignment problem as a nonconvex quadratically constrained quadratic program (QCQP) problem and relax the QCQP problem to a concave Lagrangian dual problem that provides a lower cost bound for the original problem. Then we perform relaxation tightness and observability analysis to derive criteria for certifiable optimality of the solution. Finally, simulation and real world experiments are conducted to evaluate the proposed method. The experiments show that our method provides certifiably optimal solutions even with only 2 satellites with Doppler measurements and 2D vehicle motion, while the traditional velocity-based VOBA method and the advanced GVINS alignment technique may fail or converge to local optima without notice. To support the development of GNSS-based navigation techniques in robotics, all code and data are open-sourced at https://github.com/Baoshan-Song/Certifiable-Doppler-alignment.",
        "url": "http://arxiv.org/abs/2512.20931v1",
        "published_date": "2025-12-24T04:24:33+00:00",
        "updated_date": "2025-12-24T04:24:33+00:00",
        "categories": [
            "cs.RO"
        ],
        "authors": [
            "Baoshan Song",
            "Matthew Giamou",
            "Penggao Yan",
            "Chunxi Xia",
            "Li-Ta Hsu"
        ],
        "tldr": "This paper presents a globally optimal and certifiable method for aligning a local system with GNSS, even in GNSS-degraded environments, using Lagrangian duality to solve a convexly relaxed problem with provable optimality.",
        "tldr_zh": "该论文提出了一种全局最优且可认证的方法，用于将局部系统与GNSS对齐，即使在GNSS退化的环境中， 也使用拉格朗日对偶性来解决一个凸松弛问题，具有可证明的最优性。",
        "relevance_score": 2,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 4,
        "summary_zh": "估计局部坐标系相对于全球导航卫星系统(GNSS)参考系的绝对姿态通常受到局部最小值和对卫星可用性高度依赖的影响。现有的对齐方法依赖于GNSS退化环境中无法获得的充足卫星，或者使用无法保证解的最优性的局部优化方法。 本文提出了一种全局最优求解器，可将原始伪距或多普勒测量值转化为凸松弛问题。所提出的方法是可认证的，这意味着它可以数值验证结果的正确性，填补了现有局部优化器无法解决的空白。我们首先将原始坐标系对齐问题表述为一个非凸二次约束二次规划(QCQP)问题，并将QCQP问题松弛为一个凹拉格朗日对偶问题，该问题为原始问题提供了一个较低的成本下界。然后，我们进行松弛紧度和可观测性分析，以推导解的可认证最优性的判据。最后，我们通过仿真和真实世界实验对所提出的方法进行评估。实验表明，即使只有2颗卫星的多普勒测量值和二维车辆运动，我们的方法也能提供可认证的最优解，而传统的基于速度的VOBA方法和先进的GVINS对齐技术可能会失败或在不知不觉中收敛到局部最优解。为了支持机器人技术中基于GNSS的导航技术的开发，所有代码和数据均在 https://github.com/Baoshan-Song/Certifiable-Doppler-alignment 上开源。"
    },
    {
        "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
        "summary": "To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.",
        "url": "http://arxiv.org/abs/2512.20866v1",
        "published_date": "2025-12-24T00:50:27+00:00",
        "updated_date": "2025-12-24T00:50:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haotian Lv",
            "Chao Li",
            "Jiangbo Dai",
            "Yuhui Zhang",
            "Zepeng Fan",
            "Yiqiu Tan",
            "Dawei Wang",
            "Binglei Xie"
        ],
        "tldr": "This paper presents a lightweight framework for underground pipeline recognition and spatial localization using multi-view 2D GPR images, achieving improved accuracy and robustness by integrating deep learning optimization with 3D GPR physical characteristics.",
        "tldr_zh": "本文提出了一种基于多视图2D GPR图像的地下管道识别和空间定位轻量级框架，通过将深度学习优化与3D GPR物理特性相结合，实现了更高的精度和鲁棒性。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 3,
        "summary_zh": "针对采用3D地质雷达进行地下管道检测时，多视图特征关联性弱、小目标识别精度低以及在复杂场景中鲁棒性不足等问题，本文提出了一种3D管道智能检测框架。首先，基于B/C/D-Scan三视图联合分析策略，通过使用时域有限差分（FDTD）方法获得的正演模拟结果与实际测量数据的交叉验证，建立了一种三维管道三视图特征评估方法。其次，提出了一种DCO-YOLO框架，该框架将DySample、CGLU和OutlookAttention跨维度关联机制集成到原始YOLOv11算法中，显著提高了小尺度管道边缘特征的提取能力。此外，提出一种3D-DIoU空间特征匹配算法，该算法集成了三维几何约束和中心距离惩罚项，实现了多视图标注的自动关联。三视图融合策略解决了单视图检测中固有的模糊性。基于真实城市地下管道数据的实验表明，所提出的方法在复杂的多管道场景中实现了96.2%的精度、93.3%的召回率和96.7%的平均精度均值（mAP），分别比基线模型高2.0%、2.1%和0.9%。消融实验验证了动态特征增强模块的协同优化效果，而Grad-CAM++热图可视化表明，改进后的模型显著增强了其对管道几何特征的关注能力。本研究将深度学习优化策略与3D地质雷达的物理特性相结合，为地下管道的智能识别和定位提供了一种高效可靠的新技术框架。"
    }
]