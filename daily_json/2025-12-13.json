[
    {
        "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "summary": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
        "url": "http://arxiv.org/abs/2512.10958v1",
        "published_date": "2025-12-11T18:59:58+00:00",
        "updated_date": "2025-12-11T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ao Liang",
            "Lingdong Kong",
            "Tianyi Yan",
            "Hongsi Liu",
            "Wesley Yang",
            "Ziqi Huang",
            "Wei Yin",
            "Jialong Zuo",
            "Yixuan Hu",
            "Dekai Zhu",
            "Dongyue Lu",
            "Youquan Liu",
            "Guangfeng Jiang",
            "Linfeng Li",
            "Xiangtai Li",
            "Long Zhuo",
            "Lai Xing Ng",
            "Benoit R. Cottereau",
            "Changxin Gao",
            "Liang Pan",
            "Wei Tsang Ooi",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces WorldLens, a comprehensive benchmark for evaluating driving world models across visual realism, geometric consistency, physical plausibility, and functional reliability, including a large human-annotated dataset and an evaluation agent.",
        "tldr_zh": "该论文介绍了 WorldLens，这是一个全面的基准，用于评估驾驶世界模型在视觉真实性、几何一致性、物理合理性和功能可靠性方面的表现，包括一个大型人工标注数据集和一个评估代理。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9,
        "summary_zh": "生成式世界模型正在重塑具身人工智能，使智能体能够合成逼真的 4D 驾驶环境，这些环境在视觉上具有说服力，但经常在物理或行为上失败。尽管进展迅速，但该领域仍然缺乏一种统一的方法来评估生成的环境是否保留了几何属性、遵守物理定律或支持可靠的控制。我们引入了 WorldLens，这是一个全谱基准，用于评估模型在其生成的世界中构建、理解和行为的能力。它涵盖了五个方面——生成、重构、动作跟随、下游任务和人类偏好——共同覆盖了视觉真实感、几何一致性、物理合理性和功能可靠性。在这些维度上，没有现有的世界模型能够普遍表现出色：纹理强大的模型通常违反物理定律，而几何稳定的模型则缺乏行为保真度。为了使客观指标与人类判断对齐，我们进一步构建了 WorldLens-26K，这是一个大规模的人工标注视频数据集，包含数值评分和文本解释，并开发了 WorldLens-Agent，这是一个从这些标注中提炼出来的评估模型，以实现可扩展、可解释的评分。基准、数据集和智能体共同构成了一个统一的生态系统，用于衡量世界保真度——标准化未来模型的评价方式，不仅通过它们看起来有多真实，还通过它们行为表现有多真实。"
    },
    {
        "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
        "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
        "url": "http://arxiv.org/abs/2512.10946v1",
        "published_date": "2025-12-11T18:59:46+00:00",
        "updated_date": "2025-12-11T18:59:46+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Wendi Chen",
            "Han Xue",
            "Yi Wang",
            "Fangyuan Zhou",
            "Jun Lv",
            "Yang Jin",
            "Shirun Tang",
            "Chuan Wen",
            "Cewu Lu"
        ],
        "tldr": "The paper introduces ImplicitRDP, a visual-force diffusion policy with structural slow-fast learning for contact-rich manipulation, which integrates vision and force feedback with causal attention and representation regularization, achieving superior performance compared to vision-only and hierarchical baselines.",
        "tldr_zh": "该论文介绍了ImplicitRDP，一种具有结构性慢快学习的视觉-力扩散策略，用于富接触操作。它通过因果注意力和表示正则化整合视觉和力反馈，与纯视觉和分层基线相比，实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "人类级别的富接触操作依赖于两种关键模态的不同作用：视觉提供空间丰富但时间缓慢的全局上下文，而力觉传感捕获快速、高频的局部接触动力学。由于它们在频率和信息上的根本差异，整合这些信号具有挑战性。在这项工作中，我们提出ImplicitRDP，一种统一的端到端视觉-力觉扩散策略，它在单个网络中集成了视觉规划和反应性力控制。我们引入了结构化快慢学习，一种利用因果注意力同时处理异步视觉和力觉令牌的机制，使策略能够以力觉频率执行闭环调整，同时保持动作块的时间一致性。此外，为了减轻端到端模型无法调整不同模态之间的权重的模态崩溃问题，我们提出了基于虚拟目标的表征正则化。这个辅助目标将力反馈映射到与动作相同的空间中，提供比原始力预测更强、更具物理基础的学习信号。在富接触任务上的大量实验表明，ImplicitRDP显著优于纯视觉和分层基线方法，通过简化的训练流程实现了卓越的反应性和成功率。代码和视频将在https://implicit-rdp.github.io公开发布。"
    },
    {
        "title": "Curriculum-Based Reinforcement Learning for Autonomous UAV Navigation in Unknown Curved Tubular Conduit",
        "summary": "Autonomous drone navigation in confined tubular environments remains a major challenge due to the constraining geometry of the conduits, the proximity of the walls, and the perceptual limitations inherent to such scenarios. We propose a reinforcement learning approach enabling a drone to navigate unknown three-dimensional tubes without any prior knowledge of their geometry, relying solely on local observations from LiDAR and a conditional visual detection of the tube center. In contrast, the Pure Pursuit algorithm, used as a deterministic baseline, benefits from explicit access to the centerline, creating an information asymmetry designed to assess the ability of RL to compensate for the absence of a geometric model. The agent is trained through a progressive Curriculum Learning strategy that gradually exposes it to increasingly curved geometries, where the tube center frequently disappears from the visual field. A turning-negotiation mechanism, based on the combination of direct visibility, directional memory, and LiDAR symmetry cues, proves essential for ensuring stable navigation under such partial observability conditions. Experiments show that the PPO policy acquires robust and generalizable behavior, consistently outperforming the deterministic controller despite its limited access to geometric information. Validation in a high-fidelity 3D environment further confirms the transferability of the learned behavior to a continuous physical dynamics.\n  The proposed approach thus provides a complete framework for autonomous navigation in unknown tubular environments and opens perspectives for industrial, underground, or medical applications where progressing through narrow and weakly perceptive conduits represents a central challenge.",
        "url": "http://arxiv.org/abs/2512.10934v1",
        "published_date": "2025-12-11T18:57:29+00:00",
        "updated_date": "2025-12-11T18:57:29+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Zamirddine Mari",
            "Jérôme Pasquet",
            "Julien Seinturier"
        ],
        "tldr": "This paper presents a curriculum-based reinforcement learning approach for autonomous drone navigation in unknown tubular environments, demonstrating robust and generalizable behavior that outperforms a deterministic baseline.",
        "tldr_zh": "本文提出了一种基于课程学习的强化学习方法，用于在未知管状环境中实现自主无人机导航，展示了优于确定性基线的稳健且可泛化的行为。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "在受限的管状环境中实现无人机自主导航仍然是一项重大挑战，这归因于管道的限制性几何结构、墙壁的近距离以及此类场景固有的感知局限性。我们提出了一种强化学习方法，使无人机能够在不了解其几何结构的情况下，仅依靠来自激光雷达的局部观测和对管状中心有条件的视觉检测，即可在未知的三维管道中进行导航。相反，纯追踪算法（Pure Pursuit algorithm）作为确定性基线，得益于对中心线的显式访问，从而产生了一种信息不对称性，旨在评估强化学习弥补几何模型缺失的能力。该智能体通过渐进式课程学习策略进行训练，该策略使其逐渐暴露于曲率渐增的几何结构中，在这些结构中，管状中心经常从视野中消失。一种基于直接可见性、方向记忆和激光雷达对称性线索组合的转弯协商机制，被证明对于在这样的部分可观测性条件下确保稳定导航至关重要。实验表明，PPO策略获得了稳健且可泛化的行为，尽管其对几何信息的访问有限，但始终优于确定性控制器。在高保真三维环境中的验证进一步证实了所学行为到连续物理动力学的可转移性。因此，所提出的方法为未知管状环境中的自主导航提供了一个完整的框架，并为工业、地下或医疗应用开启了前景，在这些应用中，在狭窄且感知较弱的管道中前进是一个核心挑战。"
    },
    {
        "title": "Iterative Compositional Data Generation for Robot Control",
        "summary": "Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",
        "url": "http://arxiv.org/abs/2512.10891v1",
        "published_date": "2025-12-11T18:20:49+00:00",
        "updated_date": "2025-12-11T18:20:49+00:00",
        "categories": [
            "cs.RO",
            "cs.LG"
        ],
        "authors": [
            "Anh-Quan Pham",
            "Marcel Hussing",
            "Shubhankar P. Patankar",
            "Dani S. Bassett",
            "Jorge Mendez-Mendez",
            "Eric Eaton"
        ],
        "tldr": "This paper introduces a compositional diffusion transformer for robot control that can generate data for unseen task combinations and improve performance through an iterative self-improvement loop using offline reinforcement learning.",
        "tldr_zh": "该论文提出了一种用于机器人控制的组合扩散Transformer，它可以为未见过的任务组合生成数据，并通过使用离线强化学习的迭代自改进循环来提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "收集机器人操作数据成本高昂，导致在多物体、多机器人和多环境的背景下，对于组合数量庞大的任务空间获取演示数据不切实际。虽然最近的生成模型可以为单个任务合成有用的数据，但它们无法利用机器人领域的组合结构，并且难以推广到未见过的任务组合。我们提出了一种语义组合扩散Transformer，它将转移过程分解为机器人特定、物体特定、障碍物特定和目标特定的组件，并通过注意力机制学习它们之间的交互。在有限的任务子集上训练后，我们证明了我们的模型可以零样本生成高质量的转移过程，我们可以从中学习用于未见任务组合的控制策略。然后，我们引入了一种迭代式的自我改进程序，其中合成数据通过离线强化学习进行验证，并纳入后续的训练轮次。我们的方法显著提高了相对于整体式方法和硬编码式组合基线的零样本性能，最终解决了几乎所有保留的任务，并展示了在学习到的表示中涌现出的有意义的组合结构。"
    },
    {
        "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
        "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
        "url": "http://arxiv.org/abs/2512.10949v1",
        "published_date": "2025-12-11T18:59:52+00:00",
        "updated_date": "2025-12-11T18:59:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yiwen Tang",
            "Zoey Guo",
            "Kaixin Zhu",
            "Ray Zhang",
            "Qizhi Chen",
            "Dongzhi Jiang",
            "Junli Liu",
            "Bohan Zeng",
            "Haoming Song",
            "Delin Qu",
            "Tianyi Bai",
            "Dan Xu",
            "Wentao Zhang",
            "Bin Zhao"
        ],
        "tldr": "This paper presents the first systematic study of applying reinforcement learning to text-to-3D generation, including reward design, RL algorithms, benchmarks, and a hierarchical RL approach, culminating in AR3D-R1, an RL-enhanced text-to-3D model.",
        "tldr_zh": "本文首次系统地研究了将强化学习应用于文本到3D生成，包括奖励设计、强化学习算法、基准测试和一个分层强化学习方法，最终提出了AR3D-R1，一个强化学习增强的文本到3D模型。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "强化学习(RL)先前已被证实在大语言模型和多模态模型中有效，最近已成功扩展到增强二维图像生成。然而，由于三维物体更高的空间复杂性，导致需要全局一致的几何结构和精细的局部纹理，将RL应用于三维生成仍然在很大程度上未被探索。这使得三维生成对奖励设计和RL算法尤其敏感。为了应对这些挑战，我们首次对RL在文本到三维自回归生成中的应用进行了跨多个维度的系统性研究。(1) 奖励设计：我们评估了奖励维度和模型选择，表明与人类偏好对齐至关重要，且通用多模态模型为三维属性提供了稳健的信号。(2) RL算法：我们研究了GRPO的变体，突出了令牌级别优化的有效性，并进一步研究了训练数据和迭代次数的扩展。(3) 文本到三维基准：由于现有的基准未能衡量三维生成模型中的隐式推理能力，我们引入了MME-3DR。(4) 先进的RL范式：受到三维生成自然层次结构的启发，我们提出了Hi-GRPO，它通过专用的奖励集成优化全局到局部的分层三维生成。基于这些见解，我们开发了AR3D-R1，这是第一个RL增强的文本到三维模型，擅长从粗糙形状到纹理细化的过程。我们希望这项研究能够为RL驱动的三维生成推理提供见解。代码已发布在https://github.com/Ivan-Tang-3D/3DGen-R1。"
    },
    {
        "title": "Mull-Tokens: Modality-Agnostic Latent Thinking",
        "summary": "Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.",
        "url": "http://arxiv.org/abs/2512.10941v1",
        "published_date": "2025-12-11T18:59:08+00:00",
        "updated_date": "2025-12-11T18:59:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Arijit Ray",
            "Ahmed Abdelkader",
            "Chengzhi Mao",
            "Bryan A. Plummer",
            "Kate Saenko",
            "Ranjay Krishna",
            "Leonidas Guibas",
            "Wen-Sheng Chu"
        ],
        "tldr": "The paper introduces \"Mull-Tokens,\" modality-agnostic latent tokens, pre-trained to perform intermediate reasoning using both image and text modalities and tested on spatial reasoning tasks, showing improved performance compared to baselines.",
        "tldr_zh": "该论文介绍了一种名为“Mull-Tokens”的模态无关潜在令牌，该令牌经过预训练，可以使用图像和文本模态执行中间推理，并在空间推理任务中进行了测试，显示出比基线更高的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8,
        "summary_zh": "推理超越了语言；现实世界需要关于空间、时间、可供性以及更多仅凭文字无法传达的事物进行推理。现有的多模态模型探索了使用图像推理的潜力，但它们是脆弱的且无法扩展。它们依赖于调用专门工具、图像的昂贵生成或手工制作的推理数据，以在文本和图像思考之间切换。相反，我们提供了一种更简单的替代方案——Mull-Tokens——与模态无关的潜在Token，经过预训练以在图像或文本模态中保持中间信息，从而让模型能够自由形式地思考以得出正确的答案。我们研究了受潜在推理框架启发的训练Mull-Tokens的最佳实践。我们首先使用交错的文本-图像轨迹的监督来训练Mull-Tokens，然后仅使用最终答案进行微调，无需任何监督。在四个具有挑战性的空间推理基准测试中，涉及解决拼图和采用不同视角等任务，我们证明Mull-Tokens优于几种仅使用文本推理或交错图像-文本推理的基线，与我们最强的基线相比，平均提高了+3%，在拼图求解这个推理密集的分支上达到了+16%。Mull-Tokens丰富了关于文本和视觉推理中grounding挑战的讨论，并提供了一个简单的解决方案，以抽象地在多种模态中进行思考。"
    },
    {
        "title": "OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis",
        "summary": "Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\\% in multiview NVS LLFF dataset, 60\\% in dynamic NVS Neural 3D Video benchmark, 20\\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/",
        "url": "http://arxiv.org/abs/2512.10940v1",
        "published_date": "2025-12-11T18:59:05+00:00",
        "updated_date": "2025-12-11T18:59:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiang Fan",
            "Sharath Girish",
            "Vivek Ramanujan",
            "Chaoyang Wang",
            "Ashkan Mirzaei",
            "Petr Sushko",
            "Aliaksandr Siarohin",
            "Sergey Tulyakov",
            "Ranjay Krishna"
        ],
        "tldr": "OmniView is a unified diffusion model framework for 3D/4D view synthesis that generalizes across various tasks by representing space, time, and view conditions separately, achieving state-of-the-art performance in novel view synthesis and video generation.",
        "tldr_zh": "OmniView是一个统一的扩散模型框架，用于3D/4D视图合成，通过分别表示空间、时间和视图条件，从而在各种任务中推广应用，并在新视角合成和视频生成中实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "先前将相机控制注入扩散模型的方法主要集中于4D一致性任务的特定子集：新视角合成、带相机控制的文本到视频生成、图像到视频生成等等。因此，这些碎片化的方法是在可用的3D/4D数据的不相交切片上训练的。我们引入了OmniView，一个统一的框架，可以推广到广泛的4D一致性任务。我们的方法分别表示空间、时间和视角条件，从而能够灵活地组合这些输入。例如，OmniView可以从静态、动态和多视角输入中合成新视角，在时间上向前或向后推断轨迹，并使用文本或图像提示生成具有完整相机控制的视频。OmniView在不同的基准和指标上与特定任务模型相比具有竞争力，在多视角NVS LLFF数据集上将相机条件扩散模型的图像质量评分提高了高达33％，在动态NVS Neural 3D Video基准上提高了60％，在RE-10K上的静态相机控制中提高了20％，并在文本条件视频生成中将相机轨迹误差降低了4倍。凭借在一个模型中的强大泛化能力，OmniView证明了通用4D视频模型的可行性。项目主页可访问https://snap-research.github.io/OmniView/"
    },
    {
        "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
        "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\n  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
        "url": "http://arxiv.org/abs/2512.10956v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wentao Zhou",
            "Xuweiyi Chen",
            "Vignesh Rajagopal",
            "Jeffrey Chen",
            "Rohan Chandra",
            "Zezhou Cheng"
        ],
        "tldr": "The paper introduces StereoWalker, a navigation framework that enhances Navigation Foundation Models (NFMs) with stereo vision and mid-level vision priors like depth estimation and pixel tracking, achieving state-of-the-art performance with significantly less training data.",
        "tldr_zh": "该论文介绍了StereoWalker，一个导航框架，通过立体视觉和中间层视觉先验（如深度估计和像素跟踪）增强了导航基础模型（NFMs），并以明显更少的训练数据实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "基础模型在语言和视觉领域的成功激发了对全端到端机器人导航基础模型（NFMs）的研究。NFMs直接将单目视觉输入映射到控制动作，完全忽略了中间层视觉模块（跟踪、深度估计等）。虽然视觉能力将隐式涌现的假设引人注目，但它需要大量的像素到动作的监督，而这很难获得。在动态和非结构化环境中，这一挑战尤其突出，因为稳健的导航需要精确的几何和动态理解，而单目视图中的深度-尺度模糊进一步限制了准确的空间推理。在本文中，我们表明，依赖单目视觉并忽略中间层视觉先验是低效的。\n\n我们提出了StereoWalker，它利用立体输入和显式的中间层视觉（如深度估计和密集像素跟踪）来增强NFMs。我们的直觉很简单：立体输入解决了深度-尺度模糊，并且现代中间层视觉模型在动态场景中提供了可靠的几何和运动结构。我们还策划了一个大型立体导航数据集，其中包含来自互联网立体视频的自动动作标注，以支持StereoWalker的训练并促进未来的研究。通过我们的实验，我们发现中间层视觉使StereoWalker能够以仅使用1.5%的训练数据量达到与最先进技术相当的性能，并在使用完整数据时超过最先进技术。我们还观察到，立体 vision 产生比单目输入更高的导航性能。"
    },
    {
        "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
        "summary": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
        "url": "http://arxiv.org/abs/2512.10947v1",
        "published_date": "2025-12-11T18:59:46+00:00",
        "updated_date": "2025-12-11T18:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Yang",
            "Ziyu Chen",
            "Yurong You",
            "Yan Wang",
            "Yiming Li",
            "Yuxiao Chen",
            "Boyi Li",
            "Boris Ivanovic",
            "Marco Pavone",
            "Yue Wang"
        ],
        "tldr": "The paper introduces Flex, a geometry-agnostic, learnable scene encoder for autonomous driving that utilizes a small set of tokens to efficiently encode multi-camera data, outperforming SOTA methods in both driving performance and inference throughput.",
        "tldr_zh": "本文介绍了一种名为 Flex 的几何无关、可学习的自动驾驶场景编码器，它使用一小部分 tokens 有效地编码多摄像头数据，在驾驶性能和推理吞吐量方面均优于 SOTA 方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "我们提出了 Flex，一种高效且有效的场景编码器，旨在解决端到端自动驾驶中处理大量多摄像头数据所带来的计算瓶颈。Flex 采用一小组可学习的场景令牌，联合编码来自不同摄像头和时间步长的所有图像令牌的信息。在设计上，我们的方法与几何无关，直接从数据中学习紧凑的场景表示，而无需依赖显式的 3D 先验归纳偏置，例如在先前工作中常见的鸟瞰图 (BEV)、占据或三平面表示。这种整体编码策略能够大幅压缩视觉输入，供下游基于大型语言模型 (LLM) 的策略模型使用。在包含 20,000 小时驾驶数据的大规模专有数据集上进行评估，我们的 Flex 实现了 2.2 倍的推理吞吐量提升，同时与最先进的方法相比，驾驶性能也大幅度提高。此外，我们表明这些紧凑的场景令牌在没有任何显式监督的情况下，能够涌现出场景分解的能力。我们的发现挑战了以往 3D 先验是必要的普遍假设，证明了数据驱动的联合编码策略为未来的自动驾驶系统提供了一条更具可扩展性、效率和效果的路径。"
    },
    {
        "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
        "summary": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
        "url": "http://arxiv.org/abs/2512.10945v1",
        "published_date": "2025-12-11T18:59:44+00:00",
        "updated_date": "2025-12-11T18:59:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Henghui Ding",
            "Chang Liu",
            "Shuting He",
            "Kaining Ying",
            "Xudong Jiang",
            "Chen Change Loy",
            "Yu-Gang Jiang"
        ],
        "tldr": "The paper introduces MeViS, a large-scale multi-modal dataset for referring motion expression video segmentation, designed to address the limitations of existing datasets by emphasizing motion in both videos and language.",
        "tldr_zh": "该论文介绍了MeViS，一个大规模多模态数据集，用于指代运动表达视频分割，旨在通过强调视频和语言中的运动来解决现有数据集的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8,
        "summary_zh": "本文提出了一个大规模的多模态数据集，用于指代表达式视频分割，重点在于基于对物体运动的语言描述，分割和跟踪视频中的目标物体。现有的指代视频分割数据集通常侧重于显著性物体，并且使用富含静态属性的语言表达，这有可能允许在单帧中识别目标物体。此类数据集淡化了运动在视频和语言中的作用。为了探索使用运动表达和运动推理线索进行像素级视频理解的可行性，我们引入了MeViS，一个包含33,072个由人工标注的运动表达的数据集，包括文本和音频两种形式，涵盖了复杂场景中2,006个视频中的8,171个物体。我们在MeViS支持的4个任务上，对15种现有方法进行了基准测试，包括6种指代视频物体分割 (RVOS) 方法，3种音频引导视频物体分割 (AVOS) 方法，2种指代多目标跟踪 (RMOT) 方法，以及4种针对新提出的指代运动表达生成 (RMEG) 任务的视频字幕生成方法。结果表明了现有方法在解决运动表达引导的视频理解方面的弱点和局限性。我们进一步分析了挑战，并提出了一种名为LMPM++的方法，用于RVOS/AVOS/RMOT，该方法取得了新的最先进的成果。我们的数据集提供了一个平台，能够促进复杂视频场景中运动表达引导的视频理解算法的开发。所提出的MeViS数据集和该方法的源代码可在https://henghuiding.com/MeViS/公开获取。"
    },
    {
        "title": "Any4D: Unified Feed-Forward Metric 4D Reconstruction",
        "summary": "We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.",
        "url": "http://arxiv.org/abs/2512.10935v1",
        "published_date": "2025-12-11T18:57:39+00:00",
        "updated_date": "2025-12-11T18:57:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Jay Karhade",
            "Nikhil Keetha",
            "Yuchen Zhang",
            "Tanisha Gupta",
            "Akash Sharma",
            "Sebastian Scherer",
            "Deva Ramanan"
        ],
        "tldr": "Any4D introduces a novel multi-view transformer for metric 4D reconstruction, unifying various modalities and achieving significant improvements in accuracy and efficiency compared to existing methods.",
        "tldr_zh": "Any4D 提出了一种用于度量 4D 重建的新型多视图转换器，统一了各种模态，与现有方法相比，在精度和效率方面实现了显着改进。",
        "relevance_score": 6,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 7,
        "summary_zh": "我们提出了Any4D，一种可扩展的多视角Transformer，用于度量尺度下的密集式前馈4D重建。与以往通常关注双视角密集场景流或稀疏3D点追踪的工作不同，Any4D直接为N帧生成逐像素的运动和几何预测。此外，与近期其他利用单目RGB视频进行4D重建的方法不同，当可用时，Any4D可以处理额外的模态和传感器数据，例如RGB-D帧，基于IMU的自运动以及雷达多普勒测量。该框架灵活性的关键创新之一在于对4D场景的模块化表示；具体而言，每个视角的4D预测使用多种以自我为中心的因素（深度图和相机内参，以局部相机坐标表示）和以环境为中心的因素（相机外参和场景流，以全局世界坐标表示）进行编码。我们在多种设置下实现了卓越的性能——无论是在精度（误差降低2-3倍）还是计算效率（速度快15倍）方面，为多种下游应用开辟了道路。"
    },
    {
        "title": "Decoupled Q-Chunking",
        "summary": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
        "url": "http://arxiv.org/abs/2512.10926v1",
        "published_date": "2025-12-11T18:52:51+00:00",
        "updated_date": "2025-12-11T18:52:51+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.RO",
            "stat.ML"
        ],
        "authors": [
            "Qiyang Li",
            "Seohong Park",
            "Sergey Levine"
        ],
        "tldr": "The paper introduces Decoupled Q-Chunking, a novel RL algorithm that decouples the critic's chunk length from the policy's, addressing the limitations of chunked critics in complex, long-horizon tasks by optimizing the policy against a distilled critic for partial action chunks.",
        "tldr_zh": "本文介绍了解耦Q-Chunking，这是一种新型强化学习算法，它将critic的块长度与policy的块长度分离，通过针对部分动作块优化policy来解决块状critic在复杂、长时程任务中的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "时间差分 (TD) 方法通过自举其自身的未来价值预测来高效地学习状态和动作价值，但是这种自举机制容易产生自举偏差，即价值目标中的误差会在步骤间累积，导致有偏的价值估计。最近的研究提出使用分块评论家，它估计短动作序列（“分块”）的价值，而不是单个动作的价值，从而加速价值备份。然而，从分块评论家中提取策略具有挑战性：策略必须以开环方式输出整个动作分块，这对于需要策略具有反应性的环境来说可能是次优的，并且在分块长度增长时也难以建模。我们的关键洞察是将评论家的分块长度与策略的分块长度解耦，允许策略在较短的动作分块上操作。我们提出了一种新颖的算法来实现这一点，该算法通过针对部分动作分块的蒸馏评论家来优化策略，该蒸馏评论家是通过从原始分块评论家乐观地备份，来近似当部分动作分块扩展到完整动作分块时可以实现的最大价值来构建的。这种设计保留了多步价值传播的优点，同时避免了开环次优性和学习长动作分块的动作分块策略的困难。我们在具有挑战性的、长时程的离线目标条件任务上评估了我们的方法，并表明它可靠地优于先前的方法。代码：github.com/ColinQiyangLi/dqc。"
    },
    {
        "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation",
        "summary": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.",
        "url": "http://arxiv.org/abs/2512.10925v1",
        "published_date": "2025-12-11T18:52:42+00:00",
        "updated_date": "2025-12-11T18:52:42+00:00",
        "categories": [
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Zamirddine Mari",
            "Mohamad Motasem Nawaf",
            "Pierre Drap"
        ],
        "tldr": "This paper presents a digital twin supervised reinforcement learning framework using PPO for autonomous underwater navigation of a BlueROV2, showing improved performance over DWA in cluttered environments with good simulation-to-real-world transfer.",
        "tldr_zh": "本文提出了一种数字孪生监督的强化学习框架，使用PPO算法进行BlueROV2的自主水下导航。实验表明，该方法在复杂的环境中优于DWA，且具备良好的从仿真到真实世界的迁移能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "水下环境中的自主导航仍然是一个主要挑战，原因是没有GPS、能见度降低以及水下障碍物的存在。本文以BlueROV2为例（一种广泛用于科学实验的开放平台），对这些问题进行了研究。我们提出了一种基于近端策略优化 (PPO) 算法的深度强化学习方法，该方法使用一个结合了面向目标导航信息、虚拟占据栅格以及沿操作区域边界进行光线投射的观测空间。我们将学习到的策略与参考确定性运动学规划器——动态窗口法 (DWA) 进行比较，动态窗口法通常被用作为障碍物避免的可靠基线。评估是在真实的模拟环境中进行，并辅以在物理BlueROV2上的验证，该验证由测试站点的3D数字孪生进行监督，从而有助于降低与真实世界实验相关的风险。结果表明，PPO策略在高度杂乱的环境中始终优于DWA，这主要归功于更好的局部适应性和更少的碰撞。最后，实验证明了学习到的行为从仿真到现实世界的可迁移性，证实了深度强化学习在水下机器人自主导航中的相关性。"
    },
    {
        "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
        "url": "http://arxiv.org/abs/2512.10957v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yukai Shi",
            "Weiyu Li",
            "Zihao Wang",
            "Hongyang Li",
            "Xingyu Chen",
            "Ping Tan",
            "Lei Zhang"
        ],
        "tldr": "SceneMaker is a decoupled 3D scene generation framework addressing occlusion and pose estimation challenges in open-set environments by using separate de-occlusion and pose estimation models, trained on a newly constructed dataset.",
        "tldr_zh": "SceneMaker是一个解耦的3D场景生成框架，通过使用分离的去遮挡和姿态估计模型来解决开放场景中遮挡和姿态估计的挑战，这些模型在一个新构建的数据集上进行训练。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "本文提出了一种解耦的3D场景生成框架，名为SceneMaker。由于缺乏足够的开放集去遮挡和姿态估计先验知识，现有方法难以在严重的遮挡和开放集场景下同时生成高质量的几何形状和准确的姿态。为了解决这些问题，我们首先将去遮挡模型与3D物体生成分离，并通过利用图像数据集和收集的去遮挡数据集来增强它，以获得更多样化的开放集遮挡模式。然后，我们提出了一种统一的姿态估计模型，该模型集成了全局和局部机制，用于自注意力机制和交叉注意力机制，以提高准确性。此外，我们构建了一个开放集3D场景数据集，以进一步扩展姿态估计模型的泛化能力。全面的实验表明，我们的解耦框架在室内和开放集场景中都具有优越性。我们的代码和数据集已在https://idea-research.github.io/SceneMaker/上发布。"
    },
    {
        "title": "MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos",
        "summary": "Motion capture now underpins content creation far beyond digital humans, yet most existing pipelines remain species- or template-specific. We formalize this gap as Category-Agnostic Motion Capture (CAMoCap): given a monocular video and an arbitrary rigged 3D asset as a prompt, the goal is to reconstruct a rotation-based animation such as BVH that directly drives the specific asset. We present MoCapAnything, a reference-guided, factorized framework that first predicts 3D joint trajectories and then recovers asset-specific rotations via constraint-aware inverse kinematics. The system contains three learnable modules and a lightweight IK stage: (1) a Reference Prompt Encoder that extracts per-joint queries from the asset's skeleton, mesh, and rendered images; (2) a Video Feature Extractor that computes dense visual descriptors and reconstructs a coarse 4D deforming mesh to bridge the gap between video and joint space; and (3) a Unified Motion Decoder that fuses these cues to produce temporally coherent trajectories. We also curate Truebones Zoo with 1038 motion clips, each providing a standardized skeleton-mesh-render triad. Experiments on both in-domain benchmarks and in-the-wild videos show that MoCapAnything delivers high-quality skeletal animations and exhibits meaningful cross-species retargeting across heterogeneous rigs, enabling scalable, prompt-driven 3D motion capture for arbitrary assets. Project page: https://animotionlab.github.io/MoCapAnything/",
        "url": "http://arxiv.org/abs/2512.10881v1",
        "published_date": "2025-12-11T18:09:48+00:00",
        "updated_date": "2025-12-11T18:09:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kehong Gong",
            "Zhengyu Wen",
            "Weixia He",
            "Mingxi Xu",
            "Qi Wang",
            "Ning Zhang",
            "Zhengyu Li",
            "Dongze Lian",
            "Wei Zhao",
            "Xiaoyu He",
            "Mingyuan Zhang"
        ],
        "tldr": "The paper presents MoCapAnything, a category-agnostic motion capture framework that can generate 3D animations for arbitrary rigged assets from monocular videos using a reference-guided, factorized approach and inverse kinematics. A new dataset, Truebones Zoo, is also introduced.",
        "tldr_zh": "该论文提出了MoCapAnything，一个类别无关的运动捕捉框架，它可以使用参考引导的分解方法和逆运动学，从单目视频为任意绑定的资产生成3D动画。同时还引入了一个新的数据集Truebones Zoo。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7,
        "summary_zh": "动作捕捉现在支撑着远超于数字人的内容创作，然而现有的大多数流程仍然是物种或模板特定的。我们正式将这一差距定义为类别无关的动作捕捉 (CAMoCap)：给定一个单目视频和一个任意已绑定骨骼的 3D 资产作为提示，目标是重建一个基于旋转的动画，如 BVH，可以直接驱动该特定资产。我们提出了 MoCapAnything，一个参考引导的、因子分解的框架，它首先预测 3D 关节轨迹，然后通过约束感知的逆运动学恢复资产特定的旋转。该系统包含三个可学习的模块和一个轻量级的 IK 阶段：（1）一个参考提示编码器，从资产的骨骼、网格和渲染图像中提取每个关节的查询；（2）一个视频特征提取器，计算密集的视觉描述符并重建一个粗略的 4D 形变网格，以弥合视频和关节空间之间的差距；以及（3）一个统一的运动解码器，融合这些线索以产生时间上连贯的轨迹。我们还策划了 Truebones Zoo，包含 1038 个运动片段，每个片段都提供一个标准化的骨骼-网格-渲染三元组。在领域内基准测试和真实视频上的实验表明，MoCapAnything 能够提供高质量的骨骼动画，并在异构绑定之间展现出有意义的跨物种重定向能力，从而实现可扩展的、提示驱动的、针对任意资产的 3D 动作捕捉。项目页面：https://animotionlab.github.io/MoCapAnything/"
    },
    {
        "title": "Distributionally Robust Regret Optimal Control Under Moment-Based Ambiguity Sets",
        "summary": "In this paper, we consider a class of finite-horizon, linear-quadratic stochastic control problems, where the probability distribution governing the noise process is unknown but assumed to belong to an ambiguity set consisting of all distributions whose mean and covariance lie within norm balls centered at given nominal values. To address the distributional ambiguity, we explore the design of causal affine control policies to minimize the worst-case expected regret over all distributions in the given ambiguity set. The resulting minimax optimal control problem is shown to admit an equivalent reformulation as a tractable convex program that corresponds to a regularized version of the nominal linear-quadratic stochastic control problem. While this convex program can be recast as a semidefinite program, semidefinite programs are typically solved using primal-dual interior point methods that scale poorly with the problem size in practice. To address this limitation, we propose a scalable dual projected subgradient method to compute optimal controllers to an arbitrary accuracy. Numerical experiments are presented to benchmark the proposed method against state-of-the-art data-driven and distributionally robust control design approaches.",
        "url": "http://arxiv.org/abs/2512.10906v1",
        "published_date": "2025-12-11T18:36:15+00:00",
        "updated_date": "2025-12-11T18:36:15+00:00",
        "categories": [
            "math.OC",
            "cs.LG",
            "eess.SY"
        ],
        "authors": [
            "Feras Al Taha",
            "Eilyan Bitar"
        ],
        "tldr": "The paper proposes a tractable, scalable approach to distributionally robust control for linear-quadratic systems under moment-based ambiguity, reformulating the minimax problem as a convex program and solving it with a dual projected subgradient method.",
        "tldr_zh": "该论文提出了一种针对线性二次系统的分布鲁棒控制的可处理、可扩展的方法，该方法在基于矩的模糊性下，将minimax问题重新表述为凸规划，并使用对偶投影次梯度法求解。",
        "relevance_score": 4,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 5,
        "summary_zh": "本文研究一类有限时域线性二次型随机控制问题，其中噪声过程的概率分布未知，但假设属于一个由所有均值和协方差位于以给定标称值为中心的范数球内的分布组成的模糊集。为了解决分布模糊性，我们探索设计因果仿射控制策略，以最小化给定模糊集内所有分布上的最坏情况预期遗憾。所得到的极小极大最优控制问题被证明可以等价地重新表述为一个易于处理的凸规划，该凸规划对应于标称线性二次型随机控制问题的正则化版本。虽然这个凸规划可以被表达为半定规划，但半定规划通常使用原始-对偶内点法求解，这些方法在实践中随问题规模的增长而扩展性较差。为了解决这个局限性，我们提出了一种可扩展的对偶投影次梯度方法，以计算具有任意精度的最优控制器。数值实验被用来对该方法与最先进的数据驱动和分布鲁棒控制设计方法进行基准测试。"
    },
    {
        "title": "GaussianHeadTalk: Wobble-Free 3D Talking Heads with Audio Driven Gaussian Splatting",
        "summary": "Speech-driven talking heads have recently emerged and enable interactive avatars. However, real-world applications are limited, as current methods achieve high visual fidelity but slow or fast yet temporally unstable. Diffusion methods provide realistic image generation, yet struggle with oneshot settings. Gaussian Splatting approaches are real-time, yet inaccuracies in facial tracking, or inconsistent Gaussian mappings, lead to unstable outputs and video artifacts that are detrimental to realistic use cases. We address this problem by mapping Gaussian Splatting using 3D Morphable Models to generate person-specific avatars. We introduce transformer-based prediction of model parameters, directly from audio, to drive temporal consistency. From monocular video and independent audio speech inputs, our method enables generation of real-time talking head videos where we report competitive quantitative and qualitative performance.",
        "url": "http://arxiv.org/abs/2512.10939v1",
        "published_date": "2025-12-11T18:59:02+00:00",
        "updated_date": "2025-12-11T18:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Madhav Agarwal",
            "Mingtian Zhang",
            "Laura Sevilla-Lara",
            "Steven McDonagh"
        ],
        "tldr": "This paper introduces a method called GaussianHeadTalk that uses 3D Morphable Models and audio-driven transformer-based prediction to create real-time, stable, and realistic 3D talking heads using Gaussian Splatting. It addresses the temporal instability issues of existing talking head methods.",
        "tldr_zh": "本文介绍了一种名为 GaussianHeadTalk 的方法，该方法使用 3D 形变模型和音频驱动的基于Transformer的预测，利用高斯溅射创建实时、稳定和逼真的 3D 说话人头部模型。它解决了现有说话人头部方法的时间不稳定问题。",
        "relevance_score": 2,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 4,
        "summary_zh": "语音驱动的会说话头像最近涌现，并实现了交互式化身。然而，现实世界的应用受到限制，因为当前方法虽然实现了高视觉逼真度，但速度慢，或者速度快却时序不稳定。扩散方法能够生成逼真的图像，但在单样本设置下表现欠佳。高斯溅射方法是实时的，但面部跟踪的不准确性，或者不一致的高斯映射，会导致不稳定的输出和视频伪影，这对现实场景的应用是有害的。我们通过使用3D形变模型映射高斯溅射来解决这个问题，从而生成特定人物的化身。我们引入了基于Transformer的模型参数预测，直接通过音频驱动时序一致性。仅需单目视频和独立的音频语音输入，我们的方法就能生成实时的会说话头像视频，并且我们报告了有竞争力的定量和定性性能。"
    }
]